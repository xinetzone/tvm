{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(relay-op-strategy)=\n",
    "# Relay 算子策略\n",
    "\n",
    "为了将 Relay 算子 lower 到 TOPI 库中定义的实现，需要向每个 Relay 算子注册 compute 和 schedule 函数。然而，compute 和 schedule 函数通常是针对每个 target 的，而且，甚至对于同一个 target，可能有多个算法和实现可用。为了解决问题的复杂性，引入了算子策略，允许开发者为每个算子和目标定义灵活的 lowering 策略。\n",
    "\n",
    "## 算子策略设计\n",
    "\n",
    "算子策略的基本元素是 ``OpImplementation``。它包括一对计算和调度函数、实现的名称和优先级级别（优先级级别的使用在 [Op 策略中选择实现](Op 策略中选择实现) 中有解释）。\n",
    "\n",
    "``OpStrategy`` 包含一组 ``OpSpecialization``。每个 ``OpSpecialization`` 都包含一组与 ``SpecializedCondition`` 相关联的 ``OpImplementation`` （参见 ``include/tvm/te/schedule.h`` 中的定义）。``SpecializedCondition`` 可以为 null，表示实现是普遍适用的；否则，只在满足特定条件时才考虑实现。 ``SpecializedCondition`` 由一组在合取范式张量表达式（conjunctive normal form，即 CNF）中定义的子句组成，只支持张量 shapes 上的条件。\n",
    "\n",
    "最后，策略函数或 ``FTVMStrategy`` 决定给定工作负载应该使用哪一对计算和调度函数，并且需要注册到每个 Relay 算子。``FTVMStrategy`` 是 generic 函数（参见 ``include/tvm/target/generic_func.h`` ），它可以被每个目标覆盖。函数签名为\n",
    "\n",
    "```c\n",
    "OpStrategy(const Attrs& attrs, const Array<Tensor>& inputs, const Type& out_type, const Target& target)\n",
    "```\n",
    "\n",
    "函数返回给定 op 属性、输入张量、输出类型和要编译的 target 的 ``OpStrategy``。\n",
    "\n",
    "## 编写策略函数\n",
    "\n",
    "建议开发人员用 Python 编写策略函数，因为大多数 TOPI 计算和调度函数都是用 Python 编写的。在 Python 中，在 ``pyton/tvm/relay/op/op.py`` 中提供 ``OpStrategy`` 类。它只有一个 API，就是向策略中添加实现：\n",
    "\n",
    "```python\n",
    "def add_implementation(self, compute, schedule, name=\"default\", plevel=10)\n",
    "```\n",
    "\n",
    "以 ``topk`` 为例来解释如何编写 ``FTVMStrategy`` 函数：\n",
    "\n",
    "```python\n",
    "# 添加到 python/tvm/relay/op/strategy/generic.py\n",
    "@override_native_generic_func(\"topk_strategy\")\n",
    "def topk_strategy(attrs, inputs, out_type, target):\n",
    "    strategy = _op.OpStrategy()\n",
    "    strategy.add_implementation(\n",
    "        wrap_compute_topk(topi.topk),\n",
    "        wrap_topi_schedule(topi.generic.schedule_topk),\n",
    "        name=\"topk.generic\")\n",
    "    return strategy\n",
    "\n",
    "# add to each target file in python/tvm/relay/op/strategy, e.g., x86.py, cuda.py, etc.\n",
    "@topk_strategy.register([\"cuda\", \"gpu\"])\n",
    "def topk_strategy_cuda(attrs, inputs, out_type, target):\n",
    "    strategy = _op.OpStrategy()\n",
    "    strategy.add_implementation(\n",
    "        wrap_compute_my_new_op(topi.cuda.topk),\n",
    "        wrap_topi_schedule(topi.cuda.schedule_topk),\n",
    "        name=\"topk.cuda\")\n",
    "    return strategy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this example, we use ``topi.cuda.topk`` and ``topi.cuda.schedule_topk``\n",
    "as the compute and schedule function for CUDA or GPU target, while use TOPI\n",
    "generic compute and schedule for the rest of targets.\n",
    "Note that we use two wrapper functions that wrap the topi\n",
    "compute and schedule to conform with the required function signature (\n",
    "see ``FTVMCompute`` and ``FTVMSchedule`` in ``include/tvm/relay/op_attr_types.h``).\n",
    "Usually we need to write a customized compute wrapper function for each operator\n",
    "to get different fields from op attributes.\n",
    "\n",
    "The example above shows a very basic strategy function that only\n",
    "adds one implementation in the strategy. But for many complicated operators,\n",
    "we may need to add multiple implementations that use different algorithms.\n",
    "For example, we can use both direct and winograd algorithm to\n",
    "compute a conv2d op. In order to achieve this, we can write the strategy function\n",
    "as follows:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    strategy.add_implementation(\n",
    "        wrap_compute_conv2d(topi.cuda.conv2d_nchw),\n",
    "        wrap_topi_schedule(topi.cuda.schedule_conv2d_nchw),\n",
    "        name=\"conv2d_nchw.cuda\",\n",
    "        plevel=10)\n",
    "\n",
    "    if winograd_condition:\n",
    "        strategy.add_implementation(\n",
    "            wrap_compute_conv2d(topi.cuda.conv2d_nchw_winograd),\n",
    "            wrap_topi_schedule(topi.cuda.schedule_conv2d_nchw_winograd),\n",
    "            name=\"conv2d_nchw_winograd.cuda\",\n",
    "            plevel=15)\n",
    "\n",
    "In this example, we add two implementations to the conv2d strategy where\n",
    "winograd algorithm is only added when ``winograd_condition`` is true.\n",
    "The implementation ``\"conv2d_nchw_winograd.cuda\"`` will be used to compile\n",
    "conv2d when ``winograd_condition`` is true as it has higher\n",
    "priority level (this could be changed if certain implementation is an AutoTVM\n",
    "template. See `Select Implementation from Op Strategy`_ for more\n",
    "details). Otherwise, ``\"conv2d_nchw.cuda\"`` is used.\n",
    "\n",
    "We can extend the example above to third party library implementation. For\n",
    "example, we can add the implementation that invokes kernel in the cblas\n",
    "library when cblas is included in the target.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    if \"cblas\" in target.libs:\n",
    "        strategy.add_implementation(\n",
    "            wrap_compute_dense(topi.x86.dense_cblas),\n",
    "            wrap_topi_schedule(topi.x86.schedule_dense_cblas),\n",
    "            name=\"dense_cblas.x86\",\n",
    "            plevel=15)\n",
    "\n",
    "\n",
    "Further, we can add implementation specialized for a certain range of shapes.\n",
    "The code below shows an example of dense strategy that adds an implementation\n",
    "that is specialized for ``m`` greater than 16. The main difference between\n",
    "hardcode python condition like examples above and specialized condition is that\n",
    "it allows TVM to generate multiple kernels when the input tensors have symbolic\n",
    "shapes. The compile engine will generate a dispatch function that invokes the\n",
    "specialized kernel when the corresponding condition is met; otherwise,\n",
    "invoke the kernel that has no associated specialized condition (``dense_common``\n",
    "in this example). This part is still work in progress. More details will be\n",
    "provided after it is done.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    def dense_strategy(attrs, inputs, out_type, target):\n",
    "        m = inputs[0].shape[0]\n",
    "        strategy = _op.OpStrategy()\n",
    "        strategy.add_implementation(\n",
    "            wrap_compute_dense(dense_compute1),\n",
    "            wrap_topi_schedule(dense_schedule1),\n",
    "            name=\"dense_common\")\n",
    "\n",
    "        with tvm.te.SpecializedCondition(m > 16):\n",
    "            strategy.add_implementation(\n",
    "                wrap_compute_dense(dense_compute2),\n",
    "                wrap_topi_schedule(dense_schedule2),\n",
    "                name=\"dense_for_large_m\",\n",
    "                plevel=15)\n",
    "\n",
    "        return strategy\n",
    "\n",
    "\n",
    "Register Strategy Function to An Operator\n",
    "-----------------------------------------\n",
    "\n",
    "After we define the strategy function for an operator, we can now\n",
    "register the strategy function to this operator with\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    register_strategy(\"topk\", strategy.topk_strategy)\n",
    "\n",
    "However, it takes much effort to write a strategy function for an operator.\n",
    "Therefore, we provide two other methods for simpler operators.\n",
    "\n",
    "First, for operators that have injective, broadcast, or reduction pattern, we\n",
    "can call ``register_injective_schedule``, ``register_broadcast_schedule``, and\n",
    "``register_reduce_schedule`` repsectively. The schedule function for these\n",
    "patterns are already registered by each target and can be applied to these\n",
    "operators. We assume the compute function should be the same across all targets,\n",
    "and ``FTVMCompute`` needs to be registered to the op before invoking register\n",
    "schedule.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    register_broadcast_schedule(\"add\")\n",
    "\n",
    "Second, for operators that doesn't have these common patterns mentioned before,\n",
    "but also have the same compute function for all targets, we can use\n",
    "``register_schedule`` API. It is easier to write ``FTVMSchedule`` function\n",
    "as we only need to provide which schedule function to use. The following\n",
    "code snippet shows ``FTVMSchedule`` function for pooling.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    # add to python/tvm/relay/op/strategy/generic.py\n",
    "    @generic_func\n",
    "    def schedule_pool(attrs, outs, target):\n",
    "        with target:\n",
    "            return topi.generic.schedule_pool(outs, attrs.layout)\n",
    "\n",
    "    # add to each target file in python/tvm/relay/op/strategy, e.g., x86.py, cuda.py, etc.\n",
    "    @schedule_pool.register(\"cpu\")\n",
    "    def schedule_pool_cpu(attrs, outs, target):\n",
    "        ...\n",
    "\n",
    "After we created the ``FTVMSchedule`` for an operator, we can\n",
    "register the strategy using ``register_schedule``:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    register_schedule(\"nn.max_pool2d\", strategy.schedule_pool)\n",
    "\n",
    "\n",
    "Register Strategies for A New Target\n",
    "------------------------------------\n",
    "\n",
    "There are two ways to register strategies for a new target. The more\n",
    "straightforward one is adding a new target file in the directory\n",
    "``python/tvm/relay/op/strategy``. You only need to customize the strategy for\n",
    "ops that have been implemented for this new target and reuse the generic\n",
    "strategies for the rest.\n",
    "\n",
    "Alternatively, you can also register the strategy for the new target outside the\n",
    "TVM python library. The following code snippet shows an example how to do\n",
    "so. You can find more examples in ``vta/python/vta/top/op.py``.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    @relay.op.strategy.conv2d_strategy.register(\"mytarget\")\n",
    "    def conv2d_strategy_mytarget(attrs, inputs, out_type, target):\n",
    "        ...\n",
    "\n",
    "\n",
    "## Op 策略中选择实现\n",
    "\n",
    "During the compilation, Relay compile engine needs to determine which\n",
    "implementation to use for an operator when there are multiple. The selection\n",
    "policy works as follows.\n",
    "\n",
    "When the input tensors to an operator or a fused op all have constant shapes,\n",
    "the compile engine first finds the best implementation based on AutoTVM tuning\n",
    "logs. If there is no implementation that is an AutoTVM template or all AutoTVM\n",
    "templates have fallback configs, the implementation with highest priority level\n",
    "will then be chosen. Implementations with same priority level in this case leads\n",
    "to an undefined behavior, and any of them might be selected.\n",
    "\n",
    "The selection policy for ops with symbolic input shapes is still work in\n",
    "progress. Currently, if any input tensor has a symbolic shape, only the\n",
    "implementation with highest priority level will be used for this operator. This\n",
    "will be updated after the implementation finishes.\n",
    "\n",
    "For debug purpose, you can add the following lines before you compile the Relay\n",
    "model to learn which implementation is used for each operator.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    logging.getLogger(\"te_compiler\").setLevel(logging.INFO)\n",
    "    logging.getLogger(\"te_compiler\").addHandler(logging.StreamHandler(sys.stdout))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tvm80')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d56931767869a22775ecec95b6db9cb1d4d3c8e9a80ba4de55eaee096d239e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
