{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import te, relay\n",
    "from tqdm.asyncio import tqdm\n",
    "from testing.relay.utils.tag_span import _create_span, _set_span, _verify_structural_equal_with_span\n",
    "\n",
    "\n",
    "def list_ops(expr):\n",
    "    \"\"\"list_ops\"\"\"\n",
    "\n",
    "    class OpLister(tvm.relay.ExprVisitor):\n",
    "        \"\"\"OpLister inherits from ExprVisitor\"\"\"\n",
    "\n",
    "        def visit_op(self, op):\n",
    "            if op not in self.node_set:\n",
    "                self.node_list.append(op)\n",
    "            return super().visit_op(op)\n",
    "\n",
    "        def list_nodes(self, expr):\n",
    "            self.node_set = {}\n",
    "            self.node_list = []\n",
    "            self.visit(expr)\n",
    "            return self.node_list\n",
    "\n",
    "    return OpLister().list_nodes(expr)\n",
    "   \n",
    "\n",
    "def gen_ir_module(model, inputs, use_parser_friendly_name=False):\n",
    "    \"\"\"Helper function to generate IRModule with meaningful source information\"\"\"\n",
    "\n",
    "    trace = torch.jit.trace(model, inputs)\n",
    "    input_names = [\"input{}\".format(idx) for idx, _ in enumerate(inputs)]\n",
    "    input_shapes = list(zip(input_names, [inp.shape for inp in inputs]))\n",
    "    mod, _ = relay.frontend.from_pytorch(\n",
    "        trace,\n",
    "        input_shapes,\n",
    "        use_parser_friendly_name=use_parser_friendly_name,\n",
    "    )\n",
    "    return mod\n",
    "\n",
    "def assert_shapes_match(tru, est):\n",
    "    \"\"\"Verfiy whether the shapes are equal\"\"\"\n",
    "    if tru.shape != est.shape:\n",
    "        msg = \"Output shapes {} and {} don't match\"\n",
    "        raise AssertionError(msg.format(tru.shape, est.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "input_shape = [10]\n",
    "\n",
    "class Add1(nn.Module):\n",
    "    def forward(self, *args):\n",
    "        return args[0] + args[0]\n",
    "\n",
    "class Add2(nn.Module):\n",
    "    def forward(self, *args):\n",
    "        return args[0] + 1\n",
    "\n",
    "class Add3(nn.Module):\n",
    "    def forward(self, *args):\n",
    "        ones = torch.ones(input_shape, dtype=torch.float)\n",
    "        if torch.cuda.is_available():\n",
    "            ones = ones.cuda()\n",
    "        return args[0] + ones\n",
    "\n",
    "class Add4(nn.Module):\n",
    "    def forward(self, *args):\n",
    "        ones = torch.ones([], dtype=torch.float)\n",
    "        if torch.cuda.is_available():\n",
    "            ones = ones.cuda()\n",
    "        return args[0] + ones\n",
    "input_data = torch.rand(input_shape).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = Add1().float().eval()\n",
    "baseline_input = [input_data]\n",
    "with torch.no_grad():\n",
    "    baseline_outputs = baseline_model(*[input.clone() for input in baseline_input])\n",
    "if isinstance(baseline_outputs, tuple):\n",
    "    baseline_outputs = tuple(out.cpu().numpy() for out in baseline_outputs)\n",
    "else:\n",
    "    baseline_outputs = (baseline_outputs.cpu().numpy(),)\n",
    "trace = torch.jit.trace(baseline_model, [input.clone() for input in baseline_input])\n",
    "trace = trace.float().eval()\n",
    "input_names = [f\"input{idx}\" for idx, _ in enumerate(baseline_input)]\n",
    "input_shapes = list(zip(input_names, [inp.shape for inp in baseline_input]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fn (%input0: Tensor[(10), float32] /* span=aten::add_0.input0:0:0 */) {\n",
      "  add(%input0, %input0) /* span=aten::add_0:0:0 */\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input_names = [f\"input{idx}\" for idx, _ in enumerate(baseline_input)]\n",
    "input_shapes = list(zip(input_names, [inp.shape for inp in baseline_input]))\n",
    "mod, params = relay.frontend.from_pytorch(trace, input_shapes, custom_convert_map=None)\n",
    "print(mod[\"main\"])\n",
    "for arg in mod[\"main\"].params[: len(input_names)]:\n",
    "    assert arg.name_hint in input_names\n",
    "compiled_input = dict(zip(input_names, [inp.clone().cpu().numpy() for inp in baseline_input]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 1993.49it/s]\n"
     ]
    }
   ],
   "source": [
    "kind = \"graph\"\n",
    "targets = [\"llvm\"]\n",
    "# targets = [\"llvm\", \"cuda\"]\n",
    "check_correctness = True\n",
    "rtol = 1e-5\n",
    "atol = 1e-5\n",
    "expected_ops = []\n",
    "for target in targets:\n",
    "    if not tvm.runtime.enabled(target):\n",
    "        continue\n",
    "    dev = tvm.device(target, 0)\n",
    "    exe = relay.create_executor(\n",
    "        kind, mod=mod, params=params, device=dev, target=target\n",
    "    ).evaluate()\n",
    "    result = exe(**compiled_input)\n",
    "    if not isinstance(result, list):\n",
    "        result = [result]\n",
    "\n",
    "    for i, baseline_output in tqdm(enumerate(baseline_outputs)):\n",
    "        output = result[i].numpy()\n",
    "        assert_shapes_match(baseline_output, output)\n",
    "        if check_correctness:\n",
    "            np.testing.assert_allclose(baseline_output, output, rtol=rtol, atol=atol)\n",
    "    def visit(op):\n",
    "        if isinstance(op, tvm.ir.op.Op):\n",
    "            if op.name in expected_ops:\n",
    "                expected_ops.remove(op.name)\n",
    "\n",
    "    tvm.relay.analysis.post_order_visit(mod[\"main\"].body, visit)\n",
    "\n",
    "    if expected_ops:\n",
    "        msg = \"TVM Relay do not contain expected ops {}\"\n",
    "        raise AssertionError(msg.format(expected_ops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvmz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
