{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(vta-mat-mult-opt)=\n",
        "# 分块矩阵乘法\n",
        "\n",
        "\n",
        "**原作者**: [Thierry Moreau](https://homes.cs.washington.edu/~moreau/)\n",
        "\n",
        "本教程概述了如何在 VTA 设计中使用 TVM 有效地映射矩阵乘法。建议先学习 {ref}`basic-mat-mult` 教程。\n",
        "\n",
        "在本教程中，将演示 TVM 调度优化，将大型神经网络算子分解为较小的块，以在有限的硬件加速器资源内实现计算。\n",
        "\n",
        "## RPC 设置\n",
        "\n",
        "首先编程 Pynq 的 FPGA 并构建它的 RPC 运行时。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tvm\n",
        "from tvm import te\n",
        "import vta\n",
        "import numpy as np\n",
        "from tvm import rpc\n",
        "from tvm.contrib import utils\n",
        "from vta.testing import simulator\n",
        "\n",
        "# Load VTA parameters from the 3rdparty/vta-hw/config/vta_config.json file\n",
        "env = vta.get_env()\n",
        "remote = rpc.LocalSession()\n",
        "\n",
        "# 全连接层 1024 x 1024\n",
        "batch_size = 1\n",
        "in_channels = 1024\n",
        "out_channels = 1024\n",
        "num_ops = in_channels * out_channels * batch_size * 2\n",
        "\n",
        "assert batch_size % env.BATCH == 0\n",
        "assert in_channels % env.BLOCK_IN == 0\n",
        "assert out_channels % env.BLOCK_OUT == 0\n",
        "\n",
        "# 推导出平铺的张量形状\n",
        "data_shape = (\n",
        "    batch_size // env.BATCH, \n",
        "    in_channels // env.BLOCK_IN,\n",
        "    env.BATCH, env.BLOCK_IN\n",
        ")\n",
        "weight_shape = (\n",
        "    out_channels // env.BLOCK_OUT,\n",
        "    in_channels // env.BLOCK_IN,\n",
        "    env.BLOCK_OUT,\n",
        "    env.BLOCK_IN,\n",
        ")\n",
        "output_shape = (\n",
        "    batch_size // env.BATCH, \n",
        "    out_channels // env.BLOCK_OUT, \n",
        "    env.BATCH, env.BLOCK_OUT\n",
        ")\n",
        "\n",
        "# Reduction axes\n",
        "ic = te.reduce_axis((0, in_channels // env.BLOCK_IN), name=\"ic\")\n",
        "ic_tns = te.reduce_axis((0, env.BLOCK_IN), name=\"ic_tns\")\n",
        "\n",
        "# Input placeholder tensors\n",
        "data = te.placeholder(data_shape, name=\"data\", dtype=env.inp_dtype)\n",
        "weight = te.placeholder(weight_shape, name=\"weight\", dtype=env.wgt_dtype)\n",
        "\n",
        "# Copy buffers\n",
        "data_buf = te.compute(data_shape, lambda *i: data(*i), \"data_buf\")\n",
        "weight_buf = te.compute(weight_shape, lambda *i: weight(*i), \"weight_buf\")\n",
        "\n",
        "# 声明矩阵乘法计算\n",
        "res_gemm = te.compute(\n",
        "    output_shape,\n",
        "    lambda bo, co, bi, ci: te.sum(\n",
        "        data_buf[bo, ic, bi, ic_tns].astype(env.acc_dtype)\n",
        "        * weight_buf[co, ic, ci, ic_tns].astype(env.acc_dtype),\n",
        "        axis=[ic, ic_tns],\n",
        "    ),\n",
        "    name=\"res_gem\",\n",
        ")\n",
        "\n",
        "# 为定点归一化（fix-point normalization）添加 shift stage\n",
        "res_shr = te.compute(output_shape, lambda *i: res_gemm(*i) >> env.INP_WIDTH, name=\"res_shr\")\n",
        "\n",
        "# 将值裁剪到 (0, input max value)\n",
        "inp_max = (1 << (env.INP_WIDTH - 1)) - 1\n",
        "res_max = te.compute(output_shape, lambda *i: tvm.te.max(res_shr(*i), 0), \"res_max\")\n",
        "res_min = te.compute(output_shape, lambda *i: tvm.te.min(res_max(*i), inp_max), \"res_min\")\n",
        "\n",
        "# 在返回结果之前，对输入数据类型应用类型转换\n",
        "res = te.compute(output_shape, lambda *i: res_min(*i).astype(env.inp_dtype), name=\"res\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's define tiling sizes (expressed in multiples of VTA tensor shape size)\n",
        "b_block = 1 // env.BATCH\n",
        "i_block = 128 // env.BLOCK_IN\n",
        "o_block = 64 // env.BLOCK_OUT\n",
        "\n",
        "s = te.create_schedule(res.op)\n",
        "b, oc, b_tns, oc_tns = s[res].op.axis\n",
        "b_out, b_inn = s[res].split(b, b_block)\n",
        "oc_out, oc_inn = s[res].split(oc, o_block)\n",
        "s[res].reorder(b_out, oc_out, b_inn, oc_inn)\n",
        "\n",
        "# Move intermediate computation into each output compute tile\n",
        "s[res_gemm].compute_at(s[res], oc_out)\n",
        "s[res_shr].compute_at(s[res], oc_out)\n",
        "s[res_max].compute_at(s[res], oc_out)\n",
        "s[res_min].compute_at(s[res], oc_out)\n",
        "\n",
        "# Apply additional loop split along reduction axis (input channel)\n",
        "b_inn, oc_inn, b_tns, oc_tns = s[res_gemm].op.axis\n",
        "ic_out, ic_inn = s[res_gemm].split(ic, i_block)\n",
        "\n",
        "# Reorder axes. We move the ic_out axis all the way out of the GEMM\n",
        "# loop to block along the reduction axis\n",
        "s[res_gemm].reorder(ic_out, b_inn, oc_inn, ic_inn, b_tns, oc_tns, ic_tns)\n",
        "\n",
        "# Set scope of SRAM buffers\n",
        "s[data_buf].set_scope(env.inp_scope)\n",
        "s[weight_buf].set_scope(env.wgt_scope)\n",
        "s[res_gemm].set_scope(env.acc_scope)\n",
        "s[res_shr].set_scope(env.acc_scope)\n",
        "s[res_min].set_scope(env.acc_scope)\n",
        "s[res_max].set_scope(env.acc_scope)\n",
        "\n",
        "# Block data and weight cache reads\n",
        "s[data_buf].compute_at(s[res_gemm], ic_out)\n",
        "s[weight_buf].compute_at(s[res_gemm], ic_out)\n",
        "\n",
        "# Use DMA copy pragma on DRAM->SRAM operations\n",
        "s[data_buf].pragma(s[data_buf].op.axis[0], env.dma_copy)\n",
        "s[weight_buf].pragma(s[weight_buf].op.axis[0], env.dma_copy)\n",
        "\n",
        "# Use DMA copy pragma on SRAM->DRAM operation\n",
        "# (this implies that these copies should be performed along b_inn,\n",
        "# or result axis 2)\n",
        "s[res].pragma(s[res].op.axis[2], env.dma_copy)\n",
        "# Apply tensorization over the batch tensor tile axis\n",
        "s[res_gemm].tensorize(b_tns, env.gemm)\n",
        "\n",
        "# Add an ALU pragma over the shift and clipping operations\n",
        "s[res_shr].pragma(s[res_shr].op.axis[0], env.alu)\n",
        "s[res_min].pragma(s[res_min].op.axis[0], env.alu)\n",
        "s[res_max].pragma(s[res_max].op.axis[0], env.alu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[18:26:44] /media/pc/data/lxw/ai/tvm/src/tir/transforms/arg_binder.cc:95: Warning: Trying to bind buffer to another one with lower alignment requirement  required_alignment=256, provided_alignment=64\n",
            "2023-04-20 18:26:44.318 INFO load_module /tmp/tmpv32svv50/gemm.o\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution statistics:\n",
            "\tinp_load_nbytes :            16384\n",
            "\twgt_load_nbytes :          1048576\n",
            "\tacc_load_nbytes :                0\n",
            "\tuop_load_nbytes :               20\n",
            "\tout_store_nbytes:             1024\n",
            "\tgemm_counter    :             4096\n",
            "\talu_counter     :              192\n",
            "Successful blocked matrix multiply test!\n"
          ]
        }
      ],
      "source": [
        "# Compile the TVM module\n",
        "my_gemm = vta.build(\n",
        "    s, [data, weight, res], tvm.target.Target(\"ext_dev\", host=env.target_host), name=\"my_gemm\"\n",
        ")\n",
        "temp = utils.tempdir()\n",
        "my_gemm.save(temp.relpath(\"gemm.o\"))\n",
        "remote.upload(temp.relpath(\"gemm.o\"))\n",
        "f = remote.load_module(\"gemm.o\")\n",
        "\n",
        "# Get the remote device context\n",
        "ctx = remote.ext_dev(0)\n",
        "\n",
        "# Initialize the data and weight arrays randomly in the int range of (-128, 128]\n",
        "data_np = np.random.randint(-128, 128, size=(batch_size, in_channels)).astype(data.dtype)\n",
        "weight_np = np.random.randint(-128, 128, size=(out_channels, in_channels)).astype(weight.dtype)\n",
        "\n",
        "# Apply packing to the data and weight arrays from a 2D to a 4D packed layout\n",
        "data_packed = data_np.reshape(\n",
        "    batch_size // env.BATCH, env.BATCH, in_channels // env.BLOCK_IN, env.BLOCK_IN\n",
        ").transpose((0, 2, 1, 3))\n",
        "weight_packed = weight_np.reshape(\n",
        "    out_channels // env.BLOCK_OUT, env.BLOCK_OUT, in_channels // env.BLOCK_IN, env.BLOCK_IN\n",
        ").transpose((0, 2, 1, 3))\n",
        "\n",
        "# Format the input/output arrays with tvm.nd.array to the DLPack standard\n",
        "data_nd = tvm.nd.array(data_packed, ctx)\n",
        "weight_nd = tvm.nd.array(weight_packed, ctx)\n",
        "res_nd = tvm.nd.array(np.zeros(output_shape).astype(res.dtype), ctx)\n",
        "\n",
        "# Clear stats\n",
        "if env.TARGET in [\"sim\", \"tsim\"]:\n",
        "    simulator.clear_stats()\n",
        "\n",
        "# Invoke the module to perform the computation\n",
        "f(data_nd, weight_nd, res_nd)\n",
        "\n",
        "# Verify against numpy implementation\n",
        "res_ref = np.dot(data_np.astype(env.acc_dtype), weight_np.T.astype(env.acc_dtype))\n",
        "res_ref = res_ref >> env.INP_WIDTH\n",
        "res_ref = np.clip(res_ref, 0, inp_max)\n",
        "res_ref = res_ref.astype(res.dtype)\n",
        "res_ref = res_ref.reshape(\n",
        "    batch_size // env.BATCH, env.BATCH, out_channels // env.BLOCK_OUT, env.BLOCK_OUT\n",
        ").transpose((0, 2, 1, 3))\n",
        "np.testing.assert_equal(res_ref, res_nd.asnumpy())\n",
        "\n",
        "# Print stats\n",
        "if env.TARGET in [\"sim\", \"tsim\"]:\n",
        "    sim_stats = simulator.stats()\n",
        "    print(\"Execution statistics:\")\n",
        "    for k, v in sim_stats.items():\n",
        "        print(\"\\t{:<16}: {:>16}\".format(k, v))\n",
        "\n",
        "print(\"Successful blocked matrix multiply test!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4096.0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1048576/256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 小结\n",
        "\n",
        "本教程演示了 TVM 调度原语如何为矩阵乘法示例实现分块计算。这允许将任意大的计算映射到有限的硬件加速器资源上。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
