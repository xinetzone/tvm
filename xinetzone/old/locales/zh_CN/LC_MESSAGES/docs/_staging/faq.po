# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-29 15:13+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/_staging/faq.rst:20
msgid "Frequently Asked Questions"
msgstr ""

#: ../../doc/docs/_staging/faq.rst:24
msgid "How to Install"
msgstr ""

#: ../../doc/docs/_staging/faq.rst:25
msgid "See :ref:`installation`."
msgstr ""

#: ../../doc/docs/_staging/faq.rst:29
msgid "How to add a new Hardware Backend"
msgstr ""

#: ../../doc/docs/_staging/faq.rst:31
msgid ""
"If the hardware backend has LLVM support, then we can directly generate "
"the code by setting the correct target triple as in "
":py:mod:`~tvm.target`."
msgstr ""

#: ../../doc/docs/_staging/faq.rst:33
msgid ""
"If the target hardware is a GPU, try to use the cuda, opencl or vulkan "
"backend."
msgstr ""

#: ../../doc/docs/_staging/faq.rst:34
msgid ""
"If the target hardware is a special accelerator, checkout :ref:`vta-"
"index` and :ref:`relay-bring-your-own-codegen`."
msgstr ""

#: ../../doc/docs/_staging/faq.rst:36
msgid ""
"For all of the above cases, You may want to add target specific "
"optimization templates using AutoTVM, see :ref:`tutorials-autotvm-sec`."
msgstr ""

#: ../../doc/docs/_staging/faq.rst:38
msgid ""
"Besides using LLVM's vectorization, we can also embed micro-kernels to "
"leverage hardware intrinsics, see :ref:`tutorials-tensorize`."
msgstr ""

#: ../../doc/docs/_staging/faq.rst:43
msgid "TVM's relation to Other IR/DSL Projects"
msgstr ""

#: ../../doc/docs/_staging/faq.rst:44
msgid ""
"There are usually two levels of abstractions of IR in the deep learning "
"systems. TensorFlow's XLA and Intel's ngraph both use a computation graph"
" representation. This representation is high level, and can be helpful to"
" perform generic optimizations such as memory reuse, layout "
"transformation and automatic differentiation."
msgstr ""

#: ../../doc/docs/_staging/faq.rst:49
msgid ""
"TVM adopts a low-level representation, that explicitly express the choice"
" of memory layout, parallelization pattern, locality and hardware "
"primitives etc. This level of IR is closer to directly target hardwares. "
"The low-level IR adopts ideas from existing image processing languages "
"like Halide, darkroom and loop transformation tools like loopy and "
"polyhedra-based analysis. We specifically focus on expressing deep "
"learning workloads (e.g. recurrence), optimization for different hardware"
" backends and embedding with frameworks to provide end-to-end compilation"
" stack."
msgstr ""

#: ../../doc/docs/_staging/faq.rst:60
msgid "TVM's relation to libDNN, cuDNN"
msgstr ""

#: ../../doc/docs/_staging/faq.rst:61
msgid ""
"TVM can incorporate these libraries as external calls. One goal of TVM is"
" to be able to generate high-performing kernels. We will evolve TVM an "
"incremental manner as we learn from the techniques of manual kernel "
"crafting and add these as primitives in DSL. See also top for recipes of "
"operators in TVM."
msgstr ""

#: ../../doc/docs/_staging/faq.rst:68
msgid "Security"
msgstr ""

#: ../../doc/docs/_staging/faq.rst:69
msgid "See :ref:`dev-security`"
msgstr ""

