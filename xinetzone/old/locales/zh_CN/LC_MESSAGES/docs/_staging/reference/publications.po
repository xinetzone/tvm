# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-29 15:13+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/_staging/reference/publications.rst:19
msgid "Publications"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:21
msgid ""
"TVM is developed as part of peer-reviewed research in machine learning "
"compiler framework for CPUs, GPUs, and machine learning accelerators."
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:24
msgid ""
"This document includes references to publications describing the "
"research, results, and design that use or built on top of TVM."
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:27
msgid "2018"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:29
msgid ""
"`TVM: An Automated End-to-End Optimizing Compiler for Deep Learning`__, "
"[Slides_]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:34
msgid "`Learning to Optimize Tensor Programs`__, [Slides]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:38
msgid "2020"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:40
msgid ""
"`Ansor: Generating High-Performance Tensor Programs for Deep Learning`__,"
" [Slides__] [Tutorial__]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:46
msgid "2021"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:48
msgid ""
"`Nimble: Efficiently Compiling Dynamic Neural Networks for Model "
"Inference`__, [Slides__]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:53
msgid "`Cortex: A Compiler for Recursive Deep Learning Models`__, [Slides__]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:58
msgid "`UNIT: Unifying Tensorized Instruction Compilation`__, [Slides]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:62
msgid "`Lorien: Efficient Deep Learning Workloads Delivery`__, [Slides]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:67
msgid ""
"`Bring Your Own Codegen to Deep Learning Compiler`__, [Slides] "
"[Tutorial__]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:72
msgid "2022"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:74
msgid "`DietCode: Automatic optimization for dynamic tensor program`__, [Slides]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:78
msgid ""
"`Bolt: Bridging the Gap between Auto-tuners and Hardware-native "
"Performance`__, [Slides]"
msgstr ""

#: ../../doc/docs/_staging/reference/publications.rst:82
msgid ""
"`The CoRa Tensor Compiler: Compilation for Ragged Tensors with Minimal "
"Padding`__, [Slides]"
msgstr ""

