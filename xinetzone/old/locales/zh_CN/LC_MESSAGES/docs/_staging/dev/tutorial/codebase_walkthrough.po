# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-29 15:13+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:20
msgid "TVM Codebase Walkthrough by Example"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:22
msgid ""
"Getting to know a new codebase can be a challenge. This is especially "
"true for a codebase like that of TVM, where different components interact"
" in non-obvious ways. In this guide, we try to illustrate the key "
"elements that comprise a compilation pipeline with a simple example. For "
"each important step, we show where in the codebase it is implemented. The"
" purpose is to let new developers and interested users dive into the "
"codebase more quickly."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:26
msgid "Codebase Structure Overview"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:28
msgid ""
"At the root of the TVM repository, we have following subdirectories that "
"together comprise a bulk of the codebase."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:30
msgid "``src`` - C++ code for operator compilation and deployment runtimes."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:31
msgid ""
"``src/relay`` - Implementation of Relay, a new functional IR for deep "
"learning framework."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:32
msgid ""
"``python`` - Python frontend that wraps C++ functions and objects "
"implemented in ``src``."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:33
msgid ""
"``src/topi`` - Compute definitions and backend schedules for standard "
"neural network operators."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:35
msgid ""
"Using standard Deep Learning terminology, ``src/relay`` is the component "
"that manages a computational graph, and nodes in a graph are compiled and"
" executed using infrastructure implemented in the rest of ``src``. "
"``python`` provides python bindings for the C++ API and driver code that "
"users can use to execute compilation. Operators corresponding to each "
"node are registered in ``src/relay/op``. Implementations of operators are"
" in ``topi``, and they are coded in either C++ or Python."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:37
msgid ""
"When a user invokes graph compilation by ``relay.build(...)``, the "
"following sequence of actions happens for each node in the graph:"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:39
msgid "Look up an operator implementation by querying the operator registry"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:40
msgid "Generate a compute expression and a schedule for the operator"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:41
msgid "Compile the operator into object code"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:43
msgid ""
"One of the interesting aspects of the TVM codebase is that "
"interoperability between C++ and Python is not unidirectional. Typically,"
" all code that performs heavy lifting is implemented in C++, and Python "
"bindings are provided for the user interface. This is also true in TVM, "
"but in the TVM codebase, C++ code can also call into functions defined in"
" a Python module. For example, the convolution operator is implemented in"
" Python, and its implementation is invoked from C++ code in Relay."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:47
msgid "Vector Add Example"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:49
msgid ""
"We use a simple example that uses the low level TVM API directly. The "
"example is vector addition, which is covered in detail in :ref:`tutorial-"
"tensor-expr-get-started`"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:58
msgid ""
"Here, types of ``A``, ``B``, ``C`` are ``tvm.tensor.Tensor``, defined in "
"``python/tvm/te/tensor.py``. The Python ``Tensor`` is backed by C++ "
"``Tensor``, implemented in ``include/tvm/te/tensor.h`` and "
"``src/te/tensor.cc``. All Python types in TVM can be thought of as a "
"handle to the underlying C++ type with the same name. If you look at the "
"definition of Python ``Tensor`` type below, you can see it is a subclass "
"of ``Object``."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:69
msgid ""
"The object protocol is the basis of exposing C++ types to frontend "
"languages, including Python. The way TVM implements Python wrapping is "
"not straightforward. It is briefly covered in :ref:`tvm-runtime-system`, "
"and details are in ``python/tvm/_ffi/`` if you are interested."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:71
msgid ""
"We use the ``TVM_REGISTER_*`` macro to expose C++ functions to frontend "
"languages, in the form of a :ref:`tvm-runtime-system-packed-func`. A "
"``PackedFunc`` is another mechanism by which TVM implements "
"interoperability between C++ and Python. In particular, this is what "
"makes calling Python functions from the C++ codebase very easy. You can "
"also checkout `FFI Navigator <https://github.com/tqchen/ffi-navigator>`_ "
"which allows you to navigate between python and c++ FFI calls."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:74
msgid ""
"A ``Tensor`` object has an ``Operation`` object associated with it, "
"defined in ``python/tvm/te/tensor.py``, ``include/tvm/te/operation.h``, "
"and ``src/tvm/te/operation`` subdirectory. A ``Tensor`` is an output of "
"its ``Operation`` object. Each ``Operation`` object has in turn "
"``input_tensors()`` method, which returns a list of input ``Tensor`` to "
"it. This way we can keep track of dependencies between ``Operation``."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:76
msgid ""
"We pass the operation corresponding to the output tensor ``C`` to "
"``tvm.te.create_schedule()`` function in ``python/tvm/te/schedule.py``."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:82
msgid "This function is mapped to the C++ function in ``include/tvm/schedule.h``."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:90
msgid ""
"``Schedule`` consists of collections of ``Stage`` and output "
"``Operation``."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:92
msgid ""
"``Stage`` corresponds to one ``Operation``. In the vector add example "
"above, there are two placeholder ops and one compute op, so the schedule "
"``s`` contains three stages. Each ``Stage`` holds information about a "
"loop nest structure, types of each loop (``Parallel``, ``Vectorized``, "
"``Unrolled``), and where to execute its computation in the loop nest of "
"the next ``Stage``, if any."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:94
msgid ""
"``Schedule`` and ``Stage`` are defined in ``tvm/python/te/schedule.py``, "
"``include/tvm/te/schedule.h``, and ``src/te/schedule/schedule_ops.cc``."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:96
msgid ""
"To keep it simple, we call ``tvm.build(...)`` on the default schedule "
"created by ``create_schedule()`` function above, and we must add "
"necessary thread bindings to make it runnable on GPU."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:106
msgid ""
"``tvm.build()``, defined in ``python/tvm/driver/build_module.py``, takes "
"a schedule, input and output ``Tensor``, and a target, and returns a "
":py:class:`tvm.runtime.Module` object. A :py:class:`tvm.runtime.Module` "
"object contains a compiled function which can be invoked with function "
"call syntax."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:108
msgid "The process of ``tvm.build()`` can be divided into two steps:"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:110
msgid ""
"Lowering, where a high level, initial loop nest structures are "
"transformed into a final, low level IR"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:111
msgid ""
"Code generation, where target machine code is generated from the low "
"level IR"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:113
msgid ""
"Lowering is done by ``tvm.lower()`` function, defined in "
"``python/tvm/build_module.py``. First, bound inference is performed, and "
"an initial loop nest structure is created."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:127
msgid ""
"Bound inference is the process where all loop bounds and sizes of "
"intermediate buffers are inferred. If you target the CUDA backend and you"
" use shared memory, its required minimum size is automatically determined"
" here. Bound inference is implemented in ``src/te/schedule/bound.cc``, "
"``src/te/schedule/graph.cc`` and ``src/te/schedule/message_passing.cc``."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:130
msgid ""
"``stmt``, which is the output of ``ScheduleOps()``, represents an initial"
" loop nest structure. If you have applied ``reorder`` or ``split`` "
"primitives to your schedule, then the initial loop nest already reflects "
"those changes. ``ScheduleOps()`` is defined in "
"``src/te/schedule/schedule_ops.cc``."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:132
msgid ""
"Next, we apply a number of lowering passes to ``stmt``. These passes are "
"implemented in ``src/tir/pass`` subdirectory. For example, if you have "
"applied ``vectorize`` or ``unroll`` primitives to your schedule, they are"
" applied in loop vectorization and unrolling passes below."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:147
msgid ""
"After lowering is done, ``build()`` function generates target machine "
"code from the lowered function. This code can contain SSE or AVX "
"instructions if you target x86, or PTX instructions for CUDA target. In "
"addition to target specific machine code, TVM also generates host side "
"code that is responsible for memory management, kernel launch etc."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:149
msgid ""
"Code generation is done by ``build_module()`` function, defined in "
"``python/tvm/target/codegen.py``. On the C++ side, code generation is "
"implemented in ``src/target/codegen`` subdirectory. ``build_module()`` "
"Python function will reach ``Build()`` function below in "
"``src/target/codegen/codegen.cc``:"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:153
msgid ""
"The ``Build()`` function looks up the code generator for the given target"
" in the ``PackedFunc`` registry, and invokes the function found. For "
"example, ``codegen.build_cuda`` function is registered in "
"``src/codegen/build_cuda_on.cc``, like this:"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:162
msgid ""
"The ``BuildCUDA()`` above generates CUDA kernel source from the lowered "
"IR using ``CodeGenCUDA`` class defined in "
"``src/codegen/codegen_cuda.cc``, and compile the kernel using NVRTC. If "
"you target a backend that uses LLVM, which includes x86, ARM, NVPTX and "
"AMDGPU, code generation is done primarily by ``CodeGenLLVM`` class "
"defined in ``src/codegen/llvm/codegen_llvm.cc``. ``CodeGenLLVM`` "
"translates TVM IR into LLVM IR, runs a number of LLVM optimization "
"passes, and generates target machine code."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:164
msgid ""
"The ``Build()`` function in ``src/codegen/codegen.cc`` returns a "
"``runtime::Module`` object, defined in ``include/tvm/runtime/module.h`` "
"and ``src/runtime/module.cc``. A ``Module`` object is a container for the"
" underlying target specific ``ModuleNode`` object. Each backend "
"implements a subclass of ``ModuleNode`` to add target specific runtime "
"API calls. For example, the CUDA backend implements ``CUDAModuleNode`` "
"class in ``src/runtime/cuda/cuda_module.cc``, which manages the CUDA "
"driver API. The ``BuildCUDA()`` function above wraps ``CUDAModuleNode`` "
"with ``runtime::Module`` and return it to the Python side. The LLVM "
"backend implements ``LLVMModuleNode`` in "
"``src/codegen/llvm/llvm_module.cc``, which handles JIT execution of "
"compiled code. Other subclasses of ``ModuleNode`` can be found under "
"subdirectories of ``src/runtime`` corresponding to each backend."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:166
msgid ""
"The returned module, which can be thought of as a combination of a "
"compiled function and a device API, can be invoked on TVM's NDArray "
"objects."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:177
msgid ""
"Under the hood, TVM allocates device memory and manages memory transfers "
"automatically. To do that, each backend needs to subclass ``DeviceAPI`` "
"class, defined in ``include/tvm/runtime/device_api.h``, and override "
"memory management methods to use device specific API. For example, the "
"CUDA backend implements ``CUDADeviceAPI`` in "
"``src/runtime/cuda/cuda_device_api.cc`` to use ``cudaMalloc``, "
"``cudaMemcpy`` etc."
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:179
msgid ""
"The first time you invoke the compiled module with ``fadd(a, b, c)``, "
"``GetFunction()`` method of ``ModuleNode`` is called to get a "
"``PackedFunc`` that can be used for a kernel call. For example, in "
"``src/runtime/cuda/cuda_module.cc`` the CUDA backend implements "
"``CUDAModuleNode::GetFunction()`` like this:"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:193
msgid ""
"The ``PackedFunc``'s overloaded ``operator()`` will be called, which in "
"turn calls ``operator()`` of ``CUDAWrappedFunc`` in "
"``src/runtime/cuda/cuda_module.cc``, where finally we see the "
"``cuLaunchKernel`` driver call:"
msgstr ""

#: ../../doc/docs/_staging/dev/tutorial/codebase_walkthrough.rst:223
msgid ""
"This concludes an overview of how TVM compiles and executes a function. "
"Although we did not detail TOPI or Relay, in the end, all neural network "
"operators go through the same compilation process as above. You are "
"encouraged to dive into the details of the rest of the codebase."
msgstr ""

