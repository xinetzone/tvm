# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-29 15:13+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/_staging/arch/index.rst:19
msgid "Design and Architecture"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:21
msgid ""
"This document is intended for developers who want to understand the "
"architecture of Apache TVM and/or actively develop on the project. This "
"page is organized as follows:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:24
msgid ""
"The `Overall Flow`_ gives an overview of the steps that TVM takes to turn"
" a high level description of a model into a deployable module. To get "
"started, please read this section first."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:26
msgid ""
"Brief introduction to the key components of the TVM stack. Feel free to "
"also check out the :ref:`TensorIR Deep Dive <tensor-ir-deep-dive>` and "
":ref:`Relax Deep Dive <relax-deep-dive>` for more details about the two "
"major components in the TVM stack."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:29
msgid ""
"This guide provides a few complementary views of the architecture. First,"
" we review a single end-to-end compilation flow and discuss the key data "
"structures and the transformations. This runtime-based view focuses on "
"the interactions of each components when running the compiler. Then we "
"will review the logical modules of the codebase and their relationship. "
"This part provides a static overarching view of the design."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:35
msgid "Overall Flow"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:37
msgid ""
"In this guide, we will study an example compilation flow in the compiler."
" The figure below shows the flow. At a high-level, it contains several "
"steps:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:39
msgid ""
"**Model Creation**: Create the IRModule to be optimized and compiled, "
"which contains a collection of functions that internally represent the "
"model. Users can manually construct IRModule via NNModule, TVMScript, or "
"import a pre-trained model from from Relax frontend."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:41
msgid ""
"**Transformation**: The compiler transforms an IRModule to another "
"functionally equivalent or approximately equivalent(e.g. in the case of "
"quantization) IRModule. Many of the transformations are target (backend) "
"independent. We also allow target to affect the configuration of the "
"transformation pipeline."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:44
msgid ""
"**Target Translation**: The compiler translates(codegen) the IRModule to "
"an executable format specified by the target. The target translation "
"result is encapsulated as a `runtime.Module` that can be exported, "
"loaded, and executed on the target runtime environment."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:46
msgid ""
"**Runtime Execution**: the user loads back a `runtime.Module` and runs "
"the compiled functions in the supported runtime environment."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:55
msgid "Key data structures"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:57
msgid ""
"One of the best ways to design and understand a complex system is to "
"identify the key data structures and APIs that manipulate (transform) "
"these data structures. Once we identified the key data structures, we can"
" then breakdown a system into logical components that either define a "
"collection of key data structures or transformations among the data "
"structures."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:61
msgid ""
"**IRModule** is the primary data structure used across the entire stack. "
"An IRModule (intermediate representation module) contains a collection of"
" functions. Currently, we support two primary variants of functions."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:64
msgid ""
"**relax::Function** is a high-level functional program representation. A "
"relax.Function represents high-level graph structure, usually corresponds"
" to an end-to-end model or a sub-graph of the overall model. You can view"
" a relax.Function as a computational graph with additional support for "
"control-flow, and complex data structures."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:67
msgid ""
"**tir::PrimFunc** is a low-level program representation that contains "
"elements including loop-nest choices, multi-dimensional load/store, "
"threading, and vector/tensor instructions. It is usually used to "
"represent an operator program that executes a (possibly-fused) layer in a"
" model."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:70
msgid ""
"During the compilation and transformation, all relax operators are "
"lowered to ``tir::PrimFunc`` or ``TVM PackedFunc``, which can be executed"
" directly on the target device, while the calls to relax operators are "
"lowered to calls to low-level functions (e.g. ``R.call_tir`` or "
"``R.call_dps``)."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:74
msgid "Transformations"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:76
msgid ""
"Now that we have covered the key data structures, let us talk about the "
"transformations. Each transformation could serve one of the following "
"purposes:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:78
msgid ""
"optimization: transform a program to an equivalent, possibly more "
"optimized version."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:79
msgid ""
"lowering: transform a program to a lower-level representation that is "
"closer to the target."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:82
msgid "relax transformations"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:83
msgid ""
"relax transformations contain a collection of passes that apply to relax "
"functions. The optimizations include common graph-level optimizations "
"such as constant folding and dead-code elimination for operators, and "
"backend-specific optimizations such as library dispatch."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:87
msgid "tir transformations"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:88
msgid ""
"tir transformations contain a collection of passes that apply to tir "
"functions. There are two major types of transformations:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:90
msgid ""
"**TensorIR schedule**: TensorIR schedules are designed to optimize the "
"TensorIR functions for a specific target, with user-guided instructions "
"and control how the target code is generated. For CPU targets, TIR "
"PrimFunc can generate valid code and execute on the target device without"
" schedule but with very-low performance. However, for GPU targets, the "
"schedule is essential for generating valid code with thread bindings. For"
" more details, please refer to the :ref:`TensorIR Transformation <tir-"
"transform>` section. Additionally, we provides ``MetaSchedule`` to "
"automate the search of TensorIR schedule."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:94
msgid ""
"**Lowering Passes**: These passes usually perform after the schedule is "
"applied, transforming a TIR PrimFunc into another functionally equivalent"
" PrimFunc, but closer to the target-specific representation. For example,"
" there are passes to flatten multi-dimensional access to one-dimensional "
"pointer access, to expand the intrinsics into target-specific ones, and "
"to decorate the function entry to meet the runtime calling convention."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:99
msgid ""
"Many low-level optimizations can be handled in the target phase by the "
"LLVM, CUDA C, and other target compilers. As a result, we leave low-level"
" optimizations such as register allocation"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:99
msgid ""
"to the downstream compilers and only focus on optimizations that are not "
"covered by them."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:102
msgid "cross-level transformations"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:103
msgid ""
"Apache TVM brings a unity strategy to optimize the end-to-end models. As "
"the IRModule includes both relax and tir functions, the cross-level "
"transformations are designed to mutate the IRModule by applying different"
" transformations to these two types of functions."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:106
msgid ""
"For example, ``relax.LegalizeOps`` pass mutates the IRModule by lowering "
"relax operators, add corresponding TIR PrimFunc into the IRModule, and "
"replace the relax operators with calls to the lowered TIR PrimFunc. "
"Another example is operator fusion pipeline in relax (including "
"``relax.FuseOps`` and ``relax.FuseTIR``), which fuse multiple consecutive"
" tensor operations into one. Different from the previous implementations,"
" relax fusion pipeline analyzes the pattern of TIR functions and detects "
"the best fusion rules automatically rather than human-defined operator "
"fusion patterns."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:112
msgid "Target Translation"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:114
msgid ""
"The target translation phase transforms an IRModule to the corresponding "
"target executable format. For backends such as x86 and ARM, we use the "
"LLVM IRBuilder to build in-memory LLVM IR. We can also generate source-"
"level languages such as CUDA C and OpenCL. Finally, we support direct "
"translations of a Relay function (sub-graph) to specific targets via "
"external code generators. It is important that the final code generation "
"phase is as lightweight as possible. Vast majority of transformations and"
" lowering should be performed before the target translation phase."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:121
msgid ""
"We also provide a Target structure to specify the compilation target. The"
" transformations before the target translation phase can also be affected"
" by the target — for example, a target's vector length would change the "
"vectorization behavior."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:127
msgid "Runtime Execution"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:129
msgid ""
"The main goal of TVM's runtime is to provide a minimal API for loading "
"and executing the compiled artifact in a language of their choice, "
"including Python, C++, Rust, Go, Java, and JavaScript. The code snippet "
"below shows such an example in Python:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:142
msgid ""
":py:class:`tvm.runtime.Module` encapsulates the result of compilation. A "
"runtime.Module contains a GetFunction method to obtain PackedFuncs by "
"name."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:144
msgid ""
":py:class:`tvm.runtime.PackedFunc` is a type-erased function interface "
"for both the generated functions. A runtime.PackedFunc can take arguments"
" and return values with the following types: POD types(int, float), "
"string, runtime.PackedFunc, runtime.Module, runtime.NDArray, and other "
"sub-classes of runtime.Object."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:147
msgid ""
":py:class:`tvm.runtime.Module` and :py:class:`tvm.runtime.PackedFunc` are"
" powerful mechanisms to modularize the runtime. For example, to get the "
"above `addone` function on CUDA, we can use LLVM to generate the host-"
"side code to compute the launching parameters(e.g. size of the thread "
"groups) and then call into another PackedFunc from a CUDAModule that is "
"backed by the CUDA driver API. The same mechanism can be used for OpenCL "
"kernels."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:149
msgid ""
"The above example only deals with a simple `addone` function. The code "
"snippet below gives an example of an end-to-end model execution using the"
" same interface:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:166
msgid ""
"The main take away is that runtime.Module and runtime.PackedFunc are "
"sufficient to encapsulate both operator level programs (such as addone), "
"as well as the end-to-end models."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:169
msgid "Summary and Discussions"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:171
msgid "In summary, the key data structures in the compilation flows are:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:173
msgid "IRModule: contains relay.Function and tir.PrimFunc"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:174
msgid "runtime.Module: contains runtime.PackedFunc"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:176
msgid ""
"Most parts of the compilation are transformations among the key data "
"structures."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:178
msgid ""
"relay/transform and tir/transform are determinstic rule-based "
"transformations"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:179
msgid "auto_scheduler and autotvm contains the search-based transformations"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:181
msgid ""
"Finally, the compilation flow example is only a typical use-case of the "
"TVM stack. We expose these key data structures and transformations to "
"python and C++ APIs. As a result, you can use TVM just like the way you "
"use numpy, except that the data structure of interest changes from the "
"numpy.ndarray to tvm.IRModule. Here are some example use-cases:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:185
msgid "Directly construct IRModule using the python API."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:186
msgid "Compose a custom set of transformations(e.g. customize quantization)."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:187
msgid "Manipulate the IR directly using TVM's python API."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:191
msgid "tvm/support"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:192
msgid ""
"The support module contains the most common utilities for the "
"infrastructure, such as generic arena allocator, socket, and logging."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:196
msgid "tvm/runtime"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:198
msgid ""
"The runtime serves as the foundation of the TVM stack. It provides the "
"mechanism to load and execute compiled artifacts. The runtime defines a "
"stable standard set of C APIs to interface with frontend languages such "
"as Python and Rust."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:201
msgid ""
"`runtime::Object` is one of the primary data structures in TVM runtime "
"besides the `runtime::PackedFunc`. It is a reference-counted base class "
"with a type index to support runtime type checking and downcasting. The "
"object system allows the developer to introduce new data structures to "
"the runtime, such as Array, Map, and new IR data structures."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:205
msgid ""
"Besides deployment use-cases, the compiler itself also makes heavy use of"
" TVM's runtime mechanism. All of the IR data structures are subclasses of"
" `runtime::Object`, as a result, they can be directly accessed and "
"manipulated from the Python frontend. We use the PackedFunc mechanism to "
"expose various APIs to the frontend."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:209
msgid ""
"Runtime support for different hardware backends are defined in "
"subdirectories of runtime(e.g. runtime/opencl). These hardware-specific "
"runtime modules define APIs for device memory allocation and device "
"function serialization."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:212
msgid ""
"`runtime/rpc` implements an RPC support for PackedFunc. We can use the "
"RPC mechanism to send a cross-compiled library to a remote device and "
"benchmark the execution performance. The rpc infrastructure enables data "
"collection from a wide range of hardware backends for learning-based "
"optimizations."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:231
msgid "tvm/node"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:232
msgid ""
"The node module adds additional features on top of the `runtime::Object` "
"for IR data structures. The main features include reflection, "
"serialization, structural equivalence, and hashing."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:235
msgid ""
"Thanks to the node module, we can directly access any field of the TVM's "
"IRNode by their name in Python."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:245
msgid ""
"We can also serialize arbitrary IR node into a JSON format, and load them"
" back. The ability to save/store, and inspect an IR node provides a "
"foundation for making the compiler more accessible."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:249
msgid "tvm/ir"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:250
msgid ""
"The `tvm/ir` folder contains the unified data structure and interfaces "
"across for all IR function variants. The components in `tvm/ir` are "
"shared by `tvm/relay` and `tvm/tir`, notable ones include"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:253
msgid "IRModule"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:254
msgid "Type"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:255
msgid "PassContext and Pass"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:256
msgid "Op"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:258
msgid ""
"Different variants of functions(e.g. relay.Function and tir.PrimFunc) can"
" co-exist in an IRModule. While these variants may not have the same "
"content representation, they use the same data structure to represent "
"types. As a consequence, we use the same data structure to represent "
"function (type) signatures of these variants. The unified type system "
"allows one function variant to call another function once we clearly "
"define the calling convention. This opens doors for future cross-"
"function-variant optimizations."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:264
msgid ""
"We also provide a unified PassContext for configuring the pass behavior, "
"and common composite passes to execute a pass pipeline. The following "
"code snippet gives an example of PassContext configuration."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:274
msgid ""
"Op is the common class to represent all system-defined primitive "
"operator/intrinsics. Developers can register new Ops as well as their "
"additional attributes(e.g. whether the Op is elementwise) to the system."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:284
msgid "tvm/target"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:285
msgid ""
"The target module contains all the code generators that translate an "
"IRModule to a target runtime.Module. It also provides a common `Target` "
"class that describes the target."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:291
msgid ""
"The compilation pipeline can be customized according to the target by "
"querying the attribute information in the target and builtin information "
"registered to each target id(cuda, opencl)."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:300
msgid "tvm/relax"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:302
msgid ""
"Relax is the high-level IR used to represent the computational graph of a"
" model. Various optimizations are defined in ``relax.transform``. Note "
"that Relax usually works closely the the TensorIR IRModule, most of the "
"transformations are applied on the both Relax and TensorIR functions in "
"the IRModule. Please refer to the :ref:`Relax Deep Dive <relax-deep-"
"dive>` for more details."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:307
msgid "tvm/tir"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:309
msgid ""
"TIR contains the definition of the low-level program representations. We "
"use `tir::PrimFunc` to represent functions that can be transformed by TIR"
" passes. Besides the IR data structures, the tir module also includes:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:312
msgid ""
"A set of schedule primitives to control the generated code in "
"``tir/schedule``."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:313
msgid "A set of builtin intrinsics in ``tir/tensor_intrin``."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:314
msgid "A set of analysis passes to analyze the TIR functions in ``tir/analysis``."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:315
msgid ""
"A set of transformation passes to lower or optimize the TIR functions in "
"``tir/transform``."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:317
msgid ""
"Please refer to the :ref:`TensorIR Deep Dive <tensor-ir-deep-dive>` for "
"more details."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:320
msgid "tvm/arith"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:322
msgid ""
"This module is closely tied to the TIR. One of the key problems in the "
"low-level code generation is the analysis of the indices' arithmetic "
"properties — the positiveness, variable bound, and the integer set that "
"describes the iterator space. arith module provides a collection of tools"
" that do (primarily integer) analysis. A TIR pass can use these analyses "
"to simplify and optimize the code."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:327
msgid "tvm/te and tvm/topi"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:329
msgid ""
"TE stands for Tensor Expression. TE is a domain-specific language (DSL) "
"for describing tensor computations. Importantly, a tensor expression "
"itself is not a self-contained function that can be stored into IRModule."
" We can use ``te.create_prim_func`` to convert a tensor expression to a "
"``tir::PrimFunc`` and then integrate it into the IRModule."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:333
msgid ""
"While possible to construct operators directly via TIR or tensor "
"expressions (TE) for each use case it is tedious to do so. `topi` (Tensor"
" operator inventory) provides a set of pre-defined operators defined by "
"numpy and found in common deep learning workloads."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:337
msgid "tvm/meta_schedule"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:339
msgid ""
"MetaSchedule is a system for automated search-based program optimization."
" It is designed to be a drop-in replacement for AutoTVM and "
"AutoScheduler, and can be used to optimize TensorIR schedules. Note that "
"MetaSchedule only works with static-shape workloads."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:343
msgid "tvm/dlight"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:345
msgid ""
"DLight is a set of pre-defined, easy-to-use, and performant TIR "
"schedules. DLight aims:"
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:347
msgid "Fully support **dynamic shape workloads**."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:348
msgid ""
"**Light weight**. DLight schedules provides tuning-free or (very few-"
"shots tuning) schedule with reasonable performance."
msgstr ""

#: ../../doc/docs/_staging/arch/index.rst:349
msgid ""
"**Robust**. DLight schedules are designed to be robust and general-"
"purpose for a single rule. And if the rule is not applicable, DLight not "
"raise any error and switch to the next rule automatically."
msgstr ""

