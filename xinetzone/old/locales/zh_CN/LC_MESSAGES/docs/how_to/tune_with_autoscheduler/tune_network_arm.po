# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-09 00:02+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:20003
msgid "Auto-scheduling a Neural Network for ARM CPU"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:20004
msgid ""
"**Author**: [Thierry Moreau](https://github.com/tmoreau89),             "
"[Lianmin Zheng](https://github.com/merrymercy),             [Chengfan "
"Jia](https://github.com/jcf94/)"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:20006
msgid ""
"Auto-tuning for specific devices and workloads is critical for getting "
"the best performance. This is a tutorial on how to tune a whole neural "
"network for ARM CPU with the auto-scheduler via RPC."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:20010
msgid ""
"To auto-tune a neural network, we partition the network into small "
"subgraphs and tune them independently. Each subgraph is treated as one "
"search task. A task scheduler slices the time and dynamically allocates "
"time resources to these tasks. The task scheduler predicts the impact of "
"each task on the end-to-end execution time and prioritizes the one that "
"can reduce the execution time the most."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:20016
msgid ""
"For each subgraph, we use the compute declaration in "
":code:`tvm/python/topi` to get the computational DAG in the tensor "
"expression form. We then use the auto-scheduler to construct a search "
"space of this DAG and search for good schedules (low-level "
"optimizations)."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:20021
msgid ""
"Different from the template-based `autotvm <tutorials-autotvm-sec>` which"
" relies on manual templates to define the search space, the auto-"
"scheduler does not require any schedule templates. In other words, the "
"auto-scheduler only uses the compute declarations in "
":code:`tvm/python/topi` and does not use existing schedule templates."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:20026
msgid ""
"Note that this tutorial will not run on Windows or recent versions of "
"macOS. To get it to run, you will need to wrap the body of this tutorial "
"in a :code:`if __name__ == \"__main__\":` block."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:40002
msgid "Define a Network"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:40003
msgid ""
"First, we need to define the network with relay frontend API. We can load"
" some pre-defined network from :code:`tvm.relay.testing`. We can also "
"load models from MXNet, ONNX, PyTorch, and TensorFlow (see `front end "
"tutorials<tutorial-frontend>`)."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:40008
msgid ""
"For convolutional neural networks, although auto-scheduler can work "
"correctly with any layout, we found the best performance is typically "
"achieved with NHWC layout. We also implemented more optimizations for "
"NHWC layout with the auto-scheduler. So it is recommended to convert your"
" models to NHWC layout to use the auto-scheduler. You can use "
"`ConvertLayout <convert-layout-usage>` pass to do the layout conversion "
"in TVM."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:60002
msgid "Start RPC Tracker"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:60003
msgid ""
"TVM uses RPC session to communicate with ARM boards. During tuning, the "
"tuner will send the generated code to the board and measure the speed of "
"code on the board."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:60007
msgid ""
"To scale up the tuning, TVM uses RPC Tracker to manage distributed "
"devices. The RPC Tracker is a centralized controller node. We can "
"register all devices to the tracker. For example, if we have 10 phones, "
"we can register all of them to the tracker, and run 10 measurements in "
"parallel, accelerating the tuning process."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:60012
msgid ""
"To start an RPC tracker, run this command on the host machine. The "
"tracker is required during the whole tuning process, so we need to open a"
" new terminal for this command:"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:60019
msgid "The expected output is"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:70002
msgid "Register Devices to RPC Tracker"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:70003
msgid ""
"Now we can register our devices to the tracker. The first step is to "
"build the TVM runtime for the ARM devices."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:70006
msgid ""
"For Linux: Follow this section `build-tvm-runtime-on-device` to build the"
" TVM runtime on the device. Then register the device to tracker by"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:70013
msgid "(replace :code:`[HOST_IP]` with the IP address of your host machine)"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:70015
msgid ""
"For Android: Follow this [readme "
"page](https://github.com/apache/tvm/tree/main/apps/android_rpc) to "
"install the TVM RPC APK on the android device. Make sure you can pass the"
" android rpc test. Then you have already registered your device. During "
"tuning, you have to go to developer option and enable \"Keep screen awake"
" during changing\" and charge your phone to make it stable."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:70021
msgid "After registering devices, we can confirm it by querying rpc_tracker"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:70026
msgid ""
"For example, if we have 2 Huawei mate10 pro, 11 Raspberry Pi 4B with "
"64bit OS, and 2 rk3399, the output can be"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:70039
msgid ""
"You can register multiple devices to the tracker to accelerate the "
"measurement in tuning."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:80002
msgid "Set Tuning Options"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:80003
msgid ""
"Before tuning, we should apply some configurations. Here I use a "
"Raspberry Pi 4b 4GB board as example with a 64bit OS (Ubuntu 20.04). In "
"your setting, you should modify the target and device_key accordingly. "
"set :code:`use_ndk` to True if you use android phone."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:100002
msgid "Extract Search Tasks"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:100003
msgid ""
"Next, we extract the search tasks and their weights from a network. The "
"weight of a task is the number of appearances of the task's subgraph in "
"the whole network. By using the weight, we can approximate the end-to-end"
" latency of the network as :code:`sum(latency[t] * weight[t])`, where "
":code:`latency[t]` is the latency of a task and :code:`weight[t]` is the "
"weight of the task. The task scheduler will just optimize this objective."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:120002
msgid "Tuning and Evaluation"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:120003
msgid "Now, we set some options for tuning and launch the search tasks"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:120005
msgid ""
":code:`num_measure_trials` is the number of measurement trials we can use"
" during the tuning. You can set it to a small number (e.g., 200) for a "
"fast demonstrative run. In practice, we recommend setting it around "
":code:`800 * len(tasks)`, which is typically enough for the search to "
"converge. For example, there are 29 tasks in resnet-50, so we can set it "
"as 20000. You can adjust this parameter according to your time budget."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:120011
msgid ""
"In addition, we use :code:`RecordToFile` to dump measurement records into"
" a log file, The measurement records can be used to query the history "
"best, resume the search, and do more analyses later."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:120014
msgid ""
"see :any:`auto_scheduler.TuningOptions`, "
":any:`auto_scheduler.LocalRunner` for more parameters."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:120017
msgid ""
"After auto-tuning, we can compile the network with the best schedules we "
"found. All measurement records are dumped into the log file during auto-"
"tuning, so we can read the log file and load the best schedules."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:140004
msgid ""
"During the tuning, a lot of information will be printed on the console."
"   They are used for debugging purposes. The most important info is the "
"output   of the task scheduler. The following table is a sample output."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:140039
msgid ""
"This table lists the latency and (estimated) speed of all tasks.   It "
"also lists the allocation of measurement trials for all tasks.   The last"
" line prints the total weighted latency of these tasks,   which can be a "
"rough estimation of the end-to-end execution time   of the network.   The"
" last line also prints the total number of measurement trials,   total "
"time spent on auto-tuning and the id of the next task to tune."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:140047
msgid ""
"There will also be some \"dmlc::Error\"s errors, because the   auto-"
"scheduler will try some invalid schedules.   You can safely ignore them "
"if the tuning can continue, because these   errors are isolated from the "
"main process.</p></div>"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:150004
msgid ""
"You can terminate the tuning earlier by forcibly killing this process.   "
"As long as you get at least one valid schedule for each task in the log "
"file,   you should be able to do the compilation (the secion "
"below).</p></div>"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:160002
msgid "Other Tips"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:160003
msgid ""
"During the tuning, the auto-scheduler needs to compile many programs and "
"extract feature from them. This part is CPU-intensive, so a high-"
"performance CPU with many cores is recommended for faster search."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:160006
msgid ""
"You can use :code:`python3 -m tvm.auto_scheduler.measure_record --mode "
"distill -i log.json` to distill the large log file and only save the best"
" useful records."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:160008
msgid ""
"You can resume a search from the previous log file. You just need to add "
"a new argument :code:`load_log_file` when creating the task scheduler in "
"function :code:`run_tuning`. Say, :code:`tuner = "
"auto_scheduler.TaskScheduler(tasks, task_weights, "
"load_log_file=log_file)`"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_arm.ipynb:160012
msgid ""
"If you have multiple target CPUs, you can use all of them for "
"measurements to parallelize the measurements. Check this `section "
"<tutorials-autotvm-scale-up-rpc-tracker>` to learn how to use the RPC "
"Tracker and RPC Server. To use the RPC Tracker in auto-scheduler, replace"
" the runner in :code:`TuningOptions` with "
":any:`auto_scheduler.RPCRunner`."
msgstr ""

#~ msgid ""
#~ ":download:`Download Python source code: "
#~ "tune_network_arm.py <tune_network_arm.py>`"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Jupyter notebook: "
#~ "tune_network_arm.ipynb <tune_network_arm.ipynb>`"
#~ msgstr ""

#~ msgid ""
#~ "`Gallery generated by Sphinx-Gallery "
#~ "<https://sphinx-gallery.github.io>`_"
#~ msgstr ""

#~ msgid ""
#~ "Click :ref:`here "
#~ "<sphx_glr_download_how_to_tune_with_autoscheduler_tune_network_arm.py>`"
#~ " to download the full example code"
#~ msgstr ""

#~ msgid ""
#~ "**Author**: `Thierry Moreau "
#~ "<https://github.com/tmoreau89>_`,             `Lianmin "
#~ "Zheng <https://github.com/merrymercy>_`,             "
#~ "`Chengfan Jia <https://github.com/jcf94/>`_"
#~ msgstr ""

#~ msgid ""
#~ "Different from the template-based "
#~ ":ref:`autotvm <tutorials-autotvm-sec>` which"
#~ " relies on manual templates to define"
#~ " the search space, the auto-scheduler"
#~ " does not require any schedule "
#~ "templates. In other words, the auto-"
#~ "scheduler only uses the compute "
#~ "declarations in :code:`tvm/python/topi` and "
#~ "does not use existing schedule "
#~ "templates."
#~ msgstr ""

#~ msgid ""
#~ "First, we need to define the "
#~ "network with relay frontend API. We "
#~ "can load some pre-defined network "
#~ "from :code:`tvm.relay.testing`. We can also"
#~ " load models from MXNet, ONNX, "
#~ "PyTorch, and TensorFlow (see :ref:`front "
#~ "end tutorials<tutorial-frontend>`)."
#~ msgstr ""

#~ msgid ""
#~ "For convolutional neural networks, although"
#~ " auto-scheduler can work correctly "
#~ "with any layout, we found the best"
#~ " performance is typically achieved with "
#~ "NHWC layout. We also implemented more"
#~ " optimizations for NHWC layout with "
#~ "the auto-scheduler. So it is "
#~ "recommended to convert your models to"
#~ " NHWC layout to use the auto-"
#~ "scheduler. You can use :ref:`ConvertLayout "
#~ "<convert-layout-usage>` pass to do "
#~ "the layout conversion in TVM."
#~ msgstr ""

#~ msgid ""
#~ "For Linux: Follow this section :ref"
#~ ":`build-tvm-runtime-on-device` to "
#~ "build the TVM runtime on the "
#~ "device. Then register the device to "
#~ "tracker by"
#~ msgstr ""

#~ msgid ""
#~ "For Android: Follow this `readme page"
#~ " <https://github.com/apache/tvm/tree/main/apps/android_rpc>`_ "
#~ "to install the TVM RPC APK on "
#~ "the android device. Make sure you "
#~ "can pass the android rpc test. "
#~ "Then you have already registered your"
#~ " device. During tuning, you have to"
#~ " go to developer option and enable"
#~ " \"Keep screen awake during changing\" "
#~ "and charge your phone to make it"
#~ " stable."
#~ msgstr ""

#~ msgid "Explaining the printed information during tuning"
#~ msgstr ""

#~ msgid ""
#~ "During the tuning, a lot of "
#~ "information will be printed on the "
#~ "console. They are used for debugging "
#~ "purposes. The most important info is "
#~ "the output of the task scheduler. "
#~ "The following table is a sample "
#~ "output."
#~ msgstr ""

#~ msgid ""
#~ "This table lists the latency and "
#~ "(estimated) speed of all tasks. It "
#~ "also lists the allocation of measurement"
#~ " trials for all tasks. The last "
#~ "line prints the total weighted latency"
#~ " of these tasks, which can be a"
#~ " rough estimation of the end-to-"
#~ "end execution time of the network. "
#~ "The last line also prints the "
#~ "total number of measurement trials, "
#~ "total time spent on auto-tuning "
#~ "and the id of the next task "
#~ "to tune."
#~ msgstr ""

#~ msgid ""
#~ "There will also be some \"dmlc::Error\"s"
#~ " errors, because the auto-scheduler "
#~ "will try some invalid schedules. You "
#~ "can safely ignore them if the "
#~ "tuning can continue, because these "
#~ "errors are isolated from the main "
#~ "process."
#~ msgstr ""

#~ msgid "Terminate the tuning earlier"
#~ msgstr ""

#~ msgid ""
#~ "You can terminate the tuning earlier "
#~ "by forcibly killing this process. As "
#~ "long as you get at least one "
#~ "valid schedule for each task in "
#~ "the log file, you should be able"
#~ " to do the compilation (the secion"
#~ " below)."
#~ msgstr ""

#~ msgid ""
#~ "If you have multiple target CPUs, "
#~ "you can use all of them for "
#~ "measurements to parallelize the measurements."
#~ " Check this :ref:`section <tutorials-"
#~ "autotvm-scale-up-rpc-tracker>` to "
#~ "learn how to use the RPC Tracker"
#~ " and RPC Server. To use the RPC"
#~ " Tracker in auto-scheduler, replace "
#~ "the runner in :code:`TuningOptions` with "
#~ ":any:`auto_scheduler.RPCRunner`."
#~ msgstr ""

