# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-09 00:02+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:20003
msgid ""
"Auto-scheduling Sparse Matrix Multiplication on CPU with Custom Sketch "
"Rule"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:20004
msgid "**Author**: [Chengfan Jia](https://github.com/jcf94/)"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:20006
msgid ""
"This is a tutorial on how to use the auto-scheduler to tune a sparse "
"matrix multiplication for CPUs."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:20009
msgid ""
"Auto-scheduler is designed to explore the schedule with best performance "
"for a given computation declaration automatically. While sometimes, we "
"may have a demand to try some special ops which may not been well-"
"supported by auto-scheduler's default sketch rules and result in poor "
"performance. Fortunately, auto-scheduler currently allows user to provide"
" a CustomSketch to cover these cases."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:20014
msgid ""
"We use sparse matrix multiplication as an example in this tutorial to "
"demonstrate how to implement and plug a custom sketch rule to the auto-"
"scheduler's search policy."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:20017
msgid ""
"Note that this tutorial will not run on Windows or recent versions of "
"macOS. To get it to run, you will need to wrap the body of this tutorial "
"in a :code:`if __name__ == \"__main__\":` block."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:40002
msgid "Define the computation"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:40003
msgid ""
"To begin with, let us define the computation of a sparse matmul with "
"several relu and bias add. The function should return the list of "
"input/output tensors. From these tensors, the auto-scheduler can get the "
"whole computational graph."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:60002
msgid "Special step for sparse workload"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:60003
msgid ""
"During schedule tuning, auto-scheduler will use random inputs to measure "
"the performance of a generated schedule. While we cannot directly use a "
"random array as the input of a sparse op, for the \"indices\" and "
"\"indptr\" array are meaningful for the computation."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:60007
msgid ""
"To solve this problem, we register these as special buffers, and load "
"them when process program measuring. See the "
"`tvm.auto_scheduler.measure.py` for more details."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:80002
msgid "Create the search task"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:80003
msgid ""
"We then create a search task with M=N=K=512 and dtype=\"float32\" If your"
" machine supports avx instructions, you can"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:80006
msgid "replace \"llvm\" below with \"llvm -mcpu=core-avx2\" to enable AVX2"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:80007
msgid ""
"replace \"llvm\" below with \"llvm -mcpu=skylake-avx512\" to enable "
"AVX-512"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:100002
msgid "Write the custom sketch for sparse dense op"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:100003
msgid ""
"Before tuning, we will need to define the CustomSketchRule for the sparse"
" dense op."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:100005
msgid ""
"CustomSketchRule consists of two parts: the condition function and the "
"apply function."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:100007
msgid ""
"condition function: describe when to apply this sketch rule. For example,"
" we can only apply the rule to the sparse ops by matching their name and "
"tag."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:100009
msgid ""
"apply function: describe how to generate the initial sketch. You can "
"implement it using auto-scheduler provided loop state APIs."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:120002
msgid ""
"Next, we set parameters for the auto-scheduler with the custom sketch "
"plugged in."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:120004
msgid ""
":code:`num_measure_trials` is the number of measurement trials we can use"
" during the search. We only make 10 trials in this tutorial for a fast "
"demonstration. In practice, 1000 is a good value for the search to "
"converge. You can do more trials according to your time budget."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:120007
msgid ""
"In addition, we use :code:`RecordToFile` to dump measurement records into"
" a file `sparse_dense.json`. The measurement records can be used to query"
" the history best, resume the search, and do more analyses later."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:120011
msgid "see :any:`auto_scheduler.TuningOptions` for more parameters"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:120012
msgid ""
"Here, we need to create a :code:`auto_scheduler.SketchPolicy` object, and"
" add the custom sketch rule as a `init_search_callbacks`."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:140002
msgid "Run the search"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:140003
msgid ""
"Now we get all inputs ready. We can kick off the search and let the auto-"
"scheduler do its magic. After some measurement trials, we can load the "
"best schedule from the log file and apply it."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:160002
msgid ""
"We can lower the schedule to see the IR after auto-scheduling. The auto-"
"scheduler correctly performs optimizations including multi-level tiling, "
"layout transformation, parallelization, vectorization, unrolling, and "
"operator fusion."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:180002
msgid "Check correctness and evaluate performance"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_sparse_x86.ipynb:180003
msgid "We build the binary and check its correctness and performance."
msgstr ""

#~ msgid ""
#~ ":download:`Download Python source code: "
#~ "tune_sparse_x86.py <tune_sparse_x86.py>`"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Jupyter notebook: "
#~ "tune_sparse_x86.ipynb <tune_sparse_x86.ipynb>`"
#~ msgstr ""

#~ msgid ""
#~ "`Gallery generated by Sphinx-Gallery "
#~ "<https://sphinx-gallery.github.io>`_"
#~ msgstr ""

#~ msgid ""
#~ "Click :ref:`here "
#~ "<sphx_glr_download_how_to_tune_with_autoscheduler_tune_sparse_x86.py>`"
#~ " to download the full example code"
#~ msgstr ""

#~ msgid "**Author**: `Chengfan Jia <https://github.com/jcf94/>`_"
#~ msgstr ""

#~ msgid "Tuning result example"
#~ msgstr ""

