# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-05-05 16:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:10002
msgid "基于 x86 CPU 的神经网络自动调度"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:10003
msgid ""
"**原作者**: [Lianmin Zheng](https://github.com/merrymercy), [Chengfan "
"Jia](https://github.com/jcf94/)"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:10005
msgid "针对特定设备和工作负载的自动调优对于获得最佳性能至关重要。下面是关于如何用自动调度器调优 x86 CPU 的整个神经网络的教程。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:10007
msgid "为了自动调优神经网络，需要将网络划分为小的子图，并独立地调优它们。每个子图被视为一个搜索任务。任务调度程序对时间进行切片，并动态地为这些任务分配时间资源。任务调度器预测每个任务对端到端执行时间的影响，并优先考虑能够最大程度减少执行时间的任务。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:10009
msgid ""
"对于每个子图，使用 `tvm/python/topi` 中的 compute 声明来获得张量表达式形式的计算 DAG。然后，使用自动调度器来构造 "
"DAG 的搜索空间，并搜索良好的调度（低级优化）。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:10011
msgid ""
"与基于模板的 [autotvm](../tune_with_autotvm/index) "
"依赖手动模板定义搜索空间不同，自动调度程序不需要任何调度模板。换句话说，自动调度器只使用 `tvm/python/topi` 中的 "
"compute，而不使用现有的调度模板。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:30002
msgid "定义网络"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:30004
msgid ""
"首先，需要用 relay 前端 AP I定义网络。可以从 {mod}`tvm.relay.testing` 加载一些预定义的网络。还可以从 "
"MXNet、ONNX、PyTorch 和 TensorFlow 加载模型（参见[前端教程](../compile_models/index)）。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:30006
msgid ""
"对于卷积神经网络，尽管自动调度器可以在任何布局下正确工作，但我们发现 NHWC 布局通常能获得最佳性能。我们还通过自动调度器实现了对 NHWC "
"布局的更多优化。因此，建议将模型转换为 NHWC 布局以使用自动调度器。可以使用 [ConvertLayout pass](convert-"
"layout-usage) 在 TVM 中进行布局转换。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:50002
msgid "提取搜索任务"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:50004
msgid ""
"接下来，从网络中提取搜索任务及其权重。任务的权重是该任务的子图在整个网络中出现的次数。通过使用权重，可以将网络的端到端延迟近似为 "
"`sum(latency[t] * weight[t])`，其中 `latency[t]` 是任务的延迟，`weight[t]` "
"是任务的权重。任务调度器会优化这个目标。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:70002
msgid "开始调优"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:70004
msgid "现在，设置了一些调优选项并启动搜索任务"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:70006
msgid ""
"`num_measure_trials` "
"是在调优期间可以使用的度量试验的数量。您可以将它设置为小的数字（例如，200）以进行快速演示运行。在实践中，建议将它设置在 `800 * "
"len(tasks)` 左右，这通常足以让搜索收敛。例如 resnet-50 中有 29 个任务，所以可以将其设置为 "
"20000。可以根据自己的时间预算调整该参数。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:70007
msgid "此外，使用 `RecordToFile` 将测量记录转储到日志文件中，测量记录可以用于查询历史，恢复搜索，并在以后进行更多的分析。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:70008
msgid ""
"查阅 "
"{mod}`tvm.auto_scheduler.TuningOptions`、{mod}`tvm.auto_scheduler.LocalRunner`"
" 获取更多参数。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:90002
msgid "解释在调优期间的打印信息"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:90004
msgid "在调优过程中，控制台上将打印大量信息。它们用于调试目的。最重要的信息是任务调度器的输出。下表是示例输出。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:90045
msgid ""
"该表列出了所有任务的延迟和（估计的）速度。它还列出了所有任务的测量试验分配。最后一行打印这些任务的总加权延迟，这可以粗略估计网络的端到端执行时间。最后一行还输出测试的总数、自动调优所花费的总时间和下一个要调优的任务的"
" id。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:90047
msgid ""
"也会有一些 \"tvm::Error\" "
"的错误，因为自动调度程序将尝试一些无效的调度。如果可以继续进行调优，则可以安全地忽略它们，因为这些错误与主进程隔离开来。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:100002
msgid "提前终止调优"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:100004
msgid "可以通过强制终止此进程提前终止调优。只要为日志文件中的每个任务获得至少一个有效的调度，就应该能够进行编译（见下面的部分）。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:110002
msgid "编译和评估"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:110004
msgid "在自动调优之后，可以用找到的最佳调度来编译网络。在自动调优期间，所有测量记录都被转储到日志文件中，因此可以读取日志文件并加载最佳调度。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:130002
msgid "其他技巧"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:130004
msgid "在调优过程中，自动调度器需要编译许多程序并从中提取特征。该部分是 CPU 密集型的，因此建议使用多核的高性能 CPU，以加快搜索速度。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:130005
msgid ""
"可以使用 `python3 -m tvm.auto_scheduler.measure_record --mode distill -i "
"log.json` 提取大的日志文件，只保存最好的有用的记录。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:130006
msgid ""
"可以从上一个日志文件恢复搜索。在函数 `run_tuning` 中创建任务调度器时，只需要添加新的参数 `load_log_file`，即 "
"`tuner = auto_scheduler.TaskScheduler(tasks, task_weights, "
"load_log_file=log_file)`。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autoscheduler/tune_network_x86.ipynb:130007
msgid ""
"如果有多个目标 CPU，您可以将它们都用于测量，从而使测量并行化。请查阅[如何使用 RPC 跟踪器和 RPC 服务器](tutorials-"
"autotvm-scale-up-rpc-tracker)。要在自动调度器中使用 RPC 跟踪器，请将 `TuningOptions` "
"中的运行器替换为 {any}`auto_scheduler.RPCRunner`。"
msgstr ""

#~ msgid ""
#~ ":download:`Download Python source code: "
#~ "tune_network_x86.py <tune_network_x86.py>`"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Jupyter notebook: "
#~ "tune_network_x86.ipynb <tune_network_x86.ipynb>`"
#~ msgstr ""

#~ msgid ""
#~ "`Gallery generated by Sphinx-Gallery "
#~ "<https://sphinx-gallery.github.io>`_"
#~ msgstr ""

#~ msgid ""
#~ "Click :ref:`here "
#~ "<sphx_glr_download_how_to_tune_with_autoscheduler_tune_network_x86.py>`"
#~ " to download the full example code"
#~ msgstr ""

#~ msgid ""
#~ "**Author**: `Lianmin Zheng "
#~ "<https://github.com/merrymercy>`_,             `Chengfan "
#~ "Jia <https://github.com/jcf94/>`_"
#~ msgstr ""

#~ msgid ""
#~ "To auto-tune a neural network, we"
#~ " partition the network into small "
#~ "subgraphs and tune them independently. "
#~ "Each subgraph is treated as one "
#~ "search task. A task scheduler slices "
#~ "the time and dynamically allocates time"
#~ " resources to these tasks. The task"
#~ " scheduler predicts the impact of "
#~ "each task on the end-to-end "
#~ "execution time and prioritizes the one"
#~ " that can reduce the execution time"
#~ " the most."
#~ msgstr ""

#~ msgid ""
#~ "Different from the template-based "
#~ ":ref:`autotvm <tutorials-autotvm-sec>` which"
#~ " relies on manual templates to define"
#~ " the search space, the auto-scheduler"
#~ " does not require any schedule "
#~ "templates. In other words, the auto-"
#~ "scheduler only uses the compute "
#~ "declarations in :code:`tvm/python/topi` and "
#~ "does not use existing schedule "
#~ "templates."
#~ msgstr ""

#~ msgid ""
#~ "First, we need to define the "
#~ "network with relay frontend API. We "
#~ "can load some pre-defined network "
#~ "from :code:`tvm.relay.testing`. We can also"
#~ " load models from MXNet, ONNX, "
#~ "PyTorch, and TensorFlow (see :ref:`front "
#~ "end tutorials<tutorial-frontend>`)."
#~ msgstr ""

#~ msgid ""
#~ "For convolutional neural networks, although"
#~ " auto-scheduler can work correctly "
#~ "with any layout, we found the best"
#~ " performance is typically achieved with "
#~ "NHWC layout. We also implemented more"
#~ " optimizations for NHWC layout with "
#~ "the auto-scheduler. So it is "
#~ "recommended to convert your models to"
#~ " NHWC layout to use the auto-"
#~ "scheduler. You can use :ref:`ConvertLayout "
#~ "<convert-layout-usage>` pass to do "
#~ "the layout conversion in TVM."
#~ msgstr ""

#~ msgid "Explain the printed information during tuning"
#~ msgstr ""

#~ msgid ""
#~ "During the tuning, a lot of "
#~ "information will be printed on the "
#~ "console. They are used for debugging "
#~ "purposes. The most important info is "
#~ "the output of the task scheduler. "
#~ "The following table is a sample "
#~ "output."
#~ msgstr ""

#~ msgid ""
#~ "This table lists the latency and "
#~ "(estimated) speed of all tasks. It "
#~ "also lists the allocation of measurement"
#~ " trials for all tasks. The last "
#~ "line prints the total weighted latency"
#~ " of these tasks, which can be a"
#~ " rough estimation of the end-to-"
#~ "end execution time of the network. "
#~ "The last line also prints the "
#~ "total number of measurement trials, "
#~ "total time spent on auto-tuning "
#~ "and the id of the next task "
#~ "to tune."
#~ msgstr ""

#~ msgid ""
#~ "There will also be some \"tvm::Error\"s"
#~ " errors, because the auto-scheduler "
#~ "will try some invalid schedules. You "
#~ "can safely ignore them if the "
#~ "tuning can continue, because these "
#~ "errors are isolated from the main "
#~ "process."
#~ msgstr ""

#~ msgid "Terminate the tuning earlier"
#~ msgstr ""

#~ msgid ""
#~ "You can terminate the tuning earlier "
#~ "by forcibly killing this process. As "
#~ "long as you get at least one "
#~ "valid schedule for each task in "
#~ "the log file, you should be able"
#~ " to do the compilation (the secion"
#~ " below)."
#~ msgstr ""

#~ msgid ""
#~ "If you have multiple target CPUs, "
#~ "you can use all of them for "
#~ "measurements to parallelize the measurements."
#~ " Check this :ref:`section <tutorials-"
#~ "autotvm-scale-up-rpc-tracker>` to "
#~ "learn how to use the RPC Tracker"
#~ " and RPC Server. To use the RPC"
#~ " Tracker in auto-scheduler, replace "
#~ "the runner in :code:`TuningOptions` with "
#~ ":any:`auto_scheduler.RPCRunner`."
#~ msgstr ""

#~ msgid "Auto-scheduling a Neural Network for x86 CPU"
#~ msgstr ""

#~ msgid ""
#~ "**Author**: [Lianmin "
#~ "Zheng](https://github.com/merrymercy),             [Chengfan "
#~ "Jia](https://github.com/jcf94/)"
#~ msgstr ""

#~ msgid ""
#~ "Auto-tuning for specific devices and "
#~ "workloads is critical for getting the"
#~ " best performance. This is a tutorial"
#~ " on how to tune a whole neural"
#~ " network for x86 CPU with the "
#~ "auto-scheduler."
#~ msgstr ""

#~ msgid ""
#~ "To auto-tune a neural network, we"
#~ " partition the network into small "
#~ "subgraphs and  tune them independently. "
#~ "Each subgraph is treated as one "
#~ "search task. A task scheduler slices "
#~ "the time and dynamically allocates time"
#~ " resources to these tasks. The task"
#~ " scheduler predicts the impact of "
#~ "each task on the end-to-end "
#~ "execution time and prioritizes the one"
#~ " that can reduce the execution time"
#~ " the most."
#~ msgstr ""

#~ msgid ""
#~ "For each subgraph, we use the "
#~ "compute declaration in :code:`tvm/python/topi` "
#~ "to get the computational DAG in "
#~ "the tensor expression form. We then "
#~ "use the auto-scheduler to construct "
#~ "a search space of this DAG and "
#~ "search for good schedules (low-level "
#~ "optimizations)."
#~ msgstr ""

#~ msgid ""
#~ "Different from the template-based "
#~ "`autotvm <tutorials-autotvm-sec>` which "
#~ "relies on manual templates to define "
#~ "the search space, the auto-scheduler "
#~ "does not require any schedule templates."
#~ " In other words, the auto-scheduler"
#~ " only uses the compute declarations "
#~ "in :code:`tvm/python/topi` and does not "
#~ "use existing schedule templates."
#~ msgstr ""

#~ msgid ""
#~ "Note that this tutorial will not "
#~ "run on Windows or recent versions "
#~ "of macOS. To get it to run, "
#~ "you will need to wrap the body "
#~ "of this tutorial in a :code:`if "
#~ "__name__ == \"__main__\":` block."
#~ msgstr ""

#~ msgid "Define a Network"
#~ msgstr ""

#~ msgid ""
#~ "First, we need to define the "
#~ "network with relay frontend API. We "
#~ "can load some pre-defined network "
#~ "from :code:`tvm.relay.testing`. We can also"
#~ " load models from MXNet, ONNX, "
#~ "PyTorch, and TensorFlow (see `front end"
#~ " tutorials<tutorial-frontend>`)."
#~ msgstr ""

#~ msgid ""
#~ "For convolutional neural networks, although"
#~ " auto-scheduler can work correctly "
#~ "with any layout, we found the best"
#~ " performance is typically achieved with "
#~ "NHWC layout. We also implemented more"
#~ " optimizations for NHWC layout with "
#~ "the auto-scheduler. So it is "
#~ "recommended to convert your models to"
#~ " NHWC layout to use the auto-"
#~ "scheduler. You can use `ConvertLayout "
#~ "<convert-layout-usage>` pass to do "
#~ "the layout conversion in TVM."
#~ msgstr ""

#~ msgid "Extract Search Tasks"
#~ msgstr ""

#~ msgid ""
#~ "Next, we extract the search tasks "
#~ "and their weights from a network. "
#~ "The weight of a task is the "
#~ "number of appearances of the task's "
#~ "subgraph in the whole network. By "
#~ "using the weight, we can approximate "
#~ "the end-to-end latency of the "
#~ "network as :code:`sum(latency[t] * "
#~ "weight[t])`, where :code:`latency[t]` is the"
#~ " latency of a task and "
#~ ":code:`weight[t]` is the weight of the"
#~ " task. The task scheduler will just"
#~ " optimize this objective."
#~ msgstr ""

#~ msgid "Begin Tuning"
#~ msgstr ""

#~ msgid "Now, we set some options for tuning and launch the search tasks"
#~ msgstr ""

#~ msgid ""
#~ ":code:`num_measure_trials` is the number of"
#~ " measurement trials we can use during"
#~ " the tuning. You can set it to"
#~ " a small number (e.g., 200) for "
#~ "a fast demonstrative run. In practice,"
#~ " we recommend setting it around "
#~ ":code:`800 * len(tasks)`, which is "
#~ "typically enough for the search to "
#~ "converge. For example, there are 29 "
#~ "tasks in resnet-50, so we can set"
#~ " it as 20000. You can adjust "
#~ "this parameter according to your time"
#~ " budget."
#~ msgstr ""

#~ msgid ""
#~ "In addition, we use :code:`RecordToFile` "
#~ "to dump measurement records into a "
#~ "log file, The measurement records can"
#~ " be used to query the history "
#~ "best, resume the search, and do "
#~ "more analyses later."
#~ msgstr ""

#~ msgid ""
#~ "see :any:`auto_scheduler.TuningOptions`, "
#~ ":any:`auto_scheduler.LocalRunner` for more "
#~ "parameters."
#~ msgstr ""

#~ msgid ""
#~ "During the tuning, a lot of "
#~ "information will be printed on the "
#~ "console.   They are used for debugging"
#~ " purposes. The most important info is"
#~ " the output   of the task scheduler."
#~ " The following table is a sample "
#~ "output."
#~ msgstr ""

#~ msgid ""
#~ "This table lists the latency and "
#~ "(estimated) speed of all tasks.   It "
#~ "also lists the allocation of measurement"
#~ " trials for all tasks.   The last "
#~ "line prints the total weighted latency"
#~ " of these tasks,   which can be "
#~ "a rough estimation of the end-"
#~ "to-end execution time   of the "
#~ "network.   The last line also prints "
#~ "the total number of measurement trials,"
#~ "   total time spent on auto-tuning"
#~ " and the id of the next task"
#~ " to tune."
#~ msgstr ""

#~ msgid ""
#~ "There will also be some \"tvm::Error\"s"
#~ " errors, because the   auto-scheduler "
#~ "will try some invalid schedules.   You"
#~ " can safely ignore them if the "
#~ "tuning can continue, because these   "
#~ "errors are isolated from the main "
#~ "process.</p></div>"
#~ msgstr ""

#~ msgid ""
#~ "You can terminate the tuning earlier "
#~ "by forcibly killing this process.   As"
#~ " long as you get at least one"
#~ " valid schedule for each task in "
#~ "the log file,   you should be able"
#~ " to do the compilation (the secion"
#~ " below).</p></div>"
#~ msgstr ""

#~ msgid "Compile and Evaluate"
#~ msgstr ""

#~ msgid ""
#~ "After auto-tuning, we can compile "
#~ "the network with the best schedules "
#~ "we found. All measurement records are"
#~ " dumped into the log file during "
#~ "auto-tuning, so we can read the"
#~ " log file and load the best "
#~ "schedules."
#~ msgstr ""

#~ msgid "Other Tips"
#~ msgstr ""

#~ msgid ""
#~ "During the tuning, the auto-scheduler"
#~ " needs to compile many programs and"
#~ " extract feature from them. This part"
#~ " is CPU-intensive, so a high-"
#~ "performance CPU with many cores is "
#~ "recommended for faster search."
#~ msgstr ""

#~ msgid ""
#~ "You can use :code:`python3 -m "
#~ "tvm.auto_scheduler.measure_record --mode distill -i"
#~ " log.json` to distill the large log"
#~ " file and only save the best "
#~ "useful records."
#~ msgstr ""

#~ msgid ""
#~ "You can resume a search from the"
#~ " previous log file. You just need "
#~ "to add a new argument "
#~ ":code:`load_log_file` when creating the task"
#~ " scheduler in function :code:`run_tuning`. "
#~ "Say, :code:`tuner = "
#~ "auto_scheduler.TaskScheduler(tasks, task_weights, "
#~ "load_log_file=log_file)`"
#~ msgstr ""

#~ msgid ""
#~ "If you have multiple target CPUs, "
#~ "you can use all of them for "
#~ "measurements to parallelize the measurements."
#~ " Check this `section <tutorials-autotvm-"
#~ "scale-up-rpc-tracker>` to learn "
#~ "how to use the RPC Tracker and "
#~ "RPC Server. To use the RPC Tracker"
#~ " in auto-scheduler, replace the "
#~ "runner in :code:`TuningOptions` with "
#~ ":any:`auto_scheduler.RPCRunner`."
#~ msgstr ""

