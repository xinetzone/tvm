# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-05-05 16:40+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20002
msgid "在 NVIDIA GPU 上调优 CNN"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20003
msgid ""
"**原作者**: [Lianmin Zheng](https://github.com/merrymercy), [Eddie "
"Yan](https://github.com/eqy/)"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20005
msgid "针对特定设备和工作负载的自动调优对于获得最佳性能至关重要。下面是关于如何用自动调度器调优 NVIDIA GPU 的整个卷积网络的教程。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20007
msgid "为了自动调优神经网络，需要将网络划分为小的子图，并独立地调优它们。每个子图被视为一个搜索任务。任务调度程序对时间进行切片，并动态地为这些任务分配时间资源。任务调度器预测每个任务对端到端执行时间的影响，并优先考虑能够最大程度减少执行时间的任务。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20009
msgid ""
"对于每个子图，使用 `tvm/python/topi` 中的 compute 声明来获得张量表达式形式的计算 DAG。然后，使用自动调度器来构造 "
"DAG 的搜索空间，并搜索良好的调度（低级优化）。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20011
msgid ""
"与基于模板的 [autotvm](../tune_with_autotvm/index) "
"依赖手动模板定义搜索空间不同，自动调度程序不需要任何调度模板。换句话说，自动调度器只使用 `tvm/python/topi` 中的 "
"compute，而不使用现有的调度模板。"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20013
msgid ""
"Auto-tuning for specific devices and workloads is critical for getting "
"the best performance. This is a tutorial on how to tune a whole "
"convolutional network for NVIDIA GPU."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20017
msgid ""
"The operator implementation for NVIDIA GPU in TVM is written in template "
"form. The template has many tunable knobs (tile factor, unrolling, etc). "
"We will tune all convolution and depthwise convolution operators in the "
"neural network. After tuning, we produce a log file which stores the best"
" knob values for all required operators. When the TVM compiler compiles "
"these operators, it will query this log file to get the best knob values."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20024
msgid ""
"We also released pre-tuned parameters for some NVIDIA GPUs. You can go to"
" [NVIDIA GPU Benchmark](https://github.com/apache/tvm/wiki/Benchmark"
"#nvidia-gpu) to see the results."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:20028
msgid ""
"Note that this tutorial will not run on Windows or recent versions of "
"macOS. To get it to run, you will need to wrap the body of this tutorial "
"in a :code:`if __name__ == \"__main__\":` block."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:30002
msgid "Install dependencies"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:30003
msgid ""
"To use the autotvm package in tvm, we need to install some extra "
"dependencies. (change \"3\" to \"2\" if you use python2):"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:30009
msgid ""
"To make TVM run faster during tuning, it is recommended to use cython as "
"FFI of tvm. In the root directory of tvm, execute:"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:30016
msgid "Now return to python code. Import packages."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:50002
msgid "Define Network"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:50003
msgid ""
"First we need to define the network in relay frontend API. We can load "
"some pre-defined network from :code:`tvm.relay.testing`. We can also load"
" models from MXNet, ONNX and TensorFlow."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:70002
msgid "Set Tuning Options"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:70003
msgid "Before tuning, we apply some configurations."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:90004
msgid "In general, the default value provided here works well."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:90006
msgid ""
"If you have large time budget, you can set :code:`n_trial`, "
":code:`early_stopping` larger,   which makes the tuning runs longer."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:90009
msgid ""
"If you have multiple devices, you can use all of them for measurement to"
"   accelerate the tuning process. (see the 'Scale up measurement` section"
" below).</p></div>"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:100002
msgid "Begin Tuning"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:100003
msgid ""
"Now we can extract tuning tasks from the network and begin tuning. Here, "
"we provide a simple utility function to tune a list of tasks. This "
"function is just an initial implementation which tunes them in sequential"
" order. We will introduce a more sophisticated tuning scheduler in the "
"future."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:120002
msgid "Finally, we launch tuning jobs and evaluate the end-to-end performance."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:140002
msgid "Sample Output"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:140003
msgid ""
"The tuning needs to compile many programs and extract feature from them. "
"So a high performance CPU is recommended. One sample output is listed "
"below. It takes about 4 hours to get the following output on a 32T AMD "
"Ryzen Threadripper. The tuning target is NVIDIA 1080 Ti. (You can see "
"some errors during compilation. If the tuning is not stuck, it is okay.)"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:140032
msgid ""
"As a reference baseline, the time cost of MXNet + TensorRT on resnet-18 "
"is 1.30ms. So we are a little faster."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:150004
msgid ""
"The auto tuning module is error-prone. If you always see \" 0.00/ 0.00 "
"GFLOPS\",   then there must be something wrong."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:150007
msgid ""
"First, make sure you set the correct configuration of your device.   "
"Then, you can print debug information by adding these lines in the "
"beginning   of the script. It will print every measurement result, where "
"you can find useful   error messages."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:150016
msgid ""
"Finally, always feel free to ask our community for help on "
"https://discuss.tvm.apache.org</p></div>"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:160003
msgid "Scale up measurement by using multiple devices"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:160004
msgid ""
"If you have multiple devices, you can use all of them for measurement. "
"TVM uses the RPC Tracker to manage distributed devices. The RPC Tracker "
"is a centralized controller node. We can register all devices to the "
"tracker. For example, if we have 10 GPU cards, we can register all of "
"them to the tracker, and run 10 measurements in parallel, accelerating "
"the tuning process."
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:160010
msgid ""
"To start an RPC tracker, run this command on the host machine. The "
"tracker is required during the whole tuning process, so we need to open a"
" new terminal for this command:"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:160017
msgid "The expected output is"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:160022
msgid ""
"Then open another new terminal for the RPC server. We need to start one "
"dedicated server for each device. We use a string key to distinguish the "
"types of devices. You can pick a name you like. (Note: For rocm backend, "
"there are some internal errors with the compiler, we need to add `--no-"
"fork` to the argument list.)"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:160031
msgid "After registering devices, we can confirm it by querying rpc_tracker"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:160036
msgid ""
"For example, if we have four 1080ti, two titanx and one gfx900, the "
"output can be"
msgstr ""

#: ../../xin/docs/how_to/tune_with_autotvm/tune_relay_cuda.ipynb:160048
msgid ""
"Finally, we need to change the tuning option to use RPCRunner. Use the "
"code below to replace the corresponding part above."
msgstr ""

#~ msgid ""
#~ ":download:`Download Python source code: "
#~ "tune_relay_cuda.py <tune_relay_cuda.py>`"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Jupyter notebook: "
#~ "tune_relay_cuda.ipynb <tune_relay_cuda.ipynb>`"
#~ msgstr ""

#~ msgid ""
#~ "`Gallery generated by Sphinx-Gallery "
#~ "<https://sphinx-gallery.github.io>`_"
#~ msgstr ""

#~ msgid ""
#~ "Click :ref:`here "
#~ "<sphx_glr_download_how_to_tune_with_autotvm_tune_relay_cuda.py>` "
#~ "to download the full example code"
#~ msgstr ""

#~ msgid "Auto-tuning a Convolutional Network for NVIDIA GPU"
#~ msgstr ""

#~ msgid ""
#~ "**Author**: `Lianmin Zheng "
#~ "<https://github.com/merrymercy>`_, `Eddie Yan "
#~ "<https://github.com/eqy/>`_"
#~ msgstr ""

#~ msgid ""
#~ "We also released pre-tuned parameters"
#~ " for some NVIDIA GPUs. You can "
#~ "go to `NVIDIA GPU Benchmark "
#~ "<https://github.com/apache/tvm/wiki/Benchmark#nvidia-gpu>`_ "
#~ "to see the results."
#~ msgstr ""

#~ msgid "How to set tuning options"
#~ msgstr ""

#~ msgid ""
#~ "If you have large time budget, you"
#~ " can set :code:`n_trial`, :code:`early_stopping`"
#~ " larger, which makes the tuning runs"
#~ " longer."
#~ msgstr ""

#~ msgid ""
#~ "If you have multiple devices, you "
#~ "can use all of them for "
#~ "measurement to accelerate the tuning "
#~ "process. (see the 'Scale up measurement`"
#~ " section below)."
#~ msgstr ""

#~ msgid "**Experiencing Difficulties?**"
#~ msgstr ""

#~ msgid ""
#~ "The auto tuning module is error-"
#~ "prone. If you always see \" 0.00/"
#~ " 0.00 GFLOPS\", then there must be"
#~ " something wrong."
#~ msgstr ""

#~ msgid ""
#~ "First, make sure you set the "
#~ "correct configuration of your device. "
#~ "Then, you can print debug information"
#~ " by adding these lines in the "
#~ "beginning of the script. It will "
#~ "print every measurement result, where "
#~ "you can find useful error messages."
#~ msgstr ""

#~ msgid ""
#~ "Finally, always feel free to ask "
#~ "our community for help on "
#~ "https://discuss.tvm.apache.org"
#~ msgstr ""

