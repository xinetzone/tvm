# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-06-25 10:20+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20002
msgid "部署在 CPU 上进行了修剪模型的 Hugging Face"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20004
msgid "**Author**: [Josh Fromm](https://github.com/jwfromm)"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20006
msgid ""
"本教程演示了如何使用任何剪枝模型，在这种情况下，从 [Hugging Face 的 "
"PruneBert](https://huggingface.co/huggingface/prunebert-base-"
"uncased-6-finepruned-w-distil-squad)，并使用 TVM "
"利用模型的稀疏性支持来产生实际的加速。虽然本教程的主要目的是实现对已经剪枝（Pruning）的模型的加速，但它也可能有助于估计模型剪枝后的速度。本文还提供了一个函数，该函数接受未剪枝的模型，并在指定的稀疏度下用随机和剪枝的权重替换其权重。当试图决定模型是否值得剪枝时，这可能是有用的功能。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20008
msgid "在我们开始代码之前，先讨论稀疏性和剪枝，并深入研究两种不同类型的稀疏性：结构化（structured）和非结构化（unstructured），这很有用。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20010
msgid ""
"剪枝是一种主要用于通过将权重值替换为 0 来减少模型参数大小的技术。虽然有很多方法可以选择哪些权重应该设置为 "
"0，但最直接的方法是选择最小的权重。通常，权重被修剪到所需的稀疏度百分比。例如，$95\\%$ 稀疏的模型只有 $5\\%$ "
"的权重非零。修剪到非常高的稀疏度通常需要微调或完全重新训练，因为它往往是有损的近似。尽管通过简单的压缩可以很容易地从剪枝后的模型中获得参数大小的优势，但利用稀疏性来产生运行时加速则更加复杂。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20012
msgid ""
"在结构化稀疏性中，权重被修剪以将修剪的权重聚集在一起。换句话说，它们是使用它们的值和位置进行修剪的。聚集修剪权重的好处是允许像矩阵乘法这样的算法跳过整个块。事实证明，在大多数当前可用的硬件上实现显著加速非常重要的某种程度的块稀疏性。这是因为在大多数"
" CPU 或 GPU 中加载内存时，跳过一次读取单个值并不能节省任何工作，而是需要读取整个块或瓦片并使用像向量化指令之类的东西执行。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20014
msgid "非结构化稀疏权重是仅基于原始权重值进行修剪的权重。它们可能看起来随机分散在张量中，而不像块稀疏权重那样集中在块中。在低稀疏度下，非结构化剪枝技术很难加速。然而，在高稀疏度下，许多全零值的块自然会出现，从而可能加速。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20016
msgid ""
"本教程涉及结构化稀疏性和非结构化稀疏性。Hugging Face 的 PruneBert 模型是非结构化的，但稀疏度为 "
"$95\\%$，因此即使不是最优，也可以将 TVM 的块稀疏优化应用于它。在为未修剪的模型生成随机稀疏权重时，我们使用结构稀疏性。有趣的练习是比较"
" PruneBert 的真实速度和使用虚假权重的块稀疏速度，以看到结构稀疏性的好处。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:30002
msgid "加载所需模块"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:30004
msgid "需要除了 TVM 之外的其他软件，包括 scipy、最新的 transformers 和 tensorflow 2.2+。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:50002
msgid "配置设置"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:50005
msgid "让我们从定义一些参数开始，以定义要运行的模型和稀疏性类型。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:70002
msgid "下载并转换 Transformers 模型"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:70004
msgid ""
"现在，将从 transformers 模块中获取模型，下载它，将其转换为 TensorFlow 的 graphdef 格式，为将该 "
"graphdef 转换为 relay graph 做准备，以便可以对其进行优化和部署。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:90002
msgid "转换为 Relay Graph"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:90004
msgid ""
"现在，已经拥有了将 transformers 模型转换为 relay 格式的所有工具。在下面的函数中，将导入的 graph 保存在 relay 的"
" JSON 格式中，以便每次运行此脚本时无需从 tensorflow 重新导入。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:110002
msgid "运行 Dense Graph"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:110004
msgid "让我们运行导入模型的默认版本。请注意，即使权重是稀疏的，我们也不会看到任何加速，因为我们在这些密集（但大多数是零）张量上使用的是常规的密集矩阵乘法，而不是稀疏感知的内核。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:130002
msgid "运行 Sparse Graph"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:130004
msgid "接下来，我们将把图形转换为稀疏表示，并在需要时生成虚拟稀疏权重。然后，我们将使用与密集矩阵相同的基准测试脚本来查看我们的速度提升情况！我们对图形应用了几个中继通行证来利用稀疏性。首先，我们使用`simplify_fc_transpose`将密集层的权重转置为参数。这使得更容易将其转换为矩阵乘法的稀疏版本。接下来，我们应用`bsr_dense.convert`来识别所有可以稀疏的权重矩阵，并自动替换它们。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:130006
msgid "下面的`bsr_dense.convert`调用正在做重活，通过检查模型中的权重是否至少稀疏`sparsity_threshold`百分之几来确定哪些权重可以被稀疏化。如果是，则将这些权重转换为*块压缩行格式(BSR)*。BSR实质上是一种将张量中的非零块索引化的表示形式，这使得算法可以轻松加载那些非零块并忽略张量的其余部分。一旦稀疏权重以BSR格式存在，就会应用`relay.transform.DenseToSparse`来实际替换`relay.dense`操作，使用`relay.sparse_dense`调用以实现更快的运行速度。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:150002
msgid "运行全部代码"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:150005
msgid "现在，我们只需调用所有必要的函数，根据设置的参数对模型进行基准测试。请注意，要运行此代码，您需要先取消注释最后一行。"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:170002
msgid "输出样例"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:170004
msgid "供参考，以下是在 AMD CPU 上运行脚本时的输出，显示使用稀疏性可以加速约 2.5 倍。"
msgstr ""

#~ msgid ""
#~ ":download:`Download Python source code: "
#~ "deploy_sparse.py <deploy_sparse.py>`"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Jupyter notebook: "
#~ "deploy_sparse.ipynb <deploy_sparse.ipynb>`"
#~ msgstr ""

#~ msgid ""
#~ "`Gallery generated by Sphinx-Gallery "
#~ "<https://sphinx-gallery.github.io>`_"
#~ msgstr ""

#~ msgid ""
#~ "Click :ref:`here "
#~ "<sphx_glr_download_how_to_deploy_models_deploy_sparse.py>` to"
#~ " download the full example code"
#~ msgstr ""

#~ msgid "**Author**: `Josh Fromm <https://github.com/jwfromm>`_"
#~ msgstr ""

#~ msgid ""
#~ "This tutorial demonstrates how to take"
#~ " any pruned model, in this case "
#~ "`PruneBert from Hugging Face "
#~ "<https://huggingface.co/huggingface/prunebert-base-"
#~ "uncased-6-finepruned-w-distil-squad>`_, and use "
#~ "TVM to leverage the model's sparsity "
#~ "support to produce real speedups. "
#~ "Although the primary purpose of this "
#~ "tutorial is to realize speedups on "
#~ "already pruned models, it may also "
#~ "be useful to estimate how fast a"
#~ " model would be *if* it were "
#~ "pruned. To this end, we also "
#~ "provide a function that takes an "
#~ "unpruned model and replaces its weights"
#~ " with random and pruned weights at"
#~ " a specified sparsity. This may be"
#~ " a useful feature when trying to "
#~ "decide if a model is worth pruning"
#~ " or not."
#~ msgstr ""

#~ msgid ""
#~ "Pruning is a technique primarily used"
#~ " to reduce the parameter size of "
#~ "a model by replacing weight values "
#~ "with 0s. Although many methods exist "
#~ "for choosing which weights should be "
#~ "set to 0, the most straight "
#~ "forward is by picking the weights "
#~ "with the smallest value. Typically, "
#~ "weights are pruned to a desired "
#~ "sparsity percentage. For example, a 95%"
#~ " sparse model would have only 5% "
#~ "of its weights non-zero. Pruning "
#~ "to very high sparsities often requires"
#~ " finetuning or full retraining as it"
#~ " tends to be a lossy approximation."
#~ " Although parameter size benefits are "
#~ "quite easy to obtain from a pruned"
#~ " model through simple compression, "
#~ "leveraging sparsity to yield runtime "
#~ "speedups is more complicated."
#~ msgstr ""

#~ msgid "Deploy a Hugging Face Pruned Model on CPU"
#~ msgstr ""

#~ msgid ""
#~ "This tutorial demonstrates how to take"
#~ " any pruned model, in this case "
#~ "[PruneBert from Hugging "
#~ "Face](https://huggingface.co/huggingface/prunebert-base-"
#~ "uncased-6-finepruned-w-distil-squad), and use "
#~ "TVM to leverage the model's sparsity "
#~ "support to produce real speedups. "
#~ "Although the primary purpose of this "
#~ "tutorial is to realize speedups on "
#~ "already pruned models, it may also "
#~ "be useful to estimate how fast a"
#~ " model would be *if* it were "
#~ "pruned. To this end, we also "
#~ "provide a function that takes an "
#~ "unpruned model and replaces its weights"
#~ " with random and pruned weights at"
#~ " a specified sparsity. This may be"
#~ " a useful feature when trying to "
#~ "decide if a model is worth pruning"
#~ " or not."
#~ msgstr ""

#~ msgid ""
#~ "Before we get into the code, it's"
#~ " useful to discuss sparsity and "
#~ "pruning and dig into the two "
#~ "different types of sparsity: **structured**"
#~ " and **unstructured**."
#~ msgstr ""

#~ msgid ""
#~ "Pruning is a technique primarily used"
#~ " to reduce the parameter size of "
#~ "a model by replacing weight values "
#~ "with 0s. Although many methods exist "
#~ "for choosing which weights should be "
#~ "set to 0, the most straight "
#~ "forward is by picking the weights "
#~ "with the smallest value. Typically, "
#~ "weights are pruned to a desired "
#~ "sparsity percentage. For example, a 95%"
#~ " sparse model would have only 5% "
#~ "of its weights non-zero. Pruning "
#~ "to very high sparsities often requires"
#~ " fine-tuning or full retraining as"
#~ " it tends to be a lossy "
#~ "approximation. Although parameter size "
#~ "benefits are quite easy to obtain "
#~ "from a pruned model through simple "
#~ "compression, leveraging sparsity to yield "
#~ "runtime speedups is more complicated."
#~ msgstr ""

#~ msgid ""
#~ "In structured sparsity weights are "
#~ "pruned with the goal of clustering "
#~ "pruned weights together. In other words,"
#~ " they are pruned using both their "
#~ "value and location. The benefit of "
#~ "bunching up pruned weights is that "
#~ "it allows an algorithm such as "
#~ "matrix multiplication to skip entire "
#~ "blocks. It turns out that some "
#~ "degree of *block sparsity* is very "
#~ "important to realizing significant speedups"
#~ " on most hardware available today. "
#~ "This is because when loading memory "
#~ "in most CPUs or GPUs, it doesn't"
#~ " save any work to skip reading "
#~ "a single value at a time, instead"
#~ " an entire chunk or tile is "
#~ "read in and executed using something "
#~ "like vectorized instructions."
#~ msgstr ""

#~ msgid ""
#~ "Unstructured sparse weights are those "
#~ "that are pruned only on the value"
#~ " of the original weights. They may"
#~ " appear to be scattered randomly "
#~ "throughout a tensor rather than in "
#~ "chunks like we'd see in block "
#~ "sparse weights. At low sparsities, "
#~ "unstructured pruning techniques are difficult"
#~ " to accelerate. However, at high "
#~ "sparsities many blocks of all 0 "
#~ "values will naturally appear, making it"
#~ " possible to accelerate."
#~ msgstr ""

#~ msgid ""
#~ "This tutorial interacts with both "
#~ "structured and unstructured sparsity. Hugging"
#~ " Face's PruneBert model is unstructured "
#~ "but 95% sparse, allowing us to "
#~ "apply TVM's block sparse optimizations "
#~ "to it, even if not optimally. When"
#~ " generating random sparse weights for "
#~ "an unpruned model, we do so with"
#~ " structured sparsity. A fun exercise "
#~ "is comparing the real speed of "
#~ "PruneBert with the block sparse speed"
#~ " using fake weights to see the "
#~ "benefit of structured sparsity."
#~ msgstr ""

#~ msgid "Load Required Modules"
#~ msgstr ""

#~ msgid ""
#~ "Other than TVM, scipy, the latest "
#~ "transformers, and tensorflow 2.2+ are "
#~ "required."
#~ msgstr ""

#~ msgid "Configure Settings"
#~ msgstr ""

#~ msgid ""
#~ "Let's start by defining some parameters"
#~ " that define the type of model "
#~ "and sparsity to run."
#~ msgstr ""

#~ msgid "Download and Convert Transformers Model"
#~ msgstr ""

#~ msgid ""
#~ "Now we'll grab a model from the"
#~ " transformers module, download it, convert"
#~ " it into a TensorFlow graphdef in "
#~ "preperation for converting that graphdef "
#~ "into a relay graph that we can "
#~ "optimize and deploy."
#~ msgstr ""

#~ msgid "Convert to Relay Graph"
#~ msgstr ""

#~ msgid ""
#~ "We now have all the tooling to "
#~ "get a transformers model in the "
#~ "right format for relay conversion. Let's"
#~ " import it! In the following function"
#~ " we save the imported graph in "
#~ "relay's json format so that we "
#~ "dont have to reimport from tensorflow"
#~ " each time this script is run."
#~ msgstr ""

#~ msgid "Run the Dense Graph"
#~ msgstr ""

#~ msgid ""
#~ "Let's run the default version of "
#~ "the imported model. Note that even "
#~ "if the weights are sparse, we "
#~ "won't see any speedup because we "
#~ "are using regular dense matrix "
#~ "multiplications on these dense (but "
#~ "mostly zero) tensors instead of sparse"
#~ " aware kernels."
#~ msgstr ""

#~ msgid "Run the Sparse Graph"
#~ msgstr ""

#~ msgid ""
#~ "Next we'll convert the graph into "
#~ "a sparse representation and generate "
#~ "fake sparse weights if needed. Then "
#~ "we'll use the same benchmarking script"
#~ " as dense to see how much "
#~ "faster we go! We apply a few "
#~ "relay passes to the graph to get"
#~ " it leveraging sparsity. First we use"
#~ " `simplify_fc_transpose` to use transposes "
#~ "on the weights of dense layers "
#~ "into the parameters. This makes it "
#~ "easier to convert to matrix multiplies"
#~ " to sparse versions. Next we apply"
#~ " `bsr_dense.convert` to identify all weight"
#~ " matrices that can be sparse, and "
#~ "automatically replace them."
#~ msgstr ""

#~ msgid ""
#~ "The `bsr_dense.convert` call below is "
#~ "doing the heavy lifting of identifying"
#~ " which weights in the model can "
#~ "be made sparse by checking if they"
#~ " are at least `sparsity_threshold` percent"
#~ " sparse. If so, it converts those "
#~ "weights into *Block Compressed Row "
#~ "Format (BSR)*. BSR is essentially a "
#~ "representation that indexes into the "
#~ "nonzero chunks of the tensor, making "
#~ "it easy for an algorithm to load"
#~ " those non-zero chunks and ignore "
#~ "the rest of the tensor. Once the"
#~ " sparse weights are in BSR format,"
#~ " `relay.transform.DenseToSparse` is applied to"
#~ " actually replace `relay.dense` operations "
#~ "with `relay.sparse_dense` calls that can "
#~ "be run faster."
#~ msgstr ""

#~ msgid "Run All the Code!"
#~ msgstr ""

#~ msgid ""
#~ "And that's it! Now we'll simply "
#~ "call all the needed function to "
#~ "benchmark the model according to the "
#~ "set parameters. Note that to run "
#~ "this code you'll need to uncomment "
#~ "the last line first."
#~ msgstr ""

#~ msgid "Sample Output"
#~ msgstr ""

#~ msgid ""
#~ "For reference, below is the output "
#~ "of the script when run on an "
#~ "AMD CPU and shows about a 2.5X "
#~ "speedup from using sparsity."
#~ msgstr ""

