# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-05 09:32+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/reference/api/python/relax/op.rst:19
#: ../../doc/docs/reference/api/python/relax/op.rst:22
msgid "tvm.relax.op"
msgstr ""

#: of tvm.relax.op:1
msgid "Relax core operators."
msgstr ""

#: of tvm.relax.op.unary.abs:1
msgid "Compute element-wise absolute value of the input data."
msgstr ""

#: of tvm.relax.op.base.assert_op:5 tvm.relax.op.base.call_builtin_with_ctx:4
#: tvm.relax.op.base.call_dps_packed:8 tvm.relax.op.base.call_inplace_packed:19
#: tvm.relax.op.base.call_pure_packed:13 tvm.relax.op.base.call_tir:4
#: tvm.relax.op.base.call_tir_inplace:14 tvm.relax.op.base.call_tir_with_grad:6
#: tvm.relax.op.base.hint_on_device:5 tvm.relax.op.base.invoke_closure:4
#: tvm.relax.op.base.invoke_pure_closure:10 tvm.relax.op.base.make_closure:4
#: tvm.relax.op.base.print:4 tvm.relax.op.base.register_gradient:4
#: tvm.relax.op.base.shape_of:4 tvm.relax.op.base.to_vdevice:6
#: tvm.relax.op.binary.add:4 tvm.relax.op.binary.divide:4
#: tvm.relax.op.binary.equal:4 tvm.relax.op.binary.floor_divide:4
#: tvm.relax.op.binary.greater:4 tvm.relax.op.binary.greater_equal:4
#: tvm.relax.op.binary.less:4 tvm.relax.op.binary.less_equal:4
#: tvm.relax.op.binary.maximum:4 tvm.relax.op.binary.minimum:4
#: tvm.relax.op.binary.multiply:4 tvm.relax.op.binary.not_equal:4
#: tvm.relax.op.binary.power:4 tvm.relax.op.binary.subtract:4
#: tvm.relax.op.builtin.builtin.alloc_tensor:4
#: tvm.relax.op.builtin.builtin.stop_lift_params:5
#: tvm.relax.op.ccl.ccl.allgather:4 tvm.relax.op.ccl.ccl.allreduce:4
#: tvm.relax.op.ccl.ccl.broadcast_from_worker0:4
#: tvm.relax.op.ccl.ccl.scatter_from_worker0:4 tvm.relax.op.create.arange:4
#: tvm.relax.op.create.full:4 tvm.relax.op.create.full_like:6
#: tvm.relax.op.create.ones:4 tvm.relax.op.create.ones_like:4
#: tvm.relax.op.create.tril:4 tvm.relax.op.create.triu:4
#: tvm.relax.op.create.zeros:4 tvm.relax.op.create.zeros_like:4
#: tvm.relax.op.datatype.astype:4
#: tvm.relax.op.distributed.distributed.annotate_sharding:4
#: tvm.relax.op.distributed.distributed.call_tir_local_view:6
#: tvm.relax.op.distributed.distributed.redistribute:4
#: tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:8
#: tvm.relax.op.grad.grad.avg_pool2d_backward:5
#: tvm.relax.op.grad.grad.end_checkpoint:4
#: tvm.relax.op.grad.grad.max_pool2d_backward:5
#: tvm.relax.op.grad.grad.nll_loss_backward:5 tvm.relax.op.grad.grad.no_grad:4
#: tvm.relax.op.grad.grad.start_checkpoint:25
#: tvm.relax.op.grad.grad.take_backward:5 tvm.relax.op.image.image.resize2d:12
#: tvm.relax.op.index.dynamic_strided_slice:4
#: tvm.relax.op.index.strided_slice:4 tvm.relax.op.index.take:8
#: tvm.relax.op.linear_algebra.einsum:4 tvm.relax.op.linear_algebra.linear:4
#: tvm.relax.op.linear_algebra.matmul:7 tvm.relax.op.manipulate.broadcast_to:4
#: tvm.relax.op.manipulate.collapse_sum_like:6
#: tvm.relax.op.manipulate.collapse_sum_to:15 tvm.relax.op.manipulate.concat:4
#: tvm.relax.op.manipulate.expand_dims:4 tvm.relax.op.manipulate.flatten:4
#: tvm.relax.op.manipulate.flip:4 tvm.relax.op.manipulate.layout_transform:4
#: tvm.relax.op.manipulate.permute_dims:4 tvm.relax.op.manipulate.repeat:4
#: tvm.relax.op.manipulate.reshape:14
#: tvm.relax.op.manipulate.scatter_elements:22 tvm.relax.op.manipulate.split:11
#: tvm.relax.op.manipulate.squeeze:4 tvm.relax.op.manipulate.tile:13
#: tvm.relax.op.memory.memory.alloc_storage:5
#: tvm.relax.op.memory.memory.alloc_tensor:4
#: tvm.relax.op.memory.memory.kill_storage:4
#: tvm.relax.op.memory.memory.kill_tensor:4
#: tvm.relax.op.memory.view.ensure_zero_offset:4
#: tvm.relax.op.memory.view.view:12 tvm.relax.op.nn.nn.adaptive_avg_pool1d:23
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:26
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:26 tvm.relax.op.nn.nn.attention:12
#: tvm.relax.op.nn.nn.attention_var_len:8 tvm.relax.op.nn.nn.avg_pool1d:15
#: tvm.relax.op.nn.nn.avg_pool2d:23 tvm.relax.op.nn.nn.avg_pool3d:16
#: tvm.relax.op.nn.nn.batch_norm:56 tvm.relax.op.nn.nn.conv1d:27
#: tvm.relax.op.nn.nn.conv1d_transpose:13 tvm.relax.op.nn.nn.conv2d:27
#: tvm.relax.op.nn.nn.conv2d_transpose:23 tvm.relax.op.nn.nn.conv3d:29
#: tvm.relax.op.nn.nn.cross_entropy_with_logits:12 tvm.relax.op.nn.nn.dropout:8
#: tvm.relax.op.nn.nn.gelu:9 tvm.relax.op.nn.nn.gelu_tanh:7
#: tvm.relax.op.nn.nn.group_norm:7 tvm.relax.op.nn.nn.layer_norm:20
#: tvm.relax.op.nn.nn.leakyrelu:7 tvm.relax.op.nn.nn.log_softmax:11
#: tvm.relax.op.nn.nn.max_pool1d:15 tvm.relax.op.nn.nn.max_pool2d:22
#: tvm.relax.op.nn.nn.max_pool3d:16 tvm.relax.op.nn.nn.nll_loss:11
#: tvm.relax.op.nn.nn.pad:7 tvm.relax.op.nn.nn.relu:7
#: tvm.relax.op.nn.nn.rms_norm:11 tvm.relax.op.nn.nn.silu:7
#: tvm.relax.op.nn.nn.softmax:6 tvm.relax.op.qdq.dequantize:8
#: tvm.relax.op.qdq.quantize:8
#: tvm.relax.op.sampling.multinomial_from_uniform:10
#: tvm.relax.op.search.argmax:4 tvm.relax.op.search.argmin:4
#: tvm.relax.op.search.where:8 tvm.relax.op.set.unique:8
#: tvm.relax.op.sorting.argsort:5 tvm.relax.op.sorting.sort:5
#: tvm.relax.op.sorting.topk:6 tvm.relax.op.statistical.cumprod:5
#: tvm.relax.op.statistical.cumsum:5 tvm.relax.op.statistical.max:4
#: tvm.relax.op.statistical.mean:4 tvm.relax.op.statistical.min:4
#: tvm.relax.op.statistical.prod:4 tvm.relax.op.statistical.std:4
#: tvm.relax.op.statistical.sum:4 tvm.relax.op.statistical.variance:4
#: tvm.relax.op.ternary.ewise_fma:5 tvm.relax.op.unary.abs:4
#: tvm.relax.op.unary.acos:4 tvm.relax.op.unary.acosh:4
#: tvm.relax.op.unary.asin:4 tvm.relax.op.unary.asinh:4
#: tvm.relax.op.unary.atan:4 tvm.relax.op.unary.atanh:4
#: tvm.relax.op.unary.bitwise_not:4 tvm.relax.op.unary.ceil:4
#: tvm.relax.op.unary.clip:4 tvm.relax.op.unary.cos:4 tvm.relax.op.unary.cosh:4
#: tvm.relax.op.unary.erf:4 tvm.relax.op.unary.exp:4 tvm.relax.op.unary.floor:4
#: tvm.relax.op.unary.isfinite:4 tvm.relax.op.unary.isinf:4
#: tvm.relax.op.unary.isnan:4 tvm.relax.op.unary.log:4
#: tvm.relax.op.unary.logical_not:4 tvm.relax.op.unary.negative:4
#: tvm.relax.op.unary.round:4 tvm.relax.op.unary.rsqrt:8
#: tvm.relax.op.unary.sigmoid:4 tvm.relax.op.unary.sign:4
#: tvm.relax.op.unary.sin:4 tvm.relax.op.unary.sinh:4 tvm.relax.op.unary.sqrt:4
#: tvm.relax.op.unary.square:4 tvm.relax.op.unary.tan:4
#: tvm.relax.op.unary.tanh:4
msgid "Parameters"
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:5 tvm.relax.op.ccl.ccl.allreduce:5
#: tvm.relax.op.ccl.ccl.broadcast_from_worker0:5
#: tvm.relax.op.ccl.ccl.scatter_from_worker0:5 tvm.relax.op.create.full_like:7
#: tvm.relax.op.create.ones_like:5 tvm.relax.op.create.tril:5
#: tvm.relax.op.create.triu:5 tvm.relax.op.create.zeros_like:5
#: tvm.relax.op.datatype.astype:5 tvm.relax.op.index.dynamic_strided_slice:5
#: tvm.relax.op.index.strided_slice:5 tvm.relax.op.index.take:9
#: tvm.relax.op.manipulate.broadcast_to:5 tvm.relax.op.manipulate.expand_dims:5
#: tvm.relax.op.manipulate.flatten:5 tvm.relax.op.manipulate.layout_transform:5
#: tvm.relax.op.manipulate.permute_dims:5 tvm.relax.op.manipulate.reshape:15
#: tvm.relax.op.manipulate.split:12 tvm.relax.op.manipulate.squeeze:5
#: tvm.relax.op.search.argmax:5 tvm.relax.op.search.argmin:5
#: tvm.relax.op.set.unique:9 tvm.relax.op.sorting.sort:6
#: tvm.relax.op.statistical.max:5 tvm.relax.op.statistical.mean:5
#: tvm.relax.op.statistical.min:5 tvm.relax.op.statistical.prod:5
#: tvm.relax.op.statistical.std:5 tvm.relax.op.statistical.sum:5
#: tvm.relax.op.statistical.variance:5 tvm.relax.op.unary.abs:5
#: tvm.relax.op.unary.acos:5 tvm.relax.op.unary.acosh:5
#: tvm.relax.op.unary.asin:5 tvm.relax.op.unary.asinh:5
#: tvm.relax.op.unary.atan:5 tvm.relax.op.unary.atanh:5
#: tvm.relax.op.unary.bitwise_not:5 tvm.relax.op.unary.ceil:5
#: tvm.relax.op.unary.clip:5 tvm.relax.op.unary.cos:5 tvm.relax.op.unary.cosh:5
#: tvm.relax.op.unary.erf:5 tvm.relax.op.unary.exp:5 tvm.relax.op.unary.floor:5
#: tvm.relax.op.unary.isfinite:5 tvm.relax.op.unary.isinf:5
#: tvm.relax.op.unary.isnan:5 tvm.relax.op.unary.log:5
#: tvm.relax.op.unary.logical_not:5 tvm.relax.op.unary.negative:5
#: tvm.relax.op.unary.round:5 tvm.relax.op.unary.rsqrt:9
#: tvm.relax.op.unary.sigmoid:5 tvm.relax.op.unary.sign:5
#: tvm.relax.op.unary.sin:5 tvm.relax.op.unary.sinh:5 tvm.relax.op.unary.sqrt:5
#: tvm.relax.op.unary.square:5 tvm.relax.op.unary.tan:5
#: tvm.relax.op.unary.tanh:5
msgid "x"
msgstr ""

#: of tvm.relax.op.binary.bitwise_and:-1 tvm.relax.op.binary.bitwise_or:-1
#: tvm.relax.op.binary.bitwise_xor:-1 tvm.relax.op.binary.divide:-1
#: tvm.relax.op.binary.equal:-1 tvm.relax.op.binary.floor_divide:-1
#: tvm.relax.op.binary.greater:-1 tvm.relax.op.binary.greater_equal:-1
#: tvm.relax.op.binary.less:-1 tvm.relax.op.binary.less_equal:-1
#: tvm.relax.op.binary.logical_and:-1 tvm.relax.op.binary.logical_or:-1
#: tvm.relax.op.binary.logical_xor:-1 tvm.relax.op.binary.maximum:-1
#: tvm.relax.op.binary.minimum:-1 tvm.relax.op.binary.not_equal:-1
#: tvm.relax.op.binary.power:-1 tvm.relax.op.binary.subtract:-1
#: tvm.relax.op.builtin.builtin.stop_lift_params:-1
#: tvm.relax.op.ccl.ccl.allgather:-1 tvm.relax.op.ccl.ccl.allreduce:-1
#: tvm.relax.op.ccl.ccl.broadcast_from_worker0:-1
#: tvm.relax.op.ccl.ccl.scatter_from_worker0:-1 tvm.relax.op.create.arange:-1
#: tvm.relax.op.create.full:-1 tvm.relax.op.create.full_like:-1
#: tvm.relax.op.create.ones:-1 tvm.relax.op.create.ones_like:-1
#: tvm.relax.op.create.tril:-1 tvm.relax.op.create.triu:-1
#: tvm.relax.op.create.zeros:-1 tvm.relax.op.create.zeros_like:-1
#: tvm.relax.op.datatype.astype:-1 tvm.relax.op.datatype.wrap_param:-1
#: tvm.relax.op.distributed.distributed.annotate_sharding:-1
#: tvm.relax.op.distributed.distributed.redistribute:-1
#: tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:-1
#: tvm.relax.op.grad.grad.avg_pool2d_backward:-1
#: tvm.relax.op.grad.grad.end_checkpoint:-1
#: tvm.relax.op.grad.grad.max_pool2d_backward:-1
#: tvm.relax.op.grad.grad.nll_loss_backward:-1
#: tvm.relax.op.grad.grad.no_grad:-1 tvm.relax.op.grad.grad.start_checkpoint:-1
#: tvm.relax.op.grad.grad.take_backward:-1 tvm.relax.op.image.image.resize2d:-1
#: tvm.relax.op.index.dynamic_strided_slice:-1
#: tvm.relax.op.index.strided_slice:-1 tvm.relax.op.index.take:-1
#: tvm.relax.op.linear_algebra.einsum:-1 tvm.relax.op.linear_algebra.linear:-1
#: tvm.relax.op.linear_algebra.matmul:-1
#: tvm.relax.op.manipulate.broadcast_to:-1
#: tvm.relax.op.manipulate.collapse_sum_like:-1
#: tvm.relax.op.manipulate.collapse_sum_to:-1
#: tvm.relax.op.manipulate.expand_dims:-1 tvm.relax.op.manipulate.flatten:-1
#: tvm.relax.op.manipulate.flip:-1 tvm.relax.op.manipulate.layout_transform:-1
#: tvm.relax.op.manipulate.permute_dims:-1 tvm.relax.op.manipulate.repeat:-1
#: tvm.relax.op.manipulate.reshape:-1
#: tvm.relax.op.manipulate.scatter_elements:-1 tvm.relax.op.manipulate.split:-1
#: tvm.relax.op.manipulate.squeeze:-1 tvm.relax.op.manipulate.tile:-1
#: tvm.relax.op.mask.masked_fill:-1
#: tvm.relax.op.memory.view.ensure_zero_offset:-1
#: tvm.relax.op.memory.view.view:-1 tvm.relax.op.nn.nn.adaptive_avg_pool1d:-1
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:-1
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:-1 tvm.relax.op.nn.nn.attention:-1
#: tvm.relax.op.nn.nn.attention_var_len:-1 tvm.relax.op.nn.nn.avg_pool1d:-1
#: tvm.relax.op.nn.nn.avg_pool2d:-1 tvm.relax.op.nn.nn.avg_pool3d:-1
#: tvm.relax.op.nn.nn.batch_norm:-1 tvm.relax.op.nn.nn.conv1d:-1
#: tvm.relax.op.nn.nn.conv1d_transpose:-1 tvm.relax.op.nn.nn.conv2d:-1
#: tvm.relax.op.nn.nn.conv2d_transpose:-1 tvm.relax.op.nn.nn.conv3d:-1
#: tvm.relax.op.nn.nn.cross_entropy_with_logits:-1
#: tvm.relax.op.nn.nn.dropout:-1 tvm.relax.op.nn.nn.gelu:-1
#: tvm.relax.op.nn.nn.gelu_tanh:-1 tvm.relax.op.nn.nn.group_norm:-1
#: tvm.relax.op.nn.nn.layer_norm:-1 tvm.relax.op.nn.nn.leakyrelu:-1
#: tvm.relax.op.nn.nn.log_softmax:-1 tvm.relax.op.nn.nn.max_pool1d:-1
#: tvm.relax.op.nn.nn.max_pool2d:-1 tvm.relax.op.nn.nn.max_pool3d:-1
#: tvm.relax.op.nn.nn.nll_loss:-1 tvm.relax.op.nn.nn.pad:-1
#: tvm.relax.op.nn.nn.relu:-1 tvm.relax.op.nn.nn.rms_norm:-1
#: tvm.relax.op.nn.nn.silu:-1 tvm.relax.op.nn.nn.softmax:-1
#: tvm.relax.op.sampling.multinomial_from_uniform:-1
#: tvm.relax.op.search.argmax:-1 tvm.relax.op.search.argmin:-1
#: tvm.relax.op.search.where:-1 tvm.relax.op.set.unique:-1
#: tvm.relax.op.sorting.argsort:-1 tvm.relax.op.sorting.sort:-1
#: tvm.relax.op.sorting.topk:-1 tvm.relax.op.statistical.cumprod:-1
#: tvm.relax.op.statistical.cumsum:-1 tvm.relax.op.statistical.max:-1
#: tvm.relax.op.statistical.mean:-1 tvm.relax.op.statistical.min:-1
#: tvm.relax.op.statistical.prod:-1 tvm.relax.op.statistical.std:-1
#: tvm.relax.op.statistical.sum:-1 tvm.relax.op.statistical.variance:-1
#: tvm.relax.op.ternary.ewise_fma:-1 tvm.relax.op.unary.abs:-1
#: tvm.relax.op.unary.acos:-1 tvm.relax.op.unary.acosh:-1
#: tvm.relax.op.unary.asin:-1 tvm.relax.op.unary.asinh:-1
#: tvm.relax.op.unary.atan:-1 tvm.relax.op.unary.atanh:-1
#: tvm.relax.op.unary.bitwise_not:-1 tvm.relax.op.unary.ceil:-1
#: tvm.relax.op.unary.clip:-1 tvm.relax.op.unary.cos:-1
#: tvm.relax.op.unary.cosh:-1 tvm.relax.op.unary.erf:-1
#: tvm.relax.op.unary.exp:-1 tvm.relax.op.unary.floor:-1
#: tvm.relax.op.unary.isfinite:-1 tvm.relax.op.unary.isinf:-1
#: tvm.relax.op.unary.isnan:-1 tvm.relax.op.unary.log:-1
#: tvm.relax.op.unary.logical_not:-1 tvm.relax.op.unary.negative:-1
#: tvm.relax.op.unary.round:-1 tvm.relax.op.unary.rsqrt:-1
#: tvm.relax.op.unary.sigmoid:-1 tvm.relax.op.unary.sign:-1
#: tvm.relax.op.unary.sin:-1 tvm.relax.op.unary.sinh:-1
#: tvm.relax.op.unary.sqrt:-1 tvm.relax.op.unary.square:-1
#: tvm.relax.op.unary.tan:-1 tvm.relax.op.unary.tanh:-1
msgid "relax.Expr"
msgstr ""

#: of tvm.relax.op.builtin.builtin.stop_lift_params:7
#: tvm.relax.op.nn.nn.gelu:11 tvm.relax.op.nn.nn.gelu_tanh:9
#: tvm.relax.op.nn.nn.leakyrelu:9 tvm.relax.op.nn.nn.relu:9
#: tvm.relax.op.nn.nn.silu:9 tvm.relax.op.unary.abs:6 tvm.relax.op.unary.acos:6
#: tvm.relax.op.unary.acosh:6 tvm.relax.op.unary.asin:6
#: tvm.relax.op.unary.asinh:6 tvm.relax.op.unary.atan:6
#: tvm.relax.op.unary.atanh:6 tvm.relax.op.unary.bitwise_not:6
#: tvm.relax.op.unary.ceil:6 tvm.relax.op.unary.clip:6 tvm.relax.op.unary.cos:6
#: tvm.relax.op.unary.cosh:6 tvm.relax.op.unary.erf:6 tvm.relax.op.unary.exp:6
#: tvm.relax.op.unary.floor:6 tvm.relax.op.unary.isfinite:6
#: tvm.relax.op.unary.isinf:6 tvm.relax.op.unary.isnan:6
#: tvm.relax.op.unary.log:6 tvm.relax.op.unary.logical_not:6
#: tvm.relax.op.unary.negative:6 tvm.relax.op.unary.round:6
#: tvm.relax.op.unary.rsqrt:10 tvm.relax.op.unary.sigmoid:6
#: tvm.relax.op.unary.sign:6 tvm.relax.op.unary.sin:6 tvm.relax.op.unary.sinh:6
#: tvm.relax.op.unary.sqrt:6 tvm.relax.op.unary.square:6
#: tvm.relax.op.unary.tan:6 tvm.relax.op.unary.tanh:6
msgid "The input data"
msgstr ""

#: of tvm.relax.op.base.assert_op:16 tvm.relax.op.base.call_builtin_with_ctx:15
#: tvm.relax.op.base.call_dps_packed:21
#: tvm.relax.op.base.call_inplace_packed:38
#: tvm.relax.op.base.call_pure_packed:24 tvm.relax.op.base.call_tir:20
#: tvm.relax.op.base.call_tir_inplace:39
#: tvm.relax.op.base.call_tir_with_grad:30 tvm.relax.op.base.hint_on_device:13
#: tvm.relax.op.base.invoke_closure:15 tvm.relax.op.base.invoke_pure_closure:21
#: tvm.relax.op.base.make_closure:13 tvm.relax.op.base.null_value:4
#: tvm.relax.op.base.print:12 tvm.relax.op.base.shape_of:9
#: tvm.relax.op.base.shape_to_tensor:7 tvm.relax.op.base.tensor_to_shape:7
#: tvm.relax.op.base.to_vdevice:14 tvm.relax.op.binary.add:11
#: tvm.relax.op.binary.bitwise_and:9 tvm.relax.op.binary.bitwise_or:9
#: tvm.relax.op.binary.bitwise_xor:9 tvm.relax.op.binary.divide:11
#: tvm.relax.op.binary.equal:11 tvm.relax.op.binary.floor_divide:11
#: tvm.relax.op.binary.greater:11 tvm.relax.op.binary.greater_equal:11
#: tvm.relax.op.binary.less:11 tvm.relax.op.binary.less_equal:11
#: tvm.relax.op.binary.logical_and:9 tvm.relax.op.binary.logical_or:9
#: tvm.relax.op.binary.logical_xor:9 tvm.relax.op.binary.maximum:11
#: tvm.relax.op.binary.minimum:11 tvm.relax.op.binary.multiply:11
#: tvm.relax.op.binary.not_equal:11 tvm.relax.op.binary.power:11
#: tvm.relax.op.binary.subtract:11 tvm.relax.op.builtin.builtin.alloc_tensor:19
#: tvm.relax.op.builtin.builtin.stop_lift_params:10
#: tvm.relax.op.ccl.ccl.allgather:15 tvm.relax.op.ccl.ccl.allreduce:16
#: tvm.relax.op.ccl.ccl.broadcast_from_worker0:9
#: tvm.relax.op.ccl.ccl.scatter_from_worker0:15 tvm.relax.op.create.arange:19
#: tvm.relax.op.create.full:16 tvm.relax.op.create.full_like:19
#: tvm.relax.op.create.ones:12 tvm.relax.op.create.ones_like:14
#: tvm.relax.op.create.tril:16 tvm.relax.op.create.triu:16
#: tvm.relax.op.create.zeros:12 tvm.relax.op.create.zeros_like:14
#: tvm.relax.op.datatype.astype:12 tvm.relax.op.datatype.wrap_param:10
#: tvm.relax.op.distributed.distributed.annotate_sharding:13
#: tvm.relax.op.distributed.distributed.call_tir_local_view:22
#: tvm.relax.op.distributed.distributed.redistribute:12
#: tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:19
#: tvm.relax.op.grad.grad.avg_pool2d_backward:10
#: tvm.relax.op.grad.grad.end_checkpoint:9
#: tvm.relax.op.grad.grad.max_pool2d_backward:10
#: tvm.relax.op.grad.grad.nll_loss_backward:10 tvm.relax.op.grad.grad.no_grad:9
#: tvm.relax.op.grad.grad.start_checkpoint:30
#: tvm.relax.op.grad.grad.take_backward:10 tvm.relax.op.image.image.resize2d:57
#: tvm.relax.op.index.dynamic_strided_slice:20
#: tvm.relax.op.index.strided_slice:26 tvm.relax.op.index.take:20
#: tvm.relax.op.linear_algebra.einsum:12 tvm.relax.op.linear_algebra.linear:24
#: tvm.relax.op.linear_algebra.matmul:19
#: tvm.relax.op.manipulate.broadcast_to:12
#: tvm.relax.op.manipulate.collapse_sum_like:14
#: tvm.relax.op.manipulate.collapse_sum_to:23 tvm.relax.op.manipulate.concat:14
#: tvm.relax.op.manipulate.expand_dims:14 tvm.relax.op.manipulate.flatten:9
#: tvm.relax.op.manipulate.flip:12 tvm.relax.op.manipulate.layout_transform:19
#: tvm.relax.op.manipulate.permute_dims:12 tvm.relax.op.manipulate.repeat:17
#: tvm.relax.op.manipulate.reshape:22
#: tvm.relax.op.manipulate.scatter_elements:40 tvm.relax.op.manipulate.split:22
#: tvm.relax.op.manipulate.squeeze:14 tvm.relax.op.manipulate.tile:21
#: tvm.relax.op.mask.masked_fill:11 tvm.relax.op.memory.memory.alloc_storage:20
#: tvm.relax.op.memory.memory.alloc_tensor:18
#: tvm.relax.op.memory.memory.kill_storage:9
#: tvm.relax.op.memory.memory.kill_tensor:9 tvm.relax.op.memory.view.view:34
#: tvm.relax.op.nn.nn.adaptive_avg_pool1d:39
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:42
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:42 tvm.relax.op.nn.nn.attention:75
#: tvm.relax.op.nn.nn.attention_var_len:82 tvm.relax.op.nn.nn.avg_pool1d:45
#: tvm.relax.op.nn.nn.avg_pool2d:53 tvm.relax.op.nn.nn.avg_pool3d:46
#: tvm.relax.op.nn.nn.batch_norm:88 tvm.relax.op.nn.nn.conv1d:62
#: tvm.relax.op.nn.nn.conv1d_transpose:51 tvm.relax.op.nn.nn.conv2d:62
#: tvm.relax.op.nn.nn.conv2d_transpose:61 tvm.relax.op.nn.nn.conv3d:64
#: tvm.relax.op.nn.nn.cross_entropy_with_logits:20
#: tvm.relax.op.nn.nn.dropout:16 tvm.relax.op.nn.nn.gelu:14
#: tvm.relax.op.nn.nn.gelu_tanh:12 tvm.relax.op.nn.nn.group_norm:36
#: tvm.relax.op.nn.nn.layer_norm:43 tvm.relax.op.nn.nn.leakyrelu:16
#: tvm.relax.op.nn.nn.log_softmax:21 tvm.relax.op.nn.nn.max_pool1d:45
#: tvm.relax.op.nn.nn.max_pool2d:52 tvm.relax.op.nn.nn.max_pool3d:46
#: tvm.relax.op.nn.nn.nll_loss:32 tvm.relax.op.nn.nn.pad:20
#: tvm.relax.op.nn.nn.relu:12 tvm.relax.op.nn.nn.rms_norm:28
#: tvm.relax.op.nn.nn.silu:12 tvm.relax.op.nn.nn.softmax:16
#: tvm.relax.op.qdq.dequantize:25 tvm.relax.op.qdq.quantize:25
#: tvm.relax.op.sampling.multinomial_from_uniform:33
#: tvm.relax.op.search.argmax:19 tvm.relax.op.search.argmin:19
#: tvm.relax.op.search.where:23 tvm.relax.op.set.unique:32
#: tvm.relax.op.sorting.argsort:19 tvm.relax.op.sorting.sort:17
#: tvm.relax.op.sorting.topk:30 tvm.relax.op.statistical.cumprod:22
#: tvm.relax.op.statistical.cumsum:22 tvm.relax.op.statistical.max:19
#: tvm.relax.op.statistical.mean:19 tvm.relax.op.statistical.min:19
#: tvm.relax.op.statistical.prod:19 tvm.relax.op.statistical.std:19
#: tvm.relax.op.statistical.sum:19 tvm.relax.op.statistical.variance:19
#: tvm.relax.op.ternary.ewise_fma:16 tvm.relax.op.unary.abs:9
#: tvm.relax.op.unary.acos:9 tvm.relax.op.unary.acosh:9
#: tvm.relax.op.unary.asin:9 tvm.relax.op.unary.asinh:9
#: tvm.relax.op.unary.atan:9 tvm.relax.op.unary.atanh:9
#: tvm.relax.op.unary.bitwise_not:9 tvm.relax.op.unary.ceil:9
#: tvm.relax.op.unary.clip:15 tvm.relax.op.unary.cos:9
#: tvm.relax.op.unary.cosh:9 tvm.relax.op.unary.erf:9 tvm.relax.op.unary.exp:9
#: tvm.relax.op.unary.floor:9 tvm.relax.op.unary.isfinite:9
#: tvm.relax.op.unary.isinf:9 tvm.relax.op.unary.isnan:9
#: tvm.relax.op.unary.log:9 tvm.relax.op.unary.logical_not:9
#: tvm.relax.op.unary.negative:9 tvm.relax.op.unary.round:9
#: tvm.relax.op.unary.rsqrt:13 tvm.relax.op.unary.sigmoid:9
#: tvm.relax.op.unary.sign:9 tvm.relax.op.unary.sin:9 tvm.relax.op.unary.sinh:9
#: tvm.relax.op.unary.sqrt:9 tvm.relax.op.unary.square:9
#: tvm.relax.op.unary.tan:9 tvm.relax.op.unary.tanh:9
msgid "Returns"
msgstr ""

#: of tvm.relax.op.base.assert_op:17 tvm.relax.op.base.call_inplace_packed:39
#: tvm.relax.op.base.call_pure_packed:25 tvm.relax.op.base.hint_on_device:14
#: tvm.relax.op.base.print:13 tvm.relax.op.base.shape_of:10
#: tvm.relax.op.base.shape_to_tensor:8 tvm.relax.op.base.tensor_to_shape:8
#: tvm.relax.op.base.to_vdevice:15 tvm.relax.op.binary.add:12
#: tvm.relax.op.binary.bitwise_and:10 tvm.relax.op.binary.bitwise_or:10
#: tvm.relax.op.binary.bitwise_xor:10 tvm.relax.op.binary.divide:12
#: tvm.relax.op.binary.equal:12 tvm.relax.op.binary.floor_divide:12
#: tvm.relax.op.binary.greater:12 tvm.relax.op.binary.greater_equal:12
#: tvm.relax.op.binary.less:12 tvm.relax.op.binary.less_equal:12
#: tvm.relax.op.binary.logical_and:10 tvm.relax.op.binary.logical_or:10
#: tvm.relax.op.binary.logical_xor:10 tvm.relax.op.binary.maximum:12
#: tvm.relax.op.binary.minimum:12 tvm.relax.op.binary.multiply:12
#: tvm.relax.op.binary.not_equal:12 tvm.relax.op.binary.power:12
#: tvm.relax.op.binary.subtract:12 tvm.relax.op.builtin.builtin.alloc_tensor:20
#: tvm.relax.op.builtin.builtin.stop_lift_params:11
#: tvm.relax.op.ccl.ccl.allgather:16 tvm.relax.op.ccl.ccl.allreduce:17
#: tvm.relax.op.ccl.ccl.broadcast_from_worker0:10
#: tvm.relax.op.ccl.ccl.scatter_from_worker0:16 tvm.relax.op.create.arange:20
#: tvm.relax.op.create.full:17 tvm.relax.op.create.full_like:20
#: tvm.relax.op.create.ones:13 tvm.relax.op.create.ones_like:15
#: tvm.relax.op.create.zeros:13 tvm.relax.op.create.zeros_like:15
#: tvm.relax.op.datatype.astype:13 tvm.relax.op.datatype.wrap_param:11
#: tvm.relax.op.distributed.distributed.annotate_sharding:14
#: tvm.relax.op.distributed.distributed.redistribute:13
#: tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:20
#: tvm.relax.op.grad.grad.avg_pool2d_backward:11
#: tvm.relax.op.grad.grad.end_checkpoint:10
#: tvm.relax.op.grad.grad.max_pool2d_backward:11
#: tvm.relax.op.grad.grad.nll_loss_backward:11
#: tvm.relax.op.grad.grad.no_grad:10 tvm.relax.op.grad.grad.start_checkpoint:31
#: tvm.relax.op.grad.grad.take_backward:11
#: tvm.relax.op.linear_algebra.einsum:13 tvm.relax.op.linear_algebra.linear:25
#: tvm.relax.op.linear_algebra.matmul:20
#: tvm.relax.op.manipulate.broadcast_to:13
#: tvm.relax.op.manipulate.collapse_sum_like:15
#: tvm.relax.op.manipulate.collapse_sum_to:24
#: tvm.relax.op.manipulate.expand_dims:15 tvm.relax.op.manipulate.flatten:10
#: tvm.relax.op.manipulate.layout_transform:20
#: tvm.relax.op.manipulate.permute_dims:13 tvm.relax.op.manipulate.reshape:23
#: tvm.relax.op.manipulate.scatter_elements:41
#: tvm.relax.op.manipulate.squeeze:15 tvm.relax.op.mask.masked_fill:12
#: tvm.relax.op.memory.memory.alloc_storage:21
#: tvm.relax.op.memory.memory.alloc_tensor:19
#: tvm.relax.op.memory.memory.kill_storage:10
#: tvm.relax.op.memory.memory.kill_tensor:10
#: tvm.relax.op.memory.view.ensure_zero_offset:10
#: tvm.relax.op.memory.view.view:35 tvm.relax.op.nn.nn.adaptive_avg_pool1d:40
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:43
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:43 tvm.relax.op.nn.nn.attention:76
#: tvm.relax.op.nn.nn.attention_var_len:83 tvm.relax.op.nn.nn.avg_pool1d:46
#: tvm.relax.op.nn.nn.avg_pool2d:54 tvm.relax.op.nn.nn.avg_pool3d:47
#: tvm.relax.op.nn.nn.batch_norm:89 tvm.relax.op.nn.nn.conv1d:63
#: tvm.relax.op.nn.nn.conv1d_transpose:52 tvm.relax.op.nn.nn.conv2d:63
#: tvm.relax.op.nn.nn.conv2d_transpose:62 tvm.relax.op.nn.nn.conv3d:65
#: tvm.relax.op.nn.nn.cross_entropy_with_logits:21
#: tvm.relax.op.nn.nn.dropout:17 tvm.relax.op.nn.nn.gelu:15
#: tvm.relax.op.nn.nn.gelu_tanh:13 tvm.relax.op.nn.nn.group_norm:37
#: tvm.relax.op.nn.nn.layer_norm:44 tvm.relax.op.nn.nn.leakyrelu:17
#: tvm.relax.op.nn.nn.log_softmax:22 tvm.relax.op.nn.nn.max_pool1d:46
#: tvm.relax.op.nn.nn.max_pool2d:53 tvm.relax.op.nn.nn.max_pool3d:47
#: tvm.relax.op.nn.nn.nll_loss:33 tvm.relax.op.nn.nn.pad:21
#: tvm.relax.op.nn.nn.relu:13 tvm.relax.op.nn.nn.rms_norm:29
#: tvm.relax.op.nn.nn.silu:13 tvm.relax.op.nn.nn.softmax:17
#: tvm.relax.op.qdq.dequantize:26 tvm.relax.op.qdq.quantize:26
#: tvm.relax.op.sampling.multinomial_from_uniform:34
#: tvm.relax.op.search.argmax:20 tvm.relax.op.search.argmin:20
#: tvm.relax.op.search.where:24 tvm.relax.op.statistical.cumprod:23
#: tvm.relax.op.statistical.cumsum:23 tvm.relax.op.statistical.max:20
#: tvm.relax.op.statistical.mean:20 tvm.relax.op.statistical.min:20
#: tvm.relax.op.statistical.prod:20 tvm.relax.op.statistical.std:20
#: tvm.relax.op.statistical.sum:20 tvm.relax.op.statistical.variance:20
#: tvm.relax.op.ternary.ewise_fma:17 tvm.relax.op.unary.abs:10
#: tvm.relax.op.unary.acos:10 tvm.relax.op.unary.acosh:10
#: tvm.relax.op.unary.asin:10 tvm.relax.op.unary.asinh:10
#: tvm.relax.op.unary.atan:10 tvm.relax.op.unary.atanh:10
#: tvm.relax.op.unary.bitwise_not:10 tvm.relax.op.unary.ceil:10
#: tvm.relax.op.unary.clip:16 tvm.relax.op.unary.cos:10
#: tvm.relax.op.unary.cosh:10 tvm.relax.op.unary.erf:10
#: tvm.relax.op.unary.exp:10 tvm.relax.op.unary.floor:10
#: tvm.relax.op.unary.isfinite:10 tvm.relax.op.unary.isinf:10
#: tvm.relax.op.unary.isnan:10 tvm.relax.op.unary.log:10
#: tvm.relax.op.unary.logical_not:10 tvm.relax.op.unary.negative:10
#: tvm.relax.op.unary.round:10 tvm.relax.op.unary.rsqrt:14
#: tvm.relax.op.unary.sigmoid:10 tvm.relax.op.unary.sign:10
#: tvm.relax.op.unary.sin:10 tvm.relax.op.unary.sinh:10
#: tvm.relax.op.unary.sqrt:10 tvm.relax.op.unary.square:10
#: tvm.relax.op.unary.tan:10 tvm.relax.op.unary.tanh:10
msgid "result"
msgstr ""

#: of tvm.relax.op.binary.add:13 tvm.relax.op.binary.bitwise_and:11
#: tvm.relax.op.binary.bitwise_or:11 tvm.relax.op.binary.bitwise_xor:11
#: tvm.relax.op.binary.divide:13 tvm.relax.op.binary.equal:13
#: tvm.relax.op.binary.floor_divide:13 tvm.relax.op.binary.greater:13
#: tvm.relax.op.binary.greater_equal:13 tvm.relax.op.binary.less:13
#: tvm.relax.op.binary.less_equal:13 tvm.relax.op.binary.logical_and:11
#: tvm.relax.op.binary.logical_or:11 tvm.relax.op.binary.logical_xor:11
#: tvm.relax.op.binary.maximum:13 tvm.relax.op.binary.minimum:13
#: tvm.relax.op.binary.multiply:13 tvm.relax.op.binary.not_equal:13
#: tvm.relax.op.binary.power:13 tvm.relax.op.binary.subtract:13
#: tvm.relax.op.linear_algebra.linear:26 tvm.relax.op.linear_algebra.matmul:21
#: tvm.relax.op.manipulate.flip:14 tvm.relax.op.manipulate.repeat:19
#: tvm.relax.op.manipulate.split:24 tvm.relax.op.manipulate.tile:23
#: tvm.relax.op.nn.nn.adaptive_avg_pool1d:41
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:44
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:44 tvm.relax.op.nn.nn.avg_pool1d:47
#: tvm.relax.op.nn.nn.avg_pool2d:55 tvm.relax.op.nn.nn.avg_pool3d:48
#: tvm.relax.op.nn.nn.batch_norm:90 tvm.relax.op.nn.nn.conv1d:64
#: tvm.relax.op.nn.nn.conv1d_transpose:53 tvm.relax.op.nn.nn.conv2d:64
#: tvm.relax.op.nn.nn.conv2d_transpose:63 tvm.relax.op.nn.nn.conv3d:66
#: tvm.relax.op.nn.nn.cross_entropy_with_logits:22 tvm.relax.op.nn.nn.gelu:16
#: tvm.relax.op.nn.nn.gelu_tanh:14 tvm.relax.op.nn.nn.group_norm:38
#: tvm.relax.op.nn.nn.layer_norm:45 tvm.relax.op.nn.nn.leakyrelu:18
#: tvm.relax.op.nn.nn.log_softmax:23 tvm.relax.op.nn.nn.max_pool1d:47
#: tvm.relax.op.nn.nn.max_pool2d:54 tvm.relax.op.nn.nn.max_pool3d:48
#: tvm.relax.op.nn.nn.nll_loss:34 tvm.relax.op.nn.nn.pad:22
#: tvm.relax.op.nn.nn.relu:14 tvm.relax.op.nn.nn.rms_norm:30
#: tvm.relax.op.nn.nn.silu:14 tvm.relax.op.nn.nn.softmax:18
#: tvm.relax.op.qdq.dequantize:27 tvm.relax.op.qdq.quantize:27
#: tvm.relax.op.search.argmax:21 tvm.relax.op.search.argmin:21
#: tvm.relax.op.sorting.topk:32 tvm.relax.op.statistical.max:21
#: tvm.relax.op.statistical.mean:21 tvm.relax.op.statistical.min:21
#: tvm.relax.op.statistical.prod:21 tvm.relax.op.statistical.std:21
#: tvm.relax.op.statistical.sum:21 tvm.relax.op.statistical.variance:21
#: tvm.relax.op.ternary.ewise_fma:18 tvm.relax.op.unary.abs:11
#: tvm.relax.op.unary.acos:11 tvm.relax.op.unary.acosh:11
#: tvm.relax.op.unary.asin:11 tvm.relax.op.unary.asinh:11
#: tvm.relax.op.unary.atan:11 tvm.relax.op.unary.atanh:11
#: tvm.relax.op.unary.bitwise_not:11 tvm.relax.op.unary.ceil:11
#: tvm.relax.op.unary.clip:17 tvm.relax.op.unary.cos:11
#: tvm.relax.op.unary.cosh:11 tvm.relax.op.unary.exp:11
#: tvm.relax.op.unary.floor:11 tvm.relax.op.unary.isfinite:11
#: tvm.relax.op.unary.isinf:11 tvm.relax.op.unary.isnan:11
#: tvm.relax.op.unary.log:11 tvm.relax.op.unary.logical_not:11
#: tvm.relax.op.unary.round:11 tvm.relax.op.unary.rsqrt:15
#: tvm.relax.op.unary.sigmoid:11 tvm.relax.op.unary.sign:11
#: tvm.relax.op.unary.sin:11 tvm.relax.op.unary.sinh:11
#: tvm.relax.op.unary.sqrt:11 tvm.relax.op.unary.square:11
#: tvm.relax.op.unary.tan:11 tvm.relax.op.unary.tanh:11
msgid "The computed result."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst
msgid "参数"
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst
msgid "返回类型"
msgstr ""

#: of tvm.relax.op.unary.acos:1
msgid "Compute element-wise arc cos of the input data."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:25
#: tvm.relax.op.index.strided_slice:31 tvm.relax.op.manipulate.reshape:27
#: tvm.relax.op.nn.nn.gelu:19 tvm.relax.op.nn.nn.gelu_tanh:17
#: tvm.relax.op.nn.nn.silu:17 tvm.relax.op.nn.nn.softmax:21
#: tvm.relax.op.unary.acos:14 tvm.relax.op.unary.acosh:14
#: tvm.relax.op.unary.asin:14 tvm.relax.op.unary.asinh:14
#: tvm.relax.op.unary.atan:14 tvm.relax.op.unary.atanh:14
#: tvm.relax.op.unary.cos:14 tvm.relax.op.unary.cosh:14
#: tvm.relax.op.unary.exp:14 tvm.relax.op.unary.log:14
#: tvm.relax.op.unary.rsqrt:18 tvm.relax.op.unary.sigmoid:14
#: tvm.relax.op.unary.sin:14 tvm.relax.op.unary.sinh:14
#: tvm.relax.op.unary.sqrt:14 tvm.relax.op.unary.tan:14
#: tvm.relax.op.unary.tanh:14
msgid "Note"
msgstr ""

#: of tvm.relax.op.nn.nn.gelu:20 tvm.relax.op.nn.nn.gelu_tanh:18
#: tvm.relax.op.nn.nn.silu:18 tvm.relax.op.nn.nn.softmax:22
#: tvm.relax.op.unary.acos:15 tvm.relax.op.unary.acosh:15
#: tvm.relax.op.unary.asin:15 tvm.relax.op.unary.asinh:15
#: tvm.relax.op.unary.atan:15 tvm.relax.op.unary.atanh:15
#: tvm.relax.op.unary.cos:15 tvm.relax.op.unary.cosh:15
#: tvm.relax.op.unary.exp:15 tvm.relax.op.unary.log:15
#: tvm.relax.op.unary.rsqrt:19 tvm.relax.op.unary.sigmoid:15
#: tvm.relax.op.unary.sin:15 tvm.relax.op.unary.sinh:15
#: tvm.relax.op.unary.sqrt:15 tvm.relax.op.unary.tan:15
#: tvm.relax.op.unary.tanh:15
msgid "The input tensor is required to have float dtype"
msgstr ""

#: of tvm.relax.op.unary.acosh:1
msgid "Compute element-wise arc cosh of the input data."
msgstr ""

#: of tvm.relax.op.binary.add:1
msgid "Addition with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.binary.add:5 tvm.relax.op.binary.divide:5
#: tvm.relax.op.binary.equal:5 tvm.relax.op.binary.floor_divide:5
#: tvm.relax.op.binary.greater:5 tvm.relax.op.binary.greater_equal:5
#: tvm.relax.op.binary.less:5 tvm.relax.op.binary.less_equal:5
#: tvm.relax.op.binary.maximum:5 tvm.relax.op.binary.minimum:5
#: tvm.relax.op.binary.multiply:5 tvm.relax.op.binary.not_equal:5
#: tvm.relax.op.binary.power:5 tvm.relax.op.binary.subtract:5
#: tvm.relax.op.linear_algebra.matmul:8 tvm.relax.op.search.where:14
#: tvm.relax.op.ternary.ewise_fma:6
msgid "x1"
msgstr ""

#: of tvm.relax.op.base.assert_op:-1 tvm.relax.op.base.call_builtin_with_ctx:-1
#: tvm.relax.op.base.call_dps_packed:-1
#: tvm.relax.op.base.call_inplace_packed:-1
#: tvm.relax.op.base.call_pure_packed:-1 tvm.relax.op.base.call_tir:-1
#: tvm.relax.op.base.call_tir_inplace:-1
#: tvm.relax.op.base.call_tir_with_grad:-1 tvm.relax.op.base.hint_on_device:-1
#: tvm.relax.op.base.invoke_closure:-1 tvm.relax.op.base.invoke_pure_closure:-1
#: tvm.relax.op.base.make_closure:-1 tvm.relax.op.base.print:-1
#: tvm.relax.op.base.shape_of:-1 tvm.relax.op.base.shape_to_tensor:-1
#: tvm.relax.op.base.tensor_to_shape:-1 tvm.relax.op.base.to_vdevice:-1
#: tvm.relax.op.binary.add:-1 tvm.relax.op.binary.multiply:-1
#: tvm.relax.op.builtin.builtin.alloc_tensor:-1
#: tvm.relax.op.distributed.distributed.call_tir_local_view:-1
#: tvm.relax.op.index.dynamic_strided_slice:-1
#: tvm.relax.op.memory.memory.alloc_storage:-1
#: tvm.relax.op.memory.memory.alloc_tensor:-1
#: tvm.relax.op.memory.memory.kill_storage:-1
#: tvm.relax.op.memory.memory.kill_tensor:-1 tvm.relax.op.nn.nn.avg_pool1d:-1
#: tvm.relax.op.nn.nn.avg_pool2d:-1 tvm.relax.op.nn.nn.avg_pool3d:-1
#: tvm.relax.op.nn.nn.max_pool1d:-1 tvm.relax.op.nn.nn.max_pool2d:-1
#: tvm.relax.op.nn.nn.max_pool3d:-1
msgid "Expr"
msgstr ""

#: of tvm.relax.op.binary.add:6 tvm.relax.op.binary.bitwise_and:5
#: tvm.relax.op.binary.bitwise_or:5 tvm.relax.op.binary.bitwise_xor:5
#: tvm.relax.op.binary.divide:6 tvm.relax.op.binary.equal:6
#: tvm.relax.op.binary.floor_divide:6 tvm.relax.op.binary.greater:6
#: tvm.relax.op.binary.greater_equal:6 tvm.relax.op.binary.less:6
#: tvm.relax.op.binary.less_equal:6 tvm.relax.op.binary.logical_and:5
#: tvm.relax.op.binary.logical_or:5 tvm.relax.op.binary.logical_xor:5
#: tvm.relax.op.binary.maximum:6 tvm.relax.op.binary.minimum:6
#: tvm.relax.op.binary.multiply:6 tvm.relax.op.binary.not_equal:6
#: tvm.relax.op.binary.power:6 tvm.relax.op.binary.subtract:6
#: tvm.relax.op.linear_algebra.matmul:9
msgid "The first input tensor."
msgstr ""

#: of tvm.relax.op.binary.add:7 tvm.relax.op.binary.bitwise_and:6
#: tvm.relax.op.binary.bitwise_or:6 tvm.relax.op.binary.bitwise_xor:6
#: tvm.relax.op.binary.divide:7 tvm.relax.op.binary.equal:7
#: tvm.relax.op.binary.floor_divide:7 tvm.relax.op.binary.greater:7
#: tvm.relax.op.binary.greater_equal:7 tvm.relax.op.binary.less:7
#: tvm.relax.op.binary.less_equal:7 tvm.relax.op.binary.logical_and:6
#: tvm.relax.op.binary.logical_or:6 tvm.relax.op.binary.logical_xor:6
#: tvm.relax.op.binary.maximum:7 tvm.relax.op.binary.minimum:7
#: tvm.relax.op.binary.multiply:7 tvm.relax.op.binary.not_equal:7
#: tvm.relax.op.binary.power:7 tvm.relax.op.binary.subtract:7
#: tvm.relax.op.linear_algebra.matmul:11 tvm.relax.op.search.where:18
#: tvm.relax.op.ternary.ewise_fma:9
msgid "x2"
msgstr ""

#: of tvm.relax.op.binary.add:8 tvm.relax.op.binary.bitwise_and:7
#: tvm.relax.op.binary.bitwise_or:7 tvm.relax.op.binary.bitwise_xor:7
#: tvm.relax.op.binary.divide:8 tvm.relax.op.binary.equal:8
#: tvm.relax.op.binary.floor_divide:8 tvm.relax.op.binary.greater:8
#: tvm.relax.op.binary.greater_equal:8 tvm.relax.op.binary.less:8
#: tvm.relax.op.binary.less_equal:8 tvm.relax.op.binary.logical_and:7
#: tvm.relax.op.binary.logical_or:7 tvm.relax.op.binary.logical_xor:7
#: tvm.relax.op.binary.maximum:8 tvm.relax.op.binary.minimum:8
#: tvm.relax.op.binary.multiply:8 tvm.relax.op.binary.not_equal:8
#: tvm.relax.op.binary.power:8 tvm.relax.op.binary.subtract:8
#: tvm.relax.op.linear_algebra.matmul:12
msgid "The second input tensor."
msgstr ""

#: of tvm.relax.op.binary.add:16 tvm.relax.op.manipulate.flip:17
#: tvm.relax.op.manipulate.repeat:22
#: tvm.relax.op.manipulate.scatter_elements:45 tvm.relax.op.manipulate.tile:26
#: tvm.relax.op.sampling.multinomial_from_uniform:38
#: tvm.relax.op.statistical.cumprod:28 tvm.relax.op.statistical.cumsum:28
msgid "Examples"
msgstr ""

#: of tvm.relax.op.create.arange:1
msgid "Construct a tensor with evenly spaced elements."
msgstr ""

#: of tvm.relax.op.create.arange:5
msgid "start"
msgstr ""

#: of tvm.relax.op.create.arange:-1
msgid "Union[PrimExprLike,PrimValue]"
msgstr ""

#: of tvm.relax.op.create.arange:6
msgid "The start of the interval."
msgstr ""

#: of tvm.relax.op.create.arange:8 tvm.relax.op.index.dynamic_strided_slice:11
#: tvm.relax.op.index.strided_slice:14
msgid "end"
msgstr ""

#: of tvm.relax.op.create.arange:-1
msgid "Optional[Union[PrimExprLike,PrimValue]]"
msgstr ""

#: of tvm.relax.op.create.arange:9
msgid ""
"The end of the interval. If not given, it will be set to start, and start"
" will be set to 0."
msgstr ""

#: of tvm.relax.op.create.arange:12
msgid "step"
msgstr ""

#: of tvm.relax.op.create.arange:13
msgid "The step size."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:8 tvm.relax.op.create.arange:15
#: tvm.relax.op.create.full:11 tvm.relax.op.create.full_like:14
#: tvm.relax.op.create.ones:8 tvm.relax.op.create.ones_like:9
#: tvm.relax.op.create.zeros:8 tvm.relax.op.create.zeros_like:9
#: tvm.relax.op.datatype.wrap_param:7
#: tvm.relax.op.memory.memory.alloc_storage:16
#: tvm.relax.op.memory.memory.alloc_tensor:14
#: tvm.relax.op.sampling.multinomial_from_uniform:29
#: tvm.relax.op.sorting.argsort:15 tvm.relax.op.sorting.topk:26
#: tvm.relax.op.statistical.cumprod:13 tvm.relax.op.statistical.cumsum:13
msgid "dtype"
msgstr ""

#: of tvm.relax.op.create.arange:-1 tvm.relax.op.create.full:-1
#: tvm.relax.op.create.full_like:-1 tvm.relax.op.create.ones_like:-1
#: tvm.relax.op.create.zeros_like:-1 tvm.relax.op.image.image.resize2d:-1
#: tvm.relax.op.nn.nn.conv1d:-1 tvm.relax.op.nn.nn.conv1d_transpose:-1
#: tvm.relax.op.nn.nn.conv2d:-1 tvm.relax.op.nn.nn.conv2d_transpose:-1
#: tvm.relax.op.nn.nn.conv3d:-1 tvm.relax.op.statistical.cumprod:-1
#: tvm.relax.op.statistical.cumsum:-1
msgid "Optional[Union[str, DataType]]"
msgstr ""

#: of tvm.relax.op.create.arange:16 tvm.relax.op.create.ones:9
#: tvm.relax.op.create.zeros:9
msgid "The data type of the created tensor."
msgstr ""

#: of tvm.relax.op.create.arange:21 tvm.relax.op.create.full:18
#: tvm.relax.op.create.full_like:21 tvm.relax.op.create.ones:14
#: tvm.relax.op.create.ones_like:16 tvm.relax.op.create.tril:18
#: tvm.relax.op.create.triu:18 tvm.relax.op.create.zeros:14
#: tvm.relax.op.create.zeros_like:16 tvm.relax.op.search.where:25
msgid "The result tensor."
msgstr ""

#: of tvm.relax.op.search.argmax:1
msgid "Computes the argmax of tensor elements over given axis."
msgstr ""

#: of tvm.relax.op.search.argmax:6 tvm.relax.op.search.argmin:6
#: tvm.relax.op.statistical.max:6 tvm.relax.op.statistical.mean:6
#: tvm.relax.op.statistical.min:6 tvm.relax.op.statistical.prod:6
#: tvm.relax.op.statistical.std:6 tvm.relax.op.statistical.sum:6
#: tvm.relax.op.statistical.variance:6
msgid "The input data tensor"
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:11
#: tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:15
#: tvm.relax.op.index.take:15 tvm.relax.op.manipulate.concat:9
#: tvm.relax.op.manipulate.expand_dims:8 tvm.relax.op.manipulate.split:18
#: tvm.relax.op.manipulate.squeeze:8 tvm.relax.op.nn.nn.batch_norm:72
#: tvm.relax.op.qdq.dequantize:18 tvm.relax.op.qdq.quantize:18
#: tvm.relax.op.search.argmax:8 tvm.relax.op.search.argmin:8
#: tvm.relax.op.set.unique:27 tvm.relax.op.sorting.argsort:9
#: tvm.relax.op.sorting.sort:9 tvm.relax.op.sorting.topk:13
#: tvm.relax.op.statistical.cumprod:9 tvm.relax.op.statistical.cumsum:9
#: tvm.relax.op.statistical.max:8 tvm.relax.op.statistical.mean:8
#: tvm.relax.op.statistical.min:8 tvm.relax.op.statistical.prod:8
#: tvm.relax.op.statistical.std:8 tvm.relax.op.statistical.sum:8
#: tvm.relax.op.statistical.variance:8
msgid "axis"
msgstr ""

#: of tvm.relax.op.index.take:-1 tvm.relax.op.manipulate.concat:-1
#: tvm.relax.op.search.argmax:-1 tvm.relax.op.search.argmin:-1
#: tvm.relax.op.statistical.cumprod:-1 tvm.relax.op.statistical.cumsum:-1
msgid "Optional[int]"
msgstr ""

#: of tvm.relax.op.search.argmax:9
msgid ""
"Axis along which an argmax operation is performed. The default, "
"axis=None, will compute the argmax of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.search.argmax:13 tvm.relax.op.search.argmin:13
#: tvm.relax.op.statistical.max:13 tvm.relax.op.statistical.mean:13
#: tvm.relax.op.statistical.min:13 tvm.relax.op.statistical.prod:13
#: tvm.relax.op.statistical.std:13 tvm.relax.op.statistical.sum:13
#: tvm.relax.op.statistical.variance:13
msgid "keepdims"
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:-1 tvm.relax.op.ccl.ccl.allreduce:-1
#: tvm.relax.op.index.strided_slice:-1 tvm.relax.op.nn.nn.avg_pool1d:-1
#: tvm.relax.op.nn.nn.avg_pool2d:-1 tvm.relax.op.nn.nn.avg_pool3d:-1
#: tvm.relax.op.nn.nn.batch_norm:-1 tvm.relax.op.nn.nn.group_norm:-1
#: tvm.relax.op.nn.nn.layer_norm:-1 tvm.relax.op.nn.nn.max_pool1d:-1
#: tvm.relax.op.nn.nn.max_pool2d:-1 tvm.relax.op.nn.nn.max_pool3d:-1
#: tvm.relax.op.search.argmax:-1 tvm.relax.op.search.argmin:-1
#: tvm.relax.op.sorting.argsort:-1 tvm.relax.op.sorting.sort:-1
#: tvm.relax.op.sorting.topk:-1 tvm.relax.op.statistical.cumprod:-1
#: tvm.relax.op.statistical.cumsum:-1 tvm.relax.op.statistical.max:-1
#: tvm.relax.op.statistical.mean:-1 tvm.relax.op.statistical.min:-1
#: tvm.relax.op.statistical.prod:-1 tvm.relax.op.statistical.std:-1
#: tvm.relax.op.statistical.sum:-1 tvm.relax.op.statistical.variance:-1
msgid "bool"
msgstr ""

#: of tvm.relax.op.search.argmax:14 tvm.relax.op.search.argmin:14
msgid ""
"If this is set to True, the axis being reduced is left in the result as "
"dimensions with size one. With this option, the result will broadcast "
"correctly against the input tensor."
msgstr ""

#: of tvm.relax.op.search.argmin:1
msgid "Computes the argmin of tensor elements over given axis."
msgstr ""

#: of tvm.relax.op.search.argmin:9
msgid ""
"Axis along which an argmin operation is performed. The default, "
"axis=None, will compute the argmin of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.sorting.argsort:1
msgid ""
"Performs sorting along the given axis and returns an array of indices "
"having same shape as an input array that index data in sorted order."
msgstr ""

#: of tvm.relax.op.base.hint_on_device:6 tvm.relax.op.base.to_vdevice:7
#: tvm.relax.op.image.image.resize2d:13 tvm.relax.op.linear_algebra.linear:5
#: tvm.relax.op.manipulate.collapse_sum_like:7
#: tvm.relax.op.manipulate.collapse_sum_to:16 tvm.relax.op.manipulate.flip:5
#: tvm.relax.op.manipulate.repeat:5 tvm.relax.op.manipulate.scatter_elements:23
#: tvm.relax.op.manipulate.tile:14
#: tvm.relax.op.memory.view.ensure_zero_offset:5
#: tvm.relax.op.nn.nn.adaptive_avg_pool1d:24
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:27
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:27 tvm.relax.op.nn.nn.avg_pool1d:16
#: tvm.relax.op.nn.nn.avg_pool2d:24 tvm.relax.op.nn.nn.avg_pool3d:17
#: tvm.relax.op.nn.nn.batch_norm:57 tvm.relax.op.nn.nn.conv1d:28
#: tvm.relax.op.nn.nn.conv1d_transpose:14 tvm.relax.op.nn.nn.conv2d:28
#: tvm.relax.op.nn.nn.conv2d_transpose:24 tvm.relax.op.nn.nn.conv3d:30
#: tvm.relax.op.nn.nn.dropout:9 tvm.relax.op.nn.nn.gelu:10
#: tvm.relax.op.nn.nn.gelu_tanh:8 tvm.relax.op.nn.nn.group_norm:8
#: tvm.relax.op.nn.nn.layer_norm:21 tvm.relax.op.nn.nn.leakyrelu:8
#: tvm.relax.op.nn.nn.max_pool1d:16 tvm.relax.op.nn.nn.max_pool2d:23
#: tvm.relax.op.nn.nn.max_pool3d:17 tvm.relax.op.nn.nn.relu:8
#: tvm.relax.op.nn.nn.rms_norm:12 tvm.relax.op.nn.nn.silu:8
#: tvm.relax.op.qdq.dequantize:9 tvm.relax.op.qdq.quantize:9
#: tvm.relax.op.sorting.argsort:6 tvm.relax.op.sorting.topk:7
#: tvm.relax.op.statistical.cumprod:6 tvm.relax.op.statistical.cumsum:6
msgid "data"
msgstr ""

#: of tvm.relax.op.sorting.argsort:7 tvm.relax.op.sorting.topk:8
msgid "The input data tensor."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:-1
#: tvm.relax.op.ccl.ccl.scatter_from_worker0:-1 tvm.relax.op.create.tril:-1
#: tvm.relax.op.create.triu:-1
#: tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:-1
#: tvm.relax.op.manipulate.repeat:-1 tvm.relax.op.manipulate.split:-1
#: tvm.relax.op.nn.nn.batch_norm:-1 tvm.relax.op.nn.nn.conv1d:-1
#: tvm.relax.op.nn.nn.conv1d_transpose:-1 tvm.relax.op.nn.nn.conv2d:-1
#: tvm.relax.op.nn.nn.conv2d_transpose:-1 tvm.relax.op.nn.nn.conv3d:-1
#: tvm.relax.op.nn.nn.group_norm:-1 tvm.relax.op.nn.nn.nll_loss:-1
#: tvm.relax.op.qdq.dequantize:-1 tvm.relax.op.qdq.quantize:-1
#: tvm.relax.op.sorting.argsort:-1 tvm.relax.op.sorting.sort:-1
#: tvm.relax.op.sorting.topk:-1
msgid "int"
msgstr ""

#: of tvm.relax.op.sorting.argsort:10 tvm.relax.op.sorting.topk:14
msgid "Axis long which to sort the input tensor."
msgstr ""

#: of tvm.relax.op.sorting.argsort:12 tvm.relax.op.sorting.sort:13
msgid "descending"
msgstr ""

#: of tvm.relax.op.sorting.argsort:13 tvm.relax.op.sorting.sort:14
msgid "Whether to sort in descending order, the default is False"
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:-1 tvm.relax.op.ccl.ccl.allreduce:-1
#: tvm.relax.op.image.image.resize2d:-1 tvm.relax.op.linear_algebra.einsum:-1
#: tvm.relax.op.nn.nn.adaptive_avg_pool1d:-1
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:-1
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:-1 tvm.relax.op.nn.nn.avg_pool1d:-1
#: tvm.relax.op.nn.nn.avg_pool2d:-1 tvm.relax.op.nn.nn.avg_pool3d:-1
#: tvm.relax.op.nn.nn.conv1d:-1 tvm.relax.op.nn.nn.conv1d_transpose:-1
#: tvm.relax.op.nn.nn.conv2d:-1 tvm.relax.op.nn.nn.conv2d_transpose:-1
#: tvm.relax.op.nn.nn.conv3d:-1 tvm.relax.op.nn.nn.max_pool1d:-1
#: tvm.relax.op.nn.nn.max_pool2d:-1 tvm.relax.op.nn.nn.max_pool3d:-1
#: tvm.relax.op.nn.nn.nll_loss:-1
#: tvm.relax.op.sampling.multinomial_from_uniform:-1
#: tvm.relax.op.sorting.argsort:-1 tvm.relax.op.sorting.topk:-1
msgid "str"
msgstr ""

#: of tvm.relax.op.sorting.argsort:16
msgid "The data type of the output indices."
msgstr ""

#: of tvm.relax.op.sorting.argsort:20 tvm.relax.op.sorting.sort:18
#: tvm.relax.op.sorting.topk:31
msgid "out"
msgstr ""

#: of tvm.relax.op.sorting.argsort:21
msgid "Tensor with same shape as data."
msgstr ""

#: of tvm.relax.op.unary.asin:1
msgid "Compute element-wise arc sin of the input data."
msgstr ""

#: of tvm.relax.op.unary.asinh:1
msgid "Compute element-wise arc sinh of the input data."
msgstr ""

#: of tvm.relax.op.base.assert_op:1
msgid ""
"Create a call to Relax's assert_op operation (`assert` is reserved in "
"Python, so the name must be distinct)."
msgstr ""

#: of tvm.relax.op.base.assert_op:6
msgid "condition: Union[Expr, PrimExpr]"
msgstr ""

#: of tvm.relax.op.base.assert_op:7
msgid "The assertion condition."
msgstr ""

#: of tvm.relax.op.base.assert_op:9
msgid "format_args: Optional[Union[Expr, List[Expr]]]"
msgstr ""

#: of tvm.relax.op.base.assert_op:10
msgid "Format arguments for the error message if the condition fails."
msgstr ""

#: of tvm.relax.op.base.assert_op:12 tvm.relax.op.base.print:8
msgid "format: Union[str, Expr]"
msgstr ""

#: of tvm.relax.op.base.assert_op:13
msgid "The format string or StringImm for the error message."
msgstr ""

#: of tvm.relax.op.base.assert_op:18
msgid "A Call to the Relax assert operation."
msgstr ""

#: of tvm.relax.op.datatype.astype:1
msgid "Cast input tensor to the given data type."
msgstr ""

#: of tvm.relax.op.datatype.astype:6 tvm.relax.op.datatype.wrap_param:6
#: tvm.relax.op.image.image.resize2d:14 tvm.relax.op.manipulate.broadcast_to:6
#: tvm.relax.op.manipulate.expand_dims:6 tvm.relax.op.manipulate.flatten:6
#: tvm.relax.op.manipulate.flip:6 tvm.relax.op.manipulate.permute_dims:6
#: tvm.relax.op.manipulate.reshape:16
#: tvm.relax.op.manipulate.scatter_elements:24
#: tvm.relax.op.manipulate.squeeze:6 tvm.relax.op.manipulate.tile:15
#: tvm.relax.op.mask.masked_fill:5 tvm.relax.op.memory.view.view:15
#: tvm.relax.op.nn.nn.adaptive_avg_pool1d:25
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:28
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:28 tvm.relax.op.nn.nn.avg_pool1d:17
#: tvm.relax.op.nn.nn.avg_pool2d:25 tvm.relax.op.nn.nn.avg_pool3d:18
#: tvm.relax.op.nn.nn.batch_norm:58 tvm.relax.op.nn.nn.conv1d:29
#: tvm.relax.op.nn.nn.conv1d_transpose:15 tvm.relax.op.nn.nn.conv2d:29
#: tvm.relax.op.nn.nn.conv2d_transpose:25 tvm.relax.op.nn.nn.conv3d:31
#: tvm.relax.op.nn.nn.dropout:10 tvm.relax.op.nn.nn.log_softmax:13
#: tvm.relax.op.nn.nn.max_pool1d:17 tvm.relax.op.nn.nn.max_pool2d:24
#: tvm.relax.op.nn.nn.max_pool3d:18 tvm.relax.op.nn.nn.softmax:8
#: tvm.relax.op.statistical.cumprod:7 tvm.relax.op.statistical.cumsum:7
msgid "The input data to the operator."
msgstr ""

#: of tvm.relax.op.datatype.astype:8
msgid "dtype: Union[str, DataType]"
msgstr ""

#: of tvm.relax.op.datatype.astype:9 tvm.relax.op.datatype.wrap_param:8
msgid "The target data type"
msgstr ""

#: of tvm.relax.op.datatype.astype:14 tvm.relax.op.datatype.wrap_param:12
msgid "The casted result."
msgstr ""

#: of tvm.relax.op.unary.atan:1
msgid "Compute element-wise arc tan of the input data."
msgstr ""

#: of tvm.relax.op.unary.atanh:1
msgid "Compute element-wise arc tanh of the input data."
msgstr ""

#: of tvm.relax.op.binary.bitwise_and:1
msgid "Bitwise AND Parameters ---------- x1 : relax.Expr"
msgstr ""

#: of tvm.relax.op.unary.bitwise_not:1
msgid "Compute bitwise NOT of the input data."
msgstr ""

#: of tvm.relax.op.binary.bitwise_or:1
msgid "Bitwise OR Parameters ---------- x1 : relax.Expr"
msgstr ""

#: of tvm.relax.op.binary.bitwise_xor:1
msgid "Bitwise XOR Parameters ---------- x1 : relax.Expr"
msgstr ""

#: of tvm.relax.op.manipulate.broadcast_to:1
msgid "Broadcasts a tensor to a specified shape."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:5 tvm.relax.op.create.full:5
#: tvm.relax.op.create.ones:5 tvm.relax.op.create.zeros:5
#: tvm.relax.op.manipulate.broadcast_to:8
#: tvm.relax.op.manipulate.collapse_sum_to:19
#: tvm.relax.op.manipulate.reshape:18
#: tvm.relax.op.memory.memory.alloc_tensor:11
msgid "shape"
msgstr ""

#: of tvm.relax.op.create.full:-1 tvm.relax.op.create.ones:-1
#: tvm.relax.op.create.zeros:-1 tvm.relax.op.manipulate.broadcast_to:-1
#: tvm.relax.op.manipulate.reshape:-1
msgid "Union[Tuple[PrimExprLike], Expr]"
msgstr ""

#: of tvm.relax.op.manipulate.broadcast_to:9
msgid "The target shape."
msgstr ""

#: of tvm.relax.op.manipulate.broadcast_to:14
msgid "The broadcasted tensor."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:1
msgid "Call a builtin function func."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:5
#: tvm.relax.op.base.call_dps_packed:9 tvm.relax.op.base.call_inplace_packed:20
#: tvm.relax.op.base.call_pure_packed:14 tvm.relax.op.base.make_closure:5
msgid "func"
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:6
msgid "The builtin function to be called."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:8
#: tvm.relax.op.base.call_dps_packed:12 tvm.relax.op.base.call_tir:8
#: tvm.relax.op.base.call_tir_inplace:18
#: tvm.relax.op.base.call_tir_with_grad:10 tvm.relax.op.base.invoke_closure:8
#: tvm.relax.op.base.invoke_pure_closure:14 tvm.relax.op.base.make_closure:8
#: tvm.relax.op.distributed.distributed.call_tir_local_view:10
msgid "args"
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:9
#: tvm.relax.op.base.call_dps_packed:13 tvm.relax.op.base.call_tir:9
#: tvm.relax.op.base.call_tir_inplace:19
#: tvm.relax.op.base.call_tir_with_grad:11 tvm.relax.op.base.invoke_closure:9
#: tvm.relax.op.base.invoke_pure_closure:15 tvm.relax.op.base.make_closure:9
#: tvm.relax.op.distributed.distributed.call_tir_local_view:11
msgid "The input arguments."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:11
msgid "sinfo_args: Optional[Union[StructInfo, List[StructInfo]]]"
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:12
msgid "The struct info arguments to the call node."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:16
#: tvm.relax.op.base.call_dps_packed:22 tvm.relax.op.base.call_tir:21
#: tvm.relax.op.base.call_tir_inplace:40
#: tvm.relax.op.base.call_tir_with_grad:31 tvm.relax.op.base.invoke_closure:16
#: tvm.relax.op.base.invoke_pure_closure:22 tvm.relax.op.base.null_value:5
#: tvm.relax.op.distributed.distributed.call_tir_local_view:23
msgid "ret: Call"
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:17 tvm.relax.op.base.null_value:6
msgid "The created call node."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:1
msgid "Call a destination-passing-style packed function and return the output."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:3
msgid ""
"Note: The called function is assumed to be _pure_ (other than modifying "
"the designated output arguments). If the function _does_ result in other "
"side effects, then the compiler may end up removing, reordering, or "
"repeating those effects--no guarantees can be made."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:-1
#: tvm.relax.op.builtin.builtin.alloc_tensor:-1
#: tvm.relax.op.memory.memory.alloc_storage:-1
#: tvm.relax.op.memory.memory.alloc_tensor:-1
msgid "Union[str, Expr]"
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:10
msgid "The destination-passing-style function, can be ExternFunc."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:15 tvm.relax.op.base.call_tir:11
#: tvm.relax.op.base.call_tir_inplace:29
#: tvm.relax.op.base.call_tir_with_grad:13
#: tvm.relax.op.distributed.distributed.call_tir_local_view:13
msgid "out_sinfo"
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:-1 tvm.relax.op.base.call_tir:-1
#: tvm.relax.op.base.call_tir_inplace:-1
#: tvm.relax.op.base.call_tir_with_grad:-1
msgid "Union[TensorStructInfo, List[TensorStructInfo]]"
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:16
msgid ""
"The structure info of the call_dps_packed output. It should be a single "
"or a list of TensorStructInfo. Each one denotes the structure info of a "
"returned tensor."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:23
msgid "A call node for the call_dps_packed operator."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:1
msgid ""
"Construct a call to a packed function that consumes some of its arguments"
" \"in-place\" and returns the mutated arguments (aliased), but should be "
"considered to be otherwise pure. The `inplace_indices` argument indicates"
" which of the outputs are mutated arguments."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:5
#: tvm.relax.op.base.call_pure_packed:4
msgid ""
"The resulting call will have the same semantics as calling the packed "
"function directly."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:7
msgid ""
"Note: This should be used for cases when the user knows that calling the "
"packed function with these arguments will **in reality** not cause any "
"other side effects. If it is used for a call that **does** result in "
"other side effects, then the compiler may end up removing, reordering, or"
" repeating that call, with no guarantees made about any side effects from"
" the callee."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:13
msgid ""
"Warning: This operator as treated as pure by the type system even though "
"it *is* performing side effects (mutating some arguments). It is "
"therefore incumbent upon the user to ensure that it is being used safely "
"(viz., that mutated arguments are not live after the mutation, that they "
"do not alias values live after the mutation)."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:-1
#: tvm.relax.op.base.call_pure_packed:-1
msgid "Union[str, ExternFunc]"
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:21
#: tvm.relax.op.base.call_pure_packed:15
msgid "The name (global symbol) for a PackedFunc or an ExternFunc node."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:23
#: tvm.relax.op.base.call_pure_packed:17
msgid "args: Expr"
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:24
#: tvm.relax.op.base.call_pure_packed:18
msgid "The arguments for the PackedFunc."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:26
#: tvm.relax.op.base.call_tir_inplace:21
msgid "inplace_indices"
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:-1
#: tvm.relax.op.base.call_tir_inplace:-1 tvm.relax.op.manipulate.expand_dims:-1
#: tvm.relax.op.nn.nn.group_norm:-1 tvm.relax.op.nn.nn.layer_norm:-1
#: tvm.relax.op.nn.nn.rms_norm:-1
msgid "Union[int, List[int]]"
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:27
#: tvm.relax.op.base.call_tir_inplace:22
msgid ""
"Specify which arguments should be used for in-place computations. If "
"`inplace_indices` is a single integer, it will be made into a singleton "
"list. Suppose `inplace_indices[i] = j`, where `j >= 0`. Then the `i`th "
"output will be an alias of `args[j]`. If `inplace_indices[i] = -1`, then "
"the `i`th output will be a freshly allocated tensor. At least one member "
"of `inplace_indices` must not be -1."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:34
#: tvm.relax.op.base.call_pure_packed:20
msgid "sinfo_args: Union[StructInfo, List[StructInfo]]"
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:35
#: tvm.relax.op.base.call_pure_packed:21
msgid ""
"The list of structure info arguments (giving the structural info for the "
"returned value)."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:40
#: tvm.relax.op.base.call_pure_packed:26
msgid ""
"A Relax call, corresponding to `call_pure_packed(ExternFunc(func), args, "
"DictAttrs(kwargs), sinfo_args)`"
msgstr ""

#: of tvm.relax.op.base.call_pure_packed:1
msgid ""
"Construct a call to a packed function that should be treated as pure, "
"even though packed calls are normally not treated as pure."
msgstr ""

#: of tvm.relax.op.base.call_pure_packed:6
msgid ""
"Note: This should be used for cases when the user knows that calling the "
"packed function with these arguments will **in reality** not cause any "
"side effects. If it is used for a call that **does** result in side "
"effects, then the compiler may end up removing, reordering, or repeating "
"that call, with no guarantees made about any side effects from the "
"callee."
msgstr ""

#: of tvm.relax.op.base.call_tir:1
msgid "Call a tir.prim_func and return the output."
msgstr ""

#: of tvm.relax.op.base.call_tir:5 tvm.relax.op.base.call_tir_inplace:15
#: tvm.relax.op.base.call_tir_with_grad:7
#: tvm.relax.op.distributed.distributed.call_tir_local_view:7
msgid "gvar"
msgstr ""

#: of tvm.relax.op.base.call_tir:-1 tvm.relax.op.base.call_tir_inplace:-1
#: tvm.relax.op.base.call_tir_with_grad:-1
#: tvm.relax.op.distributed.distributed.call_tir_local_view:-1
msgid "GlobalVar"
msgstr ""

#: of tvm.relax.op.base.call_tir:6 tvm.relax.op.base.call_tir_with_grad:8
#: tvm.relax.op.distributed.distributed.call_tir_local_view:8
msgid "The GlobalVar referring to a tir PrimFunc."
msgstr ""

#: of tvm.relax.op.base.call_tir:12
msgid ""
"The structure info of the call_tir output. It should be a single or a "
"list of TensorStructInfo. Each one denotes the structure info of a "
"returned tensor."
msgstr ""

#: of tvm.relax.op.base.call_tir:16 tvm.relax.op.base.call_tir_inplace:35
#: tvm.relax.op.base.call_tir_with_grad:26
#: tvm.relax.op.distributed.distributed.call_tir_local_view:18
msgid "tir_vars"
msgstr ""

#: of tvm.relax.op.base.call_tir:-1 tvm.relax.op.base.call_tir_inplace:-1
#: tvm.relax.op.base.call_tir_with_grad:-1
#: tvm.relax.op.distributed.distributed.call_tir_local_view:-1
msgid "Optional[Union[ShapeExpr, Tuple[PrimExpr], List[PrimExpr]]]"
msgstr ""

#: of tvm.relax.op.base.call_tir:17 tvm.relax.op.base.call_tir_inplace:36
#: tvm.relax.op.base.call_tir_with_grad:27
#: tvm.relax.op.distributed.distributed.call_tir_local_view:19
msgid ""
"ShapeExpr representing a tuple of integers to unpack when calling func. "
"Is null if not used"
msgstr ""

#: of tvm.relax.op.base.call_tir:22 tvm.relax.op.base.call_tir_inplace:41
msgid "A call node for the call_tir operator."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:1
msgid ""
"Call a TIR PrimFunc and return the result, doing the specified "
"computations in-place (based on the `inplace_indices` argument; outputs "
"will alias the inputs selected by in-place indices)."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:5
msgid ""
"Warning: This operator is considered pure by the type system but actually"
" mutates the arguments specified by `inplace_indices`. This operator "
"should not be used directly, but rather should be inserted by passes that"
" have checked whether it is safe to perform operations in-place (i.e., "
"none of the arguments specified as an output is aliased or is live after "
"calling call_tir_inplace)."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:11
msgid "Direct calls to this operator should be done for testing purposes only."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:16
msgid "The GlobalVar referring to a TIR PrimFunc."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:30
msgid ""
"The structure info of the call_tir_inplace output. It should be a single "
"`TensorStructInfo` or a list of `TensorStructInfo`. Each one denotes the "
"structure info of a returned tensor. If a list of `TensorStructInfo` is "
"given, the result will be a tuple of `TensorStructInfo`."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:1
msgid ""
"Call a tir.prim_func and return the output. This intrinsic will bind a te"
" gradient function (refered by te_grad_name) to the call_tir_with_grad "
"node. The te gradient function will be called by the Gradient pass."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:14
msgid ""
"The structure info of the call_tir_with_grad output. It should be a "
"single or a list of TensorStructInfo. Each one denotes the structure info"
" of a returned tensor."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:18
msgid "te_grad_name"
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:19
msgid ""
"The registered name of the te gradient function associated with the "
"call_tir_with_grad node. Must be provided as a keyword argument."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:22
msgid "te_grad_kwargs"
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:-1
msgid "Dict[str, Object], optional"
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:23
msgid ""
"The keyword arguments passed to the te gradient function. Optionally "
"provided as a keyword argument. Default: {}."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:32
msgid "A call node for the call_tir_with_grad operator."
msgstr ""

#: of tvm.relax.op.unary.ceil:1
msgid "Take ceil of input data."
msgstr ""

#: of tvm.relax.op.unary.clip:1
msgid "Clips tensor values to a specified min and max."
msgstr ""

#: of tvm.relax.op.unary.clip:8
msgid "min"
msgstr ""

#: of tvm.relax.op.unary.clip:9
msgid "The minimum value"
msgstr ""

#: of tvm.relax.op.unary.clip:11
msgid "max"
msgstr ""

#: of tvm.relax.op.unary.clip:12
msgid "The maximum value"
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_like:1
msgid "Return a summation of data to the shape of collapse_target."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_like:3
msgid "For details, please see relax.op.collapse_sum_to."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:6 tvm.relax.op.ccl.ccl.allreduce:6
#: tvm.relax.op.distributed.distributed.annotate_sharding:6
#: tvm.relax.op.distributed.distributed.redistribute:6
#: tvm.relax.op.manipulate.collapse_sum_like:8
#: tvm.relax.op.manipulate.collapse_sum_to:17 tvm.relax.op.manipulate.repeat:6
#: tvm.relax.op.set.unique:10 tvm.relax.op.sorting.sort:7
msgid "The input tensor."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_like:10
msgid "collapse_target"
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_like:11
msgid "The tensor whose shape is the shape to collapse to."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_like:16
msgid "The result tensor after summation."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:1
msgid "Return a summation of data to the given shape."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:3
msgid ""
"collapse_sum_to is intended as the backward operator of "
"tvm.relax.op.broadcast_to and other broadcast operators in the automatic "
"differentiation process."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:6
msgid ""
"We expect that data is the result of broadcasting some tensor of the "
"given shape in some broadcast operation. Thus the given `shape` and "
"`data.shape` must follow broadcast rules."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:9
msgid ""
"During computation, all axes of `data.shape` and `shape` are checked from"
" right to left. For an axis, if it follows these rules, `data` will be "
"summed over this axis: - the axis exists in `data.shape` but not in "
"`shape`, or - the axis exists in `data.shape` and equals to 1 in `shape`."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:-1
msgid "Union[Tuple[PrimExprLike], relax.Expr]"
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:20
msgid "The shape to collapse to."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:25
msgid "The result tensor of the given shape after summation."
msgstr ""

#: of tvm.relax.op.manipulate.concat:1
msgid "Concatenate the input tensors along the given axis."
msgstr ""

#: of tvm.relax.op.manipulate.concat:5
msgid "tensors"
msgstr ""

#: of tvm.relax.op.manipulate.concat:-1
msgid "Union[relax.Expr, List[relax.Expr]]"
msgstr ""

#: of tvm.relax.op.manipulate.concat:6
msgid ""
"An Expr in Tuple type, containing the tensors to be concatenated, or a "
"list of Tensors."
msgstr ""

#: of tvm.relax.op.manipulate.concat:10
msgid ""
"The axis along which the tensors are concatenated. If `axis` is `None`, "
"the input tensor is required to be flattened before concatenation."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:58 tvm.relax.op.manipulate.concat:15
msgid "result: relax.Expr"
msgstr ""

#: of tvm.relax.op.manipulate.concat:16
msgid "The concatenated tensor."
msgstr ""

#: of tvm.relax.op.unary.cos:1
msgid "Compute element-wise cos of the input data."
msgstr ""

#: of tvm.relax.op.unary.cosh:1
msgid "Compute element-wise cosh of the input data."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:1
msgid ""
"Numpy style cumprod op. Return the cumulative product of the elements "
"along a given axis."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:10
msgid ""
"Axis along which the cumulative product is computed. The default (None) "
"is to compute the cumprod over the flattened array."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:14
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are computed. If dtype is not specified, it defaults to the dtype of "
"data."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:17 tvm.relax.op.statistical.cumsum:17
msgid "exclusive"
msgstr ""

#: of tvm.relax.op.statistical.cumprod:18
msgid ""
"If false (default), all elements are included in the product.  If true, "
"the first element is excluded from the product."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:24 tvm.relax.op.statistical.cumsum:24
msgid ""
"The result has the same size as data, and the same shape as data if axis "
"is not None. If axis is None, the result is a 1-d array."
msgstr ""

#: of tvm.relax.op.statistical.cumsum:1
msgid ""
"Numpy style cumsum op. Return the cumulative inclusive sum of the "
"elements along a given axis."
msgstr ""

#: of tvm.relax.op.statistical.cumsum:10
msgid ""
"Axis along which the cumulative sum is computed. The default (None) is to"
" compute the cumsum over the flattened array."
msgstr ""

#: of tvm.relax.op.statistical.cumsum:14
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are summed. If dtype is not specified, it defaults to the dtype of data."
msgstr ""

#: of tvm.relax.op.statistical.cumsum:18
msgid ""
"If false (default), all elements are included in the sum.  If true, the "
"first element is excluded from the sum."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:1
msgid ""
"Dequantize op This operator takes input and produces dequantized output. "
"The input tensor can be of any shape. The output shape is the same as "
"input shape."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:5
msgid ""
"output = clamp(scale * (input_tensor - zero_point), out_dtype::min, "
"out_dtype::max)"
msgstr ""

#: of tvm.relax.op.qdq.dequantize:-1 tvm.relax.op.qdq.quantize:-1
msgid "tvm.relax.Expr"
msgstr ""

#: of tvm.relax.op.qdq.dequantize:10
msgid "The input tensor to be dequantized."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:81 tvm.relax.op.nn.nn.group_norm:32
#: tvm.relax.op.nn.nn.layer_norm:39 tvm.relax.op.qdq.dequantize:12
#: tvm.relax.op.qdq.quantize:12
msgid "scale"
msgstr ""

#: of tvm.relax.op.qdq.dequantize:13
msgid "The input scale."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:15 tvm.relax.op.qdq.quantize:15
msgid "zero_point"
msgstr ""

#: of tvm.relax.op.qdq.dequantize:-1 tvm.relax.op.qdq.quantize:-1
msgid "tvm.relay.Expr"
msgstr ""

#: of tvm.relax.op.qdq.dequantize:16
msgid "The input zero_point."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:19
msgid ""
"The channel axis for dequantization. Default value is -1 which "
"corresponds to the last axis."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:52 tvm.relax.op.nn.nn.conv1d:58
#: tvm.relax.op.nn.nn.conv1d_transpose:47 tvm.relax.op.nn.nn.conv2d:58
#: tvm.relax.op.nn.nn.conv2d_transpose:57 tvm.relax.op.nn.nn.conv3d:60
#: tvm.relax.op.qdq.dequantize:21 tvm.relax.op.qdq.quantize:21
msgid "out_dtype"
msgstr ""

#: of tvm.relax.op.qdq.dequantize:-1 tvm.relax.op.qdq.quantize:-1
msgid "str, optional"
msgstr ""

#: of tvm.relax.op.qdq.dequantize:22 tvm.relax.op.qdq.quantize:22
#: tvm.relax.op.sampling.multinomial_from_uniform:30
msgid "The data type of the output tensor."
msgstr ""

#: of tvm.relax.op.binary.divide:1
msgid "Division with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:1
msgid ""
"Dynamic strided slice of a tensor. `begin`, `end`, `strides` can be "
"computed at runtime."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:6
#: tvm.relax.op.index.strided_slice:6
msgid "The source tensor to be sliced."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:8
#: tvm.relax.op.index.strided_slice:11
msgid "begin"
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:9
#: tvm.relax.op.index.strided_slice:12
msgid "The indices to begin with in the slicing, inclusive."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:12
#: tvm.relax.op.index.strided_slice:15
msgid "The indices indicating end of the slice, exclusive."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:14
#: tvm.relax.op.index.strided_slice:17 tvm.relax.op.nn.nn.avg_pool1d:22
#: tvm.relax.op.nn.nn.avg_pool2d:30 tvm.relax.op.nn.nn.avg_pool3d:23
#: tvm.relax.op.nn.nn.conv1d:34 tvm.relax.op.nn.nn.conv1d_transpose:20
#: tvm.relax.op.nn.nn.conv2d:34 tvm.relax.op.nn.nn.conv2d_transpose:30
#: tvm.relax.op.nn.nn.conv3d:36 tvm.relax.op.nn.nn.max_pool1d:22
#: tvm.relax.op.nn.nn.max_pool2d:29 tvm.relax.op.nn.nn.max_pool3d:23
msgid "strides"
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:15
#: tvm.relax.op.index.strided_slice:18
msgid ""
"Specifies the stride values, it can be negative in that case, the input "
"tensor will be reversed in that particular axis. If not specified, it by "
"default is an list of ones of the same length as `axes`."
msgstr ""

#: of tvm.relax.op.create.tril:17 tvm.relax.op.create.triu:17
#: tvm.relax.op.index.dynamic_strided_slice:21
#: tvm.relax.op.index.strided_slice:27 tvm.relax.op.index.take:21
#: tvm.relax.op.manipulate.flip:13 tvm.relax.op.manipulate.repeat:18
#: tvm.relax.op.manipulate.split:23 tvm.relax.op.manipulate.tile:22
#: tvm.relax.op.set.unique:33
msgid "ret"
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:22
#: tvm.relax.op.index.strided_slice:28
msgid "The sliced result."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:26
msgid ""
"dyn_strided_slice require the input `begin`, `end` and `strides` to have "
"the same length as rank of `data` tensor."
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:1
msgid "Evaluates the Einstein summation convention on data"
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:5
msgid "operands"
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:-1
msgid "Union(List[relax.Expr], Tuple[relax.Expr])"
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:6
msgid "A list of expression."
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:8
msgid "subscripts"
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:9
msgid "The einsum expression string."
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:14
msgid "The output from the einsum op."
msgstr ""

#: of tvm.relax.op.binary.equal:1
msgid "Broadcasted element-wise test for (lhs == rhs)."
msgstr ""

#: of tvm.relax.op.unary.erf:1
msgid "Computes the error function of the input."
msgstr ""

#: of tvm.relax.op.unary.erf:11
msgid "Computed error function for each element."
msgstr ""

#: of tvm.relax.op.ternary.ewise_fma:1
msgid ""
"Elementwise fused multiply-add operator Returns elementwise result of "
":math:`x1 * x2 + x3`"
msgstr ""

#: of tvm.relax.op.ternary.ewise_fma:7
msgid "The left hand operand of the multiplication"
msgstr ""

#: of tvm.relax.op.ternary.ewise_fma:10
msgid "The right hand operand of the multiplication"
msgstr ""

#: of tvm.relax.op.ternary.ewise_fma:12
msgid "x3"
msgstr ""

#: of tvm.relax.op.ternary.ewise_fma:13
msgid "The operand of the addition"
msgstr ""

#: of tvm.relax.op.unary.exp:1
msgid "Compute element-wise exp of data."
msgstr ""

#: of tvm.relax.op.manipulate.expand_dims:1
msgid "Insert new axes at the positions given by `axis`."
msgstr ""

#: of tvm.relax.op.manipulate.expand_dims:9
msgid ""
"The axes at which the input array are expanded. All values are required "
"to lie in range `[-data.ndim - 1, data.ndim]`, with the convention of "
"negative indexing."
msgstr ""

#: of tvm.relax.op.manipulate.expand_dims:16
msgid "The transformed result."
msgstr ""

#: of tvm.relax.op.manipulate.flatten:1
msgid "Flatten all the tensor dimensions into one."
msgstr ""

#: of tvm.relax.op.manipulate.flatten:11
msgid "The flattened result."
msgstr ""

#: of tvm.relax.op.manipulate.flip:1
msgid ""
"Reverses the order of elements along given axis while preserving array "
"shape."
msgstr ""

#: of tvm.relax.op.manipulate.flip:8
#: tvm.relax.op.manipulate.scatter_elements:32
#: tvm.relax.op.nn.nn.log_softmax:15 tvm.relax.op.nn.nn.softmax:10
msgid "axis: int"
msgstr ""

#: of tvm.relax.op.manipulate.flip:9
msgid "axis to flip on"
msgstr ""

#: of tvm.relax.op.unary.floor:1
msgid "Take floor of input data."
msgstr ""

#: of tvm.relax.op.binary.floor_divide:1
msgid "Floor division with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.create.full:1
msgid "Fill array with scalar value."
msgstr ""

#: of tvm.relax.op.create.full:6 tvm.relax.op.create.ones:6
#: tvm.relax.op.create.zeros:6
msgid "The shape of the created tensor."
msgstr ""

#: of tvm.relax.op.create.full:8 tvm.relax.op.create.full_like:11
msgid "fill_value"
msgstr ""

#: of tvm.relax.op.create.full:9 tvm.relax.op.create.full_like:12
msgid "The value to fill. Must be a scalar tensor."
msgstr ""

#: of tvm.relax.op.create.full:12
msgid ""
"The data type of the created tensor. If dtype is not given, it will by "
"default use the dtype of fill_value."
msgstr ""

#: of tvm.relax.op.create.full_like:1
msgid ""
"Construct a tensor such that - its shape is the same as the input data "
"tensor's shape, - its value is filled with the input scalar fill value."
msgstr ""

#: of tvm.relax.op.create.full_like:8 tvm.relax.op.create.ones_like:6
#: tvm.relax.op.create.zeros_like:6
msgid ""
"The input tensor, which provides the shape, and dtype when the `dtype` "
"field is not specified."
msgstr ""

#: of tvm.relax.op.create.full_like:15 tvm.relax.op.create.ones_like:10
#: tvm.relax.op.create.zeros_like:10
msgid ""
"The data type of the created tensor. If dtype is not given, it will by "
"default use the dtype of the input tensor."
msgstr ""

#: of tvm.relax.op.binary.greater:1
msgid "Broadcasted element-wise test for (lhs > rhs)."
msgstr ""

#: of tvm.relax.op.binary.greater_equal:1
msgid "Broadcasted element-wise test for (lhs >= rhs)."
msgstr ""

#: of tvm.relax.op.base.hint_on_device:1
msgid ""
"It provides a hint specifying the device on which the input data should "
"be executed. This hint is utilized by RealizeVDevice to propagate the "
"virtual device.\""
msgstr ""

#: of tvm.relax.op.base.hint_on_device:7 tvm.relax.op.base.to_vdevice:8
msgid "The tensor to be copied."
msgstr ""

#: of tvm.relax.op.base.hint_on_device:9 tvm.relax.op.base.to_vdevice:10
msgid "dst_device"
msgstr ""

#: of tvm.relax.op.base.hint_on_device:-1 tvm.relax.op.base.to_vdevice:-1
msgid "VDevice"
msgstr ""

#: of tvm.relax.op.base.hint_on_device:10
msgid "The destination device where the data is supposed to be executed."
msgstr ""

#: of tvm.relax.op.base.hint_on_device:15
msgid "The result."
msgstr ""

#: of tvm.relax.op.base.invoke_closure:1
msgid "Invoke a closure."
msgstr ""

#: of tvm.relax.op.base.invoke_closure:5
#: tvm.relax.op.base.invoke_pure_closure:11
msgid "closure"
msgstr ""

#: of tvm.relax.op.base.invoke_closure:6
#: tvm.relax.op.base.invoke_pure_closure:12
msgid "The VMClosure object."
msgstr ""

#: of tvm.relax.op.base.invoke_closure:11
#: tvm.relax.op.base.invoke_pure_closure:17
msgid "type_args: Union[List[StructInfo], StructInfo]"
msgstr ""

#: of tvm.relax.op.base.invoke_closure:12
#: tvm.relax.op.base.invoke_pure_closure:18
msgid "The structure info arguments of the CallNode"
msgstr ""

#: of tvm.relax.op.base.invoke_closure:17
msgid "A call to `invoke_closure`."
msgstr ""

#: of tvm.relax.op.base.invoke_pure_closure:1
msgid "Invoke a closure and indicate to the compiler that it is pure."
msgstr ""

#: of tvm.relax.op.base.invoke_pure_closure:3
msgid ""
"Note: This should be used for cases when the user knows that calling the "
"closure with these arguments will **in reality** not cause any side "
"effects. If it is used for a call that _does_ result in side effects, "
"then the compiler may end up removing, reordering, or repeating that "
"call, with no guarantees made about any side effects from the callee."
msgstr ""

#: of tvm.relax.op.base.invoke_pure_closure:23
msgid "A call to `invoke_pure_closure`."
msgstr ""

#: of tvm.relax.op.unary.isfinite:1
msgid "Check if input value is finite."
msgstr ""

#: of tvm.relax.op.unary.isinf:1
msgid "Check if input value is infinite."
msgstr ""

#: of tvm.relax.op.unary.isnan:1
msgid "Check if input value is Nan."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:1
msgid "Modifies the layout of a tensor."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:6
msgid "The input tensor to the operator."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:8
msgid "index_map"
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:-1
msgid "Union[Callable, IndexMap]"
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:9
msgid "The transformation to apply."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:11
msgid "pad_value"
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:-1
msgid "Optional[Union[int, float, PrimValue]]"
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:12
msgid ""
"The value used for padding if the transformation results in implicit "
"padding. If not specified, any value can be used."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:15
msgid "axis_separators"
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:-1
msgid "Optional[Union[int, IndexMap.AXIS_SEPARATOR]]"
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:16
msgid "The axis_separators for index_map to create non flat buffers."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:21
msgid "The transformed tensor."
msgstr ""

#: of tvm.relax.op.binary.less:1
msgid "Broadcasted element-wise test for (lhs < rhs)."
msgstr ""

#: of tvm.relax.op.binary.less_equal:1
msgid "Broadcasted element-wise test for (lhs <= rhs)."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:1
msgid "Applies a linear transformation to the incoming data: y = xA^T + b"
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:6
msgid "The input data."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:8 tvm.relax.op.nn.nn.conv1d:31
#: tvm.relax.op.nn.nn.conv1d_transpose:17 tvm.relax.op.nn.nn.conv2d:31
#: tvm.relax.op.nn.nn.conv2d_transpose:27 tvm.relax.op.nn.nn.conv3d:33
#: tvm.relax.op.nn.nn.rms_norm:15
msgid "weight"
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:9
msgid "The weight tensor."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:11 tvm.relax.op.nn.nn.rms_norm:18
msgid "bias"
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:-1
msgid "Optional[Expr]"
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:12
msgid "The bias tensor."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:14
#: tvm.relax.op.linear_algebra.matmul:14
msgid "out_dtype: Optional[Union[str, DataType]]"
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:15
#: tvm.relax.op.linear_algebra.matmul:15
msgid ""
"The data type of the matmul result. When it is not specified, the output "
"dtype will be the same as input dtype."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:19
#: tvm.relax.op.sampling.multinomial_from_uniform:5
msgid "Notes"
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:20
msgid ""
"Relax does not regard the Linear Op as a primitive Op, while combine the "
"transpose, matmul and add op to implement it."
msgstr ""

#: of tvm.relax.op.unary.log:1
msgid "Compute element-wise natural logarithm of the input data."
msgstr ""

#: of tvm.relax.op.binary.logical_and:1
msgid "Logical AND Parameters ---------- x1 : relax.Expr"
msgstr ""

#: of tvm.relax.op.unary.logical_not:1
msgid "Compute logical NOT of the input data."
msgstr ""

#: of tvm.relax.op.binary.logical_or:1
msgid "Logical OR Parameters ---------- x1 : relax.Expr"
msgstr ""

#: of tvm.relax.op.binary.logical_xor:1
msgid "Logical XOR Parameters ---------- x1 : relax.Expr"
msgstr ""

#: of tvm.relax.op.base.make_closure:1
msgid "Create a closure with free variables and return the closure."
msgstr ""

#: of tvm.relax.op.base.make_closure:6
msgid "The closure, can be ExternFunc or PrimFunc."
msgstr ""

#: of tvm.relax.op.base.make_closure:14
msgid "ret: Object"
msgstr ""

#: of tvm.relax.op.base.make_closure:15
msgid "The VMClosure."
msgstr ""

#: of tvm.relax.op.mask.masked_fill:1
msgid ""
"Fill a tensor by a specified value in places defined by a mask. "
"Parameters ---------- x : relax.Expr"
msgstr ""

#: of tvm.relax.op.mask.masked_fill:6
msgid "mask"
msgstr ""

#: of tvm.relax.op.mask.masked_fill:7
msgid "The mask."
msgstr ""

#: of tvm.relax.op.mask.masked_fill:8
msgid "value"
msgstr ""

#: of tvm.relax.op.mask.masked_fill:9
msgid "The value to set in the input tensor."
msgstr ""

#: of tvm.relax.op.mask.masked_fill:13
msgid "The filled tensor."
msgstr ""

#: of tvm.relax.op.linear_algebra.matmul:1
msgid ""
"General matrix multiplication of two tensors, with broadcasting on "
"batched dimensions."
msgstr ""

#: of tvm.relax.op.linear_algebra.matmul:3
msgid ""
"The semantics and output shape deduction rule is specified as https"
"://data-apis.org/array-"
"api/latest/API_specification/generated/array_api.matmul.html."
msgstr ""

#: of tvm.relax.op.statistical.max:1
msgid "Computes the max of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.max:-1 tvm.relax.op.statistical.mean:-1
#: tvm.relax.op.statistical.min:-1 tvm.relax.op.statistical.prod:-1
#: tvm.relax.op.statistical.std:-1 tvm.relax.op.statistical.sum:-1
#: tvm.relax.op.statistical.variance:-1
msgid "Optional[Union[int, List[int]]]"
msgstr ""

#: of tvm.relax.op.statistical.max:9
msgid ""
"Axis or axes along which a max operation is performed. The default, "
"axis=None, will compute the max of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.statistical.max:14 tvm.relax.op.statistical.mean:14
#: tvm.relax.op.statistical.min:14 tvm.relax.op.statistical.prod:14
#: tvm.relax.op.statistical.std:14 tvm.relax.op.statistical.sum:14
#: tvm.relax.op.statistical.variance:14
msgid ""
"If this is set to True, the axes which are reduced are left in the result"
" as dimensions with size one. With this option, the result will broadcast"
" correctly against the input tensor."
msgstr ""

#: of tvm.relax.op.binary.maximum:1
msgid "Element-wise maximum"
msgstr ""

#: of tvm.relax.op.statistical.mean:1
msgid "Computes the mean of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.mean:9
msgid ""
"Axis or axes along which a mean operation is performed. The default, "
"axis=None, will compute the mean of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.statistical.min:1
msgid "Computes the min of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.min:9
msgid ""
"Axis or axes along which a min operation is performed. The default, "
"axis=None, will compute the min of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.binary.minimum:1
msgid "Element-wise minimum"
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:1
msgid ""
"Returns a tensor where each row contains the index sampled from the "
"multinomial probability distribution located in the corresponding row of "
"tensor prob."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:6
msgid ""
"For better cpu performance, use 'vm.builtin.multinomial_from_uniform'. "
"For accurate results, ensure probabilities are between 0 and 1 and sum to"
" 1."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:11
msgid "prob"
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:12
msgid ""
"A 2-D tensor of shape (batch, vocab_size) representing probability "
"distributions. Each row is a distribution across vocabulary for a batch, "
"where: Values range from [0, 1], indicating the probability of each "
"vocabulary item. The sum of values in each row is 1, forming a valid "
"distribution."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:17
msgid "uniform_sample"
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:18
msgid ""
"The uniformly sampled 2-D tensor with the shape (n, 1). Values range from"
" 0 to 1, indicating probabilities sampled uniformly."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:21
msgid "sample_indices"
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:22
msgid ""
"The 2-D tensor with the shape [n, 1], which indicates the specific "
"probability distribution to sample from. The value of sample_indices[i] "
"determines that the ith token should be sampled from the "
"sample_indices[i]th probability distribution. For instance, if there are "
"3 distinct probability distributions and the requirement is to sample 2, "
"3, and 4 tokens from each, then sample_indices would be [0, 0, 1, 1, 1, "
"2, 2, 2, 2]."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:35
msgid "The computed tensor with shape (n, 1)."
msgstr ""

#: of tvm.relax.op.binary.multiply:1
msgid "Multiplication with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.unary.negative:1
msgid "Compute element-wise negative of the input data."
msgstr ""

#: of tvm.relax.op.unary.negative:11
msgid "The computed result"
msgstr ""

#: of tvm.relax.op.binary.not_equal:1
msgid "Broadcasted element-wise test for (lhs != rhs)."
msgstr ""

#: of tvm.relax.op.base.null_value:1
msgid "Create a call node that represents a null value object."
msgstr ""

#: of tvm.relax.op.create.ones:1
msgid "Construct a tensor of all ones, with the input shape and dtype."
msgstr ""

#: of tvm.relax.op.create.ones:-1 tvm.relax.op.create.zeros:-1
#: tvm.relax.op.datatype.wrap_param:-1
msgid "Union[str, DataType]"
msgstr ""

#: of tvm.relax.op.create.ones_like:1
msgid "Construct a tensor with all ones, with shape of the input tensor shape."
msgstr ""

#: of tvm.relax.op.manipulate.permute_dims:1
msgid "Permutes the dimensions of an array."
msgstr ""

#: of tvm.relax.op.index.strided_slice:8 tvm.relax.op.manipulate.permute_dims:8
#: tvm.relax.op.nn.nn.group_norm:23 tvm.relax.op.nn.nn.layer_norm:30
#: tvm.relax.op.nn.nn.rms_norm:21
msgid "axes"
msgstr ""

#: of tvm.relax.op.manipulate.permute_dims:-1
msgid "Optional[List[int]]"
msgstr ""

#: of tvm.relax.op.manipulate.permute_dims:9
msgid ""
"The target axes order. If not specified, permute_dims will reverse the "
"order of all axes."
msgstr ""

#: of tvm.relax.op.manipulate.permute_dims:14
msgid "The transposed result."
msgstr ""

#: of tvm.relax.op.binary.power:1
msgid "Power with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.base.print:1
msgid "Print op to print the values"
msgstr ""

#: of tvm.relax.op.base.print:5
msgid "values"
msgstr ""

#: of tvm.relax.op.base.print:-1
msgid "List[Expr]"
msgstr ""

#: of tvm.relax.op.base.print:6
msgid "The values to print."
msgstr ""

#: of tvm.relax.op.base.print:9
msgid "The format string or StringImm."
msgstr ""

#: of tvm.relax.op.base.print:14
msgid "A relax Call, which will print the value during runtime."
msgstr ""

#: of tvm.relax.op.statistical.prod:1
msgid "Computes the product of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.prod:9
msgid ""
"Axis or axes along which a product is performed. The default, axis=None, "
"will compute the product of all elements of the input tensor. Negative "
"indexing is supported."
msgstr ""

#: of tvm.relax.op.qdq.quantize:1
msgid ""
"Quantize op This operator takes input and produces quantized output. The "
"input tensor can be of any shape. The output shape is the same as input "
"shape."
msgstr ""

#: of tvm.relax.op.qdq.quantize:5
msgid ""
"Q_output = clamp((round(input_tensor/scale) + zero_point), "
"out_dtype::min, out_dtype::max)"
msgstr ""

#: of tvm.relax.op.qdq.quantize:10
msgid "The input tensor to be quantized."
msgstr ""

#: of tvm.relax.op.qdq.quantize:13
msgid "The output scale."
msgstr ""

#: of tvm.relax.op.qdq.quantize:16
msgid "The output zero_point."
msgstr ""

#: of tvm.relax.op.qdq.quantize:19
msgid ""
"The channel axis for quantization. Default value is -1 which corresponds "
"to the last axis."
msgstr ""

#: of tvm.relax.op.base.register_gradient:1
msgid "Register operator gradient function for a relax operator."
msgstr ""

#: of tvm.relax.op.base.register_gradient:5
msgid "op_name: str"
msgstr ""

#: of tvm.relax.op.base.register_gradient:6
msgid "The name of the op."
msgstr ""

#: of tvm.relax.op.base.register_gradient:8
msgid ""
"fgradient: function (orig_var: Var, orig_call: Call, output_grad: Var, "
"ctx: BlockBuilder)"
msgstr ""

#: of tvm.relax.op.base.register_gradient:9
msgid "-> partials: List[Expr]"
msgstr ""

#: of tvm.relax.op.base.register_gradient:10
msgid "The gradient function being used."
msgstr ""

#: of tvm.relax.op.base.register_gradient:12
msgid "level: int"
msgstr ""

#: of tvm.relax.op.base.register_gradient:13
msgid "The priority level"
msgstr ""

#: of tvm.relax.op.manipulate.repeat:1
msgid "Repeats elements of an array."
msgstr ""

#: of tvm.relax.op.manipulate.repeat:8 tvm.relax.op.manipulate.tile:17
msgid "repeats"
msgstr ""

#: of tvm.relax.op.manipulate.repeat:9
msgid "The number of repetitions."
msgstr ""

#: of tvm.relax.op.manipulate.repeat:11
msgid "axis: Optional[int]"
msgstr ""

#: of tvm.relax.op.manipulate.repeat:12
msgid ""
"The axis along which to repeat values. The negative numbers are "
"interpreted counting from the backward. By default, use the flattened "
"input array, and return a flat output array."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:1
msgid "Reshape the input array."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:3
msgid ""
"``-1`` infers the dimension of the output shape by using the remainder of"
" the input dimensions keeping the size of the new array same as that of "
"the input array. At most one dimension of shape can be -1."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:19
msgid "The new shape. Should be compatible with the original shape."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:24
msgid "The reshaped result."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:28
msgid ""
"The ``-1`` inference is only performed at compile-time. That is to say, "
"in any case the dimension length of ``-1`` cannot be inferred in compile-"
"time, an error will be thrown."
msgstr ""

#: of tvm.relax.op.unary.round:1
msgid "Rounds each element of the input data to nearest integer."
msgstr ""

#: of tvm.relax.op.unary.rsqrt:1
msgid "Compute element-wise reciprocal square root of the input data."
msgstr ""

#: of tvm.relax.op.unary.rsqrt:3
msgid "1/sqrt(x)"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:1
msgid ""
"ONNX style scatter elements. This operation updates its value in `data` "
"to values specified by `updates` at specific index positions specified by"
" `indices`. For example, in 2D tensor, the update corresponding to the "
"[i][j] entry is performed as below:"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:11
msgid ""
"When the `reduction` is set to some reduction function `f`, the update "
"corresponding to [i][j] entry is performed as below:"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:19
msgid "Where `f` is update, add, mul, mean, max, min."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:26
msgid "indices: relax.Expr"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:27
msgid "The index positions to update in `data`."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:29
msgid "updates: relax.Expr"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:30
msgid "Values to replace to."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:33
msgid "Axis to scatter on."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:35
msgid "reduction: str"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:36
msgid ""
"Type of reduction to apply: update, add, mul, mean, max, min. It is "
"\"update\" by default."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:42
msgid "The result has the same size as data, and the same shape as data"
msgstr ""

#: of tvm.relax.op.base.shape_of:1
msgid "Get shape of a tensor."
msgstr ""

#: of tvm.relax.op.base.shape_of:5
msgid "expr"
msgstr ""

#: of tvm.relax.op.base.shape_of:6
msgid "The input Expr."
msgstr ""

#: of tvm.relax.op.base.shape_of:11
msgid "A relax Call, which gets the shape of the input"
msgstr ""

#: of tvm.relax.op.base.shape_to_tensor:1
msgid "Convert shape to tensor expr. Parameters ---------- expr : Expr"
msgstr ""

#: of tvm.relax.op.base.shape_to_tensor:5 tvm.relax.op.base.tensor_to_shape:5
msgid "The input Expr"
msgstr ""

#: of tvm.relax.op.base.shape_to_tensor:9
msgid "A relax Call, which transforms the shape values to the tensor"
msgstr ""

#: of tvm.relax.op.unary.sigmoid:1
msgid "Compute element-wise sigmoid of the input data."
msgstr ""

#: of tvm.relax.op.unary.sign:1
msgid ""
"Returns an indication of the sign of a number for each element of the "
"input data."
msgstr ""

#: of tvm.relax.op.unary.sin:1
msgid "Compute element-wise sin of the input data."
msgstr ""

#: of tvm.relax.op.unary.sinh:1
msgid "Compute element-wise sinh of the input data."
msgstr ""

#: of tvm.relax.op.sorting.sort:1
msgid ""
"Performs sorting along the given axis and returns an array in sorted "
"order."
msgstr ""

#: of tvm.relax.op.sorting.sort:10
msgid ""
"Axis along which to sort the input tensor. By default the last axis of "
"the input is used."
msgstr ""

#: of tvm.relax.op.sorting.sort:19
msgid "Sorted tensor."
msgstr ""

#: of tvm.relax.op.manipulate.split:1
msgid "Split input tensor along axis by sections or indices."
msgstr ""

#: of tvm.relax.op.manipulate.split:3
msgid ""
"If indices_or_sections is an integer, the input will be divided equally "
"along given axis (if possible). Last section will be smaller if the "
"tensor size along the given dimension is not divisible by the integer."
msgstr ""

#: of tvm.relax.op.manipulate.split:7
msgid ""
"If indices_or_sections is a tuple of mixture of int or PrimExpr, the "
"entries indicate the indices where along axis the array is split."
msgstr ""

#: of tvm.relax.op.manipulate.split:13
msgid "The tensor to be split."
msgstr ""

#: of tvm.relax.op.manipulate.split:15
msgid "indices_or_sections"
msgstr ""

#: of tvm.relax.op.manipulate.split:-1
msgid "Union[int, List[PrimExprLike]]"
msgstr ""

#: of tvm.relax.op.manipulate.split:16
msgid "Indices or sections to split into. Accepts an int or a list."
msgstr ""

#: of tvm.relax.op.manipulate.split:19
msgid "The axis over which to split."
msgstr ""

#: of tvm.relax.op.unary.sqrt:1
msgid "Compute element-wise square root of the input data."
msgstr ""

#: of tvm.relax.op.unary.square:1
msgid "Squares each element of the input data."
msgstr ""

#: of tvm.relax.op.manipulate.squeeze:1
msgid "Squeeze axes in the array."
msgstr ""

#: of tvm.relax.op.manipulate.squeeze:-1
msgid "Optional[Union[int, List[int]]"
msgstr ""

#: of tvm.relax.op.manipulate.squeeze:9
msgid ""
"The set of axes to remove. If axis = None, remove all axis of dimensions "
"1. If any specified axis has dimension that does not equal 1, it is an "
"error."
msgstr ""

#: of tvm.relax.op.manipulate.squeeze:16
msgid "The squeezed result."
msgstr ""

#: of tvm.relax.op.statistical.std:1
msgid "Computes the standard deviation of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.std:9
msgid ""
"Axis or axes along which a standard deviation is performed. The default, "
"axis=None, will compute the std of all elements of the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.index.strided_slice:1
msgid "Strided slice of a tensor."
msgstr ""

#: of tvm.relax.op.index.strided_slice:-1
msgid "List[int]"
msgstr ""

#: of tvm.relax.op.index.strided_slice:9
msgid "Axes along which slicing is applied."
msgstr ""

#: of tvm.relax.op.index.strided_slice:-1
msgid "List[PrimExprLike]"
msgstr ""

#: of tvm.relax.op.index.strided_slice:-1
msgid "Optional[List[PrimExprLike]]"
msgstr ""

#: of tvm.relax.op.index.strided_slice:22
msgid "assume_inbound"
msgstr ""

#: of tvm.relax.op.index.strided_slice:23
msgid ""
"Whether to assume the indices are in bound. If it is set to false, out of"
" bound indices will be clipped to the bound."
msgstr ""

#: of tvm.relax.op.index.strided_slice:32
msgid ""
"strided_slice require the input `begin`, `end` and `strides` to have the "
"same length as `axes`."
msgstr ""

#: of tvm.relax.op.binary.subtract:1
msgid "Subtraction with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.statistical.sum:1
msgid "Computes the sum of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.sum:9
msgid ""
"Axis or axes along which a sum is performed. The default, axis=None, will"
" sum all of the elements of the input tensor. Negative indexing is "
"supported."
msgstr ""

#: of tvm.relax.op.index.take:1
msgid ""
"Take elements from a tensor along an axis. Its semantic is mostly similar"
" to `numpy.take` "
"(https://numpy.org/doc/stable/reference/generated/numpy.take.html), which"
" can cover `torch.take` "
"(https://pytorch.org/docs/stable/generated/torch.take.html) and "
"`onnx.gather` "
"(https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Gather-13)."
msgstr ""

#: of tvm.relax.op.index.take:10
msgid "The source tensor."
msgstr ""

#: of tvm.relax.op.index.take:12
msgid "indices"
msgstr ""

#: of tvm.relax.op.index.take:13
msgid "The indices of the values to extract."
msgstr ""

#: of tvm.relax.op.index.take:16
msgid ""
"The axis over which to select values. If it is none, the input tensor is "
"required to be one-dimensional."
msgstr ""

#: of tvm.relax.op.index.take:22
msgid "The taken result."
msgstr ""

#: of tvm.relax.op.unary.tan:1
msgid "Compute element-wise tan of the input data."
msgstr ""

#: of tvm.relax.op.unary.tanh:1
msgid "Compute element-wise tanh of the input data."
msgstr ""

#: of tvm.relax.op.base.tensor_to_shape:1
msgid "Convert tensor to shape expr. Parameters ---------- expr : Expr"
msgstr ""

#: of tvm.relax.op.base.tensor_to_shape:9
msgid "A relax Call, which transforms the tensor values to the shape"
msgstr ""

#: of tvm.relax.op.manipulate.tile:1
msgid "Construct an array by repeating data the number of times given by repeats."
msgstr ""

#: of tvm.relax.op.manipulate.tile:3
msgid ""
"If repeats has length l, and data has dimension d, the result will have "
"dimension of max(l, d)."
msgstr ""

#: of tvm.relax.op.manipulate.tile:5
msgid ""
"If d < l, data is promoted to be l-dimensional by prepending new axes. So"
" a shape (3,) Tensor is promoted to (1, 3) for 2-D replication, or shape "
"(1, 1, 3) for 3-D replication. If this is not the desired behavior, "
"promote data to d-dimensions manually before calling this function."
msgstr ""

#: of tvm.relax.op.manipulate.tile:9
msgid ""
"If d > l, reps is promoted to length d by pre-pending 1's to it. Thus for"
" a data of shape (2, 3, 4, 5), a reps of (2, 2) is treated as (1, 1, 2, "
"2)."
msgstr ""

#: of tvm.relax.op.manipulate.tile:-1
msgid "Union[int, Tuple[int], List[int]]"
msgstr ""

#: of tvm.relax.op.manipulate.tile:18
msgid "The number of repetitions of data along each axis."
msgstr ""

#: of tvm.relax.op.base.to_vdevice:1
msgid ""
"Copy data to the destination device. This operator helps data "
"transferring between difference devices for heterogeneous execution."
msgstr ""

#: of tvm.relax.op.base.to_vdevice:11
msgid "The destination device where the data is copied to."
msgstr ""

#: of tvm.relax.op.base.to_vdevice:16
msgid "The copied result."
msgstr ""

#: of tvm.relax.op.sorting.topk:1
msgid "Get the top k elements in an input tensor along the given axis."
msgstr ""

#: of tvm.relax.op.sorting.topk:3
msgid ""
"ret_type specifies the return type, can be one of (\"both\", \"values\", "
"\"indices\")."
msgstr ""

#: of tvm.relax.op.create.tril:9 tvm.relax.op.create.triu:9
#: tvm.relax.op.sorting.topk:10
msgid "k"
msgstr ""

#: of tvm.relax.op.sorting.topk:11
msgid "Number of top elements to select. Return all elements if k < 1."
msgstr ""

#: of tvm.relax.op.sorting.topk:16
msgid "ret_type: str"
msgstr ""

#: of tvm.relax.op.sorting.topk:17
msgid ""
"The return type [both, values, indices]. \"both\": return both top k data"
" and indices. \"values\": return top k data only. \"indices\": return top"
" k indices only."
msgstr ""

#: of tvm.relax.op.sorting.topk:22
msgid "largest"
msgstr ""

#: of tvm.relax.op.sorting.topk:23
msgid ""
"Whether to return largest or smallest elements. The k smallest elements "
"are returned if largest is False."
msgstr ""

#: of tvm.relax.op.sorting.topk:27
msgid "The data type of the indices output."
msgstr ""

#: of tvm.relax.op.sorting.topk:-1
msgid "relax.Expr or List[relax.Expr]"
msgstr ""

#: of tvm.relax.op.create.tril:1
msgid "Return the lower triangular part of a matrix or a batch of matrices."
msgstr ""

#: of tvm.relax.op.create.tril:6
msgid ""
"The tensor that tril will be applied to. It is required to have at least "
"two dimensions."
msgstr ""

#: of tvm.relax.op.create.tril:10
msgid ""
"The index indicating the diagonal above which to zero elements. If k = 0,"
" the diagonal is the main diagonal. If k < 0, the diagonal is below the "
"main diagonal. If k > 0, the diagonal is above the main diagonal."
msgstr ""

#: of tvm.relax.op.create.triu:1
msgid "Return the upper triangular part of a matrix or a batch of matrices."
msgstr ""

#: of tvm.relax.op.create.triu:6
msgid ""
"The tensor that triu will be applied to. It is required to have at least "
"two dimensions."
msgstr ""

#: of tvm.relax.op.create.triu:10
msgid ""
"The index indicating the diagonal below which to zero elements. If k = 0,"
" the diagonal is the main diagonal. If k < 0, the diagonal is below the "
"main diagonal. If k > 0, the diagonal is above the main diagonal."
msgstr ""

#: of tvm.relax.op.set.unique:1
msgid ""
"Find the unique elements in a given tensor. In addition, it optionally "
"returns - the indices of the input tensor that give the unique values; - "
"the indices of the unique tensor that reconstruct the input tensor; - the"
" number of times each unique value comes up in the input tensor."
msgstr ""

#: of tvm.relax.op.set.unique:12
msgid "sorted"
msgstr ""

#: of tvm.relax.op.set.unique:-1
msgid "Union[bool, Expr]"
msgstr ""

#: of tvm.relax.op.set.unique:13
msgid ""
"Whether to sort the unique elements in ascending order before returning "
"as output."
msgstr ""

#: of tvm.relax.op.set.unique:16
msgid "return_index"
msgstr ""

#: of tvm.relax.op.set.unique:17
msgid ""
"Whether to return an additional tensor with indices for where elements in"
" the unique tensor come from the original input."
msgstr ""

#: of tvm.relax.op.set.unique:20
msgid "return_inverse"
msgstr ""

#: of tvm.relax.op.set.unique:21
msgid ""
"Whether to return an additional tensor with indices for where elements in"
" the original input ended up in the returned unique list."
msgstr ""

#: of tvm.relax.op.set.unique:24
msgid "return_counts"
msgstr ""

#: of tvm.relax.op.set.unique:25
msgid ""
"Whether to return an additional tensor with counts of each unique "
"elements."
msgstr ""

#: of tvm.relax.op.set.unique:-1
msgid "Optional"
msgstr ""

#: of tvm.relax.op.set.unique:28
msgid ""
"The dimension to apply unique. If not specified, the unique values of the"
" flattened input are returned."
msgstr ""

#: of tvm.relax.op.set.unique:34
msgid "The created relax call with"
msgstr ""

#: of tvm.relax.op.statistical.variance:1
msgid "Computes the variance of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.variance:9
msgid ""
"Axis or axes along which a variance operation is performed. The default, "
"axis=None, will compute the variance of all elements in the input tensor."
" Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.search.where:1
msgid ""
"Selecting elements from either the input tensors depending on the value "
"of the condition."
msgstr ""

#: of tvm.relax.op.search.where:4
msgid ""
"For a given position, return the corresponding value in `x1` if "
"`condition` is True, and return the corresponding value in `x2` "
"otherwise."
msgstr ""

#: of tvm.relax.op.search.where:9
msgid "condition"
msgstr ""

#: of tvm.relax.op.search.where:10
msgid ""
"When True, yield `x1`; otherwise, yield `x2`. Must be broadcasting "
"compatible with `x1` and `x2`. Must have boolean dtype."
msgstr ""

#: of tvm.relax.op.search.where:15
msgid ""
"The first input tensor. Must be broadcasting compatible with `condition` "
"and `x2`."
msgstr ""

#: of tvm.relax.op.search.where:19
msgid ""
"The second input tensor. Must be broadcasting compatible with `condition`"
" and `x1`."
msgstr ""

#: of tvm.relax.op.datatype.wrap_param:1
msgid ""
"Cast input tensor which is model param to data type if the dtype of the "
"input data is not the same as the given dtype. Parameters ---------- data"
" : relax.Expr"
msgstr ""

#: of tvm.relax.op.create.zeros:1
msgid "Construct a tensor of all zeros, with the input shape and dtype."
msgstr ""

#: of tvm.relax.op.create.zeros_like:1
msgid "Construct a tensor with all zeros, with shape of the input tensor shape."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:28
msgid "tvm.relax.op.nn"
msgstr ""

#: of tvm.relax.op.nn:1
msgid "Neural network related operators."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:1
msgid "1D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation "
"across each window represented by W."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:7
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, in_channels, width)`, to produce an output Tensor "
"with shape (batch_size, in_channels, output_width)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:12
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:12
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:12
msgid ""
"The pooling kernel and stride sizes are automatically chosen for desired "
"output sizes."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:15
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:15
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:15
msgid "For output_size:"
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output width."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size) for any input (NCW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:27
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:30
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:30
msgid "output_size"
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:-1
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:-1
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:-1
msgid "Optional[Union[int, Tuple[int, int]]]"
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:28
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:31
msgid ""
"Output height and width. If not specified, it will be the same as the "
"input height and width. If specified, it is required to have length "
"either 1 or 2."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:26
#: tvm.relax.op.nn.nn.adaptive_avg_pool1d:32
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:35
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:35 tvm.relax.op.nn.nn.avg_pool1d:38
#: tvm.relax.op.nn.nn.avg_pool2d:46 tvm.relax.op.nn.nn.avg_pool3d:39
#: tvm.relax.op.nn.nn.max_pool1d:38 tvm.relax.op.nn.nn.max_pool2d:45
#: tvm.relax.op.nn.nn.max_pool3d:39
msgid "layout"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:27
#: tvm.relax.op.nn.nn.adaptive_avg_pool1d:33
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:36
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:36 tvm.relax.op.nn.nn.avg_pool1d:39
#: tvm.relax.op.nn.nn.avg_pool2d:47 tvm.relax.op.nn.nn.avg_pool3d:40
#: tvm.relax.op.nn.nn.conv1d:50 tvm.relax.op.nn.nn.conv1d_transpose:39
#: tvm.relax.op.nn.nn.conv2d:50 tvm.relax.op.nn.nn.conv2d_transpose:49
#: tvm.relax.op.nn.nn.conv3d:52 tvm.relax.op.nn.nn.max_pool1d:39
#: tvm.relax.op.nn.nn.max_pool2d:46 tvm.relax.op.nn.nn.max_pool3d:40
msgid "Layout of the input."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:35
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:38
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:38 tvm.relax.op.nn.nn.avg_pool1d:41
#: tvm.relax.op.nn.nn.avg_pool2d:49 tvm.relax.op.nn.nn.avg_pool3d:42
#: tvm.relax.op.nn.nn.conv1d:55 tvm.relax.op.nn.nn.conv1d_transpose:44
#: tvm.relax.op.nn.nn.conv2d:55 tvm.relax.op.nn.nn.conv2d_transpose:54
#: tvm.relax.op.nn.nn.conv3d:57 tvm.relax.op.nn.nn.max_pool1d:41
#: tvm.relax.op.nn.nn.max_pool2d:48 tvm.relax.op.nn.nn.max_pool3d:42
msgid "out_layout"
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:-1
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:-1
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:-1 tvm.relax.op.nn.nn.avg_pool1d:-1
#: tvm.relax.op.nn.nn.avg_pool2d:-1 tvm.relax.op.nn.nn.avg_pool3d:-1
#: tvm.relax.op.nn.nn.conv1d:-1 tvm.relax.op.nn.nn.conv1d_transpose:-1
#: tvm.relax.op.nn.nn.conv2d:-1 tvm.relax.op.nn.nn.conv2d_transpose:-1
#: tvm.relax.op.nn.nn.conv3d:-1 tvm.relax.op.nn.nn.max_pool1d:-1
#: tvm.relax.op.nn.nn.max_pool2d:-1 tvm.relax.op.nn.nn.max_pool3d:-1
msgid "Optional[str]"
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:36
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:39
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:39 tvm.relax.op.nn.nn.avg_pool1d:42
#: tvm.relax.op.nn.nn.avg_pool2d:50 tvm.relax.op.nn.nn.avg_pool3d:43
#: tvm.relax.op.nn.nn.conv1d:56 tvm.relax.op.nn.nn.conv1d_transpose:45
#: tvm.relax.op.nn.nn.conv2d:56 tvm.relax.op.nn.nn.conv2d_transpose:55
#: tvm.relax.op.nn.nn.conv3d:58 tvm.relax.op.nn.nn.max_pool1d:42
#: tvm.relax.op.nn.nn.max_pool2d:49 tvm.relax.op.nn.nn.max_pool3d:43
msgid "Layout of the output. If not specified, it is the same as data_layout"
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:1
msgid "2D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D average value calculation "
"across each window represented by WxH."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with shape (batch_size, in_channels, output_height, output_width)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output height and width."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size x output_size) for any input (NCHW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:22
msgid ""
"If a tuple of integers (height, width) are provided for output_size, the "
"output size is (N x C x height x width) for any input (NCHW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:1
msgid "3D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation "
"across each window represented by WxH."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, in_channels, depth, height, width)`, to produce an "
"output Tensor with shape (batch_size, in_channels, output_depth, "
"output_height, output_width)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:16
msgid ""
"If this argument is not provided, input depth, height and width will be "
"used as output depth, height and width."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size x output_size x output_size) for any input (NCDHW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:22
msgid ""
"If a tuple of integers (depth, height, width) are provided for "
"output_size, the output size is (N x C x depth x height x width) for any "
"input (NCDHW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:31
msgid ""
"Output height and width. If not specified, it will be the same as the "
"input height and width. If specified, it is required to have length "
"either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:1
msgid "Computes fused multi head attention."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:3
msgid "All input tensors are of 4-D tensors with BSNH layout."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:5
msgid ""
"FMA(Q, K, V) = \\text{Softmax}(Q @ K^T) @ V\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:9
msgid "The input tensor is required to have float16 dtype"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:13
msgid "query: relax.Expr"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:14
msgid ""
"The input query to the operator. The layout of the input query should be "
"(batch_size, seq_len, num_head, head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:17
msgid "key: relax.Expr"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:18
msgid ""
"The input key to the operator. The layout of the input key should be "
"(batch_size, seq_len_kv, num_head, head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:21
msgid "value: relax.Expr"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:22
msgid ""
"The input value to the operator. The layout of the input value should be "
"(batch_size, seq_len_kv, num_head, head_dim_v)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:25
msgid "bias: Optional[Expr]"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:26
msgid ""
"The optional attention bias to the operator. The layout of the attention "
"bias should be a 4-D tensor ending with seq_len_kv, and broadcastable to "
"(batch_size, num_head, seq_len, seq_len_kv)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:30 tvm.relax.op.nn.nn.attention_var_len:37
msgid "scale: Optional[float]"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:31 tvm.relax.op.nn.nn.attention_var_len:38
msgid ""
"The scale value to be applied to the attention score, by default 1 / "
"sqrt(head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:33 tvm.relax.op.nn.nn.attention_var_len:40
msgid "causal_mask: Optional[str]"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:34 tvm.relax.op.nn.nn.attention_var_len:41
msgid ""
"The optional causal mask, i.e. 'TopLeft' and 'BottomRight'. For "
"'TopLeft', the mask matrix is as `np.tril(*, k=0)`, while for "
"'BottomRight', the mask matrix is as `np.tril(*, k=abs(seq_len - "
"seq_len_kv))` For example, with seq_len = 4, seq_len_kv = 2, mask for "
"'TopLeft':"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:47 tvm.relax.op.nn.nn.attention:64
#: tvm.relax.op.nn.nn.attention_var_len:54
#: tvm.relax.op.nn.nn.attention_var_len:71
msgid "mask for 'BottomRight':"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:56 tvm.relax.op.nn.nn.attention_var_len:63
msgid "with seq_len = 2, seq_len_kv = 4, mask for 'TopLeft':"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:71 tvm.relax.op.nn.nn.attention_var_len:78
msgid "window_size: Optional[int]"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:72 tvm.relax.op.nn.nn.attention_var_len:79
msgid "The size of the window for sliding-window attention."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:77
msgid ""
"The computed result. The layout of the output should be (batch_size, "
"seq_len, num_head, head_dim_v)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:1
msgid ""
"Computes fused multi head attention over batched sequences of variable "
"lengths."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:3
msgid ""
"Given concatenated inputs and sequence lengths information, this operator"
" computes attention for all sequences more efficiently than calling the "
"normal attention operator for each sequence individually."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:9
msgid "queries: relax.Expr"
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:10
msgid ""
"The input queries concatenated along the second axis. Its shape must be "
"(1, total_seq_len, num_head, head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:13
msgid "keys: relax.Expr"
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:14
msgid ""
"The input keys concatenated along the second axis. Its shape must be (1, "
"total_seq_len_kv, num_head, head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:17
msgid "values: relax.Expr"
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:18
msgid ""
"The input values concatenated along the second axis. Its shape must be "
"(1, total_seq_len_kv, num_head, head_dim_v)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:21
msgid "seqstart_q: Optional[Expr]"
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:22
msgid ""
"The cumsum of query sequence lengths, prepended with 0. Its dtype must be"
" int32. For example, if the lengths of the sequences that are batched are"
" [2, 5, 3], this tensor has values [0, 2, 7, 10]."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:26
msgid "seqstart_k: Optional[Expr]"
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:27
msgid ""
"The cumsum of key sequence lengths, prepended with 0. By default it is "
"the same as seqstart_q."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:30
msgid "max_seqlen_q: Optional[Expr]"
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:31
msgid "The maximum query sequence length in the batch. It must be int32."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:33
msgid "max_seqlen_k: Optional[Expr]"
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:34
msgid ""
"The maximum key sequence length in the batch. It must be int32. By "
"default it is the same as max_seqlen_q."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:84
msgid "The computed result with shape `(1, total_seq_len, num_head, head_dim_v)`."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:1
msgid "1D average pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:6
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, channels, width)`, to produce an output Tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:10 tvm.relax.op.nn.nn.avg_pool3d:11
#: tvm.relax.op.nn.nn.max_pool1d:10 tvm.relax.op.nn.nn.max_pool3d:11
msgid ""
"The ceil_mode is used to take ceil or floor while computing out shape. "
"count_include_pad indicates including or excluding padded input values in"
" computation. This operator accepts data layout specification."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:19 tvm.relax.op.nn.nn.avg_pool2d:27
#: tvm.relax.op.nn.nn.avg_pool3d:20 tvm.relax.op.nn.nn.max_pool1d:19
#: tvm.relax.op.nn.nn.max_pool2d:26 tvm.relax.op.nn.nn.max_pool3d:20
msgid "pool_size"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:-1 tvm.relax.op.nn.nn.conv1d:-1
#: tvm.relax.op.nn.nn.conv1d_transpose:-1
msgid "Union[int, Tuple[int]]"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:20
msgid "The size of window for pooling. It is required to have length is 1."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:23
msgid "The strides of pooling. It is required to have length is 1."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:25 tvm.relax.op.nn.nn.avg_pool2d:33
#: tvm.relax.op.nn.nn.avg_pool3d:26 tvm.relax.op.nn.nn.conv1d:37
#: tvm.relax.op.nn.nn.conv1d_transpose:23 tvm.relax.op.nn.nn.conv2d:37
#: tvm.relax.op.nn.nn.conv2d_transpose:33 tvm.relax.op.nn.nn.conv3d:39
#: tvm.relax.op.nn.nn.max_pool1d:25 tvm.relax.op.nn.nn.max_pool2d:32
#: tvm.relax.op.nn.nn.max_pool3d:26
msgid "padding"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:-1 tvm.relax.op.nn.nn.avg_pool2d:-1
#: tvm.relax.op.nn.nn.conv1d:-1 tvm.relax.op.nn.nn.conv2d:-1
#: tvm.relax.op.nn.nn.conv2d_transpose:-1 tvm.relax.op.nn.nn.max_pool1d:-1
#: tvm.relax.op.nn.nn.max_pool2d:-1 tvm.relax.op.nn.nn.max_pool3d:-1
msgid "Union[int, Tuple[int, int]]"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:26 tvm.relax.op.nn.nn.max_pool1d:26
msgid "The padding for pooling. It is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:28 tvm.relax.op.nn.nn.avg_pool2d:36
#: tvm.relax.op.nn.nn.avg_pool3d:29 tvm.relax.op.nn.nn.conv1d:41
#: tvm.relax.op.nn.nn.conv1d_transpose:30 tvm.relax.op.nn.nn.conv2d:41
#: tvm.relax.op.nn.nn.conv2d_transpose:40 tvm.relax.op.nn.nn.conv3d:43
#: tvm.relax.op.nn.nn.max_pool1d:28 tvm.relax.op.nn.nn.max_pool2d:35
#: tvm.relax.op.nn.nn.max_pool3d:29
msgid "dilation"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:29
msgid "The dilation of pooling. It is required to have length is 1."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:31 tvm.relax.op.nn.nn.avg_pool2d:39
#: tvm.relax.op.nn.nn.avg_pool3d:32 tvm.relax.op.nn.nn.max_pool1d:31
#: tvm.relax.op.nn.nn.max_pool2d:38 tvm.relax.op.nn.nn.max_pool3d:32
msgid "ceil_mode"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:32 tvm.relax.op.nn.nn.avg_pool2d:40
#: tvm.relax.op.nn.nn.avg_pool3d:33 tvm.relax.op.nn.nn.max_pool1d:32
#: tvm.relax.op.nn.nn.max_pool2d:39 tvm.relax.op.nn.nn.max_pool3d:33
msgid ""
"A boolean indicating if use ceil or floor to compute the output shape. By"
" using ceil, every element in the input tensor will be covered by a "
"sliding window."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:35 tvm.relax.op.nn.nn.avg_pool2d:43
#: tvm.relax.op.nn.nn.avg_pool3d:36 tvm.relax.op.nn.nn.max_pool1d:35
#: tvm.relax.op.nn.nn.max_pool2d:42 tvm.relax.op.nn.nn.max_pool3d:36
msgid "count_include_pad"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:-1 tvm.relax.op.nn.nn.avg_pool2d:-1
#: tvm.relax.op.nn.nn.avg_pool3d:-1 tvm.relax.op.nn.nn.max_pool1d:-1
#: tvm.relax.op.nn.nn.max_pool2d:-1 tvm.relax.op.nn.nn.max_pool3d:-1
msgid "bool, optional"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:36 tvm.relax.op.nn.nn.avg_pool2d:44
#: tvm.relax.op.nn.nn.avg_pool3d:37 tvm.relax.op.nn.nn.max_pool1d:36
#: tvm.relax.op.nn.nn.max_pool2d:43 tvm.relax.op.nn.nn.max_pool3d:37
msgid "To include padding to compute the average."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:1 tvm.relax.op.nn.nn.avg_pool3d:1
msgid "2D average pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D avarage value calculation "
"with in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:6 tvm.relax.op.nn.nn.max_pool2d:6
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with the following rule:"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:10 tvm.relax.op.nn.nn.max_pool2d:10
msgid "with data of shape (b, c, h, w) and pool_size (kh, kw)"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:12
msgid ""
"\\mbox{out}(b, c, y, x)  = \\frac{1}{kh * kw} \\sum_{m=0, \\ldots, kh-1}\n"
"    \\sum_{n=0, \\ldots, kw-1}\n"
"    \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x +"
" n)"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:18 tvm.relax.op.nn.nn.max_pool2d:17
msgid ""
"Padding is applied to data before the computation. ceil_mode is used to "
"take ceil or floor while computing out shape. This operator accepts data "
"layout specification."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:28 tvm.relax.op.nn.nn.max_pool2d:27
msgid ""
"The size of window for pooling. It is required to have length either 1 or"
" 2."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:31 tvm.relax.op.nn.nn.max_pool2d:30
msgid "The strides of pooling. It is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:-1 tvm.relax.op.nn.nn.avg_pool3d:-1
#: tvm.relax.op.nn.nn.conv1d:-1 tvm.relax.op.nn.nn.conv1d_transpose:-1
#: tvm.relax.op.nn.nn.conv2d:-1 tvm.relax.op.nn.nn.conv2d_transpose:-1
#: tvm.relax.op.nn.nn.conv3d:-1 tvm.relax.op.nn.nn.max_pool1d:-1
#: tvm.relax.op.nn.nn.max_pool2d:-1 tvm.relax.op.nn.nn.max_pool3d:-1
msgid "Union[int, Tuple[int, ...]]"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:34 tvm.relax.op.nn.nn.max_pool2d:33
msgid "The padding for pooling. It is required to have length either 1, 2 or 4."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:37 tvm.relax.op.nn.nn.max_pool2d:36
msgid "The dilation of pooling. It is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:7 tvm.relax.op.nn.nn.max_pool3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, channels, depth, height, width)`, to produce an "
"output Tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:-1 tvm.relax.op.nn.nn.conv3d:-1
msgid "Union[int, Tuple[int, int, int]]"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:21 tvm.relax.op.nn.nn.max_pool3d:21
msgid ""
"The size of window for pooling. It is required to have length either 1 or"
" 3."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:24 tvm.relax.op.nn.nn.max_pool3d:24
msgid "The strides of pooling. It is required to have length either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:27 tvm.relax.op.nn.nn.max_pool3d:27
msgid "The padding for pooling. It is required to have length either 1, 3 or 6."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:30 tvm.relax.op.nn.nn.max_pool3d:30
msgid "The dilation of pooling. It is required to have length either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:1
msgid "Batch normalization layer (Ioffe and Szegedy, 2014)."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:3
msgid ""
"Normalizes the input at each batch, i.e. applies a transformation that "
"maintains the mean activation close to 0 and the activation standard "
"deviation close to 1."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:7
msgid ""
"data\\_mean[i] = mean(data[:,i,:,...]) \\\\\n"
"data\\_var[i] = var(data[:,i,:,...])"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:12
msgid "Both *mean* and *var* returns a scalar by treating the input as a vector."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:14
msgid ""
"Then compute the normalized output, which has the same shape as input, as"
" following:"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:16
msgid ""
"out[:,i,:,...] = \\frac{data[:,i,:,...] - "
"data\\_mean[i]}{\\sqrt{data\\_var[i]+\\epsilon}}\n"
"    * gamma[i] + beta[i]"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:21
msgid ""
"Assume the input has size *k* on axis 1, then both ``gamma`` and ``beta``"
" have shape *(k,)*."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:24
msgid ""
"Besides the inputs and the outputs, this operator accepts two auxiliary "
"states, ``moving_mean`` and ``moving_var``, which are *k*-length vectors."
" They are global statistics for the whole dataset, which are updated by"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:33
msgid ""
"The parameter ``axis`` specifies which axis of the input shape denotes "
"the 'channel' (separately normalized groups).  The default is 1. "
"Specifying -1 sets the channel axis to be the last item in the input "
"shape."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:39
msgid "This operator has two modes:"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:41
msgid "Training mode."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:42
msgid "Use the mean and var computed from THIS batch to normalize."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:43
msgid "Update and then return the running mean and running var."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:45
msgid "Inference mode."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:46
msgid "Use the running_mean and running_var parameters to normalize."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:47
msgid ""
"Do not update the running mean and running var. Just return the original "
"value."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:49
msgid ""
"In the legalization stage, this operator will be legalized to the "
"training mode by default."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:51
msgid ""
"You can use tvm.relax.transform.DecomposeOpsForInference to decompose the"
" operator, so it executes the inference mode computation. Similarly, use "
"tvm.relax.transform.DecomposeOpsForTraining to execute the training mode "
"computation."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:60 tvm.relax.op.nn.nn.group_norm:11
#: tvm.relax.op.nn.nn.layer_norm:24
msgid "gamma"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:61 tvm.relax.op.nn.nn.group_norm:12
#: tvm.relax.op.nn.nn.layer_norm:25
msgid "The gamma scale factor."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:63 tvm.relax.op.nn.nn.group_norm:14
#: tvm.relax.op.nn.nn.layer_norm:27
msgid "beta"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:64 tvm.relax.op.nn.nn.group_norm:15
#: tvm.relax.op.nn.nn.layer_norm:28
msgid "The beta offset factor."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:66
msgid "moving_mean"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:67
msgid "Running mean of input."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:69
msgid "moving_var"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:70
msgid "Running variance of input."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:73
msgid "The axis along which the normalization is applied."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:75 tvm.relax.op.nn.nn.group_norm:26
#: tvm.relax.op.nn.nn.layer_norm:33 tvm.relax.op.nn.nn.rms_norm:24
msgid "epsilon"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:-1 tvm.relax.op.nn.nn.dropout:-1
#: tvm.relax.op.nn.nn.group_norm:-1 tvm.relax.op.nn.nn.layer_norm:-1
#: tvm.relax.op.nn.nn.rms_norm:-1
msgid "float"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:76 tvm.relax.op.nn.nn.group_norm:27
#: tvm.relax.op.nn.nn.layer_norm:34
msgid "Small float added to variance to avoid dividing by zero."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:78 tvm.relax.op.nn.nn.group_norm:29
#: tvm.relax.op.nn.nn.layer_norm:36
msgid "center"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:79 tvm.relax.op.nn.nn.group_norm:30
#: tvm.relax.op.nn.nn.layer_norm:37
msgid "Indicating if the beta offset will be added to the normalized tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:82 tvm.relax.op.nn.nn.group_norm:33
#: tvm.relax.op.nn.nn.layer_norm:40
msgid "Indicating if the gamma scale will be multiplied."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:84
msgid "momentum"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:85
msgid "The value used for the moving_mean and moving_var update."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:1
msgid "1D convolution."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:3
msgid ""
"This operator takes the weight as the 1D convolution kernel and convolves"
" it with data to produce an output."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:7
msgid ""
"In the default case, where the data_layout is `NCW` and kernel_layout is "
"`OIW`, conv1d takes in a data Tensor with shape `(batch_size, "
"in_channels, width)`, and a weight Tensor with shape `(channels, "
"in_channels, kernel_w)`, where `kernel_w` is the length of the `W` kernel"
" dimension, to produce an output Tensor with the following rule:"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:14
msgid ""
"\\mbox{out}[b, c, x] = \\sum_{dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides} * x + dx] *\n"
"   \\mbox{weight}[c, k, dx]"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:20
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCW` for data and `OIW` for weight), perform the computation, "
"then convert to the out_layout."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:32 tvm.relax.op.nn.nn.conv1d_transpose:18
#: tvm.relax.op.nn.nn.conv2d:32 tvm.relax.op.nn.nn.conv2d_transpose:28
#: tvm.relax.op.nn.nn.conv3d:34
msgid "The weight expressions."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:35 tvm.relax.op.nn.nn.conv1d_transpose:21
msgid "The strides of convolution. It is required to have length 1."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:38 tvm.relax.op.nn.nn.conv1d_transpose:24
msgid ""
"The padding of convolution on both sides of inputs before convolution. It"
" is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:42
msgid ""
"Specifies the dilation rate to be used for dilated convolution. It is "
"required to have length 1."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:45 tvm.relax.op.nn.nn.conv1d_transpose:34
#: tvm.relax.op.nn.nn.conv2d:45 tvm.relax.op.nn.nn.conv2d_transpose:44
#: tvm.relax.op.nn.nn.conv3d:47
msgid "groups"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:46 tvm.relax.op.nn.nn.conv1d_transpose:35
#: tvm.relax.op.nn.nn.conv2d:46 tvm.relax.op.nn.nn.conv2d_transpose:45
#: tvm.relax.op.nn.nn.conv3d:48
msgid ""
"Number of groups to split the input into for grouped convolution. The "
"number of input and output channels should be divisible by the number of "
"groups."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:49 tvm.relax.op.nn.nn.conv1d_transpose:38
#: tvm.relax.op.nn.nn.conv2d:49 tvm.relax.op.nn.nn.conv2d_transpose:48
#: tvm.relax.op.nn.nn.conv3d:51
msgid "data_layout"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:52 tvm.relax.op.nn.nn.conv1d_transpose:41
#: tvm.relax.op.nn.nn.conv2d:52 tvm.relax.op.nn.nn.conv2d_transpose:51
#: tvm.relax.op.nn.nn.conv3d:54
msgid "kernel_layout"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:53 tvm.relax.op.nn.nn.conv1d_transpose:42
#: tvm.relax.op.nn.nn.conv2d:53 tvm.relax.op.nn.nn.conv2d_transpose:52
#: tvm.relax.op.nn.nn.conv3d:55
msgid "Layout of the weight."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:59
msgid "Specifies the output data type for mixed precision conv1d."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:1
msgid "1D transposed convolution operator."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:3
msgid "This operator can be seen as the gradient operator of conv1d."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:5
#, python-format
msgid ""
"The output shape can be explained in the simple case when `data_layout =="
" \"NCW\"` and `kernel_layout == \"IOW\"`. Suppose `data` has shape `(N, "
"in_channel, in_w)`, `weight` has shape `(in_channel, out_channel, "
"weight_w)`, we need to assure that `in_channel % groups == 0`. The shape "
"of the output will be `(N, out_channel * groups, out_w)`, where"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:10
msgid ""
"`out_w = ((in_w - 1) * strides[0] + weight_w - 2 * padding[0] + "
"output_padding[0])`"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:27
#: tvm.relax.op.nn.nn.conv2d_transpose:37
msgid "output_padding"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:-1
#: tvm.relax.op.nn.nn.conv2d_transpose:-1
msgid "Union[int, Tuple[int, ...]], optional"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:28
#: tvm.relax.op.nn.nn.conv2d_transpose:38
msgid "Used to disambiguate the output shape."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:31
msgid ""
"Specifies the dilation rate to be used for dilated convolution. It is "
"required to have length either 1."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:48 tvm.relax.op.nn.nn.conv2d:59
#: tvm.relax.op.nn.nn.conv2d_transpose:58 tvm.relax.op.nn.nn.conv3d:61
msgid "Specifies the output data type for mixed precision conv2d."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:1
msgid "2D convolution."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:3 tvm.relax.op.nn.nn.conv3d:3
msgid ""
"This operator takes the weight as the convolution kernel and convolves it"
" with data to produce an output."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` and kernel_layout is"
" `OIHW`, conv2d takes in a data Tensor with shape `(batch_size, "
"in_channels, height, width)`, and a weight Tensor with shape `(channels, "
"in_channels, kernel_h, kernel_w)`, where `kernel_h` and `kernel_w` is the"
" lengths of the `H` and `W` kernel dimensions, to produce an output "
"Tensor with the following rule:"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:14
msgid ""
"\\mbox{out}[b, c, y, x] = \\sum_{dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * y  + dy, \\mbox{strides}[1] * "
"x + dx] *\n"
"   \\mbox{weight}[c, k, dy, dx]"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:20
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCHW` for data and `OIHW` for weight), perform the computation, "
"then convert to the out_layout."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:35 tvm.relax.op.nn.nn.conv2d_transpose:31
msgid "The strides of convolution. It is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:38 tvm.relax.op.nn.nn.conv2d_transpose:34
msgid ""
"The padding of convolution on both sides of inputs before convolution. It"
" is required to have length either 1, 2 or 4."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:42 tvm.relax.op.nn.nn.conv2d_transpose:41
msgid ""
"Specifies the dilation rate to be used for dilated convolution. It is "
"required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:1
msgid "Two dimensional transposed convolution operator."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:3
msgid ""
"This operator is intended to be the gradient operator of conv2d. That "
"means, if"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:5
msgid "`out = conv2d(data, weight, strides, padding, dilation)`,"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:7
msgid "The gradient w.r.t. data can be calculated as follows:"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:9
msgid ""
"`data_grad = conv2d_transpose(out_grad, weight, strides, padding, "
"output_padding, dilation)`,"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:11
msgid "where `output_padding` is a parameter used to determine the output shape."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:13
#, python-format
msgid ""
"The output shape can be explained in the simple case when `data_layout =="
" \"NCHW\"` and `kernel_layout == \"IOHW\"`. Suppose `data` has shape `(N,"
" in_channel, in_h, in_w)`, `weight` has shape `(in_channel, out_channel, "
"weight_h, weight_w)`, we need to assure that `in_channel % groups == 0`. "
"The shape of the output will be `(N, out_channel * groups, out_h, "
"out_w)`, where"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:19
msgid ""
"`out_h = ((in_h - 1) * strides[0] + weight_h - 2 * padding[0] + "
"output_padding[0])`"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:20
msgid ""
"`out_w = ((in_w - 1) * strides[1] + weight_w - 2 * padding[1] + "
"output_padding[1])`"
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:1
msgid "3D convolution."
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` and kernel_layout "
"is `OIDHW`, conv3d takes in a data Tensor with shape `(batch_size, "
"in_channels, depth, height, width)`, and a weight Tensor with shape "
"`(channels, in_channels, kernel_d, kernel_h, kernel_w)`, where "
"`kernel_d`, `kernel_h`, and `kernel_w` are the lengths of the `D`, `H`, "
"and `W` kernel dimensions, to produce an output Tensor with the following"
" rule:"
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:14
msgid ""
"\\mbox{out}[b, c, z, y, x] = \\sum_{dz, dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * z + dz,\n"
"   \\mbox{strides}[1] * y  + dy,\n"
"   \\mbox{strides}[2] * x + dx] *\n"
"   \\mbox{weight}[c, k, dz, dy, dx]"
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:22
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCDHW` for data and `OIDHW` for weight), perform the "
"computation, then convert to the out_layout."
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:37
msgid "The strides of convolution. It is required to have length either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:40
msgid ""
"The padding of convolution on both sides of inputs before convolution. It"
" is required to have length either 1, 3 or 6."
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:44
msgid ""
"Specifies the dilation rate to be used for dilated convolution. It is "
"required to have length either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:1
msgid "CrossEntropy with logits between the predictions and labels."
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:3
msgid ""
"The shape of predictions and labels must be the same. And when ndim >= 2,"
" the first dimension is regarded as the batch_size N. In this case the "
"computed result will divide by N to perform a mean reduction."
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:7
msgid ""
"\\text{cross\\_entropy\\_with\\_logits}(x_i, y_i) = \\frac{\\sum_i -x_i "
"\\cdot y_i}{N}"
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:13
#: tvm.relax.op.nn.nn.nll_loss:12
msgid "predictions"
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:14
msgid "The predictions."
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:16
msgid "labels"
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:17
msgid "The labels (the ground truth values)."
msgstr ""

#: of tvm.relax.op.nn.nn.dropout:1
msgid "Applies the dropout operation to the input tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.dropout:3
msgid ""
"During training, each element of the input is set to zero with "
"probability ``p``. The whole array is scaled by ``1/(1-p)`` to keep the "
"expected sum of the input unchanged."
msgstr ""

#: of tvm.relax.op.nn.nn.dropout:12
msgid "rate"
msgstr ""

#: of tvm.relax.op.nn.nn.dropout:13
msgid "The probability for an element to be reset to 0."
msgstr ""

#: of tvm.relax.op.nn.nn.dropout:18
msgid ""
"The result of dropout, which is a tuple of two tensors. The first one is "
"the original tensor and the second one is a mask tensor (1.0 where "
"element not dropped, 0.0 where dropped)"
msgstr ""

#: of tvm.relax.op.nn.nn.gelu:1
msgid "Gaussian Error Linear Units function"
msgstr ""

#: of tvm.relax.op.nn.nn.gelu:3
msgid ""
"\\text{GeLU}(x) = 0.5 * x * (1 + \\text{erf}(x * 0.5**0.5))\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.gelu:6
msgid "where :math:`erf` is the Gauss Error function."
msgstr ""

#: of tvm.relax.op.nn.nn.gelu_tanh:1
msgid "Gaussian Error Linear Units function with tanh approximation"
msgstr ""

#: of tvm.relax.op.nn.nn.gelu_tanh:3
msgid ""
"\\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt(2 / \\pi) * (x + "
"0.044715 * x^3)))\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:1
msgid ""
"Group normalization (Yuxin Wu and et al., 2016). Applies group "
"normalization to the n-dimensional input array. This operator takes an "
"n-dimensional input array. First separate the input array into groups "
"along the channel axis. Then apply layer normalization to each group."
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:9
msgid "Input to which group_norm will be applied."
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:17
msgid "num_groups"
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:18
msgid "Number of groups to separate the channels into."
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:20
msgid "channel_axis"
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:21
msgid "The index of the channel axis in the input data."
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:24
msgid ""
"The axes that along which the normalization is applied (excluding the "
"group axis)"
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:1
msgid ""
"Layer normalization (Lei Ba and et al., 2016). Applies layer "
"normalization to the n-dimensional input array. This operator takes an "
"n-dimensional input array and normalizes the input using the given axis:"
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:6
msgid ""
"out = \\frac{data - mean(data, axis)}{\\sqrt{var(data, axis)+\\epsilon}}\n"
"    * gamma + beta"
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:11
msgid ""
"Unlike batch normalization, the mean and var are computed along the "
"channel dimension."
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:13
msgid ""
"Assume the input has size k on axis 1, then both gamma and beta have "
"shape (k,)."
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:17 tvm.relax.op.nn.nn.log_softmax:8
msgid "This operator can be optimized away for inference."
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:22
msgid "Input to which layer_norm will be applied."
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:31 tvm.relax.op.nn.nn.rms_norm:22
msgid "The axes that along which the normalization is applied."
msgstr ""

#: of tvm.relax.op.nn.nn.leakyrelu:1 tvm.relax.op.nn.nn.relu:1
msgid "Rectified linear unit."
msgstr ""

#: of tvm.relax.op.nn.nn.leakyrelu:3
msgid ""
"text{LeakyReLU, negative_slope}(x) = max(x, 0) + negative_slope * min(x, "
"0)\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.leakyrelu:11
msgid "alpha: float"
msgstr ""

#: of tvm.relax.op.nn.nn.leakyrelu:12
msgid ""
"Controls the angle of the negative slope, used for nagative inputs. "
"Default value is 0.01"
msgstr ""

#: of tvm.relax.op.nn.nn.log_softmax:1
msgid "Computes log softmax."
msgstr ""

#: of tvm.relax.op.nn.nn.log_softmax:3
msgid ""
"\\text{log\\_softmax}(x_i) = \\log\\left( \\frac{\\exp(x_i)}{\\sum_j "
"\\exp(x_j)}\\right)"
msgstr ""

#: of tvm.relax.op.nn.nn.log_softmax:12 tvm.relax.op.nn.nn.pad:8
#: tvm.relax.op.nn.nn.softmax:7
msgid "data: relax.Expr"
msgstr ""

#: of tvm.relax.op.nn.nn.log_softmax:16
msgid ""
"The axis to sum over when computing log softmax. If not specified, it is "
"by default the last axis of the input tensor. Supports negative indexing."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:1
msgid "1D maximum pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:3
msgid ""
"This operator takes data as input and does 1D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:6
msgid ""
"IIn the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, channels, width)`, to produce an output Tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:20
msgid "The size of window for pooling. It is required to have length either 1."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:23
msgid "The strides of pooling. It is required to have length either 1."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:29
msgid "The dilation of pooling. It is required to have length either 1."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool2d:1
msgid "2D maximum pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool2d:3
msgid ""
"This operator takes data as input and does 2D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool2d:12
msgid ""
"\\mbox{out}(b, c, y, x)  = \\max_{m=0, \\ldots, kh-1} \\max_{n=0, "
"\\ldots, kw-1}\n"
"     \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x "
"+ n)"
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool3d:1
msgid "3D maximum pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool3d:3
msgid ""
"This operator takes data as input and does 3D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:1
msgid "Negative log likelihood loss."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:3
msgid ""
"`output[n, i_1, i_2, ..., i_k] = -p * w`, where - `p = predictions[n, t, "
"i_1, i_2, i_k]`, - `t = targets[n, i_1, i_2, ..., i_k]`, - `w = "
"weights[t] if t != ignore_index else 0`"
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:8
msgid "result = reduction(output)"
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:13
msgid ""
"The predictions. Should be a `(k+2)-D` Tensor with shape `(N, C, d_1, "
"d_2, ..., d_k)` where C is the number of target classes."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:16
msgid "targets"
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:17
msgid ""
"The target value of each prediction. Should be a `(k+1)-D` Tensor with "
"shape `(N, d_1, d_2, ..., d_k)`. Must be of int dtype."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:20
msgid "weights"
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:-1
msgid "Optional[relax.Expr]"
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:21
msgid ""
"The weight of each target value. Should be a `1-D` Tensor with shape "
"`(C,)`. If not specified, it is treated as if having all ones."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:24
msgid "reduction"
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:25
msgid ""
"The reduction method to apply to the output. Possible values are "
"\"mean\", \"sum\" and \"none\"."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:28
msgid "ignore_index"
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:29
msgid "The target value to ignore."
msgstr ""

#: of tvm.relax.op.nn.nn.pad:1
msgid "Padding"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:3
msgid ""
"This operator takes in a tensor and pads each axis by the specified "
"widths using the specified value."
msgstr ""

#: of tvm.relax.op.nn.nn.pad:9
msgid "The input data to the operator"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:10
msgid "pad_width: tuple of <tuple of <int>>, required"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:11
msgid ""
"Number of values padded to the edges of each axis, in the format of "
"((before_1, after_1), ..., (before_N, after_N))"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:13
msgid "pad_value: float"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:14
msgid "The value used for padding"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:15
msgid "pad_mode: 'constant', 'edge', 'reflect'"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:16
msgid ""
"'constant' pads with constant_value pad_value 'edge' pads using the edge "
"values of the input array 'reflect' pads by reflecting values with "
"respect to the edge"
msgstr ""

#: of tvm.relax.op.nn.nn.relu:3
msgid ""
"\\text{ReLU}(x) = \\max(x, 0)\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:1
msgid ""
"Root mean square normalization (Biao Zhang and et al., 2019). Applies "
"root mean square normalization to the n-dimensional input array. This "
"operator takes an n-dimensional input array and normalizes the input "
"using the given axis:"
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:6
msgid "out = \\frac{data}{\\sqrt{mean(data, axis)+\\epsilon}} * weight + bias"
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:13
msgid "Input to which rms_norm will be applied."
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:16
msgid "The scale factor."
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:19
msgid "The offset factor."
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:25
msgid "Small float added to square mean to avoid dividing by zero."
msgstr ""

#: of tvm.relax.op.nn.nn.silu:1
msgid "Sigmoid Linear Unit function"
msgstr ""

#: of tvm.relax.op.nn.nn.silu:3
msgid ""
"\\text{SiLU}(x) = x * \\text{sigmoid}(x)\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.softmax:1
msgid "Computes softmax."
msgstr ""

#: of tvm.relax.op.nn.nn.softmax:3
msgid ""
"\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.softmax:11
msgid ""
"The axis to sum over when computing softmax. If not specified, it is by "
"default the last axis of the input tensor. Supports negative indexing."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:34
msgid "tvm.relax.op.builtin"
msgstr ""

#: of tvm.relax.op.builtin:1
msgid "Relax builtin operators."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:1
msgid ""
"Construct a Call to allocate a tensor with specific shape, dtype, "
"runtime_device_index."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:6
#: tvm.relax.op.memory.memory.alloc_tensor:12
msgid "The shape of the tensor to be allocated."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:9
#: tvm.relax.op.memory.memory.alloc_tensor:15
msgid "The datatype of the tensor to be allocated."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:11
msgid "runtime_device_index"
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:-1
#: tvm.relax.op.memory.memory.alloc_storage:-1
#: tvm.relax.op.memory.memory.alloc_tensor:-1
msgid "Union[int, Expr]"
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:12
msgid ""
"The device index indicating on which device the tensor is to be allocated"
" at runtime. Index -1 is reserved for the host device."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:15
#: tvm.relax.op.memory.memory.alloc_storage:13
msgid "storage_scope"
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:16
#: tvm.relax.op.memory.memory.alloc_storage:14
msgid "The storage scope to allocate the storage to."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:-1
#: tvm.relax.op.memory.memory.alloc_storage:-1
#: tvm.relax.op.memory.memory.alloc_tensor:-1
#: tvm.relax.op.memory.memory.kill_storage:-1
#: tvm.relax.op.memory.memory.kill_tensor:-1
msgid "Call"
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:21
#: tvm.relax.op.memory.memory.alloc_tensor:20
msgid "A relax Call, which gets the allocated tensor."
msgstr ""

#: of tvm.relax.op.builtin.builtin.stop_lift_params:1
msgid ""
"An indicator that the consumers of input tensor should not be lifted to "
"transform_params function"
msgstr ""

#: of tvm.relax.op.builtin.builtin.stop_lift_params:6
msgid "x: relax.Expr"
msgstr ""

#: of tvm.relax.op.builtin.builtin.stop_lift_params:12
msgid "The result tensor that is the same as input tensor"
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:40
msgid "tvm.relax.op.ccl"
msgstr ""

#: of tvm.relax.op.ccl:1
msgid "CCL related operators."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:1
msgid "AllGather operator"
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:8
#: tvm.relax.op.ccl.ccl.scatter_from_worker0:8
#: tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:12
msgid "num_worker"
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:9
msgid "The number of workers to gather data from."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:11 tvm.relax.op.ccl.ccl.allreduce:12
msgid "in_group"
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:12
msgid "Whether the gather operation performs globally or in group as default."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:17
msgid "The result of allgather."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allreduce:1
msgid "Allreduce operator"
msgstr ""

#: of tvm.relax.op.ccl.ccl.allreduce:8
msgid "op_type"
msgstr ""

#: of tvm.relax.op.ccl.ccl.allreduce:9
msgid ""
"The type of reduction operation to be applied to the input data. Now "
"\"sum\", \"prod\", \"min\", \"max\" and \"avg\" are supported."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allreduce:13
msgid "Whether the reduction operation performs globally or in group as default."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allreduce:18
msgid "The result of allreduce."
msgstr ""

#: of tvm.relax.op.ccl.ccl.broadcast_from_worker0:1
msgid "Broadcast data from worker-0 to all other workers."
msgstr ""

#: of tvm.relax.op.ccl.ccl.broadcast_from_worker0:6
msgid "The tensor to be broadcast."
msgstr ""

#: of tvm.relax.op.ccl.ccl.broadcast_from_worker0:11
msgid "The same tensor, which has been broadcast to all other workers."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:1
msgid ""
"Perform a scatter operation from worker-0, chunking the given buffer into"
" equal parts."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:6
msgid ""
"The buffer to be divided into equal parts and sent to each worker "
"accordingly."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:9
msgid ""
"The number of workers, i.e. the number of parts the given buffer should "
"be chunked into."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:12
msgid "The dimension of the tensor to be scattered. Default is 0."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:17
msgid "Chunked Tensor received by different workers."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:46
msgid "tvm.relax.op.distributed"
msgstr ""

#: of tvm.relax.op.distributed:1
msgid "Operators serving for distributed Relax."
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:1
msgid "Annotate sharding plan for tensor"
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:5
#: tvm.relax.op.distributed.distributed.redistribute:5
#: tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:9
#: tvm.relax.op.grad.grad.end_checkpoint:5 tvm.relax.op.grad.grad.no_grad:5
#: tvm.relax.op.grad.grad.start_checkpoint:26
msgid "input"
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:7
#: tvm.relax.op.distributed.distributed.redistribute:7
msgid "device_mesh: DeviceMesh"
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:8
msgid "The device mesh of the sharding plan"
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:9
#: tvm.relax.op.distributed.distributed.redistribute:9
msgid "placement: Placement"
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:10
msgid "The placement of the sharding plan"
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:15
msgid "The tensor unmodified."
msgstr ""

#: of tvm.relax.op.distributed.distributed.call_tir_local_view:1
msgid ""
"Call a tir.prim_func and return the output. The prim_func should be a "
"worker-local function that is actually executed on each worker, instead "
"of the unpartitioned function. The output of this operator is DTensor or "
"a tuple of DTensors."
msgstr ""

#: of tvm.relax.op.distributed.distributed.call_tir_local_view:-1
msgid "Union[DTensorStructInfo, List[DTensorStructInfo]]"
msgstr ""

#: of tvm.relax.op.distributed.distributed.call_tir_local_view:14
msgid ""
"The structure info of the call_tir output. It should be a single or a "
"list of DTensorStructInfo. Each one denotes the structure info of a "
"returned tensor."
msgstr ""

#: of tvm.relax.op.distributed.distributed.call_tir_local_view:24
msgid "A call node for the call_tir_local_view operator."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute:1
msgid "Redistribute tensor"
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute:8
msgid "The device mesh after redistribution"
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute:10
msgid "The placement after redistribution"
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute:14
msgid "The tensor after redistribution."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:1
msgid "Slice tensor into several parts along one axis,"
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:2
msgid ""
"and each worker takes one part. input.struct_info.shape[axis] % "
"num_workers == 0 is required. Each worker must have an identical copy of "
"the input. This is a specialized version of redistribute op."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:10
msgid "The buffer to be sliced into equal parts."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:13
msgid ""
"The number of workers, i.e. the number of parts the given buffer should "
"be sliced into."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:16
msgid "The axis of the tensor to be sliced."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:21
msgid "Sliced Tensor kept by each device."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:52
msgid "tvm.relax.op.grad"
msgstr ""

#: of tvm.relax.op.grad:1
msgid "Operators serving for finding gradient of relax operators."
msgstr ""

#: of tvm.relax.op.grad.grad.avg_pool2d_backward:1
msgid ""
"Backward operator of relax.nn.avg_pool2d. All parameters except "
"output_grad is the same as relax.nn.avg_pool2d. Returns the gradient "
"w.r.t. data."
msgstr ""

#: of tvm.relax.op.grad.grad.avg_pool2d_backward:6
#: tvm.relax.op.grad.grad.max_pool2d_backward:6
#: tvm.relax.op.grad.grad.nll_loss_backward:6
#: tvm.relax.op.grad.grad.take_backward:6
msgid "output_grad"
msgstr ""

#: of tvm.relax.op.grad.grad.avg_pool2d_backward:7
msgid "The gradient w.r.t. the result of avg_pool2d."
msgstr ""

#: of tvm.relax.op.grad.grad.avg_pool2d_backward:12
#: tvm.relax.op.grad.grad.max_pool2d_backward:12
msgid "The gradient w.r.t. data."
msgstr ""

#: of tvm.relax.op.grad.grad.end_checkpoint:1
msgid "Mark the end of checkpoint stage. See tvm.relax.op.grad.start_checkpoint."
msgstr ""

#: of tvm.relax.op.grad.grad.end_checkpoint:6
msgid "The output of the checkpoint stage."
msgstr ""

#: of tvm.relax.op.grad.grad.end_checkpoint:11
#: tvm.relax.op.grad.grad.start_checkpoint:32
msgid "The same tensor as the input."
msgstr ""

#: of tvm.relax.op.grad.grad.max_pool2d_backward:1
msgid ""
"Backward operator of relax.nn.max_pool2d. All parameters except "
"output_grad is the same as relax.nn.max_pool2d. Returns the gradient "
"w.r.t. data."
msgstr ""

#: of tvm.relax.op.grad.grad.max_pool2d_backward:7
msgid "The gradient w.r.t. the result of max_pool2d."
msgstr ""

#: of tvm.relax.op.grad.grad.nll_loss_backward:1
msgid ""
"Backward operator of relax.nn.nll_loss. All parameters except output_grad"
" is the same as relax.nn.nll_loss. Returns the gradient w.r.t. "
"predictions."
msgstr ""

#: of tvm.relax.op.grad.grad.nll_loss_backward:7
msgid "The gradient w.r.t. the result of nll_loss."
msgstr ""

#: of tvm.relax.op.grad.grad.nll_loss_backward:12
msgid "The gradient w.r.t. predictions."
msgstr ""

#: of tvm.relax.op.grad.grad.no_grad:1
msgid "No gradient dummy operator w.r.t. the input."
msgstr ""

#: of tvm.relax.op.grad.grad.no_grad:6
msgid "The corresponding input tensor."
msgstr ""

#: of tvm.relax.op.grad.grad.no_grad:11
msgid "The no-gradient representation w.r.t. input."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:1
msgid ""
"Mark the start of the checkpoint stage. The computation between "
"start_checkpoint and end_checkpoint will be marked as the checkpoint "
"stage."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:4
msgid ""
"Rather than storing all intermediate activations of the entire "
"computation graph for computing backward, the checkpointed stage does not"
" save intermediate activations, and instead recomputes them in backward "
"process."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:8
msgid ""
"For instance, ``` a = relax.Var(\"a\", relax.TensorStructInfo((2, 2), "
"\"float32\")) b = relax.Var(\"b\", relax.TensorStructInfo((2, 2), "
"\"float32\")) c = a * 2 d = b * 2 c_cp = start_checkpoint(c) d_cp = "
"start_checkpoint(d) e = c_cp + d_cp e_out = end_checkpoint(e) ``` Then "
"`e` will be recomputed in the backward stage."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:21
msgid ""
"See tvm.relax.transform.Gradient, tvm.relax.testing.nn.checkpoint, "
"tvm.relax.op.grad.end_checkpoint for more information."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:27
msgid "The tensor marking the input of the checkpoint stage."
msgstr ""

#: of tvm.relax.op.grad.grad.take_backward:1
msgid ""
"Backward operator of relax.take. All parameters except output_grad is the"
" same as relax.take. Returns the gradient w.r.t. x."
msgstr ""

#: of tvm.relax.op.grad.grad.take_backward:7
msgid "The gradient w.r.t. the result of take."
msgstr ""

#: of tvm.relax.op.grad.grad.take_backward:12
msgid "The gradient w.r.t. x."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:58
msgid "tvm.relax.op.image"
msgstr ""

#: of tvm.relax.op.image:1
msgid "Image operators."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:1
msgid "Image resize2d operator."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:3
msgid ""
"This operator takes data as input and does 2D scaling to the given scale "
"factor. In the default case, where the data_layout is `NCHW` with data of"
" shape (n, c, h, w) out will have a shape (n, c, size[0], size[1])"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:8
msgid ""
"method indicates the algorithm to be used while calculating the out value"
" and method can be one of (\"linear\", \"nearest_neighbor\", \"cubic\")"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:16
msgid "size: Union[Expr, PrimExprLike, Tuple[PrimExprLike]]"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:17
msgid ""
"The out size to which the image will be resized. If specified as a list, "
"it is required to have length either 1 or 2. If specified as an Expr, it "
"is required to have ndim 2."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:21
msgid "roi: Optional[Union[float, Tuple[float]]]"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:22
msgid ""
"The region of interest for cropping the input image. Expected to be of "
"size 4, and format [start_h, start_w, end_h, end_w]. Only used if "
"coordinate_transformation_mode is tf_crop_and_resize."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:29
msgid "method"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:30
msgid "Scale method to used [nearest_neighbor, linear, cubic]."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:32
msgid "coordinate_transformation_mode"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:33
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. Definitions can be found in "
"topi/image/resize.py. [half_pixel, align_corners, asymmetric, "
"pytorch_half_pixel, tf_half_pixel_for_nn, and tf_crop_and_resize]."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:39
msgid "rounding_method: str"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:40
msgid ""
"indicates how to find the \"nearest\" pixel in nearest_neighbor method "
"[round, floor, ceil]"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:43
msgid "cubic_alpha: float"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:44
msgid "Spline Coefficient for bicubic interpolation"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:46
msgid "cubic_exclude: int"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:47
msgid "Flag to exclude exterior of the image during bicubic interpolation"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:49
msgid "extrapolation_value: float"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:50
msgid "Fill value to use when roi is outside of the image"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:53
msgid ""
"The dtype of the output tensor. It it is not specified, the output will "
"have the same dtype as input if not specified."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:59
msgid "The resized result."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:64
msgid "tvm.relax.op.memory"
msgstr ""

#: of tvm.relax.op.memory:1
msgid "Relax memory primitives."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:1
msgid ""
"Construct a Call to allocate a storage with specific size, "
"virtual_device_index, storage_scope and dtype."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:6
msgid "size"
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:7
msgid "The size of the storage to be allocated."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:9
msgid "virtual_device_index"
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:10
msgid ""
"The virtual device index indicating on which device the storage is to be "
"allocated. Index -1 is reserved for the host device."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:17
msgid "The datatype of the storage to be allocated."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:22
msgid "A relax Call, which gets the allocated storage."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_tensor:1
msgid ""
"Construct a Call to allocate a tensor on a certain storage starting from "
"the given offset."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_tensor:5
#: tvm.relax.op.memory.memory.kill_storage:5
msgid "storage"
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_tensor:6
msgid "The storage to allocate the tensor to."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_tensor:8
msgid "offset"
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_tensor:9
msgid "The storage offset to allocate the tensor."
msgstr ""

#: of tvm.relax.op.memory.view.ensure_zero_offset:1
msgid "Ensure the tensor has elem_offset == 0. A copy will be made if necessary."
msgstr ""

#: of tvm.relax.op.memory.view.ensure_zero_offset:6
msgid "The input tensor"
msgstr ""

#: of tvm.relax.op.memory.view.ensure_zero_offset:9
msgid "Results"
msgstr ""

#: of tvm.relax.op.memory.view.ensure_zero_offset:11
msgid "The tensor with elem_offset == 0"
msgstr ""

#: of tvm.relax.op.memory.memory.kill_storage:1
msgid "Construct a Call to kill a storage."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_storage:6
msgid "The storage to be killed."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_storage:11
msgid "A relax Call to kill a storage."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_tensor:1
msgid "Construct a Call to kill a tensor."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_tensor:5
msgid "tensor"
msgstr ""

#: of tvm.relax.op.memory.memory.kill_tensor:6
msgid "The tensor to be killed."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_tensor:11
msgid "A relax Call to kill a tensor."
msgstr ""

#: of tvm.relax.op.memory.view.view:1
msgid "Provide a view into an existing tensor"
msgstr ""

#: of tvm.relax.op.memory.view.view:3
msgid ""
"The view may have a different shape, may be a different datatype, and may"
" start at an offset relative to the source array."
msgstr ""

#: of tvm.relax.op.memory.view.view:6
msgid ""
"Regardless of which combination of these options are used, the view may "
"never access memory that was not accessible through the input `data` "
"array.  This restriction applies even if the `data` array is itself a "
"view into a shared backing array."
msgstr ""

#: of tvm.relax.op.memory.view.view:13
msgid "data : relax.Expr"
msgstr ""

#: of tvm.relax.op.memory.view.view:17
msgid "shape : Optional[Union[Sequence[PrimExprLike], Expr]]"
msgstr ""

#: of tvm.relax.op.memory.view.view:19
msgid ""
"The target shape.  Should be a `relax.ShapeExpr`, or a collection that "
"can be converted to a `relax.ShapeExpr`."
msgstr ""

#: of tvm.relax.op.memory.view.view:22
msgid "dtype : Optional[Expr]"
msgstr ""

#: of tvm.relax.op.memory.view.view:24
msgid ""
"The target datatype.  Should be a `relax.ShapeExpr`, or a collection that"
" can be converted to a `relax.ShapeExpr`."
msgstr ""

#: of tvm.relax.op.memory.view.view:27
msgid "relative_byte_offset: Optional[Expr]"
msgstr ""

#: of tvm.relax.op.memory.view.view:29
msgid ""
"The offset of the output NDArray, relative to the byte offset of `data`."
"  If `None`, the offset of the view is the same as the offset of `data`."
msgstr ""

#: of tvm.relax.op.memory.view.view:36
msgid "The tensor view"
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:70
msgid "tvm.relax.op.op_attrs"
msgstr ""

#: of tvm.relax.op.op_attrs:1
msgid "The attributes node used for Relax operators"
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool2DAttrs:1
msgid "Attributes for 2d adaptive pool operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ArgmaxArgminAttrs:1
msgid "Attributes for argmax/argmin operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ArgsortAttrs:1
msgid "Attributes for argsort operator"
msgstr ""

#: of tvm.relax.op.op_attrs.AstypeAttrs:1
msgid "Attributes used in astype operator"
msgstr ""

#: of tvm.relax.op.op_attrs.BatchNormAttrs:1
msgid "Attributes used in batch_norm operator"
msgstr ""

#: of tvm.relax.op.op_attrs.CallTIRWithGradAttrs:1
msgid "Attributes used in call_tir_with_grad operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ConcatAttrs:1
msgid "Attributes for concat operator"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv2DAttrs:1
msgid "Attributes for nn.conv2d"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv2DTransposeAttrs:1
msgid "Attributes for nn.conv2d_transpose"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv3DAttrs:1
msgid "Attributes for nn.conv3d"
msgstr ""

#: of tvm.relax.op.op_attrs.DropoutAttrs:1
msgid "Attributes for dropout operator"
msgstr ""

#: of tvm.relax.op.op_attrs.EinsumAttrs:1
msgid "Attributes for einsum operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ExpandDimsAttrs:1
msgid "Attributes for expand_dims operator"
msgstr ""

#: of tvm.relax.op.op_attrs.FlipAttrs:1
msgid "Attributes for flip operator"
msgstr ""

#: of tvm.relax.op.op_attrs.InitAttrs:1
msgid ""
"Attributes used in full/full_like, ones/ones_like, and zeros/zeros_like "
"operator"
msgstr ""

#: of tvm.relax.op.op_attrs.LayerNormAttrs:1
msgid "Attributes used in layer_norm operator"
msgstr ""

#: of tvm.relax.op.op_attrs.LayoutTransformAttrs:1
msgid "Attributes used in layout_transform operator"
msgstr ""

#: of tvm.relax.op.op_attrs.MatmulAttrs:1
msgid "Attributes for matmul operator"
msgstr ""

#: of tvm.relax.op.op_attrs.PermuteDimsAttrs:1
msgid "Attributes for permute_dims operator"
msgstr ""

#: of tvm.relax.op.op_attrs.Pool2DAttrs:1
msgid "Attributes for nn.max_pool2d"
msgstr ""

#: of tvm.relax.op.op_attrs.RepeatAttrs:1
msgid "Attributes for repeat operator"
msgstr ""

#: of tvm.relax.op.op_attrs.Resize2DAttrs:1
msgid "Attributes used in image resize2d operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ScanopAttrs:1
msgid "Attributes for scan operators"
msgstr ""

#: of tvm.relax.op.op_attrs.SoftmaxAttrs:1
msgid "Attributes for nn.softmax"
msgstr ""

#: of tvm.relax.op.op_attrs.SortAttrs:1
msgid "Attributes for sort operator"
msgstr ""

#: of tvm.relax.op.op_attrs.SplitAttrs:1
msgid "Attributes used in split operator"
msgstr ""

#: of tvm.relax.op.op_attrs.SqueezeAttrs:1
msgid "Attributes for squeeze operator"
msgstr ""

#: of tvm.relax.op.op_attrs.StatisticalAttrs:1
msgid "Attributes used in statistical operator"
msgstr ""

#: of tvm.relax.op.op_attrs.StridedSliceAttrs:1
msgid "Attributes used in strided_slice operator"
msgstr ""

#: of tvm.relax.op.op_attrs.TakeAttrs:1
msgid "Attributes used in take operator"
msgstr ""

#: of tvm.relax.op.op_attrs.TileAttrs:1
msgid "Attributes for tile operator"
msgstr ""

#: of tvm.relax.op.op_attrs.TopKAttrs:1
msgid "Attributes for topk operators"
msgstr ""

#: of tvm.relax.op.op_attrs.TriluAttrs:1
msgid "Attributes used in tril and triu operator"
msgstr ""

