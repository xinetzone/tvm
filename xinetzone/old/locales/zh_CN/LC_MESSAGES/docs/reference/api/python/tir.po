# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-08-31 17:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/reference/api/python/tir.rst:21
msgid "tvm.tir"
msgstr ""

#: ../../doc/docs/reference/api/python/tir.rst:30
msgid "tvm.tir.transform"
msgstr ""

#: ../../doc/docs/reference/api/python/tir.rst:38
msgid "tvm.tir.analysis"
msgstr ""

#: ../../doc/docs/reference/api/python/tir.rst:47
msgid "tvm.tir.stmt_functor"
msgstr ""

#~ msgid ":py:obj:`BijectiveLayout <tvm.tir.BijectiveLayout>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`BlockScope <tvm.tir.BlockScope>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Buffer <tvm.tir.Buffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`DataProducer <tvm.tir.DataProducer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Layout <tvm.tir.Layout>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Stmt <tvm.tir.Stmt>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`StmtSRef <tvm.tir.StmtSRef>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`PrimFuncPass <tvm.tir.transform.PrimFuncPass>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Buffer <tvm.tir.analysis.Buffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Object <tvm.tir.analysis.Object>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`PrimExpr <tvm.tir.analysis.PrimExpr>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Stmt <tvm.tir.analysis.Stmt>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`BijectiveLayout <tvm.tir.BijectiveLayout>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`BlockScope <tvm.tir.BlockScope>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Buffer <tvm.tir.Buffer>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`DataProducer <tvm.tir.DataProducer>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Layout <tvm.tir.Layout>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Stmt <tvm.tir.Stmt>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`StmtSRef <tvm.tir.StmtSRef>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`PrimFuncPass <tvm.tir.transform.PrimFuncPass>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Buffer <tvm.tir.analysis.Buffer>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Object <tvm.tir.analysis.Object>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`PrimExpr <tvm.tir.analysis.PrimExpr>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Stmt <tvm.tir.analysis.Stmt>`\\"
#~ msgstr ""

#~ msgid "Namespace for Tensor-level IR"
#~ msgstr ""

#~ msgid "**Classes:**"
#~ msgstr ""

#~ msgid ":py:obj:`Add <tvm.tir.Add>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Add node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Allocate <tvm.tir.Allocate>`\\ \\(buffer\\_var\\,"
#~ " dtype\\, extents\\, ...\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Allocate node."
#~ msgstr ""

#~ msgid ":py:obj:`And <tvm.tir.And>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "And node."
#~ msgstr ""

#~ msgid ":py:obj:`Any <tvm.tir.Any>`\\ \\(\\[span\\]\\)"
#~ msgstr ""

#~ msgid "Any node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AssertStmt <tvm.tir.AssertStmt>`\\ "
#~ "\\(condition\\, message\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "AssertStmt node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AttrStmt <tvm.tir.AttrStmt>`\\ \\(node\\, "
#~ "attr\\_key\\, value\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "AttrStmt node."
#~ msgstr ""

#~ msgid "Bijective mapping for two layouts (src-layout and dst-layout)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Block <tvm.tir.Block>`\\ \\(iter\\_vars\\, "
#~ "reads\\, writes\\, name\\_hint\\, body\\)"
#~ msgstr ""

#~ msgid "Block node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BlockRealize <tvm.tir.BlockRealize>`\\ "
#~ "\\(iter\\_values\\, predicate\\, block\\)"
#~ msgstr ""

#~ msgid "BlockRealize node."
#~ msgstr ""

#~ msgid ""
#~ "An object corresponds to each block "
#~ "sref in the sref tree, which "
#~ "tracks the producer-consumer dependency "
#~ "between blocks."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Broadcast <tvm.tir.Broadcast>`\\ \\(value\\, "
#~ "lanes\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Broadcast node."
#~ msgstr ""

#~ msgid "Symbolic data buffer in TVM."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferLoad <tvm.tir.BufferLoad>`\\ \\(buffer\\,"
#~ " indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Buffer load node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferRealize <tvm.tir.BufferRealize>`\\ "
#~ "\\(buffer\\, bounds\\, condition\\, body\\)"
#~ msgstr ""

#~ msgid "Buffer realize node."
#~ msgstr ""

#~ msgid ":py:obj:`BufferRegion <tvm.tir.BufferRegion>`\\ \\(buffer\\, region\\)"
#~ msgstr ""

#~ msgid "BufferRegion node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferStore <tvm.tir.BufferStore>`\\ \\(buffer\\,"
#~ " value\\, indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Buffer store node."
#~ msgstr ""

#~ msgid ":py:obj:`Call <tvm.tir.Call>`\\ \\(dtype\\, op\\, args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Call node."
#~ msgstr ""

#~ msgid ":py:obj:`CallEffectKind <tvm.tir.CallEffectKind>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Possible kinds of Call effects."
#~ msgstr ""

#~ msgid ":py:obj:`Cast <tvm.tir.Cast>`\\ \\(dtype\\, value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Cast expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CommReducer <tvm.tir.CommReducer>`\\ \\(lhs\\,"
#~ " rhs\\, result\\, identity\\_element\\)"
#~ msgstr ""

#~ msgid "Commutative reduce operator"
#~ msgstr ""

#~ msgid ":py:obj:`Div <tvm.tir.Div>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Div node."
#~ msgstr ""

#~ msgid ":py:obj:`EQ <tvm.tir.EQ>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "EQ node."
#~ msgstr ""

#~ msgid ":py:obj:`Evaluate <tvm.tir.Evaluate>`\\ \\(value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Evaluate node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FloatImm <tvm.tir.FloatImm>`\\ \\(dtype\\, "
#~ "value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Float constant."
#~ msgstr ""

#~ msgid ":py:obj:`FloorDiv <tvm.tir.FloorDiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "FloorDiv node."
#~ msgstr ""

#~ msgid ":py:obj:`FloorMod <tvm.tir.FloorMod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "FloorMod node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`For <tvm.tir.For>`\\ \\(loop\\_var\\, "
#~ "min\\_val\\, extent\\, kind\\, body\\[\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "For node."
#~ msgstr ""

#~ msgid ":py:obj:`ForKind <tvm.tir.ForKind>`\\ \\(value\\)"
#~ msgstr ""

#~ msgid "The kind of the for loop."
#~ msgstr ""

#~ msgid ":py:obj:`GE <tvm.tir.GE>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "GE node."
#~ msgstr ""

#~ msgid ":py:obj:`GT <tvm.tir.GT>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "GT node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`IfThenElse <tvm.tir.IfThenElse>`\\ "
#~ "\\(condition\\, then\\_case\\, else\\_case\\)"
#~ msgstr ""

#~ msgid "IfThenElse node."
#~ msgstr ""

#~ msgid ":py:obj:`IntImm <tvm.tir.IntImm>`\\ \\(dtype\\, value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Int constant."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`IterVar <tvm.tir.IterVar>`\\ \\(dom\\, "
#~ "var\\, iter\\_type\\[\\, thread\\_tag\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Represent iteration variable."
#~ msgstr ""

#~ msgid ":py:obj:`LE <tvm.tir.LE>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "LE node."
#~ msgstr ""

#~ msgid ":py:obj:`LT <tvm.tir.LT>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "LT node."
#~ msgstr ""

#~ msgid ""
#~ "Layout is composed of upper cases, "
#~ "lower cases and numbers, where upper "
#~ "case indicates a primal axis and "
#~ "the corresponding lower case with factor"
#~ " size indicates the subordinate axis."
#~ msgstr ""

#~ msgid ":py:obj:`Let <tvm.tir.Let>`\\ \\(var\\, value\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Let node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LetStmt <tvm.tir.LetStmt>`\\ \\(var\\, "
#~ "value\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "LetStmt node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Load <tvm.tir.Load>`\\ \\(dtype\\, "
#~ "buffer\\_var\\, index\\[\\, predicate\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Load node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MatchBufferRegion <tvm.tir.MatchBufferRegion>`\\ "
#~ "\\(buffer\\, source\\)"
#~ msgstr ""

#~ msgid "MatchBufferRegion node."
#~ msgstr ""

#~ msgid ":py:obj:`Max <tvm.tir.Max>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Max node."
#~ msgstr ""

#~ msgid ":py:obj:`Min <tvm.tir.Min>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Min node."
#~ msgstr ""

#~ msgid ":py:obj:`Mod <tvm.tir.Mod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Mod node."
#~ msgstr ""

#~ msgid ":py:obj:`Mul <tvm.tir.Mul>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Mul node."
#~ msgstr ""

#~ msgid ":py:obj:`NE <tvm.tir.NE>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "NE node."
#~ msgstr ""

#~ msgid ":py:obj:`Not <tvm.tir.Not>`\\ \\(a\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Not node."
#~ msgstr ""

#~ msgid ":py:obj:`Or <tvm.tir.Or>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Or node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Prefetch <tvm.tir.Prefetch>`\\ \\(buffer\\, "
#~ "bounds\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Prefetch node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PrimFunc <tvm.tir.PrimFunc>`\\ \\(params\\, "
#~ "body\\[\\, ret\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "A function declaration expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ProducerLoad <tvm.tir.ProducerLoad>`\\ "
#~ "\\(producer\\, indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Producer load node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ProducerRealize <tvm.tir.ProducerRealize>`\\ "
#~ "\\(producer\\, bounds\\, condition\\, ...\\)"
#~ msgstr ""

#~ msgid "ProducerRealize node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ProducerStore <tvm.tir.ProducerStore>`\\ "
#~ "\\(producer\\, value\\, indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "ProducerStore node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Ramp <tvm.tir.Ramp>`\\ \\(base\\, stride\\,"
#~ " lanes\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Ramp node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Reduce <tvm.tir.Reduce>`\\ \\(combiner\\, "
#~ "src\\, rdom\\, condition\\, ...\\)"
#~ msgstr ""

#~ msgid "Reduce node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Schedule <tvm.tir.Schedule>`\\ \\(mod\\, "
#~ "\\*\\[\\, seed\\, debug\\_mask\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "The user-facing schedule class"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ScheduleState <tvm.tir.ScheduleState>`\\ "
#~ "\\(mod\\, \\*\\[\\, debug\\_mask\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "The state of scheduling, which exposes"
#~ " a `Replace` method as the primary"
#~ " resort for all the scheduling "
#~ "primitives to manipulate the TensorIR."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Select <tvm.tir.Select>`\\ \\(condition\\, "
#~ "true\\_value\\, false\\_value\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Select node."
#~ msgstr ""

#~ msgid ":py:obj:`SeqStmt <tvm.tir.SeqStmt>`\\ \\(seq\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Sequence of statements."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Shuffle <tvm.tir.Shuffle>`\\ \\(vectors\\, "
#~ "indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Shuffle node."
#~ msgstr ""

#~ msgid ":py:obj:`SizeVar <tvm.tir.SizeVar>`\\ \\(name\\, dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Symbolic variable to represent a tensor index size"
#~ msgstr ""

#~ msgid "Base class of all the statements."
#~ msgstr ""

#~ msgid ""
#~ "An object that refers to schedulable "
#~ "elements in the TensorIR, aka \"sref\"."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Store <tvm.tir.Store>`\\ \\(buffer\\_var\\, "
#~ "value\\, index\\[\\, predicate\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Store node."
#~ msgstr ""

#~ msgid ":py:obj:`StringImm <tvm.tir.StringImm>`\\ \\(value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "String constant."
#~ msgstr ""

#~ msgid ":py:obj:`Sub <tvm.tir.Sub>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Sub node."
#~ msgstr ""

#~ msgid ":py:obj:`TensorIntrin <tvm.tir.TensorIntrin>`\\ \\(desc\\, impl\\)"
#~ msgstr ""

#~ msgid "A tensor intrinsic."
#~ msgstr ""

#~ msgid ":py:obj:`Var <tvm.tir.Var>`\\ \\(name\\, dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Symbolic variable."
#~ msgstr ""

#~ msgid ":py:obj:`While <tvm.tir.While>`\\ \\(condition\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "While node."
#~ msgstr ""

#~ msgid "**Exceptions:**"
#~ msgstr ""

#~ msgid ":py:obj:`ScheduleError <tvm.tir.ScheduleError>`\\"
#~ msgstr ""

#~ msgid "Error that happens during TensorIR scheduling."
#~ msgstr ""

#~ msgid "**Functions:**"
#~ msgstr ""

#~ msgid ":py:obj:`abs <tvm.tir.abs>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Get absolute value of the input element-wise."
#~ msgstr ""

#~ msgid ":py:obj:`acos <tvm.tir.acos>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take acos of input x."
#~ msgstr ""

#~ msgid ":py:obj:`acosh <tvm.tir.acosh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid ":py:obj:`all <tvm.tir.all>`\\ \\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new expression of the intersection of all conditions in the"
#~ msgstr ""

#~ msgid ":py:obj:`any <tvm.tir.any>`\\ \\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new experssion of the union of all conditions in the arguments"
#~ msgstr ""

#~ msgid ":py:obj:`asin <tvm.tir.asin>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take asin of input x."
#~ msgstr ""

#~ msgid ":py:obj:`asinh <tvm.tir.asinh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take asinh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`atan <tvm.tir.atan>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take atan of input x."
#~ msgstr ""

#~ msgid ":py:obj:`atan2 <tvm.tir.atan2>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Take arctan2(x1, x2)."
#~ msgstr ""

#~ msgid ":py:obj:`atanh <tvm.tir.atanh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take atanh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bijective_layout <tvm.tir.bijective_layout>`\\ "
#~ "\\(src\\_layout\\, dst\\_layout\\)"
#~ msgstr ""

#~ msgid "Create a bijective layout mapping."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_extern <tvm.tir.call_extern>`\\ \\(dtype\\,"
#~ " func\\_name\\, \\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling a extern function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_intrin <tvm.tir.call_intrin>`\\ \\(dtype\\,"
#~ " func\\_name\\, \\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling an intrinsic function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_llvm_intrin <tvm.tir.call_llvm_intrin>`\\ "
#~ "\\(dtype\\, name\\, \\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling a llvm intrinsic function"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_llvm_pure_intrin "
#~ "<tvm.tir.call_llvm_pure_intrin>`\\ \\(dtype\\, name\\, "
#~ "\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling a pure llvm intrinsic function"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_packed <tvm.tir.call_packed>`\\ "
#~ "\\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by call an external packed function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_pure_extern <tvm.tir.call_pure_extern>`\\ "
#~ "\\(dtype\\, func\\_name\\, \\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling a pure extern function."
#~ msgstr ""

#~ msgid ":py:obj:`ceil <tvm.tir.ceil>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take ceil of float input x."
#~ msgstr ""

#~ msgid ":py:obj:`clz <tvm.tir.clz>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Count leading zero bits of an integer x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`comm_reducer <tvm.tir.comm_reducer>`\\ "
#~ "\\(fcombine\\, fidentity\\[\\, name\\]\\)"
#~ msgstr ""

#~ msgid "Create a commutative reducer for reduction."
#~ msgstr ""

#~ msgid ":py:obj:`copysign <tvm.tir.copysign>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Change the sign of x1 to that of x2, element-wise."
#~ msgstr ""

#~ msgid ":py:obj:`cos <tvm.tir.cos>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take cos of input x."
#~ msgstr ""

#~ msgid ":py:obj:`cosh <tvm.tir.cosh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take cosh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`decl_buffer <tvm.tir.decl_buffer>`\\ "
#~ "\\(shape\\[\\, dtype\\, name\\, data\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Declare a new symbolic buffer."
#~ msgstr ""

#~ msgid ":py:obj:`div <tvm.tir.div>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute a / b as in C/C++ semantics."
#~ msgstr ""

#~ msgid ":py:obj:`erf <tvm.tir.erf>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take gauss error function of the input x."
#~ msgstr ""

#~ msgid ":py:obj:`exp <tvm.tir.exp>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take exponential of input x."
#~ msgstr ""

#~ msgid ":py:obj:`exp10 <tvm.tir.exp10>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Calculate 10**x"
#~ msgstr ""

#~ msgid ":py:obj:`exp2 <tvm.tir.exp2>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Calculate 2**x"
#~ msgstr ""

#~ msgid ":py:obj:`floor <tvm.tir.floor>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take floor of float input x."
#~ msgstr ""

#~ msgid ":py:obj:`floordiv <tvm.tir.floordiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the floordiv of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`floormod <tvm.tir.floormod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the floormod of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`fmod <tvm.tir.fmod>`\\ \\(x\\, y\\)"
#~ msgstr ""

#~ msgid "Return the remainder of x divided by y with the same sign as x."
#~ msgstr ""

#~ msgid ":py:obj:`hypot <tvm.tir.hypot>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Equivalent to sqrt(x1**2 + x2**2), element-wise."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`if_then_else <tvm.tir.if_then_else>`\\ \\(cond\\,"
#~ " t\\, f\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Conditional selection expression."
#~ msgstr ""

#~ msgid ":py:obj:`indexdiv <tvm.tir.indexdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute floor(a / b) where a and b are non-negative."
#~ msgstr ""

#~ msgid ":py:obj:`indexmod <tvm.tir.indexmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the remainder of indexdiv."
#~ msgstr ""

#~ msgid ":py:obj:`isfinite <tvm.tir.isfinite>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is finite."
#~ msgstr ""

#~ msgid ":py:obj:`isinf <tvm.tir.isinf>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is infinite."
#~ msgstr ""

#~ msgid ":py:obj:`isnan <tvm.tir.isnan>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is Nan."
#~ msgstr ""

#~ msgid ":py:obj:`layout <tvm.tir.layout>`\\ \\(layout\\_str\\)"
#~ msgstr ""

#~ msgid "Create a layout node from a string."
#~ msgstr ""

#~ msgid ":py:obj:`ldexp <tvm.tir.ldexp>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Returns x1 * (2 ** x2)."
#~ msgstr ""

#~ msgid ":py:obj:`log <tvm.tir.log>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log of input x."
#~ msgstr ""

#~ msgid ":py:obj:`log10 <tvm.tir.log10>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log10 of input x."
#~ msgstr ""

#~ msgid ":py:obj:`log1p <tvm.tir.log1p>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log(x + 1) with respect to input x."
#~ msgstr ""

#~ msgid ":py:obj:`log2 <tvm.tir.log2>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log2 of input x."
#~ msgstr ""

#~ msgid ":py:obj:`max <tvm.tir.max>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
#~ msgstr ""

#~ msgid "Create a max expression over axis."
#~ msgstr ""

#~ msgid ":py:obj:`max_value <tvm.tir.max_value>`\\ \\(dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "maximum value of dtype"
#~ msgstr ""

#~ msgid ":py:obj:`min <tvm.tir.min>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
#~ msgstr ""

#~ msgid "Create a min expression over axis."
#~ msgstr ""

#~ msgid ":py:obj:`min_value <tvm.tir.min_value>`\\ \\(dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "minimum value of dtype"
#~ msgstr ""

#~ msgid ":py:obj:`nearbyint <tvm.tir.nearbyint>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Round elements of the array to the nearest integer."
#~ msgstr ""

#~ msgid ":py:obj:`nextafter <tvm.tir.nextafter>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Return the next floating-point value after x1 towards x2."
#~ msgstr ""

#~ msgid ":py:obj:`popcount <tvm.tir.popcount>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Count the number of set bits in input x."
#~ msgstr ""

#~ msgid ":py:obj:`power <tvm.tir.power>`\\ \\(x\\, y\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "x power y"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`q_multiply_shift <tvm.tir.q_multiply_shift>`\\ "
#~ "\\(x\\, y\\, q\\, s\\)"
#~ msgstr ""

#~ msgid ""
#~ "Execute a multiplication between two "
#~ "Q-numbers x and y followed by a"
#~ " right shift s."
#~ msgstr ""

#~ msgid ":py:obj:`ret <tvm.tir.ret>`\\ \\(val\\)"
#~ msgstr ""

#~ msgid "Create a tir return expression"
#~ msgstr ""

#~ msgid ":py:obj:`round <tvm.tir.round>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`rsqrt <tvm.tir.rsqrt>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take reciprocal of square root of input x."
#~ msgstr ""

#~ msgid ":py:obj:`sigmoid <tvm.tir.sigmoid>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Quick function to get sigmoid"
#~ msgstr ""

#~ msgid ":py:obj:`sin <tvm.tir.sin>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take sin of input x."
#~ msgstr ""

#~ msgid ":py:obj:`sinh <tvm.tir.sinh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take sinh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`sqrt <tvm.tir.sqrt>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take square root of input x."
#~ msgstr ""

#~ msgid ":py:obj:`stmt_list <tvm.tir.stmt_list>`\\ \\(stmt\\)"
#~ msgstr ""

#~ msgid "Make list of stmt from blocks."
#~ msgstr ""

#~ msgid ":py:obj:`stmt_seq <tvm.tir.stmt_seq>`\\ \\(\\*args\\)"
#~ msgstr ""

#~ msgid "Make sequence of statements"
#~ msgstr ""

#~ msgid ":py:obj:`sum <tvm.tir.sum>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
#~ msgstr ""

#~ msgid "Create a sum expression over axis."
#~ msgstr ""

#~ msgid ":py:obj:`tan <tvm.tir.tan>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take tan of input x."
#~ msgstr ""

#~ msgid ":py:obj:`tanh <tvm.tir.tanh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take hyperbolic tanh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`trace <tvm.tir.trace>`\\ \\(args\\[\\, trace\\_action\\]\\)"
#~ msgstr ""

#~ msgid "Trace tensor data at the runtime."
#~ msgstr ""

#~ msgid ":py:obj:`trunc <tvm.tir.trunc>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Get truncated value of the input."
#~ msgstr ""

#~ msgid ":py:obj:`truncdiv <tvm.tir.truncdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the truncdiv of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`truncmod <tvm.tir.truncmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the truncmod of two expressions."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "The left hand operand."
#~ msgstr ""

#~ msgid "The right hand operand."
#~ msgstr ""

#~ msgid "The location of this itervar in the source code."
#~ msgstr ""

#~ msgid "The buffer variable."
#~ msgstr ""

#~ msgid "The data type of the buffer."
#~ msgstr ""

#~ msgid "The extents of the allocate"
#~ msgstr ""

#~ msgid "The condition."
#~ msgstr ""

#~ msgid "The body statement."
#~ msgstr ""

#~ msgid "Additional annotation hints"
#~ msgstr ""

#~ msgid "span"
#~ msgstr ""

#~ msgid "Optional[Span]"
#~ msgstr ""

#~ msgid "The assert condition."
#~ msgstr ""

#~ msgid "The error message."
#~ msgstr ""

#~ msgid "The node to annotate the attribute"
#~ msgstr ""

#~ msgid "Attribute type key."
#~ msgstr ""

#~ msgid "The value of the attribute"
#~ msgstr ""

#~ msgid ""
#~ "Bijective mapping for two layouts "
#~ "(src-layout and dst-layout). It "
#~ "provides shape and index conversion "
#~ "between each other."
#~ msgstr ""

#~ msgid ""
#~ "Do not construct directly, use "
#~ ":any:`bijective_layout` instead. See the "
#~ "documentation of :any:`bijective_layout` for "
#~ "more details."
#~ msgstr ""

#~ msgid "source layout."
#~ msgstr ""

#~ msgid "destination layout."
#~ msgstr ""

#~ msgid ":obj:`bijective_layout`"
#~ msgstr ""

#~ msgid "Declare a layout"
#~ msgstr ""

#~ msgid "**Methods:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`backward_index "
#~ "<tvm.tir.BijectiveLayout.backward_index>`\\ \\(index\\)"
#~ msgstr ""

#~ msgid "Given the indices of the dst-layout, infer the src index."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`backward_shape "
#~ "<tvm.tir.BijectiveLayout.backward_shape>`\\ \\(shape\\)"
#~ msgstr ""

#~ msgid "Given the shape of the dst-layout, infer the src shape."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`forward_index "
#~ "<tvm.tir.BijectiveLayout.forward_index>`\\ \\(index\\)"
#~ msgstr ""

#~ msgid "Given the indices of the src-layout, infer the dst index."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`forward_shape "
#~ "<tvm.tir.BijectiveLayout.forward_shape>`\\ \\(shape\\)"
#~ msgstr ""

#~ msgid "Given the shape of the src-layout, infer the dst shape."
#~ msgstr ""

#~ msgid "The indices in dst-layout."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "**src_index** -- The inferred indices in src-layout."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "The shape in dst-layout."
#~ msgstr ""

#~ msgid "**src_shape** -- The inferred shape in src-layout."
#~ msgstr ""

#~ msgid "The indices in src-layout."
#~ msgstr ""

#~ msgid "**dst_index** -- The inferred indices in dst-layout."
#~ msgstr ""

#~ msgid "The shape in src-layout."
#~ msgstr ""

#~ msgid "**dst_shape** -- The inferred shape in dst-layout."
#~ msgstr ""

#~ msgid "The block Variable."
#~ msgstr ""

#~ msgid "The read buffer regions of the block."
#~ msgstr ""

#~ msgid "The write buffer regions of the block."
#~ msgstr ""

#~ msgid "the name_hint of the block."
#~ msgstr ""

#~ msgid "The body of the block."
#~ msgstr ""

#~ msgid "The init block of the reduction block"
#~ msgstr ""

#~ msgid "The buffer allocations"
#~ msgstr ""

#~ msgid "The subregion buffer match"
#~ msgstr ""

#~ msgid "Additional annotation hints."
#~ msgstr ""

#~ msgid "The location of this block in the source code."
#~ msgstr ""

#~ msgid "The binding values of the block var."
#~ msgstr ""

#~ msgid "The predicate of the block."
#~ msgstr ""

#~ msgid "The block to realize"
#~ msgstr ""

#~ msgid "The location of this block_realize in the source code."
#~ msgstr ""

#~ msgid "Glossary:"
#~ msgstr ""

#~ msgid ""
#~ "Block scope: A contiguous subtree of "
#~ "the sref tree, rooted at each "
#~ "block sref, whose components are:"
#~ msgstr ""

#~ msgid "scope root: a block sref"
#~ msgstr ""

#~ msgid "internal srefs: loop srefs"
#~ msgstr ""

#~ msgid "scope leaves: block srefs"
#~ msgstr ""

#~ msgid ""
#~ "Child block: The scope leaf blocks "
#~ "under the scope root or a specific"
#~ " internal sref"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_deps_by_dst <tvm.tir.BlockScope.get_deps_by_dst>`\\"
#~ " \\(block\\)"
#~ msgstr ""

#~ msgid "Get all dependencies whose `dst` is the target `block`."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_deps_by_src <tvm.tir.BlockScope.get_deps_by_src>`\\"
#~ " \\(block\\)"
#~ msgstr ""

#~ msgid "Get all dependencies whose `src` is the target`block`."
#~ msgstr ""

#~ msgid "The queried block"
#~ msgstr ""

#~ msgid "**blocks** -- The dependencies"
#~ msgstr ""

#~ msgid "The value of the expression."
#~ msgstr ""

#~ msgid "The lanes of the expression."
#~ msgstr ""

#~ msgid ""
#~ "Buffer provide a way to represent "
#~ "data layout specialization of data "
#~ "structure in TVM."
#~ msgstr ""

#~ msgid ""
#~ "Do not construct directly, use "
#~ ":py:func:`~decl_buffer` instead. See the "
#~ "documentation of :py:func:`decl_buffer` for "
#~ "more details."
#~ msgstr ""

#~ msgid ":obj:`decl_buffer`"
#~ msgstr ""

#~ msgid "Declare a buffer"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`access_ptr <tvm.tir.Buffer.access_ptr>`\\ "
#~ "\\(access\\_mask\\[\\, ptr\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Get an access pointer to the head of buffer."
#~ msgstr ""

#~ msgid ":py:obj:`scope <tvm.tir.Buffer.scope>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Return the storage scope associated with this buffer."
#~ msgstr ""

#~ msgid ":py:obj:`vload <tvm.tir.Buffer.vload>`\\ \\(begin\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Generate an Expr that loads dtype from begin index."
#~ msgstr ""

#~ msgid ":py:obj:`vstore <tvm.tir.Buffer.vstore>`\\ \\(begin\\, value\\)"
#~ msgstr ""

#~ msgid "Generate a Stmt that store value into begin index."
#~ msgstr ""

#~ msgid ""
#~ "This is the recommended method to "
#~ "get buffer data ptress when interacting"
#~ " with external functions."
#~ msgstr ""

#~ msgid ""
#~ "The access pattern MASK. Indicate "
#~ "whether the access will read or "
#~ "write to the data content."
#~ msgstr ""

#~ msgid ""
#~ "The data type of the result "
#~ "pointer. Do not specify unless we "
#~ "want to cast pointer to specific "
#~ "type."
#~ msgstr ""

#~ msgid ""
#~ "The number of lanes for the data"
#~ " type. This value is greater than "
#~ "one for vector types."
#~ msgstr ""

#~ msgid ""
#~ "The offset of pointer. We can use"
#~ " it to offset by the number of"
#~ " elements from the address of ptr."
#~ msgstr ""

#~ msgid "实际案例"
#~ msgstr ""

#~ msgid ""
#~ "Return the storage scope associated with"
#~ " this buffer. :returns: **scope** -- "
#~ "The storage scope associated with this"
#~ " buffer. :rtype: str"
#~ msgstr ""

#~ msgid "The beginning index in unit of Buffer.dtype"
#~ msgstr ""

#~ msgid ""
#~ "The data type to be loaded, can"
#~ " be vector type which have lanes "
#~ "that is multiple of Buffer.dtype"
#~ msgstr ""

#~ msgid "**load** -- The corresponding load expression."
#~ msgstr ""

#~ msgid "The value to be stored."
#~ msgstr ""

#~ msgid "**store** -- The corresponding store stmt."
#~ msgstr ""

#~ msgid "The buffer to be loaded."
#~ msgstr ""

#~ msgid "The buffer indices."
#~ msgstr ""

#~ msgid "The buffer."
#~ msgstr ""

#~ msgid "The value we to be stored."
#~ msgstr ""

#~ msgid "The realize condition."
#~ msgstr ""

#~ msgid "The body of the statement."
#~ msgstr ""

#~ msgid "The buffer of the buffer region"
#~ msgstr ""

#~ msgid "The region array of the buffer region"
#~ msgstr ""

#~ msgid "The indices location to be stored."
#~ msgstr ""

#~ msgid "The return data type"
#~ msgstr ""

#~ msgid "The function to be called, or the name to the global tvm.Op"
#~ msgstr ""

#~ msgid "The input arguments to the call"
#~ msgstr ""

#~ msgid "The data type"
#~ msgstr ""

#~ msgid "The value of the function."
#~ msgstr ""

#~ msgid "The left arguments of the reducer."
#~ msgstr ""

#~ msgid "The right arguments of the reducer."
#~ msgstr ""

#~ msgid "The reduction results."
#~ msgstr ""

#~ msgid "The identity elements."
#~ msgstr ""

#~ msgid "The expression to be evalued."
#~ msgstr ""

#~ msgid "The constant value."
#~ msgstr ""

#~ msgid "The loop variable."
#~ msgstr ""

#~ msgid "The beginning value."
#~ msgstr ""

#~ msgid "The length of the loop."
#~ msgstr ""

#~ msgid "The type of the for."
#~ msgstr ""

#~ msgid "The thread this loop binds to. Only valid if kind is ThreadBinding"
#~ msgstr ""

#~ msgid ""
#~ "ForKind can change the control flow "
#~ "semantics of the loop and need to"
#~ " be considered in all TIR passes."
#~ msgstr ""

#~ msgid "The expression"
#~ msgstr ""

#~ msgid "The statement to execute if condition is true."
#~ msgstr ""

#~ msgid "The statement to execute if condition is false."
#~ msgstr ""

#~ msgid "IterVar represents axis iterations in the computation."
#~ msgstr ""

#~ msgid "The domain of the iteration."
#~ msgstr ""

#~ msgid "The internal variable that is used for iteration."
#~ msgstr ""

#~ msgid "The iteration type."
#~ msgstr ""

#~ msgid "The thread type tag."
#~ msgstr ""

#~ msgid ":obj:`te.thread_axis`"
#~ msgstr ""

#~ msgid "Create thread axis IterVar."
#~ msgstr ""

#~ msgid ":obj:`te.reduce_axis`"
#~ msgstr ""

#~ msgid "Create reduce axis IterVar."
#~ msgstr ""

#~ msgid ""
#~ "Layout is composed of upper cases, "
#~ "lower cases and numbers, where upper "
#~ "case indicates a primal axis and "
#~ "the corresponding lower case with factor"
#~ " size indicates the subordinate axis. "
#~ "For example, NCHW16c can describe a "
#~ "5-D tensor of [batch_size, channel, "
#~ "height, width, channel_block]. Here "
#~ "subordinate axis channel_block=16 is the "
#~ "factor size of the primal axis C"
#~ " (channel)."
#~ msgstr ""

#~ msgid ":obj:`layout`"
#~ msgstr ""

#~ msgid ":py:obj:`factor_of <tvm.tir.Layout.factor_of>`\\ \\(axis\\)"
#~ msgstr ""

#~ msgid "Get the factor size of the subordinate axis."
#~ msgstr ""

#~ msgid ":py:obj:`index_of <tvm.tir.Layout.index_of>`\\ \\(axis\\)"
#~ msgstr ""

#~ msgid "Get the index of an axis"
#~ msgstr ""

#~ msgid "The axis name, need to be [a-z,A-Z]"
#~ msgstr ""

#~ msgid ""
#~ "**factor** -- the size of the "
#~ "subordinate-axis of axis (if axis is"
#~ " a primal-axis), or the size of"
#~ " axis itself (if axis is a "
#~ "subordinate-axis). Return -1 if axis "
#~ "is not in the layout."
#~ msgstr ""

#~ msgid "**index** -- The index of the axis, -1 if not found."
#~ msgstr ""

#~ msgid "The variable in the binding."
#~ msgstr ""

#~ msgid "The value in to be binded."
#~ msgstr ""

#~ msgid "The body expression."
#~ msgstr ""

#~ msgid "The data type."
#~ msgstr ""

#~ msgid "The buffer variable in the load expression."
#~ msgstr ""

#~ msgid "The index in the load."
#~ msgstr ""

#~ msgid "The load predicate."
#~ msgstr ""

#~ msgid "The target buffer"
#~ msgstr ""

#~ msgid "The region of source buffer"
#~ msgstr ""

#~ msgid "The input value"
#~ msgstr ""

#~ msgid "The buffer to be prefetched."
#~ msgstr ""

#~ msgid "The bounds to be prefetched."
#~ msgstr ""

#~ msgid "List of input parameters to the function."
#~ msgstr ""

#~ msgid "The body of the function."
#~ msgstr ""

#~ msgid "The return type annotation of the function."
#~ msgstr ""

#~ msgid "The buffer binding map."
#~ msgstr ""

#~ msgid "Attributes of the function, can be None"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`script <tvm.tir.PrimFunc.script>`\\ "
#~ "\\(\\[tir\\_prefix\\, show\\_meta\\]\\)"
#~ msgstr ""

#~ msgid "Print IRModule into TVMScript"
#~ msgstr ""

#~ msgid ":py:obj:`specialize <tvm.tir.PrimFunc.specialize>`\\ \\(param\\_map\\)"
#~ msgstr ""

#~ msgid "Specialize parameters of PrimFunc"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`with_body <tvm.tir.PrimFunc.with_body>`\\ "
#~ "\\(new\\_body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new PrimFunc with the same set signatures but a new body."
#~ msgstr ""

#~ msgid "The tir namespace prefix"
#~ msgstr ""

#~ msgid "Whether to show meta information"
#~ msgstr ""

#~ msgid "**script** -- The TVM Script of the PrimFunc"
#~ msgstr ""

#~ msgid "The mapping from function params to the instance"
#~ msgstr ""

#~ msgid "We can define a Meta TIR function with symbolic shape:"
#~ msgstr ""

#~ msgid "Then we can make it specialized with given shapes or buffers."
#~ msgstr ""

#~ msgid "The specialized function:"
#~ msgstr ""

#~ msgid "**func** -- The new function with parameter specialized"
#~ msgstr ""

#~ msgid "The new body."
#~ msgstr ""

#~ msgid "**new_func** -- The created new function."
#~ msgstr ""

#~ msgid "The data producer."
#~ msgstr ""

#~ msgid "The bound of realize"
#~ msgstr ""

#~ msgid "The realize body"
#~ msgstr ""

#~ msgid "The storage scope associated with this realization"
#~ msgstr ""

#~ msgid "The index arguments of the store."
#~ msgstr ""

#~ msgid "The base expression."
#~ msgstr ""

#~ msgid "The stride of the ramp."
#~ msgstr ""

#~ msgid "The combiner."
#~ msgstr ""

#~ msgid "The source expression."
#~ msgstr ""

#~ msgid "The iteration domain"
#~ msgstr ""

#~ msgid "The reduce condition."
#~ msgstr ""

#~ msgid "The value index."
#~ msgstr ""

#~ msgid "The initial value for output. This can be an int, float or ProducerLoad"
#~ msgstr ""

#~ msgid ""
#~ "A schedule is a set of "
#~ "transformations that change the order of"
#~ " computation but preserve the semantics "
#~ "of computation. Some example of "
#~ "schedules: 1) Split a loop into "
#~ "two; 2) Reorder two loops; 3) "
#~ "Inline the computation of a specific "
#~ "buffer into its consumer"
#~ msgstr ""

#~ msgid ""
#~ "The schedule class stores auxiliary "
#~ "information to schedule correctly and "
#~ "efficiently."
#~ msgstr ""

#~ msgid ""
#~ "Link to tutorial: "
#~ "https://tvm.apache.org/docs/tutorials/language/schedule_primitives.html"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`annotate <tvm.tir.Schedule.annotate>`\\ "
#~ "\\(block\\_or\\_loop\\, ann\\_key\\, ann\\_val\\)"
#~ msgstr ""

#~ msgid "Annotate a block/loop with a key value pair"
#~ msgstr ""

#~ msgid ":py:obj:`bind <tvm.tir.Schedule.bind>`\\ \\(loop\\, thread\\_axis\\)"
#~ msgstr ""

#~ msgid "Bind the input loop to the given thread axis."
#~ msgstr ""

#~ msgid ":py:obj:`blockize <tvm.tir.Schedule.blockize>`\\ \\(loop\\)"
#~ msgstr ""

#~ msgid "Convert the subtree rooted at a specific loop into a block."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cache_read <tvm.tir.Schedule.cache_read>`\\ "
#~ "\\(block\\, read\\_buffer\\_index\\, ...\\)"
#~ msgstr ""

#~ msgid "Create a block that reads a buffer region into a read cache."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cache_write <tvm.tir.Schedule.cache_write>`\\ "
#~ "\\(block\\, write\\_buffer\\_index\\, ...\\)"
#~ msgstr ""

#~ msgid "Create a block that reads a buffer region into a write cache."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`compute_at <tvm.tir.Schedule.compute_at>`\\ "
#~ "\\(block\\, loop\\[\\, preserve\\_unit\\_loops\\]\\)"
#~ msgstr ""

#~ msgid "Compute-At."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`compute_inline <tvm.tir.Schedule.compute_inline>`\\ "
#~ "\\(block\\)"
#~ msgstr ""

#~ msgid "Inline a block into its consumer(s)."
#~ msgstr ""

#~ msgid ":py:obj:`copy <tvm.tir.Schedule.copy>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns a copy of the schedule, "
#~ "including both the state and the "
#~ "symbol table, * guaranteeing that * "
#~ "1) SRef tree is completely "
#~ "reconstructed; * 2) The IRModule being"
#~ " scheduled is untouched; * 3) All "
#~ "the random variables are valid in "
#~ "the copy, pointing to the corresponding"
#~ " sref * reconstructed"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`decompose_reduction "
#~ "<tvm.tir.Schedule.decompose_reduction>`\\ \\(block\\, "
#~ "loop\\)"
#~ msgstr ""

#~ msgid "Decompose a reduction block into two separate blocks."
#~ msgstr ""

#~ msgid ":py:obj:`enter_postproc <tvm.tir.Schedule.enter_postproc>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "A no-op that marks the start of postprocessing phase of scheduling"
#~ msgstr ""

#~ msgid ":py:obj:`fork_seed <tvm.tir.Schedule.fork_seed>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Returns a forked random state as seed for new schedules"
#~ msgstr ""

#~ msgid ":py:obj:`fuse <tvm.tir.Schedule.fuse>`\\ \\(\\*loops\\)"
#~ msgstr ""

#~ msgid "Fuse a list of consecutive loops into one."
#~ msgstr ""

#~ msgid ":py:obj:`get <tvm.tir.Schedule.get>`\\ \\(rand\\_var\\_or\\_sref\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns: - the corresponding Block that"
#~ " a BlockRV evaluates to; - the "
#~ "corresponding For that a LoopRV "
#~ "evaluates to; - the corresponding "
#~ "integer that a ExprRV evaluates to; "
#~ "- the corresponding Block that a "
#~ "block sref points to; - the "
#~ "corresponding For that a loop sref "
#~ "points to;"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_block <tvm.tir.Schedule.get_block>`\\ "
#~ "\\(name\\[\\, func\\_name\\]\\)"
#~ msgstr ""

#~ msgid "Retrieve a block in a specific function with its name"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_child_blocks <tvm.tir.Schedule.get_child_blocks>`\\"
#~ " \\(block\\_or\\_loop\\)"
#~ msgstr ""

#~ msgid "Get the leaf blocks of a specific block/loop"
#~ msgstr ""

#~ msgid ":py:obj:`get_consumers <tvm.tir.Schedule.get_consumers>`\\ \\(block\\)"
#~ msgstr ""

#~ msgid "Get the consumers of a specific block"
#~ msgstr ""

#~ msgid ":py:obj:`get_loops <tvm.tir.Schedule.get_loops>`\\ \\(block\\)"
#~ msgstr ""

#~ msgid "Get the parent loops of the block in its scope, from outer to inner"
#~ msgstr ""

#~ msgid ":py:obj:`get_producers <tvm.tir.Schedule.get_producers>`\\ \\(block\\)"
#~ msgstr ""

#~ msgid "Get the producers of a specific block"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_sref <tvm.tir.Schedule.get_sref>`\\ "
#~ "\\(rand\\_var\\_or\\_stmt\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns the corresponding sref to the"
#~ " given 1) LoopRV 2) BlockRV 3) "
#~ "Block 4) For"
#~ msgstr ""

#~ msgid ":py:obj:`parallel <tvm.tir.Schedule.parallel>`\\ \\(loop\\)"
#~ msgstr ""

#~ msgid "Parallelize the input loop."
#~ msgstr ""

#~ msgid ":py:obj:`remove_rv <tvm.tir.Schedule.remove_rv>`\\ \\(rand\\_var\\)"
#~ msgstr ""

#~ msgid "Remove a random variable from the symbol table"
#~ msgstr ""

#~ msgid ":py:obj:`reorder <tvm.tir.Schedule.reorder>`\\ \\(\\*ordered\\_loops\\)"
#~ msgstr ""

#~ msgid "Reorder a list of loops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reverse_compute_at "
#~ "<tvm.tir.Schedule.reverse_compute_at>`\\ \\(block\\, "
#~ "loop\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Reverse-Compute-At."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reverse_compute_inline "
#~ "<tvm.tir.Schedule.reverse_compute_inline>`\\ \\(block\\)"
#~ msgstr ""

#~ msgid "Inline a block into its only producer."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`rfactor <tvm.tir.Schedule.rfactor>`\\ \\(loop\\,"
#~ " factor\\_axis\\)"
#~ msgstr ""

#~ msgid "Factorize an associative reduction block by the specified loop."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sample_categorical "
#~ "<tvm.tir.Schedule.sample_categorical>`\\ \\(candidates\\, "
#~ "probs\\[\\, decision\\]\\)"
#~ msgstr ""

#~ msgid "Sample an integer given the probability distribution"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sample_compute_location "
#~ "<tvm.tir.Schedule.sample_compute_location>`\\ \\(block\\[\\, "
#~ "decision\\]\\)"
#~ msgstr ""

#~ msgid "Sample a compute-at location of the given block"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sample_perfect_tile "
#~ "<tvm.tir.Schedule.sample_perfect_tile>`\\ \\(loop\\, "
#~ "n\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Sample the factors to perfect tile a specific loop"
#~ msgstr ""

#~ msgid ":py:obj:`seed <tvm.tir.Schedule.seed>`\\ \\(seed\\)"
#~ msgstr ""

#~ msgid "Seed the randomness"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`set_scope <tvm.tir.Schedule.set_scope>`\\ "
#~ "\\(block\\, buffer\\_index\\, storage\\_scope\\)"
#~ msgstr ""

#~ msgid ""
#~ "Set the storage scope of a buffer,"
#~ " where the buffer is specified by "
#~ "the a block and a write-index"
#~ msgstr ""

#~ msgid ":py:obj:`show <tvm.tir.Schedule.show>`\\ \\(rand\\_var\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns a string representation of the"
#~ " value that the random variable "
#~ "evaluates to"
#~ msgstr ""

#~ msgid ":py:obj:`split <tvm.tir.Schedule.split>`\\ \\(loop\\, factors\\)"
#~ msgstr ""

#~ msgid "Split a loop into a list of consecutive loops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`storage_align <tvm.tir.Schedule.storage_align>`\\ "
#~ "\\(block\\, buffer\\_index\\, axis\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Set alignment requirement for specific "
#~ "dimension such that stride[axis] == k"
#~ " * factor + offset for some k."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tensorize <tvm.tir.Schedule.tensorize>`\\ "
#~ "\\(block\\_or\\_loop\\, tensor\\_intrin\\)"
#~ msgstr ""

#~ msgid "Tensorize the computation enclosed by loop with the tensor intrinsic."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`unannotate <tvm.tir.Schedule.unannotate>`\\ "
#~ "\\(block\\_or\\_loop\\, ann\\_key\\)"
#~ msgstr ""

#~ msgid "Unannotate a block/loop's annotation with key ann_key"
#~ msgstr ""

#~ msgid ":py:obj:`unroll <tvm.tir.Schedule.unroll>`\\ \\(loop\\)"
#~ msgstr ""

#~ msgid "Unroll the input loop."
#~ msgstr ""

#~ msgid ":py:obj:`vectorize <tvm.tir.Schedule.vectorize>`\\ \\(loop\\)"
#~ msgstr ""

#~ msgid "Vectorize the input loop."
#~ msgstr ""

#~ msgid "**Attributes:**"
#~ msgstr ""

#~ msgid ":py:obj:`mod <tvm.tir.Schedule.mod>`\\"
#~ msgstr ""

#~ msgid "Returns the AST of the module being scheduled"
#~ msgstr ""

#~ msgid ":py:obj:`state <tvm.tir.Schedule.state>`\\"
#~ msgstr ""

#~ msgid "Returns the ScheduleState in the current schedule class"
#~ msgstr ""

#~ msgid ":py:obj:`trace <tvm.tir.Schedule.trace>`\\"
#~ msgstr ""

#~ msgid "Returns the internally maintained trace of scheduling program execution"
#~ msgstr ""

#~ msgid "The block/loop to be annotated"
#~ msgstr ""

#~ msgid "The annotation key"
#~ msgstr ""

#~ msgid "The annotation value"
#~ msgstr ""

#~ msgid "Before annotate, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do annotate:"
#~ msgstr ""

#~ msgid "After applying annotate, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Bind the input loop to the given"
#~ " thread axis. It requires: 1) The "
#~ "scope block that the loop is in"
#~ " should have stage-pipeline property "
#~ "2) All the blocks under the loop"
#~ " are complete blocks or reduction "
#~ "blocks, and have affine bindings 3) "
#~ "For each block under the loop, if"
#~ " the thread axis starts with "
#~ "\"threadIdx`, the loop can only be "
#~ "contained in data-parallel block iter"
#~ " and reduction block iters' bindings. "
#~ "Otherwise the loop can only be "
#~ "contained in data-parallel block iters'"
#~ " bindings"
#~ msgstr ""

#~ msgid "The loop to be bound to the thread axis"
#~ msgstr ""

#~ msgid ""
#~ "The thread axis to be bound to "
#~ "the loop. Possible candidates: - "
#~ "blockIdx.x/y/z - threadIdx.x/y/z - "
#~ "vthread.x/y/z - vthread (It is a "
#~ "legacy behavior that will be deprecated."
#~ " Please use `vthread.x/y/z` instead.)"
#~ msgstr ""

#~ msgid "Before bind, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do bind:"
#~ msgstr ""

#~ msgid "After applying bind, the IR becomes:"
#~ msgstr ""

#~ msgid "The root of the subtree."
#~ msgstr ""

#~ msgid "**result** -- The new block."
#~ msgstr ""

#~ msgid "Before blockize, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do set_scope:"
#~ msgstr ""

#~ msgid "After applying blockize, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "blockize requires there is exactly one"
#~ " block under the given loop and "
#~ "the bindings of the block are "
#~ "divisible by the subspace represented by"
#~ " the loops starting at the given "
#~ "loop."
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a read cache. It "
#~ "requires:"
#~ msgstr ""

#~ msgid "There is at most one block who write the buffer in the scope."
#~ msgstr ""

#~ msgid "The scope block have stage-pipeline property."
#~ msgstr ""

#~ msgid "The consumer block of the target buffer."
#~ msgstr ""

#~ msgid "The index of the buffer in block's read region."
#~ msgstr ""

#~ msgid "The target storage scope."
#~ msgstr ""

#~ msgid "**cached_block** -- The block of the cache stage"
#~ msgstr ""

#~ msgid "Before cache_read, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and cache_read:"
#~ msgstr ""

#~ msgid "After applying cache_read, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a write cache. It "
#~ "requires:"
#~ msgstr ""

#~ msgid "There is only one block who write the buffer in the scope."
#~ msgstr ""

#~ msgid "The producer block of the target buffer."
#~ msgstr ""

#~ msgid "The index of the buffer in block's write region."
#~ msgstr ""

#~ msgid "Before cache_write, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and cache_write:"
#~ msgstr ""

#~ msgid "After applying cache_write, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Compute-At. Move a producer block "
#~ "under the specific loop, and regenerate"
#~ " the loops induced by the block "
#~ "so that the buffer region produced "
#~ "by the producer block could cover "
#~ "those regions consumed by its consumer"
#~ " blocks under the given loop. It "
#~ "requires:"
#~ msgstr ""

#~ msgid ""
#~ "`block` and `loop` are under the "
#~ "same scope, `loop` is not the "
#~ "ancestor of `block`"
#~ msgstr ""

#~ msgid "The scope block has stage-pipeline property"
#~ msgstr ""

#~ msgid ""
#~ "3) The subtree of the scope block,"
#~ " where the given block is in, "
#~ "satisfies the compact dataflow condition. "
#~ "i.e. all the blocks in the scope"
#~ " block's subtree must be either "
#~ "complete block or reduction block"
#~ msgstr ""

#~ msgid ""
#~ "4) The block is not an output "
#~ "block with regard to the scope "
#~ "block, i.e. the buffers written by "
#~ "the block are allocated under the "
#~ "scope block"
#~ msgstr ""

#~ msgid "All the consumers of the block are under the given loop"
#~ msgstr ""

#~ msgid "The block to be moved"
#~ msgstr ""

#~ msgid "The loop where the block to be moved under"
#~ msgstr ""

#~ msgid "Whether to keep the trivial loops whose extents are 1"
#~ msgstr ""

#~ msgid "Before compute-at, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do compute-at:"
#~ msgstr ""

#~ msgid "After applying compute-at, the IR becomes:"
#~ msgstr ""

#~ msgid "Inline a block into its consumer(s). It requires:"
#~ msgstr ""

#~ msgid "The block is a complete non-root block, which only produces one buffer"
#~ msgstr ""

#~ msgid "The block must not be the only leaf in the scope."
#~ msgstr ""

#~ msgid ""
#~ "The body of the block must be "
#~ "a BufferStore statement in the form "
#~ "of, ``A[i, j, k, ...] = ...`` "
#~ "where the indices of the LHS are"
#~ " all distinct atomic variables, and "
#~ "no variables other than those indexing"
#~ " variables are allowed in the "
#~ "statement."
#~ msgstr ""

#~ msgid "The block to be inlined to its consumer(s)"
#~ msgstr ""

#~ msgid "Before compute-inline, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do compute-inline:"
#~ msgstr ""

#~ msgid "After applying compute-inline, the IR becomes:"
#~ msgstr ""

#~ msgid "**copy** -- A new copy of the schedule"
#~ msgstr ""

#~ msgid ""
#~ "The init block, which is translated "
#~ "from the init statement of the "
#~ "reduction block;"
#~ msgstr ""

#~ msgid "The update block, which is the original block without init statement."
#~ msgstr ""

#~ msgid "The init block is inserted right before the given loop."
#~ msgstr ""

#~ msgid "The schedule primitive requires:"
#~ msgstr ""

#~ msgid "The input block is a reduction block."
#~ msgstr ""

#~ msgid "The input loop is the ancestor of the block."
#~ msgstr ""

#~ msgid ""
#~ "The input loop is not lower than"
#~ " all the loops related to reduce "
#~ "block var."
#~ msgstr ""

#~ msgid "The reduction block to be decomposed"
#~ msgstr ""

#~ msgid "The loop above which the init block is inserted before."
#~ msgstr ""

#~ msgid "**init_block** -- The init block"
#~ msgstr ""

#~ msgid "Before decompose-reduction, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do decompose-reduction with specified loop:"
#~ msgstr ""

#~ msgid "After applying decompose-reduction, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "**seed** -- The forked random state, "
#~ "not the same as the current random"
#~ " state"
#~ msgstr ""

#~ msgid ""
#~ "Fuse a list of consecutive loops "
#~ "into one. It requires: 1) The "
#~ "loops can't have annotations or thread"
#~ " bindings. 2) The (i+1)-th loop must"
#~ " be the only child of the i-th"
#~ " loop. 3) All loops must start "
#~ "with 0. 4) The domain of a "
#~ "loop to be fused cannot depend on"
#~ " another loop to be fused."
#~ msgstr ""

#~ msgid "The loops to be fused"
#~ msgstr ""

#~ msgid "**fused_loop** -- The new loop after fusion"
#~ msgstr ""

#~ msgid "Before applying fuse, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do fuse:"
#~ msgstr ""

#~ msgid "After applying fuse, the IR becomes:"
#~ msgstr ""

#~ msgid "The random variable / sref to be evaluated"
#~ msgstr ""

#~ msgid "**result** -- The corresponding result"
#~ msgstr ""

#~ msgid "The name of the block"
#~ msgstr ""

#~ msgid "The name of the function"
#~ msgstr ""

#~ msgid ""
#~ "**block** -- The block retrieved "
#~ "IndexError is raised if 0 or "
#~ "multiple blocks exist with the specific"
#~ " name."
#~ msgstr ""

#~ msgid "The query block/loop"
#~ msgstr ""

#~ msgid "**blocks** -- A list of leaf blocks inside a specific block/loop"
#~ msgstr ""

#~ msgid "The block in the query"
#~ msgstr ""

#~ msgid "**consumers** -- A list of consumers of the given block"
#~ msgstr ""

#~ msgid "The query block"
#~ msgstr ""

#~ msgid ""
#~ "**loops** -- A list of loops above"
#~ " the given block in its scope, "
#~ "from outer to inner"
#~ msgstr ""

#~ msgid "**producers** -- A list of producers of the given block"
#~ msgstr ""

#~ msgid ""
#~ "Parallelize the input loop. It requires:"
#~ " 1) The scope block that the "
#~ "loop is in should have stage-"
#~ "pipeline property 2) All the blocks "
#~ "under the loop are complete blocks "
#~ "or reduction blocks, and have affine "
#~ "bindings 3) For each block under "
#~ "the loop, the loop can only be "
#~ "contained in data-parallel block iters'"
#~ " bindings"
#~ msgstr ""

#~ msgid "The loop to be parallelized"
#~ msgstr ""

#~ msgid "Before parallel, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do parallel:"
#~ msgstr ""

#~ msgid "After applying parallel, the IR becomes:"
#~ msgstr ""

#~ msgid "The random variable to be removed"
#~ msgstr ""

#~ msgid ""
#~ "Reorder a list of loops. It "
#~ "doesn't require the loops to be "
#~ "consecutive. It requires: 1) The loops"
#~ " are in the same chain. That "
#~ "means: the loops can be ordered to"
#~ " [l_1, l_2, ... , l_n] where "
#~ "l_i is an ancestor of l_{i+1} and"
#~ " there are only single-branch loops"
#~ " between l_1 and l_n (which also "
#~ "indicates they are under the same "
#~ "scope). 2) After reordering, the domain"
#~ " of an outer loop cannot depend "
#~ "on any of the inner loops. 3) "
#~ "For every block under the loop "
#~ "nests, its block binding must be "
#~ "affine, and the block variables must "
#~ "be either data parallel or reduction."
#~ " 4) No duplicated loops are allowed"
#~ " in the arguments."
#~ msgstr ""

#~ msgid "The loops in the new order"
#~ msgstr ""

#~ msgid "Before reorder, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reorder:"
#~ msgstr ""

#~ msgid "After applying reorder, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Reverse-Compute-At. Move a consumer "
#~ "block under the specific loop, and "
#~ "regenerate the loops induced by the "
#~ "block so that the buffer region "
#~ "consumed by the consumer block could "
#~ "cover those regions produced by its "
#~ "producer blocks under the given loop."
#~ " It requires:"
#~ msgstr ""

#~ msgid "All the producers of the block are under the given loop"
#~ msgstr ""

#~ msgid "Before reverse-compute-at, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reverse-compute-at:"
#~ msgstr ""

#~ msgid "After applying reverse-compute-at, the IR becomes:"
#~ msgstr ""

#~ msgid "Inline a block into its only producer. It requires:"
#~ msgstr ""

#~ msgid ""
#~ "The block is a complete non-root"
#~ " block, which only produces and "
#~ "consumes one buffer"
#~ msgstr ""

#~ msgid ""
#~ "The only producer of the block is"
#~ " a read-after-write producer and "
#~ "a complete non-root block"
#~ msgstr ""

#~ msgid ""
#~ "The body of the block must be "
#~ "a BufferStore statement in the form "
#~ "of, ``B[f(i, j, k, ...)] = g(i,"
#~ " j, k, A[i, j, k, ...] ...)``"
#~ " where the indices of each "
#~ "`BufferLoad` on the RHS are all "
#~ "distinct atomic variables, and no "
#~ "variables other than those indexing "
#~ "variables are allowed in the statement."
#~ msgstr ""

#~ msgid "The block to be inlined to its producer"
#~ msgstr ""

#~ msgid "Before reverse-compute-inline, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reverse-compute-inline:"
#~ msgstr ""

#~ msgid "After applying reverse-compute-inline, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "An associative reduction cannot be "
#~ "parallelized directly, because it leads "
#~ "to potential race condition during "
#~ "accumulation. Alternatively, the reduction "
#~ "could be factorized on a loop with"
#~ " the following steps: - Step 1: "
#~ "evenly slice the reduction into `n` "
#~ "separate chunks, where `n` is the "
#~ "loop extent - Step 2: compute the"
#~ " chunks separately and write the "
#~ "result into `n` intermediate buffers; -"
#~ " Step 3: accumulate the `n` separate"
#~ " buffer into the result buffer. Note"
#~ " that the Step 2 above introduces "
#~ "opportunities for parallelization."
#~ msgstr ""

#~ msgid ""
#~ "RFactor is a schedule primitive that "
#~ "implements the transformation described above:"
#~ " Given a block that writes to "
#~ "buffer `B`, it factorizes a loop "
#~ "of extent `n`."
#~ msgstr ""

#~ msgid ""
#~ "For example, the pseudocode below "
#~ "accumulates `B[i] = sum(A[i, : , :"
#~ " ])`:"
#~ msgstr ""

#~ msgid ""
#~ "Suppose RFactor is applied on the "
#~ "innermost loop `k` and `factor_axis ="
#~ " 1`. RFactor then creates an "
#~ "intermediate buffer and two blocks."
#~ msgstr ""

#~ msgid ""
#~ "1. The intermediate buffer, or \"rf-"
#~ "buffer\" is a buffer of rank "
#~ "`ndim(B) + 1` and size `size(B) *"
#~ " n`, whose shape expands from "
#~ "`shape(B)` by adding an axis of "
#~ "`n` at the position specified by "
#~ "`factor_axis`. For example,"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 0  => shape(B_rf) = [n, 1, 2, 3]"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 1  => shape(B_rf) = [1, n, 2, 3]"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 2  => shape(B_rf) = [1, 2, n, 3]"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 3  => shape(B_rf) = [1, 2, 3, n]"
#~ msgstr ""

#~ msgid ""
#~ "2. The rfactor block, or \"rf-"
#~ "block\", is a block that writes to"
#~ " the `rf-buffer` without accumulating "
#~ "over the loop `k`, i.e. the loop"
#~ " `k` is converted from a reduction"
#~ " loop to a data parallel loop. "
#~ "In our example, the rf-block is:"
#~ msgstr ""

#~ msgid ""
#~ "3. The write-back block, or "
#~ "`wb-block`, is a block that "
#~ "accumulates the rf-buffer into the "
#~ "result buffer. All the reduction loops"
#~ " are removed except the loop `k` "
#~ "for accumulation. In our example, the"
#~ " wb-block is:"
#~ msgstr ""

#~ msgid "The loop outside block for which we want to do rfactor"
#~ msgstr ""

#~ msgid ""
#~ "The position where the new dimension "
#~ "is placed in the new introduced "
#~ "rfactor buffer"
#~ msgstr ""

#~ msgid ""
#~ "**rf_block** -- The block which computes"
#~ " partial results over each slices "
#~ "(i.e., the first block as described "
#~ "in the above illustration)"
#~ msgstr ""

#~ msgid "Before rfactor, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do rfactor:"
#~ msgstr ""

#~ msgid "After applying rfactor, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Rfactor requires: 1) `loop` has only "
#~ "one child block, and it is a "
#~ "reduction block; 2) `loop` is a "
#~ "reduction loop, i.e. the loop variable"
#~ " is bound to only reduction variables"
#~ " in the block binding; 3) `loop` "
#~ "is not parallelized, vectorized, unrolled "
#~ "or bound to any thread axis; 4)"
#~ " The block scope that `loop` is "
#~ "in is a staged-pipeline; 5) The"
#~ " outermost loop outside the reduction "
#~ "block should has the reduction block "
#~ "as its first child block; 6) The"
#~ " outermost reduction loop should have "
#~ "only one child block; 7) An unary"
#~ " extent loop that is not bound "
#~ "to any reduction or data parallel "
#~ "variables in the block binding should"
#~ " not appear under some reduction "
#~ "loop; 8) The reduction block should "
#~ "write to only one buffer, and its"
#~ " init and body are both simple "
#~ "`BufferStore`s, and the pattern is "
#~ "registered as an associative reducer. "
#~ "The pre-defined patterns include: plus,"
#~ " multiplication, min and max; 9) Each"
#~ " of the loops on top of the "
#~ "block cannot be bound to a data"
#~ " parallel and a reduction block "
#~ "binding at the same time; 10) "
#~ "`factor_axis` should be in range "
#~ "`[-ndim(B) - 1, ndim(B)]`, where `B` "
#~ "is the buffer that the reduction "
#~ "block writes to. Negative indexing is"
#~ " normalized according to numpy convention."
#~ msgstr ""

#~ msgid "The candidates to be sampled from"
#~ msgstr ""

#~ msgid "The probability of each candidate"
#~ msgstr ""

#~ msgid "The sampling decision, if any"
#~ msgstr ""

#~ msgid "**result** -- The random variable sampled from candidates"
#~ msgstr ""

#~ msgid "The block whose compute-at location is to be sampled"
#~ msgstr ""

#~ msgid "The sampling decision"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The sampled loop where "
#~ "the input block is to be computed"
#~ " at"
#~ msgstr ""

#~ msgid "The loop to be tiled"
#~ msgstr ""

#~ msgid "The number of tiles to be sampled"
#~ msgstr ""

#~ msgid "The maximum tile size allowed to be sampled in the innermost loop"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- A list of length "
#~ "`n`, the random perfect tile sizes "
#~ "sampled"
#~ msgstr ""

#~ msgid "The new random seed, -1 if use device random, otherwise non-negative"
#~ msgstr ""

#~ msgid "The producer block of the buffer"
#~ msgstr ""

#~ msgid "The index of the buffer in block's write region"
#~ msgstr ""

#~ msgid "The storage scope to be set"
#~ msgstr ""

#~ msgid "Before set_scope, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "After applying set_scope, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Set_scope requires the buffer to be "
#~ "an intermediate buffer defined via "
#~ "`alloc_buffer`."
#~ msgstr ""

#~ msgid "The random variable to be evaluated"
#~ msgstr ""

#~ msgid "**str_repr** -- The string representation"
#~ msgstr ""

#~ msgid ""
#~ "Split a loop into a list of "
#~ "consecutive loops. It requires: 1) The"
#~ " loop can't have annotation or thread"
#~ " binding. 2) The loop must start "
#~ "with 0. Predicates may be added to"
#~ " ensure the total loop numbers keeps"
#~ " unchanged. In `factors`, at most one"
#~ " of the factors can be None, "
#~ "which will be automatically inferred."
#~ msgstr ""

#~ msgid "The loop to be split"
#~ msgstr ""

#~ msgid ""
#~ "The splitting factors Potential inputs "
#~ "are: - None - ExprRV - Positive"
#~ " constant integers"
#~ msgstr ""

#~ msgid "**split_loops** -- The new loops after split"
#~ msgstr ""

#~ msgid "Before split, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do split:"
#~ msgstr ""

#~ msgid "After applying split, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Set alignment requirement for specific "
#~ "dimension such that stride[axis] == k"
#~ " * factor + offset for some k."
#~ " This is useful to set memory "
#~ "layout for more friendly memory access"
#~ " pattern. For example, we can set "
#~ "alignment to be factor=2, offset=1 to"
#~ " avoid bank conflict for thread "
#~ "access on higher dimension in GPU "
#~ "shared memory."
#~ msgstr ""

#~ msgid "The producer block of the buffer."
#~ msgstr ""

#~ msgid "The dimension to be specified for alignment."
#~ msgstr ""

#~ msgid "The factor multiple of alignment."
#~ msgstr ""

#~ msgid "The required offset factor."
#~ msgstr ""

#~ msgid "Before storage_align, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do storage_align:"
#~ msgstr ""

#~ msgid "After applying storage_align, the IR becomes:"
#~ msgstr ""

#~ msgid "After lowering passes, buffer B will have strides as [129, 1]."
#~ msgstr ""

#~ msgid ""
#~ "Storage_align requires the buffer to be"
#~ " an intermediate buffer defined via "
#~ "`alloc_buffer`."
#~ msgstr ""

#~ msgid "The loop to be tensorized."
#~ msgstr ""

#~ msgid "The tensor intrin or the name of the tensor intrin."
#~ msgstr ""

#~ msgid "Before tensorize, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Declare and register the tensor intrinsic:"
#~ msgstr ""

#~ msgid "Create the schedule and do tensorize:"
#~ msgstr ""

#~ msgid "After applying tensorize, the IR becomes:"
#~ msgstr ""

#~ msgid "The block/loop to be unannotated"
#~ msgstr ""

#~ msgid "Before unannotate, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "After applying unannotate, the IR becomes:"
#~ msgstr ""

#~ msgid "Unroll the input loop. It requires nothing"
#~ msgstr ""

#~ msgid "The loop to be unrolled"
#~ msgstr ""

#~ msgid "Before unroll, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do unroll:"
#~ msgstr ""

#~ msgid "After applying unroll, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Vectorize the input loop. It requires:"
#~ " 1) The scope block that the "
#~ "loop is in should have stage-"
#~ "pipeline property 2) All the blocks "
#~ "under the loop are complete blocks "
#~ "or reduction blocks, and have affine "
#~ "bindings 3) For each block under "
#~ "the loop, the loop can only be "
#~ "contained in data-parallel block iters'"
#~ " bindings"
#~ msgstr ""

#~ msgid "The loop to be vectorized"
#~ msgstr ""

#~ msgid "Before vectorize, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do vectorize:"
#~ msgstr ""

#~ msgid "After applying vectorize, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "The data structure contains the "
#~ "following information 1) The AST being"
#~ " scheduled (mod) 2) The sref tree "
#~ "of schedulable statements (indicated by "
#~ "the srefs) 3) The dependency information"
#~ " of each block scope (block_info) 4)"
#~ " A reverse mapping from the AST "
#~ "nodes to that in the sref tree "
#~ "(get_sref) 5) A debug flag, if "
#~ "set, extra checking is enabled "
#~ "(debug_mask)"
#~ msgstr ""

#~ msgid "The AST of the module being scheduled"
#~ msgstr ""

#~ msgid ""
#~ "Do extra correctness checking after the"
#~ " object construction and each time "
#~ "after calling the Replace method."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_block_scope "
#~ "<tvm.tir.ScheduleState.get_block_scope>`\\ \\(block\\_sref\\)"
#~ msgstr ""

#~ msgid "Get the BlockScope correpsonding to the block sref"
#~ msgstr ""

#~ msgid ":py:obj:`get_sref <tvm.tir.ScheduleState.get_sref>`\\ \\(stmt\\)"
#~ msgstr ""

#~ msgid "Return the corresponding sref that points to the stmt"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`replace <tvm.tir.ScheduleState.replace>`\\ "
#~ "\\(src\\_sref\\, tgt\\_stmt\\[\\, "
#~ "block\\_sref\\_reuse\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Replace the part of the AST, as"
#~ " being pointed to by `src_sref`, with"
#~ " a specific statement `tgt_stmt`, and "
#~ "maintain the sref tree accordingly."
#~ msgstr ""

#~ msgid "The block sref to be retrieved"
#~ msgstr ""

#~ msgid "**sref** -- The corresponding sref"
#~ msgstr ""

#~ msgid "The schedulable statement in the TensorIR to be retrieved for its sref"
#~ msgstr ""

#~ msgid ""
#~ "Replace the part of the AST, as"
#~ " being pointed to by `src_sref`, with"
#~ " a specific statement `tgt_stmt`, and "
#~ "maintain the sref tree accordingly. "
#~ "Replace will try to perform copy "
#~ "on write as much as possible when"
#~ " the ScheduleState holds the only "
#~ "copy to the IRModule and IR nodes."
#~ msgstr ""

#~ msgid ""
#~ "Only 3 types of replacements are "
#~ "allowed: from `src_sref->stmt` to `tgt_stmt`."
#~ " 1) Block -> Block 2) Loop ->"
#~ " Loop 3) Loop -> BlockRealize"
#~ msgstr ""

#~ msgid "The sref to the statement to be replaced in the TensorIR AST"
#~ msgstr ""

#~ msgid "The statement to be replaced to"
#~ msgstr ""

#~ msgid ""
#~ "Maps an old block (to be replaced"
#~ " in the subtree under `src_sref->stmt`) "
#~ "to a new block (replaced to, in"
#~ " the subtree under `tgt_stmt`), and "
#~ "enforces reuse of srefs between them "
#~ "(rather than create new srefs) i.e. "
#~ "after being replaced, the sref that "
#~ "points to the old block will point"
#~ " to the new one"
#~ msgstr ""

#~ msgid ""
#~ "The reuse of loop srefs are "
#~ "detected automatically according to the "
#~ "reuse of loop vars."
#~ msgstr ""

#~ msgid ""
#~ "Select may compute both true_value and"
#~ " false_value. Use :py:class:`tvm.tir.if_then_else` "
#~ "instead if you want to get a "
#~ "conditional expression that only evaluates "
#~ "the correct branch."
#~ msgstr ""

#~ msgid "The condition expression."
#~ msgstr ""

#~ msgid "The value to take when condition is true."
#~ msgstr ""

#~ msgid "The value to take when condition is false."
#~ msgstr ""

#~ msgid "The statements"
#~ msgstr ""

#~ msgid "The vectors"
#~ msgstr ""

#~ msgid "The indices"
#~ msgstr ""

#~ msgid "which is greater or equal to zero."
#~ msgstr ""

#~ msgid "The name"
#~ msgstr ""

#~ msgid ""
#~ "Glossary - Block sref: An StmtSref "
#~ "that points to a TensorIR block. -"
#~ " Loop sref: An StmtSRef that points"
#~ " to a TensorIR for loop. - "
#~ "Parent sref: The parent sref of an"
#~ " sref is the block/loop sref that "
#~ "points to its closest schedulable "
#~ "statement of its ancestors on the "
#~ "TensorIR AST. - Root sref: Sref to"
#~ " the root block. Every sref has "
#~ "exactly one parent sref except for "
#~ "root sref. - Sref tree: The "
#~ "parent-children-relationship of srefs that"
#~ " forms a tree, uniquely determined by"
#~ " the TensorIR AST."
#~ msgstr ""

#~ msgid ":py:obj:`inline_mark <tvm.tir.StmtSRef.inline_mark>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "A special StmtSRef, which doesn't point"
#~ " to any stmt in the AST, only"
#~ " serving as a \"mark\" to hint "
#~ "compute-at to do the work of "
#~ "compute-inline"
#~ msgstr ""

#~ msgid ":py:obj:`root_mark <tvm.tir.StmtSRef.root_mark>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "A special StmtSRef, which doesn't point"
#~ " to any stmt in the AST, only"
#~ " serving as a \"mark\" to hint "
#~ "compute-at to do nothing"
#~ msgstr ""

#~ msgid ":py:obj:`parent <tvm.tir.StmtSRef.parent>`\\"
#~ msgstr ""

#~ msgid "The parent sref"
#~ msgstr ""

#~ msgid ":py:obj:`stmt <tvm.tir.StmtSRef.stmt>`\\"
#~ msgstr ""

#~ msgid "The block/for stmt the object refers to"
#~ msgstr ""

#~ msgid "The buffer Variable."
#~ msgstr ""

#~ msgid "The value we want to store."
#~ msgstr ""

#~ msgid "The index in the store expression."
#~ msgstr ""

#~ msgid "The store predicate."
#~ msgstr ""

#~ msgid "The function to describe the computation."
#~ msgstr ""

#~ msgid "The function of the implementation for the execution."
#~ msgstr ""

#~ msgid ":py:obj:`get <tvm.tir.TensorIntrin.get>`\\ \\(name\\)"
#~ msgstr ""

#~ msgid "Look up a tensor intrinsic by its name."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`register <tvm.tir.TensorIntrin.register>`\\ "
#~ "\\(name\\, desc\\, impl\\)"
#~ msgstr ""

#~ msgid "Register a tensor intrinsic with its name."
#~ msgstr ""

#~ msgid "The name of the TensorIntrin to look up."
#~ msgstr ""

#~ msgid "**result** -- The TensorIntrin with the specified name."
#~ msgstr ""

#~ msgid "The name of the TensorIntrin to register."
#~ msgstr ""

#~ msgid "The termination condition."
#~ msgstr ""

#~ msgid "Input argument."
#~ msgstr ""

#~ msgid "The location of this operator in the source code."
#~ msgstr ""

#~ msgid "**y** -- The result."
#~ msgstr ""

#~ msgid "arguments"
#~ msgstr ""

#~ msgid "List of symbolic boolean expressions"
#~ msgstr ""

#~ msgid "**expr** -- Expression"
#~ msgstr ""

#~ msgid "**bijective_layout** -- The created bijective layout"
#~ msgstr ""

#~ msgid "The data type of the result."
#~ msgstr ""

#~ msgid "The extern function name."
#~ msgstr ""

#~ msgid "Positional arguments."
#~ msgstr ""

#~ msgid "**call** -- The call expression."
#~ msgstr ""

#~ msgid ""
#~ "Intrinsics can be overloaded with "
#~ "multiple data types via the intrinsic"
#~ " translation rule."
#~ msgstr ""

#~ msgid "The intrinsic function name."
#~ msgstr ""

#~ msgid "The name of the llvm intrinsic function."
#~ msgstr ""

#~ msgid "Poistional arguments."
#~ msgstr ""

#~ msgid ""
#~ "The argument to packed function can "
#~ "be Expr or Buffer. The argument is"
#~ " the corresponding POD type when Expr"
#~ " is presented."
#~ msgstr ""

#~ msgid ""
#~ "When the argument is Buffer, the "
#~ "corresponding PackedFunc will recieve an "
#~ "TVMArrayHandle whose content is valid "
#~ "during the callback period. If the "
#~ "PackedFunc is a python callback, then"
#~ " the corresponding argument is NDArray."
#~ msgstr ""

#~ msgid ":obj:`te.extern`"
#~ msgstr ""

#~ msgid "Create tensor with extern function call."
#~ msgstr ""

#~ msgid "Input 32 or 64 bit integer. The result is undefined if the input is 0."
#~ msgstr ""

#~ msgid "A binary function which takes two Expr as input to return a Expr."
#~ msgstr ""

#~ msgid "A function which takes a type string as input to return a const Expr."
#~ msgstr ""

#~ msgid ""
#~ "**reducer** -- A function which creates"
#~ " a reduce expression over axis. There"
#~ " are two ways to use it:  1."
#~ " accept (expr, axis, where) to "
#~ "produce an Reduce Expr on    specified"
#~ " axis; 2. simply use it with "
#~ "multiple Exprs."
#~ msgstr ""

#~ msgid ""
#~ "**reducer** -- A function which creates"
#~ " a reduce expression over axis. There"
#~ " are two ways to use it:"
#~ msgstr ""

#~ msgid "accept (expr, axis, where) to produce an Reduce Expr on specified axis;"
#~ msgstr ""

#~ msgid "simply use it with multiple Exprs."
#~ msgstr ""

#~ msgid "示例"
#~ msgstr ""

#~ msgid ""
#~ "Normally buffer is created automatically "
#~ "during lower and build. This is "
#~ "only needed if user want to "
#~ "specify their own buffer layout."
#~ msgstr ""

#~ msgid "See the note below for detailed discussion on usage of buffer."
#~ msgstr ""

#~ msgid "The shape of the buffer."
#~ msgstr ""

#~ msgid "The name of the buffer."
#~ msgstr ""

#~ msgid "The data pointer in the buffer."
#~ msgstr ""

#~ msgid "The stride of the buffer."
#~ msgstr ""

#~ msgid ""
#~ "The beginning offset of the array "
#~ "to data. In terms of number of "
#~ "elements of dtype."
#~ msgstr ""

#~ msgid ""
#~ "The storage scope of the buffer, "
#~ "if not global. If scope equals "
#~ "empty string, it means it is "
#~ "global memory."
#~ msgstr ""

#~ msgid ""
#~ "The alignment of data pointer in "
#~ "bytes. If -1 is passed, the "
#~ "alignment will be set to TVM's "
#~ "internal default."
#~ msgstr ""

#~ msgid ""
#~ "The factor of elem_offset field, when"
#~ " set, elem_offset is required to be"
#~ " multiple of offset_factor. If 0 is"
#~ " pssed, the alignment will be set "
#~ "to 1. if non-zero is passed, "
#~ "we will created a Var for "
#~ "elem_offset if elem_offset is not None."
#~ msgstr ""

#~ msgid ""
#~ "auto_broadcast buffer allows one to "
#~ "implement broadcast computation without "
#~ "considering whether dimension size equals "
#~ "to one. TVM maps buffer[i][j][k] -> "
#~ "buffer[i][0][k] if dimension j's shape "
#~ "equals 1."
#~ msgstr ""

#~ msgid "The location of the decl_buffer creation in the source."
#~ msgstr ""

#~ msgid "**buffer** -- The created buffer"
#~ msgstr ""

#~ msgid ""
#~ "Here's an example of how broadcast "
#~ "buffer can be used to define a "
#~ "symbolic broadcast operation,"
#~ msgstr ""

#~ msgid ""
#~ "Buffer data structure reflects the "
#~ "DLTensor structure in dlpack. While "
#~ "DLTensor data structure is very general,"
#~ " it is usually helpful to create "
#~ "function that only handles specific case"
#~ " of data structure and make compiled"
#~ " function benefit from it."
#~ msgstr ""

#~ msgid ""
#~ "If user pass strides and elem_offset "
#~ "is passed as None when constructing "
#~ "the function, then the function will "
#~ "be specialized for the DLTensor that "
#~ "is compact and aligned. If user "
#~ "pass a fully generic symbolic array "
#~ "to the strides, then the resulting "
#~ "function becomes fully generic."
#~ msgstr ""

#~ msgid "The left hand operand, known to be non-negative."
#~ msgstr ""

#~ msgid "The right hand operand, known to be non-negative."
#~ msgstr ""

#~ msgid "The location of this operator in the source."
#~ msgstr ""

#~ msgid "**res** -- The result expression."
#~ msgstr ""

#~ msgid "When operands are integers, returns truncdiv(a, b, span)."
#~ msgstr ""

#~ msgid "The left hand operand"
#~ msgstr ""

#~ msgid "The right hand operand"
#~ msgstr ""

#~ msgid "**z** -- The result."
#~ msgstr ""

#~ msgid "The condition"
#~ msgstr ""

#~ msgid "The result expression if cond is true."
#~ msgstr ""

#~ msgid "The result expression if cond is false."
#~ msgstr ""

#~ msgid "**result** -- The result of conditional expression."
#~ msgstr ""

#~ msgid ""
#~ "Unlike Select, if_then_else will not "
#~ "execute the branch that does not "
#~ "satisfy the condition. You can use "
#~ "it to guard against out of bound"
#~ " access. Unlike Select, if_then_else cannot"
#~ " be vectorized if some lanes in "
#~ "the vector have different conditions."
#~ msgstr ""

#~ msgid ""
#~ "Use this function to split non-"
#~ "negative indices. This function may take"
#~ " advantage of operands' non-negativeness."
#~ msgstr ""

#~ msgid "Compute the remainder of indexdiv. a and b are non-negative."
#~ msgstr ""

#~ msgid ""
#~ "A layout representation is composed of"
#~ " upper cases, lower cases and "
#~ "numbers, where upper case indicates a"
#~ " primal axis and the corresponding "
#~ "lower case with factor size indicates"
#~ " the subordinate axis. For example, "
#~ "NCHW16c can describe a 5-D tensor "
#~ "of [batch_size, channel, height, width, "
#~ "channel_block]. Here subordinate axis "
#~ "channel_block=16 is the factor size of"
#~ " the primal axis C (channel)."
#~ msgstr ""

#~ msgid "**layout** -- The created layout"
#~ msgstr ""

#~ msgid "The reduction IterVar axis"
#~ msgstr ""

#~ msgid "Filtering predicate of the reduction."
#~ msgstr ""

#~ msgid "**value** -- The result value."
#~ msgstr ""

#~ msgid "**value** -- The maximum value of dtype."
#~ msgstr ""

#~ msgid "**value** -- The minimum value of dtype."
#~ msgstr ""

#~ msgid ""
#~ "Round elements of the array to the"
#~ " nearest integer. This intrinsic uses "
#~ "llvm.nearbyint instead of llvm.round which "
#~ "is faster but will results different "
#~ "from te.round. Notably nearbyint rounds "
#~ "according to the rounding mode, whereas"
#~ " te.round (llvm.round) ignores that. For"
#~ " differences between the two see: "
#~ "https://en.cppreference.com/w/cpp/numeric/math/round "
#~ "https://en.cppreference.com/w/cpp/numeric/math/nearbyint"
#~ msgstr ""

#~ msgid "The exponent"
#~ msgstr ""

#~ msgid ""
#~ "Execute a multiplication between two "
#~ "Q-numbers x and y followed by a"
#~ " right shift s. The mathematical "
#~ "expression is:"
#~ msgstr ""

#~ msgid "out = round(x*y*2^-s)"
#~ msgstr ""

#~ msgid ""
#~ "More about Q-numbers here: "
#~ "https://en.wikipedia.org/wiki/Q_(number_format) The "
#~ "rounding rule is to the nearest "
#~ "value, rounding half up (i.e., "
#~ "round(x.1) = x and round (x.5) ="
#~ " x+1)"
#~ msgstr ""

#~ msgid "First Q-number"
#~ msgstr ""

#~ msgid "Second Q-number"
#~ msgstr ""

#~ msgid "Number of fractional bits in x and y. Needs to be > 0"
#~ msgstr ""

#~ msgid "Integer shift"
#~ msgstr ""

#~ msgid ""
#~ "The returned tir expression, whose data"
#~ " type is int, float or void "
#~ "pointer."
#~ msgstr ""

#~ msgid "**ret** -- The return expression"
#~ msgstr ""

#~ msgid "**stmt_list** -- The unpacked list of statements"
#~ msgstr ""

#~ msgid "List of statements to be combined as sequence."
#~ msgstr ""

#~ msgid "**stmt** -- The combined statement."
#~ msgstr ""

#~ msgid ""
#~ "The trace function allows to trace "
#~ "specific tensor at the runtime. The "
#~ "tracing value should come as last "
#~ "argument. The trace action should be "
#~ "specified, by default tvm.default_trace_action "
#~ "is used."
#~ msgstr ""

#~ msgid "The name of the trace action."
#~ msgstr ""

#~ msgid ":obj:`tvm.tir.call_packed`"
#~ msgstr ""

#~ msgid "Creates packed function."
#~ msgstr ""

#~ msgid ""
#~ "The truncated value of the scalar "
#~ "x is the nearest integer i which"
#~ " is closer to zero than x is."
#~ msgstr ""

#~ msgid "This is the default integer division behavior in C."
#~ msgstr ""

#~ msgid "Namespace of all TIR transformations"
#~ msgstr ""

#~ msgid ":py:obj:`Apply <tvm.tir.transform.Apply>`\\ \\(ftransform\\)"
#~ msgstr ""

#~ msgid "Apply ftransform to each function in the Module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BF16CastElimination "
#~ "<tvm.tir.transform.BF16CastElimination>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Eliminate verbose casting between fp32 "
#~ "and bf16 Checks if the AST has "
#~ "the pattern: castto32(castto16(some_fp32_op(...))) "
#~ "The verbose casting is generated by "
#~ "BF16Promote for multiple bf16 Ops in "
#~ "a row."
#~ msgstr ""

#~ msgid ":py:obj:`BF16Legalize <tvm.tir.transform.BF16Legalize>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Legalize bf16 typed Ops."
#~ msgstr ""

#~ msgid ":py:obj:`BF16Promote <tvm.tir.transform.BF16Promote>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Promote bf16 to fp32."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BF16TypeLowering "
#~ "<tvm.tir.transform.BF16TypeLowering>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Replace all bf16 type with uint16."
#~ msgstr ""

#~ msgid ":py:obj:`CoProcSync <tvm.tir.transform.CoProcSync>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Detect and insert sync points to co-processor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CombineContextCall "
#~ "<tvm.tir.transform.CombineContextCall>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Combine context calls in the host function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CommonSubexprElimTIR "
#~ "<tvm.tir.transform.CommonSubexprElimTIR>`\\ "
#~ "\\(\\[enable\\_cse\\_tir\\]\\)"
#~ msgstr ""

#~ msgid "Replace redundant computations by new variables."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CompactBufferAllocation "
#~ "<tvm.tir.transform.CompactBufferAllocation>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Compact the buffer access region."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ConvertBlocksToOpaque "
#~ "<tvm.tir.transform.ConvertBlocksToOpaque>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Substitute all the block vars with "
#~ "the PrimExprs they are bound to, "
#~ "indicated by the corresponding iter_values "
#~ "in BlockRealize, and then convert the"
#~ " blocks into opaque ones by removing"
#~ " all the iter_values in BlockRealize "
#~ "and iter_vars in Block."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ConvertForLoopsToSerial "
#~ "<tvm.tir.transform.ConvertForLoopsToSerial>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Convert Parallel For Loops to Serial For Loops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DecorateDeviceScope "
#~ "<tvm.tir.transform.DecorateDeviceScope>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Decorate all the function's body as device function."
#~ msgstr ""

#~ msgid ":py:obj:`Filter <tvm.tir.transform.Filter>`\\ \\(fcond\\)"
#~ msgstr ""

#~ msgid "Filter functions by the calling convention attribute."
#~ msgstr ""

#~ msgid ":py:obj:`FlattenBuffer <tvm.tir.transform.FlattenBuffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Flatten the multi-dimensional BufferLoad "
#~ "and BufferStore to single dimensional "
#~ "Load/Store."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`HoistIfThenElse <tvm.tir.transform.HoistIfThenElse>`\\"
#~ " \\(\\[variant\\]\\)"
#~ msgstr ""

#~ msgid "Hoist loop-invariant IfThenElse nodes to outside the eligible loops."
#~ msgstr ""

#~ msgid ":py:obj:`InferFragment <tvm.tir.transform.InferFragment>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Infer the TensorCore fragment infomation using tensor intrinsics."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectCopyIntrin "
#~ "<tvm.tir.transform.InjectCopyIntrin>`\\ \\(pragma\\_key\\, "
#~ "fintrin\\)"
#~ msgstr ""

#~ msgid "Inject virtual thread loops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectDoubleBuffer "
#~ "<tvm.tir.transform.InjectDoubleBuffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Inject double buffer statements."
#~ msgstr ""

#~ msgid ":py:obj:`InjectPrefetch <tvm.tir.transform.InjectPrefetch>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Inject prefetch instructions into stmt."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectRollingBuffer "
#~ "<tvm.tir.transform.InjectRollingBuffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Inject rolling buffer statements."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectVirtualThread "
#~ "<tvm.tir.transform.InjectVirtualThread>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InstrumentBoundCheckers "
#~ "<tvm.tir.transform.InstrumentBoundCheckers>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Instruments bound checkers."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LegalizePackedCalls "
#~ "<tvm.tir.transform.LegalizePackedCalls>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Legalize packed calls to have its arguments wrapped in TVMValues"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LiftAttrScope <tvm.tir.transform.LiftAttrScope>`\\ "
#~ "\\(attr\\_key\\)"
#~ msgstr ""

#~ msgid "Lift common attrs with attr_key to outer scope."
#~ msgstr ""

#~ msgid ":py:obj:`LoopPartition <tvm.tir.transform.LoopPartition>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerCrossThreadReduction "
#~ "<tvm.tir.transform.LowerCrossThreadReduction>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Lower cross-thread reduction from thread"
#~ " bindings to intrinsic function calls."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerCustomDatatypes "
#~ "<tvm.tir.transform.LowerCustomDatatypes>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower custom datatypes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerDeviceStorageAccessInfo "
#~ "<tvm.tir.transform.LowerDeviceStorageAccessInfo>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower attached storage access information on device."
#~ msgstr ""

#~ msgid ":py:obj:`LowerInitBlock <tvm.tir.transform.LowerInitBlock>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower block init stmt into IfThenElse statements."
#~ msgstr ""

#~ msgid ":py:obj:`LowerIntrin <tvm.tir.transform.LowerIntrin>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower target specific intrinsic calls."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerMatchBuffer "
#~ "<tvm.tir.transform.LowerMatchBuffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Remove match buffers inside the block."
#~ msgstr ""

#~ msgid ":py:obj:`LowerTVMBuiltin <tvm.tir.transform.LowerTVMBuiltin>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower tvm builtin intrinsics."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerThreadAllreduce "
#~ "<tvm.tir.transform.LowerThreadAllreduce>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower cross thread alleduce."
#~ msgstr ""

#~ msgid ":py:obj:`LowerWarpMemory <tvm.tir.transform.LowerWarpMemory>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower warp memory access to low-level device related function calls."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MakePackedAPI <tvm.tir.transform.MakePackedAPI>`\\ "
#~ "\\(\\[num\\_unpacked\\_params\\]\\)"
#~ msgstr ""

#~ msgid "Transform the PrimFuncs in the module to a packed func API."
#~ msgstr ""

#~ msgid ":py:obj:`MakeUnpackedAPI <tvm.tir.transform.MakeUnpackedAPI>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Transform the PrimFuncs in the module"
#~ " to a C API compatible with "
#~ "internal calls."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MergeDynamicSharedMemoryAllocations "
#~ "<tvm.tir.transform.MergeDynamicSharedMemoryAllocations>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "This pass merges multiple TIR-level "
#~ "dynamic shared memory allocations into "
#~ "one allocation."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`NarrowDataType <tvm.tir.transform.NarrowDataType>`\\"
#~ " \\(target\\_bits\\)"
#~ msgstr ""

#~ msgid "Narrow down PrimExpr datatype in stmt to target_bits."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PlanAndUpdateBufferAllocationLocation "
#~ "<tvm.tir.transform.PlanAndUpdateBufferAllocationLocation>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Locate the buffer allocation to the "
#~ "exact position (usually is the lca "
#~ "of buffer access)."
#~ msgstr ""

#~ msgid ":py:obj:`RemoveNoOp <tvm.tir.transform.RemoveNoOp>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Remove No Op from the Stmt."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RewriteUnsafeSelect "
#~ "<tvm.tir.transform.RewriteUnsafeSelect>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Detect and rewrite unsafe select that contains memory access."
#~ msgstr ""

#~ msgid ":py:obj:`Simplify <tvm.tir.transform.Simplify>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Run arithmetic simplifications on the statements and expressions."
#~ msgstr ""

#~ msgid ":py:obj:`SkipAssert <tvm.tir.transform.SkipAssert>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Skip assert stmt."
#~ msgstr ""

#~ msgid ":py:obj:`SplitHostDevice <tvm.tir.transform.SplitHostDevice>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Split the function into a host function and device functions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`StorageFlatten <tvm.tir.transform.StorageFlatten>`\\"
#~ " \\(cache\\_line\\_size\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Flatten the multi-dimensional read/write to 1D."
#~ msgstr ""

#~ msgid ":py:obj:`StorageRewrite <tvm.tir.transform.StorageRewrite>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Rewrite storage allocation pattern."
#~ msgstr ""

#~ msgid ":py:obj:`TextureFlatten <tvm.tir.transform.TextureFlatten>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Flatten the multi-dimensional read/write to 2D."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ThreadSync <tvm.tir.transform.ThreadSync>`\\ "
#~ "\\(storage\\_scope\\)"
#~ msgstr ""

#~ msgid "Insert sync between parallel read/write of shared buffers."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`UnifyThreadBinding "
#~ "<tvm.tir.transform.UnifyThreadBinding>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Unify all the thread bindings for "
#~ "\"blockIdx.x/y/z\", \"threadIdx.x/y/z\", and "
#~ "\"vthread.x/y/z\"."
#~ msgstr ""

#~ msgid ":py:obj:`UnrollLoop <tvm.tir.transform.UnrollLoop>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Unroll the constant loop marked by unroll."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`VectorizeLoop <tvm.tir.transform.VectorizeLoop>`\\ "
#~ "\\(\\[enable\\_vectorize\\]\\)"
#~ msgstr ""

#~ msgid "Lower vectorization loops."
#~ msgstr ""

#~ msgid ":py:obj:`VerifyMemory <tvm.tir.transform.VerifyMemory>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Verify if func contains illegal host side direct memory access."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`prim_func_pass <tvm.tir.transform.prim_func_pass>`\\"
#~ " \\(\\[pass\\_func\\, opt\\_level\\, name\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Decorate a function pass."
#~ msgstr ""

#~ msgid "A pass that works on each :py:func:`tvm.tir.PrimFunc` in a module."
#~ msgstr ""

#~ msgid "This function is a thin wrapper around tvm.tir.transform.prim_func_pass"
#~ msgstr ""

#~ msgid "The transformation pass."
#~ msgstr ""

#~ msgid "**fpass** -- The result pass"
#~ msgstr ""

#~ msgid ""
#~ "Eliminate verbose casting between fp32 "
#~ "and bf16 Checks if the AST has "
#~ "the pattern: castto32(castto16(some_fp32_op(...))) "
#~ "The verbose casting is generated by "
#~ "BF16Promote for multiple bf16 Ops in "
#~ "a row. e.g.: X[i] + Y[i] + "
#~ "T[i] => bf16((float32(bf16((float32(X[i]) + "
#~ "float32(Y[i])))) + float32(T[i]))) After this"
#~ " pass: bf16(float32(X[i]) + float32(Y[i]) +"
#~ " float32(T[i]))"
#~ msgstr ""

#~ msgid ""
#~ "Legalize bf16 typed Ops. Runs "
#~ "BF16Promote, BF16CastElimination and "
#~ "BF16TypeLowering"
#~ msgstr ""

#~ msgid ""
#~ "Promote bf16 to fp32. Add a cast"
#~ " to fp32 before Ops, then add a"
#~ " cast back to bf16."
#~ msgstr ""

#~ msgid ""
#~ "Replace all bf16 type with uint16. "
#~ "Also lower the casting between fp32 "
#~ "and bf16"
#~ msgstr ""

#~ msgid ""
#~ "Compact the buffer access region. by "
#~ "removing the buffer regions that are "
#~ "not accessed, i.e. narrowing the buffer"
#~ " shape and adjust the access region"
#~ " if necessary."
#~ msgstr ""

#~ msgid ""
#~ "Before narrowing, ``B`` is a ``[16, "
#~ "16]`` buffer, but only a skinny "
#~ "vector ``B[i, 0:16]`` is accessed."
#~ msgstr ""

#~ msgid ""
#~ "This pass narrows the buffer shape "
#~ "and adjust its accessed region "
#~ "accordingly.  In this particular case, "
#~ "because only a ``1 * 16`` vector"
#~ " of ``B`` is accessed, the pass "
#~ "narrows ``B`` to shape ``[1, 16]``, "
#~ "and changes the access to ``B[i, "
#~ "j]`` to ``B[0, j]``."
#~ msgstr ""

#~ msgid "The condition of the filtering."
#~ msgstr ""

#~ msgid ""
#~ "Flatten the multi-dimensional BufferLoad "
#~ "and BufferStore to single dimensional "
#~ "Load/Store. Also remove Block to ensure"
#~ " that the flattened TIR can not "
#~ "be scheduled again."
#~ msgstr ""

#~ msgid ""
#~ "The variant of the pass. variant "
#~ "can have any one of following "
#~ "values [\"basic\", None(Default)].  The basic"
#~ " variant supports basic hoisting scenarios"
#~ " where it expects the For & If"
#~ " Nodes are in place consecutively and"
#~ " does not involve global scope "
#~ "variables or more advanced scenarios.  "
#~ "Default variant supports all hoisting "
#~ "scenarios,i.e., {\"Basic\" + \"Advanced\"} "
#~ "supported with control with PassContext "
#~ "configs like below:      "
#~ "config={\"tir.HoistIfThenElse\": "
#~ "{\"support_block_scope_hosting\": True}}"
#~ msgstr ""

#~ msgid ""
#~ "The variant of the pass. variant "
#~ "can have any one of following "
#~ "values [\"basic\", None(Default)]."
#~ msgstr ""

#~ msgid ""
#~ "The basic variant supports basic "
#~ "hoisting scenarios where it expects the"
#~ " For & If Nodes are in place"
#~ " consecutively and does not involve "
#~ "global scope variables or more advanced"
#~ " scenarios."
#~ msgstr ""

#~ msgid ""
#~ "Default variant supports all hoisting "
#~ "scenarios,i.e., {\"Basic\" + \"Advanced\"} "
#~ "supported with control with PassContext "
#~ "configs like below:"
#~ msgstr ""

#~ msgid ""
#~ "config={\"tir.HoistIfThenElse\": "
#~ "{\"support_block_scope_hosting\": True}}"
#~ msgstr ""

#~ msgid "The pragma key for hint of copy."
#~ msgstr ""

#~ msgid ""
#~ "The function with signature copyintrin(src,"
#~ " dst, pad_before, pad_after, pad_value)"
#~ msgstr ""

#~ msgid "The attribute key to be checked."
#~ msgstr ""

#~ msgid ""
#~ "See tvm::datatypes::Registry for more "
#~ "information on adding custom datatypes."
#~ msgstr ""

#~ msgid "Run this pass after all storage access analysis finish."
#~ msgstr ""

#~ msgid ""
#~ "Remove match buffers inside the block."
#~ " Also, it will validate the binding."
#~ msgstr ""

#~ msgid ""
#~ "Number of parameters that we hope "
#~ "to directly pass via normal arguments"
#~ " following the PackedFunc input signature."
#~ " If it is specified as -1 or"
#~ " it is less than the number of"
#~ " arguments, the pass will packed "
#~ "arguments still."
#~ msgstr ""

#~ msgid "The target bit configuration."
#~ msgstr ""

#~ msgid "Run this pass after StorageFlatten."
#~ msgstr ""

#~ msgid ""
#~ "Locate the buffer allocation to the "
#~ "exact position (usually is the lca "
#~ "of buffer access). This pass will "
#~ "inject opaque block with alloc_buffers "
#~ "at the allocation site."
#~ msgstr ""

#~ msgid ""
#~ "A pass that works on each "
#~ ":py:func:`tvm.tir.PrimFunc` in a module. A "
#~ "function pass class should be created"
#~ " through py:func:`tvm.tir.transform.function_pass`."
#~ msgstr ""

#~ msgid "The size of CPU cache line."
#~ msgstr ""

#~ msgid "Whether to create bound attributes."
#~ msgstr ""

#~ msgid ""
#~ "Moves the allocation to outer most "
#~ "possible scope. Trying to share space"
#~ " between allocations to make a static"
#~ " allocation plan when possible."
#~ msgstr ""

#~ msgid ""
#~ "Unify all the thread bindings for "
#~ "\"blockIdx.x/y/z\", \"threadIdx.x/y/z\", and "
#~ "\"vthread.x/y/z\". Before the unification, two"
#~ " vars that are bound to a "
#~ "thread axis (e.g., \"threadIdx.x\") use "
#~ "different IterVars and variables in "
#~ "their AttrStmts. After the unification, "
#~ "we use a consolidated IterVar and "
#~ "a variable for them."
#~ msgstr ""

#~ msgid ""
#~ "`vthread` is a legacy behavior that "
#~ "will be deprecated, though thread "
#~ "bindings of `vthread` are still also "
#~ "unified in this pass. Please use "
#~ "`vthread.x`, `vthread.y` and `vthread.z` "
#~ "instead."
#~ msgstr ""

#~ msgid ""
#~ "This pass also automatically attach "
#~ "pragma unroll tag to loops which "
#~ "meets the standard."
#~ msgstr ""

#~ msgid ""
#~ "Whether vectorization is enabled. Will "
#~ "lower to scalar loop when it is"
#~ " turned off."
#~ msgstr ""

#~ msgid ""
#~ "This function returns a callback when"
#~ " pass_func is provided. Otherwise, it "
#~ "returns the created function pass using"
#~ " the given optimization function."
#~ msgstr ""

#~ msgid "The transformation function or class."
#~ msgstr ""

#~ msgid "The optimization level of this module pass."
#~ msgstr ""

#~ msgid ""
#~ "The name of the function pass. The"
#~ " name could be empty. In this "
#~ "case, the name of the optimization "
#~ "function will be used as the pass"
#~ " name."
#~ msgstr ""

#~ msgid "The list of passes that the function pass is dependent on."
#~ msgstr ""

#~ msgid ""
#~ "**create_function_pass** -- A decorator will"
#~ " be returned if pass_func is not "
#~ "provided, otherwise return the decorated "
#~ "result. The returned decorator has two"
#~ " behaviors depending on the input: A"
#~ " new FunctionPass will be returned "
#~ "when we decorate a pass function. "
#~ "A new FunctionPass class will be "
#~ "returned when we decorate a class "
#~ "type."
#~ msgstr ""

#~ msgid "The following code block decorates a function pass class."
#~ msgstr ""

#~ msgid ""
#~ "The following code creates a function"
#~ " pass by decorating a user defined"
#~ " transform function."
#~ msgstr ""

#~ msgid "Namespace of all TIR analysis utils."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Block <tvm.tir.analysis.Block>`\\ "
#~ "\\(iter\\_vars\\, reads\\, writes\\, name\\_hint\\,"
#~ " body\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferRegion <tvm.tir.analysis.BufferRegion>`\\ "
#~ "\\(buffer\\, region\\)"
#~ msgstr ""

#~ msgid "Base class for all tvm's runtime objects."
#~ msgstr ""

#~ msgid "Base class of all primitive expressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PrimFunc <tvm.tir.analysis.PrimFunc>`\\ "
#~ "\\(params\\, body\\[\\, ret\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Var <tvm.tir.analysis.Var>`\\ \\(name\\, "
#~ "dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`apply_prim_func_arg_and_result_memory_constraints "
#~ "<tvm.tir.analysis.apply_prim_func_arg_and_result_memory_constraints>`\\"
#~ " \\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns func written to capture the "
#~ "memory (aka storage) scope constraints "
#~ "for each of the func's parameters "
#~ "given by arg_and_result_memory_scopes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`calculate_workspace_bytes "
#~ "<tvm.tir.analysis.calculate_workspace_bytes>`\\ \\(func\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Calculate the workspace size in bytes"
#~ " needed by the TIR allocates inside"
#~ " the TIR PrimFunc."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`detect_buffer_access_lca "
#~ "<tvm.tir.analysis.detect_buffer_access_lca>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid ""
#~ "Detect the lowest common ancestor(LCA) "
#~ "of buffer access, including both "
#~ "high-level access(BufferLoad, BufferStore) and"
#~ " low-level access(Load, Store and "
#~ "opaque access)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`expr_deep_equal <tvm.tir.analysis.expr_deep_equal>`\\"
#~ " \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Deeply compare two nested expressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_block_access_region "
#~ "<tvm.tir.analysis.get_block_access_region>`\\ \\(block\\, "
#~ "buffer\\_var\\_map\\)"
#~ msgstr ""

#~ msgid "Detect which regions of tensors in this block are read or written to."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_block_read_write_region "
#~ "<tvm.tir.analysis.get_block_read_write_region>`\\ \\(block\\,"
#~ " ...\\)"
#~ msgstr ""

#~ msgid "Auto detect the block read/write region according to its body stmt."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_prim_func_arg_and_result_memory_constraints "
#~ "<tvm.tir.analysis.get_prim_func_arg_and_result_memory_constraints>`\\"
#~ " \\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns the memory (aka storage) scope"
#~ " constraints for all the arguments "
#~ "and result of func."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`verify_gpu_code <tvm.tir.analysis.verify_gpu_code>`\\"
#~ " \\(func\\, constraints\\)"
#~ msgstr ""

#~ msgid "Verify if module contains illegal host side direct memory access."
#~ msgstr ""

#~ msgid ":py:obj:`verify_memory <tvm.tir.analysis.verify_memory>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid ":py:obj:`verify_ssa <tvm.tir.analysis.verify_ssa>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid "Verify if the func is in SSA form."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`access_ptr <tvm.tir.analysis.Buffer.access_ptr>`\\ "
#~ "\\(access\\_mask\\[\\, ptr\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`scope <tvm.tir.analysis.Buffer.scope>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`vload <tvm.tir.analysis.Buffer.vload>`\\ "
#~ "\\(begin\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`vstore <tvm.tir.analysis.Buffer.vstore>`\\ "
#~ "\\(begin\\, value\\)"
#~ msgstr ""

#~ msgid ""
#~ "PrimExpr is used in the low-level"
#~ " code optimizations and integer analysis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`script <tvm.tir.analysis.PrimFunc.script>`\\ "
#~ "\\(\\[tir\\_prefix\\, show\\_meta\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`specialize <tvm.tir.analysis.PrimFunc.specialize>`\\"
#~ " \\(param\\_map\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`with_body <tvm.tir.analysis.PrimFunc.with_body>`\\ "
#~ "\\(new\\_body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns func written to capture the "
#~ "memory (aka storage) scope constraints "
#~ "for each of the func's parameters "
#~ "given by arg_and_result_memory_scopes. However, "
#~ "arg_and_result_memory_scopes should be w.r.t. "
#~ "the func's representation as a Relay "
#~ "Function of relay_func_type before lowering"
#~ " and conversion to DPS."
#~ msgstr ""

#~ msgid "Visible for testing."
#~ msgstr ""

#~ msgid ""
#~ "CAUTION: This is experimental. The "
#~ "resulting PrimFunc may not have fully"
#~ " accounted for all new memory scopes."
#~ msgstr ""

#~ msgid "The function to retrieve constraints from."
#~ msgstr ""

#~ msgid "The type of the Relay Function from which the func was derived."
#~ msgstr ""

#~ msgid ""
#~ "Memory constraints for funcs args and"
#~ " result in Relay form. The empty "
#~ "string denotes 'no constraint'."
#~ msgstr ""

#~ msgid "**result** -- The rewritten func."
#~ msgstr ""

#~ msgid "The function to be detected."
#~ msgstr ""

#~ msgid "The byte alignment required for each tensor"
#~ msgstr ""

#~ msgid "**result** -- Workspace size in bytes."
#~ msgstr ""

#~ msgid ""
#~ "Detect the lowest common ancestor(LCA) "
#~ "of buffer access, including both "
#~ "high-level access(BufferLoad, BufferStore) and"
#~ " low-level access(Load, Store and "
#~ "opaque access). The LCA may be a"
#~ " For loop or a Block."
#~ msgstr ""

#~ msgid "**result** -- Map from buffer to the LCA of all access to it."
#~ msgstr ""

#~ msgid "The left operand."
#~ msgstr ""

#~ msgid "The right operand."
#~ msgstr ""

#~ msgid "**result** -- The comparison result"
#~ msgstr ""

#~ msgid ""
#~ "This function does not remap variable"
#~ " bindings, it will not return true"
#~ " for (let x = 1 in x +"
#~ " 1) vs (let y = 1 in y"
#~ " + 1), unless x.same_as(y). Use "
#~ "py:func:`tvm.ir.structural_equal` to handle "
#~ "structural variable remapping."
#~ msgstr ""

#~ msgid ""
#~ "Due to the restriction of not "
#~ "remapping variables, this function can "
#~ "run faster than StructuralEqual and can"
#~ " be used as a utility function "
#~ "during arithmetic simplifications."
#~ msgstr ""

#~ msgid ""
#~ "Always consider py:func:`tvm.ir.structural_equal` "
#~ "first, which handles the structural "
#~ "remapping."
#~ msgstr ""

#~ msgid ":obj:`tvm.ir.structural_equal`"
#~ msgstr ""

#~ msgid "Regions are sorted by order of appearance in the AST."
#~ msgstr ""

#~ msgid "The block in which we are detecting read/write regions."
#~ msgstr ""

#~ msgid ""
#~ "The outside buffers which may access "
#~ "the block. Mapping from buffer var "
#~ "to the buffer"
#~ msgstr ""

#~ msgid ""
#~ "**result** --  Array of access regions."
#~ " There are three arrays of "
#~ "BufferRegion:     - first: read regions"
#~ "     - second: write regions     - "
#~ "third: opaque regions"
#~ msgstr ""

#~ msgid "**result** --"
#~ msgstr ""

#~ msgid "Array of access regions. There are three arrays of BufferRegion:"
#~ msgstr ""

#~ msgid "first: read regions"
#~ msgstr ""

#~ msgid "second: write regions"
#~ msgstr ""

#~ msgid "third: opaque regions"
#~ msgstr ""

#~ msgid "An opaque access will be counted as both a read and a write access"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- An array only consisting"
#~ " of the read regions and write "
#~ "regions of the input block"
#~ msgstr ""

#~ msgid ""
#~ "Returns the memory (aka storage) scope"
#~ " constraints for all the arguments "
#~ "and result of func. However the "
#~ "result will be w.r.t. the func's "
#~ "representation as a Relay Function of"
#~ " relay_func_type before lowering and "
#~ "conversion to DPS."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- Memory scope constraints "
#~ "for funcs args and result in Relay"
#~ " form. The empty string denotes 'no"
#~ " constraint'."
#~ msgstr ""

#~ msgid "The module to be verified."
#~ msgstr ""

#~ msgid "The attribute constraints."
#~ msgstr ""

#~ msgid "**result** -- The result of verification."
#~ msgstr ""

#~ msgid "Statement functor utilities for IR transformations"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ir_transform <tvm.tir.stmt_functor.ir_transform>`\\ "
#~ "\\(stmt\\, preorder\\, postorder\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Recursively visit and transform ir nodes in post DFS order."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`post_order_visit "
#~ "<tvm.tir.stmt_functor.post_order_visit>`\\ \\(stmt\\, "
#~ "fvisit\\)"
#~ msgstr ""

#~ msgid "Recursively visit the ir in post DFS order node, apply fvisit"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`substitute <tvm.tir.stmt_functor.substitute>`\\ "
#~ "\\(node\\, vmap\\)"
#~ msgstr ""

#~ msgid "Substitute the var specified by vmap."
#~ msgstr ""

#~ msgid "The input to be transformed."
#~ msgstr ""

#~ msgid ""
#~ "The function called in before recursive"
#~ " mutation If preorder returns None, "
#~ "then the transform will proceed to "
#~ "recursive call. If preorder returns a"
#~ " not None tvm.tir.Stmt/Expr, the "
#~ "transformer will simply return it and"
#~ " won't do further recursion."
#~ msgstr ""

#~ msgid "The function called after recursive mutation."
#~ msgstr ""

#~ msgid "List of types that we only enable."
#~ msgstr ""

#~ msgid "**result** -- The result."
#~ msgstr ""

#~ msgid "Each node is guaranteed to be visited only once."
#~ msgstr ""

#~ msgid "The visitor function."
#~ msgstr ""

#~ msgid "The input."
#~ msgstr ""

#~ msgid "The variable mapping."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AllocateConst <tvm.tir.AllocateConst>`\\ "
#~ "\\(buffer\\_var\\, dtype\\, extents\\, ...\\)"
#~ msgstr ""

#~ msgid "Allocate constant node."
#~ msgstr ""

#~ msgid ":py:obj:`BlockDependenceInfo <tvm.tir.BlockDependenceInfo>`\\ \\(mod\\)"
#~ msgstr ""

#~ msgid ""
#~ "An object that helps build and "
#~ "query block level dependences using the"
#~ " 2 core objects BlockScope and "
#~ "StmtSRef"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DeclBuffer <tvm.tir.DeclBuffer>`\\ \\(buffer\\,"
#~ " body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "DeclBuffer node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`IndexMap <tvm.tir.IndexMap>`\\ "
#~ "\\(initial\\_indices\\, final\\_indices\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "A mapping from multi-dimensional indices"
#~ " to another set of multi-dimensional"
#~ " indices"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ScheduleState <tvm.tir.ScheduleState>`\\ "
#~ "\\(mod\\, \\*\\[\\, debug\\_mask\\, "
#~ "enable\\_check\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`TVMBackendAllocWorkspace "
#~ "<tvm.tir.TVMBackendAllocWorkspace>`\\ \\(device\\_type\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Backend function to allocate temporal workspace"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`TVMBackendFreeWorkspace "
#~ "<tvm.tir.TVMBackendFreeWorkspace>`\\ \\(device\\_type\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Backend function to free temporal workspace."
#~ msgstr ""

#~ msgid ":py:obj:`add <tvm.tir.add>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Generic add operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`address_of <tvm.tir.address_of>`\\ "
#~ "\\(buffer\\_load\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Returns the address of an element in the buffer"
#~ msgstr ""

#~ msgid ":py:obj:`assume <tvm.tir.assume>`\\ \\(\\[cond\\]\\)"
#~ msgstr ""

#~ msgid "Provide a true statement that can be used for simplifications"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitwise_and <tvm.tir.bitwise_and>`\\ \\(x\\, "
#~ "y\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take bitwise and of two values"
#~ msgstr ""

#~ msgid ":py:obj:`bitwise_not <tvm.tir.bitwise_not>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take bitwise not of input value"
#~ msgstr ""

#~ msgid ":py:obj:`bitwise_or <tvm.tir.bitwise_or>`\\ \\(x\\, y\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take bitwise or of two values"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitwise_xor <tvm.tir.bitwise_xor>`\\ \\(x\\, "
#~ "y\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take bitwise xor of two values"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_cpacked <tvm.tir.call_cpacked>`\\ "
#~ "\\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_cpacked_lowered <tvm.tir.call_cpacked_lowered>`\\"
#~ " \\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Lowered version of call c-packed."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_packed_lowered <tvm.tir.call_packed_lowered>`\\"
#~ " \\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Lowered version of call packed."
#~ msgstr ""

#~ msgid ":py:obj:`call_tir <tvm.tir.call_tir>`\\ \\(global\\_var\\, \\*args\\)"
#~ msgstr ""

#~ msgid "Performs a call into another PrimFunc in the same IRModule"
#~ msgstr ""

#~ msgid ":py:obj:`ceildiv <tvm.tir.ceildiv>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Generic ceildiv operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`create_barriers <tvm.tir.create_barriers>`\\ "
#~ "\\(barrier\\_count\\)"
#~ msgstr ""

#~ msgid "TVM intrinsic to create N barriers"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`end_profile_intrinsic "
#~ "<tvm.tir.end_profile_intrinsic>`\\ \\(id\\)"
#~ msgstr ""

#~ msgid ""
#~ "End profile intrinsic. Parameters ----------"
#~ " id : int     The intrinsic id. "
#~ "Returns ------- call : PrimExpr     The"
#~ " call expression."
#~ msgstr ""

#~ msgid ":py:obj:`infinity <tvm.tir.infinity>`\\ \\(dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "infinity value of dtype"
#~ msgstr ""

#~ msgid ":py:obj:`isnullptr <tvm.tir.isnullptr>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is nullptr."
#~ msgstr ""

#~ msgid ":py:obj:`layout <tvm.tir.layout>`\\ \\(layout\\_str\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`likely <tvm.tir.likely>`\\ \\(cond\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Mark condition as likely."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`lookup_param <tvm.tir.lookup_param>`\\ "
#~ "\\(param\\_name\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Returns the param by name"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`mma_fill <tvm.tir.mma_fill>`\\ \\(dtype\\, "
#~ "local\\_size\\, local\\_ptr\\, offset\\)"
#~ msgstr ""

#~ msgid "TVM intrinsic for zero-initalizing an MMA accumulation registor"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`mma_store <tvm.tir.mma_store>`\\ \\(dtype\\, "
#~ "m\\, n\\, dst\\_ptr\\, src\\_ptr\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for storing the result "
#~ "of PTX MMA into a destination "
#~ "pointer"
#~ msgstr ""

#~ msgid ":py:obj:`multiply <tvm.tir.multiply>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Generic multiply operator."
#~ msgstr ""

#~ msgid ":py:obj:`pow <tvm.tir.pow>`\\ \\(x\\, y\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_arrive_barrier <tvm.tir.ptx_arrive_barrier>`\\ "
#~ "\\(barrier\\_id\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx barrier arrival"
#~ " using mbarrier.arrive https://docs.nvidia.com/cuda"
#~ "/parallel-thread-execution/index.html#parallel-"
#~ "synchronization-and-communication-instructions-"
#~ "mbarrier-arrive"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_arrive_barrier_expect_tx "
#~ "<tvm.tir.ptx_arrive_barrier_expect_tx>`\\ \\(barrier\\_id\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx barrier arrival"
#~ " with expect tx using "
#~ "mbarrier.arrive.expect_tx https://docs.nvidia.com/cuda"
#~ "/parallel-thread-execution/index.html#parallel-"
#~ "synchronization-and-communication-instructions-"
#~ "mbarrier-arrive https://docs.nvidia.com/cuda/parallel-"
#~ "thread-execution/index.html#parallel-synchronization-"
#~ "and-communication-instructions-mbarrier-"
#~ "expect-tx-operation"
#~ msgstr ""

#~ msgid ":py:obj:`ptx_commit_group <tvm.tir.ptx_commit_group>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx async copy "
#~ "commit https://docs.nvidia.com/cuda/parallel-thread-"
#~ "execution/index.html#data-movement-and-"
#~ "conversion-instructions-cp-async-commit-"
#~ "group"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_cp_async <tvm.tir.ptx_cp_async>`\\ "
#~ "\\(dtype\\, shared\\_ptr\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx async copy "
#~ "from global to shared memory using "
#~ "cp.async https://docs.nvidia.com/cuda/parallel-thread-"
#~ "execution/index.html#data-movement-and-"
#~ "conversion-instructions-cp-async"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_cp_async_barrier <tvm.tir.ptx_cp_async_barrier>`\\"
#~ " \\(barrier\\_id\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx async copy "
#~ "barrier using cp.async.mbarrier.arrive "
#~ "https://docs.nvidia.com/cuda/parallel-thread-"
#~ "execution/index.html#parallel-synchronization-and-"
#~ "communication-instructions-cp-async-mbarrier-"
#~ "arrive"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_cp_async_bulk <tvm.tir.ptx_cp_async_bulk>`\\ "
#~ "\\(dtype\\, shared\\_ptr\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx async copy "
#~ "from global to shared memory using "
#~ "cp.async.bulk https://docs.nvidia.com/cuda/parallel-"
#~ "thread-execution/index.html#data-movement-and-"
#~ "conversion-instructions-cp-async-bulk"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_init_barrier_thread_count "
#~ "<tvm.tir.ptx_init_barrier_thread_count>`\\ \\(barrier\\_id\\,"
#~ " ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx barrier "
#~ "initialization of thread count using "
#~ "mbarrier.init https://docs.nvidia.com/cuda/parallel-"
#~ "thread-execution/index.html#parallel-synchronization-"
#~ "and-communication-instructions-mbarrier-init"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_ldmatrix <tvm.tir.ptx_ldmatrix>`\\ "
#~ "\\(dtype\\, trans\\, num\\, type\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx load matrix "
#~ "from shared memory https://docs.nvidia.com/cuda"
#~ "/parallel-thread-execution/index.html#warp-level-"
#~ "matrix-instructions-ldmatrix"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_mma <tvm.tir.ptx_mma>`\\ \\(dtype\\, "
#~ "shape\\, A\\_layout\\, B\\_layout\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx tensor core "
#~ "mma instructions https://docs.nvidia.com/cuda/parallel-"
#~ "thread-execution/index.html#warp-level-matrix-"
#~ "instructions-for-mma"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_mma_sp <tvm.tir.ptx_mma_sp>`\\ \\(dtype\\,"
#~ " shape\\, A\\_layout\\, B\\_layout\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for sparse tensor core "
#~ "ptx instructions https://docs.nvidia.com/cuda/parallel-"
#~ "thread-execution/index.html#warp-level-matrix-"
#~ "instructions-for-sparse-mma"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ptx_wait_barrier <tvm.tir.ptx_wait_barrier>`\\ "
#~ "\\(barrier\\_id\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx barrier wait "
#~ "using mbarrier.try_wait https://docs.nvidia.com/cuda"
#~ "/parallel-thread-execution/index.html#parallel-"
#~ "synchronization-and-communication-instructions-"
#~ "mbarrier-test-wait-mbarrier-try-wait"
#~ msgstr ""

#~ msgid ":py:obj:`ptx_wait_group <tvm.tir.ptx_wait_group>`\\ \\(num\\)"
#~ msgstr ""

#~ msgid ""
#~ "TVM intrinsic for ptx async copy "
#~ "wait https://docs.nvidia.com/cuda/parallel-thread-"
#~ "execution/index.html#data-movement-and-"
#~ "conversion-instructions-cp-async-wait-group"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`q_multiply_shift_per_axis "
#~ "<tvm.tir.q_multiply_shift_per_axis>`\\ \\(x\\, y\\, "
#~ "ls\\, rs\\, q\\, ...\\)"
#~ msgstr ""

#~ msgid "Execute a multiplication between two Q-numbers x and y"
#~ msgstr ""

#~ msgid ":py:obj:`reinterpret <tvm.tir.reinterpret>`\\ \\(dtype\\, value\\)"
#~ msgstr ""

#~ msgid ":py:obj:`shift_left <tvm.tir.shift_left>`\\ \\(x\\, y\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Return the result of x left shifted by y bits."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`shift_right <tvm.tir.shift_right>`\\ \\(x\\, "
#~ "y\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Return the result of x right shifted by y bits."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`start_profile_intrinsic "
#~ "<tvm.tir.start_profile_intrinsic>`\\ \\(id\\)"
#~ msgstr ""

#~ msgid ""
#~ "Start profile intrinsic. Parameters ----------"
#~ " id : int     The intrinsic id. "
#~ "Returns ------- call : PrimExpr     The"
#~ " call expression."
#~ msgstr ""

#~ msgid ":py:obj:`subtract <tvm.tir.subtract>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Generic subtract operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_access_ptr <tvm.tir.tvm_access_ptr>`\\ "
#~ "\\(ptype\\, data\\, offset\\, extent\\, ...\\)"
#~ msgstr ""

#~ msgid "Get head access address with memory access pattern info"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_bmma_sync <tvm.tir.tvm_bmma_sync>`\\ "
#~ "\\(fragment\\_d\\, index\\_d\\, ...\\)"
#~ msgstr ""

#~ msgid "TVM intrinsic for tensor core bmma_sync operators"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_check_return <tvm.tir.tvm_check_return>`\\ "
#~ "\\(expected\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Return new on stack dtype[num] "
#~ "Parameters ---------- expected : int     "
#~ "The expected return code. return_unexpected"
#~ " : int     The unexpected return "
#~ "code. nested_call : PrimExpr     The "
#~ "call expression to check return. Returns"
#~ " ------- call : PrimExpr     The call"
#~ " expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_fill_fragment <tvm.tir.tvm_fill_fragment>`\\ "
#~ "\\(fragment\\, m\\, n\\, k\\, index\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "TVM intrinsic for tensor core fill_fragment operators"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_load_matrix_sync <tvm.tir.tvm_load_matrix_sync>`\\"
#~ " \\(fragment\\, m\\, n\\, k\\, ...\\)"
#~ msgstr ""

#~ msgid "TVM intrinsic for tensor core load operators"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_mma_sync <tvm.tir.tvm_mma_sync>`\\ "
#~ "\\(fragment\\_d\\, index\\_d\\, ...\\)"
#~ msgstr ""

#~ msgid "TVM intrinsic for tensor core mma_sync operators"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_stack_alloca <tvm.tir.tvm_stack_alloca>`\\ "
#~ "\\(dtype\\_str\\, num\\)"
#~ msgstr ""

#~ msgid "Return new on stack dtype[num]"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_stack_make_array <tvm.tir.tvm_stack_make_array>`\\"
#~ " \\(data\\, shape\\, strides\\, ...\\)"
#~ msgstr ""

#~ msgid "Allocate a NDArray(DLTensor) on stack, return the handle"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_stack_make_shape <tvm.tir.tvm_stack_make_shape>`\\"
#~ " \\(\\*args\\)"
#~ msgstr ""

#~ msgid "Allocate a shape tuple on stack, return the handle"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_store_matrix_sync "
#~ "<tvm.tir.tvm_store_matrix_sync>`\\ \\(fragment\\, m\\, "
#~ "n\\, k\\, ...\\)"
#~ msgstr ""

#~ msgid "TVM intrinsic for tensor core store operators"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_struct_get <tvm.tir.tvm_struct_get>`\\ "
#~ "\\(arr\\, index\\, field\\, dtype\\)"
#~ msgstr ""

#~ msgid "Get struct field value in array"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_struct_set <tvm.tir.tvm_struct_set>`\\ "
#~ "\\(arr\\, index\\, field\\, value\\)"
#~ msgstr ""

#~ msgid "Set value in struct field in array"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tvm_thread_allreduce <tvm.tir.tvm_thread_allreduce>`\\"
#~ " \\(\\*freduce\\_args\\)"
#~ msgstr ""

#~ msgid "Perform allreduce inside threadblock."
#~ msgstr ""

#~ msgid ":py:obj:`tvm_throw_last_error <tvm.tir.tvm_throw_last_error>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Throw TVMGetLastError()"
#~ msgstr ""

#~ msgid ":py:obj:`tvm_tuple <tvm.tir.tvm_tuple>`\\ \\(\\*value\\)"
#~ msgstr ""

#~ msgid "Create a tuple structure in value field of AttrStmt"
#~ msgstr ""

#~ msgid ":py:obj:`type_annotation <tvm.tir.type_annotation>`\\ \\(dtype\\)"
#~ msgstr ""

#~ msgid "Create a type annotation expression"
#~ msgstr ""

#~ msgid ":py:obj:`undef <tvm.tir.undef>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Returns an initialized but arbitrary value"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`vectorcombine <tvm.tir.vectorcombine>`\\ "
#~ "\\(dtype\\, vec1\\, vec2\\)"
#~ msgstr ""

#~ msgid "Concat two vectors"
#~ msgstr ""

#~ msgid ":py:obj:`vectorhigh <tvm.tir.vectorhigh>`\\ \\(dtype\\, vec\\)"
#~ msgstr ""

#~ msgid "Get the high level half of the vector"
#~ msgstr ""

#~ msgid ":py:obj:`vectorlow <tvm.tir.vectorlow>`\\ \\(dtype\\, vec\\)"
#~ msgstr ""

#~ msgid "Get the low level half of the vector"
#~ msgstr ""

#~ msgid "Parameters"
#~ msgstr ""

#~ msgid "a"
#~ msgstr ""

#~ msgid "PrimExpr"
#~ msgstr ""

#~ msgid "b"
#~ msgstr ""

#~ msgid "buffer_var"
#~ msgstr ""

#~ msgid "Var"
#~ msgstr ""

#~ msgid "dtype"
#~ msgstr ""

#~ msgid "str"
#~ msgstr ""

#~ msgid "extents"
#~ msgstr ""

#~ msgid "list of Expr"
#~ msgstr ""

#~ msgid "condition"
#~ msgstr ""

#~ msgid "body"
#~ msgstr ""

#~ msgid "Stmt"
#~ msgstr ""

#~ msgid "annotations: Optional[Mapping[str, Object]]"
#~ msgstr ""

#~ msgid "data_or_idx"
#~ msgstr ""

#~ msgid "Union[NDArray, int]"
#~ msgstr ""

#~ msgid ""
#~ "If an NDArray, this is the const"
#~ " data associated with the constant.  "
#~ "If an integer, this is the index"
#~ " into the \"constants\" attribute of "
#~ "the `IRModule` that contains the "
#~ "`AllocateConst`."
#~ msgstr ""

#~ msgid "annotations"
#~ msgstr ""

#~ msgid "Optional[Map]"
#~ msgstr ""

#~ msgid "Additional annotations about the allocation."
#~ msgstr ""

#~ msgid "message"
#~ msgstr ""

#~ msgid "tvm.tir.Stmt"
#~ msgstr ""

#~ msgid "node"
#~ msgstr ""

#~ msgid "Node"
#~ msgstr ""

#~ msgid "attr_key"
#~ msgstr ""

#~ msgid "value"
#~ msgstr ""

#~ msgid "src_layout"
#~ msgstr ""

#~ msgid "str or Layout"
#~ msgstr ""

#~ msgid "dst_layout"
#~ msgstr ""

#~ msgid "See Also"
#~ msgstr ""

#~ msgid "bijective_layout : Declare a layout"
#~ msgstr ""

#~ msgid "index: Array of Expr"
#~ msgstr ""

#~ msgid "Returns"
#~ msgstr ""

#~ msgid "src_index: Array of Expr"
#~ msgstr ""

#~ msgid "The inferred indices in src-layout."
#~ msgstr ""

#~ msgid "shape: Array of Expr"
#~ msgstr ""

#~ msgid "src_shape: Array of Expr"
#~ msgstr ""

#~ msgid "The inferred shape in src-layout."
#~ msgstr ""

#~ msgid "dst_index: Array of Expr"
#~ msgstr ""

#~ msgid "The inferred indices in dst-layout."
#~ msgstr ""

#~ msgid "dst_shape: Array of Expr"
#~ msgstr ""

#~ msgid "The inferred shape in dst-layout."
#~ msgstr ""

#~ msgid "iter_vars"
#~ msgstr ""

#~ msgid "List[IterVar]"
#~ msgstr ""

#~ msgid "reads"
#~ msgstr ""

#~ msgid "List[BufferRegion]"
#~ msgstr ""

#~ msgid "writes: List[BufferRegion]"
#~ msgstr ""

#~ msgid "name_hint: str"
#~ msgstr ""

#~ msgid "body: Stmt"
#~ msgstr ""

#~ msgid "init: Optional[Stmt]"
#~ msgstr ""

#~ msgid "alloc_buffers: Optional[list[Buffer]]"
#~ msgstr ""

#~ msgid "match_buffers: Optional[List[MatchBufferRegion]]"
#~ msgstr ""

#~ msgid ""
#~ "The data structures exposed are: 1) "
#~ "sref2scope: Mapping from the srefs to"
#~ " its corresponding BlockScope 2) stmt2ref:"
#~ " Mapping from blocks to corresponding "
#~ "StmtSRefs"
#~ msgstr ""

#~ msgid ""
#~ "Note that this object does not "
#~ "store SRefs to loops as the "
#~ "purpose is only to expose block "
#~ "level dependences. This provides the "
#~ "advantage that the scope block (parent"
#~ " block) for a given block sref "
#~ "can be directly accessed as sref->parent"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_block_scope "
#~ "<tvm.tir.BlockDependenceInfo.get_block_scope>`\\ "
#~ "\\(block\\_sref\\)"
#~ msgstr ""

#~ msgid ":py:obj:`get_sref <tvm.tir.BlockDependenceInfo.get_sref>`\\ \\(block\\)"
#~ msgstr ""

#~ msgid "Return the corresponding sref that points to the block"
#~ msgstr ""

#~ msgid "block_sref"
#~ msgstr ""

#~ msgid "StmtSRef"
#~ msgstr ""

#~ msgid "scope"
#~ msgstr ""

#~ msgid "The corresponding BlockScope"
#~ msgstr ""

#~ msgid "stmt"
#~ msgstr ""

#~ msgid "Block"
#~ msgstr ""

#~ msgid "The block for which the sref is to be retrived"
#~ msgstr ""

#~ msgid "sref"
#~ msgstr ""

#~ msgid "The corresponding sref"
#~ msgstr ""

#~ msgid "iter_values"
#~ msgstr ""

#~ msgid "List[PrimExpr]"
#~ msgstr ""

#~ msgid "predicate"
#~ msgstr ""

#~ msgid "Union[PrimExpr, bool]"
#~ msgstr ""

#~ msgid "block"
#~ msgstr ""

#~ msgid "block: StmtSRef"
#~ msgstr ""

#~ msgid "blocks: List[Dependency]"
#~ msgstr ""

#~ msgid "The dependencies"
#~ msgstr ""

#~ msgid "lanes"
#~ msgstr ""

#~ msgid "int"
#~ msgstr ""

#~ msgid "decl_buffer : Declare a buffer"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_flattened_buffer "
#~ "<tvm.tir.Buffer.get_flattened_buffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Generate a Buffer that is a flattened version of this buffer."
#~ msgstr ""

#~ msgid ":py:obj:`offset_of <tvm.tir.Buffer.offset_of>`\\ \\(indices\\)"
#~ msgstr ""

#~ msgid "Determine the offset of the provided indices in the flattened buffer."
#~ msgstr ""

#~ msgid ""
#~ "Return the storage scope associated with"
#~ " this buffer. Returns ------- scope :"
#~ " str     The storage scope associated "
#~ "with this buffer."
#~ msgstr ""

#~ msgid "access_mask"
#~ msgstr ""

#~ msgid "ptr_type"
#~ msgstr ""

#~ msgid "str, optional"
#~ msgstr ""

#~ msgid "content_lanes: int, optional"
#~ msgstr ""

#~ msgid "offset: Expr, optional"
#~ msgstr ""

#~ msgid "extent: Expr, optional"
#~ msgstr ""

#~ msgid "The extent of pointer."
#~ msgstr ""

#~ msgid "Examples"
#~ msgstr ""

#~ msgid "flattened"
#~ msgstr ""

#~ msgid "Buffer"
#~ msgstr ""

#~ msgid "The corresponding flat buffer."
#~ msgstr ""

#~ msgid "indices : Union[PrimExpr, List[PrimExpr]]"
#~ msgstr ""

#~ msgid "The indices of the element in the original buffer."
#~ msgstr ""

#~ msgid "flattened_indices: List[PrimExpr]"
#~ msgstr ""

#~ msgid "The offset indices of the element in the flattened buffer."
#~ msgstr ""

#~ msgid ""
#~ "Return the storage scope associated with"
#~ " this buffer. Returns ------- scope :"
#~ " str"
#~ msgstr ""

#~ msgid "The storage scope associated with this buffer."
#~ msgstr ""

#~ msgid "begin"
#~ msgstr ""

#~ msgid "Array of Expr"
#~ msgstr ""

#~ msgid "load"
#~ msgstr ""

#~ msgid "Expr"
#~ msgstr ""

#~ msgid "The corresponding load expression."
#~ msgstr ""

#~ msgid "store"
#~ msgstr ""

#~ msgid "The corresponding store stmt."
#~ msgstr ""

#~ msgid "buffer"
#~ msgstr ""

#~ msgid "indices"
#~ msgstr ""

#~ msgid "bounds"
#~ msgstr ""

#~ msgid "List[Range]"
#~ msgstr ""

#~ msgid "region"
#~ msgstr ""

#~ msgid "op"
#~ msgstr ""

#~ msgid "Union[RelayExpr, str]"
#~ msgstr ""

#~ msgid "args"
#~ msgstr ""

#~ msgid "lhs"
#~ msgstr ""

#~ msgid "List[Var]"
#~ msgstr ""

#~ msgid "rhs"
#~ msgstr ""

#~ msgid "result"
#~ msgstr ""

#~ msgid "identity_element"
#~ msgstr ""

#~ msgid "buffer: Buffer"
#~ msgstr ""

#~ msgid "The buffer being declared."
#~ msgstr ""

#~ msgid "The body statement to be executed."
#~ msgstr ""

#~ msgid "span: Optional[Span]"
#~ msgstr ""

#~ msgid "The location of this DeclBuffer in the source code."
#~ msgstr ""

#~ msgid "float"
#~ msgstr ""

#~ msgid "loop_var"
#~ msgstr ""

#~ msgid "min_val"
#~ msgstr ""

#~ msgid "extent"
#~ msgstr ""

#~ msgid "kind"
#~ msgstr ""

#~ msgid "ForKind"
#~ msgstr ""

#~ msgid "thread_binding: Optional[tir.IterVar]"
#~ msgstr ""

#~ msgid "annotations: tvm.ir.Map"
#~ msgstr ""

#~ msgid "note"
#~ msgstr ""

#~ msgid "then_case"
#~ msgstr ""

#~ msgid "else_case"
#~ msgstr ""

#~ msgid "initial_indices"
#~ msgstr ""

#~ msgid "Variables representing the indices prior to remapping."
#~ msgstr ""

#~ msgid "final_indices"
#~ msgstr ""

#~ msgid "Expressions defining the indices after remapping."
#~ msgstr ""

#~ msgid "inverse_index_map"
#~ msgstr ""

#~ msgid "Union[Callable, Optional[IndexMap]]"
#~ msgstr ""

#~ msgid ""
#~ "The optional pre-defined inverse index"
#~ " map. When this is defined, "
#~ "IndexMap::Inverse will return the pre-"
#~ "defined inverse index map. Otherwise, "
#~ "the inverse index map will be "
#~ "computed on the fly. It is the "
#~ "user's responsibility to ensure the "
#~ "correctness of the pre-defined inverse"
#~ " index map."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`from_func <tvm.tir.IndexMap.from_func>`\\ "
#~ "\\(mapping\\_function\\[\\, ndim\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Create an index map from a function"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`from_func_with_separators "
#~ "<tvm.tir.IndexMap.from_func_with_separators>`\\ "
#~ "\\(mapping\\_function\\)"
#~ msgstr ""

#~ msgid ":py:obj:`inverse <tvm.tir.IndexMap.inverse>`\\ \\(shape\\)"
#~ msgstr ""

#~ msgid "Return the inverse of the map"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`is_equivalent_to <tvm.tir.IndexMap.is_equivalent_to>`\\"
#~ " \\(other\\_map\\)"
#~ msgstr ""

#~ msgid "Return if the index maps are equivalent."
#~ msgstr ""

#~ msgid ":py:obj:`map_indices <tvm.tir.IndexMap.map_indices>`\\ \\(indices\\)"
#~ msgstr ""

#~ msgid "Apply the index map to a set of indices"
#~ msgstr ""

#~ msgid ":py:obj:`map_ndarray <tvm.tir.IndexMap.map_ndarray>`\\ \\(arr\\_src\\)"
#~ msgstr ""

#~ msgid "Apply thie index map to transform the layout of the input NDArray"
#~ msgstr ""

#~ msgid ":py:obj:`map_shape <tvm.tir.IndexMap.map_shape>`\\ \\(shape\\)"
#~ msgstr ""

#~ msgid "Apply the index map to a buffer shape"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`non_surjective_inverse "
#~ "<tvm.tir.IndexMap.non_surjective_inverse>`\\ \\(shape\\)"
#~ msgstr ""

#~ msgid "mapping_function : Callable"
#~ msgstr ""

#~ msgid ""
#~ "The function to map from source "
#~ "indices to target indices. The function"
#~ " should accept `tir.Var` parameters and "
#~ "return a either a `tir.PrimExpr`, or "
#~ "a list of `tir.PrimExpr`. Returning a"
#~ " `tir.PrimExpr` is equivalent to returning"
#~ " a list of length 1 containing "
#~ "that `tir.PrimExpr`."
#~ msgstr ""

#~ msgid "ndim: Optional[int]"
#~ msgstr ""

#~ msgid ""
#~ "The dimensionality of the buffer to "
#~ "which this transformation should be "
#~ "applied.  If mapping_function uses variadic"
#~ " argument `*args`, `ndim` must be "
#~ "specified.  If mapping_function does not "
#~ "use variadic arguments, ndim is "
#~ "optional."
#~ msgstr ""

#~ msgid "index_map: IndexMap"
#~ msgstr ""

#~ msgid "Returns an IndexMap representing the `mapping_function`."
#~ msgstr ""

#~ msgid ""
#~ "The function to map from source "
#~ "indices to target indices. The function"
#~ " should accept tir.Var parameters and "
#~ "return either a `tir.PrimExpr` or a "
#~ "list.  Each element of the returned "
#~ "list should be either a `tir.PrimExpr`"
#~ " or the object `IndexMap.AXIS_SEPARATOR`.  "
#~ "Returning a `tir.PrimExpr` is equivalent "
#~ "to returning a list of length 1"
#~ " containing that `tir.PrimExpr`."
#~ msgstr ""

#~ msgid ""
#~ "The dimensionality of the buffer to "
#~ "which this transformation should be "
#~ "applied.  If mapping_function uses variadic"
#~ " argument `*args`, ndim must be "
#~ "specified.  If mapping_function does not "
#~ "use variadic arguments, ndim is "
#~ "optional."
#~ msgstr ""

#~ msgid "index_dtype"
#~ msgstr ""

#~ msgid "The default index dtype to use for input iters in the mapping function."
#~ msgstr ""

#~ msgid "ret: Tuple[IndexMap, List[int]]"
#~ msgstr ""

#~ msgid ""
#~ "Returns a tuple whose first element "
#~ "is an IndexMap representing the "
#~ "`mapping_function`, and whose second index "
#~ "is a list of indices at which "
#~ "`IndexMap.AXIS_SEPARATOR` occurred."
#~ msgstr ""

#~ msgid "Throws an error if the function is not bijective."
#~ msgstr ""

#~ msgid "shape: List[Union[Range,PrimExpr]]"
#~ msgstr ""

#~ msgid ""
#~ "The region over which the inverse "
#~ "should be determined. Used for "
#~ "validating that the mapping is bijective"
#~ " over this range."
#~ msgstr ""

#~ msgid "inverse : IndexMap"
#~ msgstr ""

#~ msgid "The inverse"
#~ msgstr ""

#~ msgid "other_map: IndexMap"
#~ msgstr ""

#~ msgid "The IndexMap to which the comparison should be made."
#~ msgstr ""

#~ msgid "is_equivalent: bool"
#~ msgstr ""

#~ msgid ""
#~ "True if the two mappings represent "
#~ "the same transformation, otherwise False"
#~ msgstr ""

#~ msgid "The indices to be mapped"
#~ msgstr ""

#~ msgid "The mapped indices"
#~ msgstr ""

#~ msgid "arr_src"
#~ msgstr ""

#~ msgid "runtime.NDArray"
#~ msgstr ""

#~ msgid "The NDArray to be transformed"
#~ msgstr ""

#~ msgid "arr_dst"
#~ msgstr ""

#~ msgid "The transformed NDArray"
#~ msgstr ""

#~ msgid "shape"
#~ msgstr ""

#~ msgid "The buffer shape to be mapped"
#~ msgstr ""

#~ msgid "The mapped shape"
#~ msgstr ""

#~ msgid "Can be applied to transformations that introduce padding."
#~ msgstr ""

#~ msgid ""
#~ "The region over which the inverse "
#~ "should be determined. Used for "
#~ "determining the predicate."
#~ msgstr ""

#~ msgid "result : Tuple[IndexMap, PrimExpr]"
#~ msgstr ""

#~ msgid ""
#~ "The inverse, and a predicate for "
#~ "which the inverse maps to a valid"
#~ " index in the input range."
#~ msgstr ""

#~ msgid "dom"
#~ msgstr ""

#~ msgid "Range"
#~ msgstr ""

#~ msgid "var"
#~ msgstr ""

#~ msgid "Union[Var, str]"
#~ msgstr ""

#~ msgid "iter_type"
#~ msgstr ""

#~ msgid "thread_tag"
#~ msgstr ""

#~ msgid ""
#~ "te.thread_axis: Create thread axis IterVar."
#~ " te.reduce_axis: Create reduce axis "
#~ "IterVar."
#~ msgstr ""

#~ msgid "layout : Declare a layout"
#~ msgstr ""

#~ msgid "axis"
#~ msgstr ""

#~ msgid "factor"
#~ msgstr ""

#~ msgid ""
#~ "the size of the subordinate-axis "
#~ "of axis (if axis is a primal-"
#~ "axis), or the size of axis itself"
#~ " (if axis is a subordinate-axis). "
#~ "Return -1 if axis is not in "
#~ "the layout."
#~ msgstr ""

#~ msgid "index"
#~ msgstr ""

#~ msgid "The index of the axis, -1 if not found."
#~ msgstr ""

#~ msgid "source"
#~ msgstr ""

#~ msgid "BufferRegion"
#~ msgstr ""

#~ msgid "list of Range"
#~ msgstr ""

#~ msgid "params: List[Union[tvm.tir.Var, tvm.tir.Buffer]]"
#~ msgstr ""

#~ msgid "body: tvm.tir.Stmt"
#~ msgstr ""

#~ msgid "ret_type: tvm.ir.Type"
#~ msgstr ""

#~ msgid "buffer_map"
#~ msgstr ""

#~ msgid "Map[tvm.tir.Var, tvm.tir.Buffer]"
#~ msgstr ""

#~ msgid "attrs: Optional[tvm.Attrs]"
#~ msgstr ""

#~ msgid "param_map"
#~ msgstr ""

#~ msgid "Mapping[Var, Union[PrimExpr, Buffer]]"
#~ msgstr ""

#~ msgid "func"
#~ msgstr ""

#~ msgid "PrimFunc"
#~ msgstr ""

#~ msgid "The new function with parameter specialized"
#~ msgstr ""

#~ msgid "new_body"
#~ msgstr ""

#~ msgid "new_func"
#~ msgstr ""

#~ msgid "The created new function."
#~ msgstr ""

#~ msgid "producer"
#~ msgstr ""

#~ msgid "DataProducer"
#~ msgstr ""

#~ msgid "list of range"
#~ msgstr ""

#~ msgid "storage_scope"
#~ msgstr ""

#~ msgid "base"
#~ msgstr ""

#~ msgid "stride"
#~ msgstr ""

#~ msgid "ramp stride"
#~ msgstr ""

#~ msgid "combiner"
#~ msgstr ""

#~ msgid "CommReducer"
#~ msgstr ""

#~ msgid "src"
#~ msgstr ""

#~ msgid "rdom"
#~ msgstr ""

#~ msgid "list of IterVar"
#~ msgstr ""

#~ msgid "value_index"
#~ msgstr ""

#~ msgid "init"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`__init__ <tvm.tir.Schedule.__init__>`\\ \\(mod\\,"
#~ " \\*\\[\\, seed\\, debug\\_mask\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Construct a TensorIR schedule class from an IRModule"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`_create_non_traced "
#~ "<tvm.tir.Schedule._create_non_traced>`\\ \\(mod\\, "
#~ "\\*\\[\\, seed\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Construct a non-traced TensorIR schedule class from an IRModule."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`add_unit_loop <tvm.tir.Schedule.add_unit_loop>`\\ "
#~ "\\(block\\_or\\_loop\\)"
#~ msgstr ""

#~ msgid "Create a new unit loop on top of the specific block or loop."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`blockize <tvm.tir.Schedule.blockize>`\\ "
#~ "\\(target\\[\\, preserve\\_unit\\_iters\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Convert multiple blocks or the subtree"
#~ " rooted at a specific loop into "
#~ "a block."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cache_index <tvm.tir.Schedule.cache_index>`\\ "
#~ "\\(block\\, storage\\_scope\\[\\, cse\\_thresh\\]\\)"
#~ msgstr ""

#~ msgid "Create a block to cache precomputed index for later use."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cache_inplace <tvm.tir.Schedule.cache_inplace>`\\ "
#~ "\\(block\\, read\\_buffer\\_index\\, ...\\)"
#~ msgstr ""

#~ msgid "Create blocks that reads & write a buffer region into a cache block."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cache_read <tvm.tir.Schedule.cache_read>`\\ "
#~ "\\(block\\, read\\_buffer\\_index\\, ...\\[\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`can_decompose_padding "
#~ "<tvm.tir.Schedule.can_decompose_padding>`\\ \\(block\\, "
#~ "loop\\)"
#~ msgstr ""

#~ msgid "Check whether the block match padding pattern and can be decomposed."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`compute_at <tvm.tir.Schedule.compute_at>`\\ "
#~ "\\(block\\, loop\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`decompose_padding "
#~ "<tvm.tir.Schedule.decompose_padding>`\\ \\(block\\, "
#~ "loop\\)"
#~ msgstr ""

#~ msgid ""
#~ "Decompose a block of padding computation"
#~ " pattern into two separate blocks."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`fuse <tvm.tir.Schedule.fuse>`\\ \\(\\*loops\\[\\,"
#~ " preserve\\_unit\\_iters\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_output_blocks "
#~ "<tvm.tir.Schedule.get_output_blocks>`\\ \\(scope\\_block\\)"
#~ msgstr ""

#~ msgid ""
#~ "Get the list of output blocks "
#~ "within the given scope An output "
#~ "block is a block which has atleast"
#~ " one buffer being written to, but "
#~ "is not allocated within the PrimFunc"
#~ msgstr ""

#~ msgid ":py:obj:`merge <tvm.tir.Schedule.merge>`\\ \\(\\*loops\\)"
#~ msgstr ""

#~ msgid "Merge a list of loops into one."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pad_einsum <tvm.tir.Schedule.pad_einsum>`\\ "
#~ "\\(block\\, padding\\)"
#~ msgstr ""

#~ msgid "Pad the computation of Einsum."
#~ msgstr ""

#~ msgid ":py:obj:`reindex <tvm.tir.Schedule.reindex>`\\ \\(block\\, buffer\\)"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that read/write a "
#~ "buffer region into a read/write cache"
#~ " with reindexing."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reindex_cache_read "
#~ "<tvm.tir.Schedule.reindex_cache_read>`\\ \\(block\\, "
#~ "read\\_buffer\\_index\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a read cache using "
#~ "customized indices specified by index "
#~ "map."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reindex_cache_write "
#~ "<tvm.tir.Schedule.reindex_cache_write>`\\ \\(block\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a write cache using "
#~ "customized indices specified by index "
#~ "map."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reorder_block_iter_var "
#~ "<tvm.tir.Schedule.reorder_block_iter_var>`\\ \\(block\\, "
#~ "new\\_order\\)"
#~ msgstr ""

#~ msgid "Reorder the itervars inside a given block."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`rolling_buffer <tvm.tir.Schedule.rolling_buffer>`\\ "
#~ "\\(block\\, write\\_buffer\\_index\\)"
#~ msgstr ""

#~ msgid ""
#~ "Compute the target buffer via rolling"
#~ " buffering, select the outermost rollable"
#~ " axis with a positive bound overlap"
#~ " that appears in the block's ancestor"
#~ " loops as `rolling axis`, fold and"
#~ " circularize the buffer along the "
#~ "rolling dimension, append block predicate "
#~ "to avoid recomputing overlapping elements."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sample_partitioned_tile "
#~ "<tvm.tir.Schedule.sample_partitioned_tile>`\\ \\(loop\\, "
#~ "n\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Sample the factors to a partitioned tile for a specific loop"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`set_axis_separator "
#~ "<tvm.tir.Schedule.set_axis_separator>`\\ \\(block\\, "
#~ "buffer\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Set the axis separator of a "
#~ "buffer, where the buffer is specified"
#~ " by a block and a read or "
#~ "write index."
#~ msgstr ""

#~ msgid ""
#~ "Set the storage scope of a buffer,"
#~ " where the buffer is specified by "
#~ "the a block and a write-index."
#~ msgstr ""

#~ msgid ":py:obj:`show <tvm.tir.Schedule.show>`\\ \\(\\*args\\, \\*\\*kwargs\\)"
#~ msgstr ""

#~ msgid "A sugar for print highlighted TVM script."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`split <tvm.tir.Schedule.split>`\\ \\(loop\\, "
#~ "factors\\[\\, preserve\\_unit\\_iters\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tensorize <tvm.tir.Schedule.tensorize>`\\ "
#~ "\\(block\\_or\\_loop\\, tensor\\_intrin\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`transform_block_layout "
#~ "<tvm.tir.Schedule.transform_block_layout>`\\ \\(block\\, "
#~ "index\\_map\\)"
#~ msgstr ""

#~ msgid "Apply a transformation represented by IndexMap to block"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`transform_layout <tvm.tir.Schedule.transform_layout>`\\"
#~ " \\(block\\, buffer\\, index\\_map\\[\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Apply a transformation represented by IndexMap to buffer"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`unsafe_hide_buffer_access "
#~ "<tvm.tir.Schedule.unsafe_hide_buffer_access>`\\ \\(block\\, "
#~ "buf\\_type\\, ...\\)"
#~ msgstr ""

#~ msgid "Hide some buffer access in a given block."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`unsafe_set_dtype <tvm.tir.Schedule.unsafe_set_dtype>`\\"
#~ " \\(block\\, buffer\\_index\\, dtype\\)"
#~ msgstr ""

#~ msgid ""
#~ "Set the data type of a buffer, "
#~ "where the buffer is specified by "
#~ "the a block and write-index."
#~ msgstr ""

#~ msgid ":py:obj:`work_on <tvm.tir.Schedule.work_on>`\\ \\(func\\_name\\)"
#~ msgstr ""

#~ msgid "Instruct the schedule to work on a function in the IRModule."
#~ msgstr ""

#~ msgid ":py:obj:`func_working_on <tvm.tir.Schedule.func_working_on>`\\"
#~ msgstr ""

#~ msgid ""
#~ "Returns the GlobalVar of the func "
#~ "that the schedule is currently working"
#~ " on"
#~ msgstr ""

#~ msgid "mod"
#~ msgstr ""

#~ msgid "Union[PrimFunc, IRModule]"
#~ msgstr ""

#~ msgid "The IRModule or PrimFunc to be scheduled"
#~ msgstr ""

#~ msgid "seed: Optional[int]"
#~ msgstr ""

#~ msgid ""
#~ "The seed value for schedule's random "
#~ "state Note that None and -1 means"
#~ " use device random, otherwise only "
#~ "integer between 1 and 2147483647 is "
#~ "allowed."
#~ msgstr ""

#~ msgid "debug_mask"
#~ msgstr ""

#~ msgid "Union[str, int]"
#~ msgstr ""

#~ msgid ""
#~ "Do extra correctness checking after the"
#~ " class creation and each time after"
#~ " calling the Replace method. Possible "
#~ "choices of `debug_mask`: 1) \"all\" -"
#~ " Turn on all the checks 2) "
#~ "\"none\" - Turn off all the checks"
#~ " 3) An integer - Turn on checks"
#~ " according to the bitmasks provided "
#~ "in ScheduleDebugMask"
#~ msgstr ""

#~ msgid "error_render_level"
#~ msgstr ""

#~ msgid "str = \"detail\""
#~ msgstr ""

#~ msgid ""
#~ "The level of error rendering. Choices:"
#~ " \"detail\", \"fast\", \"none\". - "
#~ "\"detail\": Render a detailed error "
#~ "message, with the TIR and error "
#~ "locations printed - \"fast: Show a "
#~ "simple error message without rendering "
#~ "or string manipulation - \"none\": Do"
#~ " not show any error message."
#~ msgstr ""

#~ msgid "enable_check"
#~ msgstr ""

#~ msgid "bool = True"
#~ msgstr ""

#~ msgid ""
#~ "The default schedule checks are too "
#~ "strict and might prevent us performing"
#~ " some valid schedules. `enable_check` is"
#~ " an argument to control whether we"
#~ " enable prerequisite checks for some "
#~ "schedule primitives or not: - true: "
#~ "perform prerequisite check before applying "
#~ "some schedules. - false: do not "
#~ "perform some check before applying "
#~ "schedules, but still raise error if "
#~ "schedule fails."
#~ msgstr ""

#~ msgid ""
#~ "It's user duty to guarantee schedule "
#~ "correctness if `enable_check` is set to"
#~ " `False`."
#~ msgstr ""

#~ msgid "Note"
#~ msgstr ""

#~ msgid "The checks performed includes: 1) VerifySRefTree 2) VerifyCachedFlags"
#~ msgstr ""

#~ msgid "block_or_loop"
#~ msgstr ""

#~ msgid "Union[LoopRV, BlockRV]"
#~ msgstr ""

#~ msgid "The block above which the new loop is created"
#~ msgstr ""

#~ msgid "new_loop"
#~ msgstr ""

#~ msgid "LoopRV"
#~ msgstr ""

#~ msgid "The new unit loop"
#~ msgstr ""

#~ msgid "Before add_unit_loop, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do add-unit-loop:"
#~ msgstr ""

#~ msgid "After applying add-unit-loop, the IR becomes:"
#~ msgstr ""

#~ msgid "block_or_loop: Union[BlockRV, LoopRV]"
#~ msgstr ""

#~ msgid "ann_key"
#~ msgstr ""

#~ msgid "ann_val"
#~ msgstr ""

#~ msgid "AnnotationValueT"
#~ msgstr ""

#~ msgid "loop"
#~ msgstr ""

#~ msgid "thread_axis"
#~ msgstr ""

#~ msgid "target"
#~ msgstr ""

#~ msgid "LoopRV or List[BlockRV]"
#~ msgstr ""

#~ msgid "The root of the subtree or the specified blocks."
#~ msgstr ""

#~ msgid "preserve_unit_iters"
#~ msgstr ""

#~ msgid "bool"
#~ msgstr ""

#~ msgid "Whether or not to preserve unit iterators in block bindings"
#~ msgstr ""

#~ msgid "BlockRV"
#~ msgstr ""

#~ msgid "The new block."
#~ msgstr ""

#~ msgid ""
#~ "Create a block to cache precomputed "
#~ "index for later use. if there is"
#~ " no index computation, keep unchanged."
#~ msgstr ""

#~ msgid "Union[BlockRV, str]"
#~ msgstr ""

#~ msgid "The target block operates on the target buffer."
#~ msgstr ""

#~ msgid "storage_scope: str"
#~ msgstr ""

#~ msgid "The storage scope of cached block."
#~ msgstr ""

#~ msgid "cse_thresh: int"
#~ msgstr ""

#~ msgid ""
#~ "The repeat threshold that determines a"
#~ " common sub expr, default 0 means "
#~ "cache all index computation."
#~ msgstr ""

#~ msgid "cached_blocks"
#~ msgstr ""

#~ msgid "List[BlockRV]"
#~ msgstr ""

#~ msgid "The blocks of the stage writing the cache buffers"
#~ msgstr ""

#~ msgid "Before cache_inplace, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and cache_index:"
#~ msgstr ""

#~ msgid "After applying cache_index, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Create blocks that reads & write a"
#~ " buffer region into a cache block."
#~ " It requires the target block both"
#~ " read & write the target buffer. "
#~ "Mainly for inplace operation."
#~ msgstr ""

#~ msgid "read_buffer_index: int"
#~ msgstr ""

#~ msgid ""
#~ "The index of the buffer in block's"
#~ " read region, the unique name of "
#~ "a read buffer in the block, or "
#~ "a Buffer object that is within the"
#~ " blocks read region."
#~ msgstr ""

#~ msgid "The blocks of the cache stage, read cache first, write cache second"
#~ msgstr ""

#~ msgid "Create the schedule and cache_inplace:"
#~ msgstr ""

#~ msgid "After applying cache_inplace, the IR becomes:"
#~ msgstr ""

#~ msgid "buffer: Union[int, str, Buffer]"
#~ msgstr ""

#~ msgid "consumer_blocks: Optional[List[Union[BlockRV, str]]]"
#~ msgstr ""

#~ msgid ""
#~ "An optional list of consumers that "
#~ "should read from the cache. If not"
#~ " specified, all consumers will use "
#~ "the cache."
#~ msgstr ""

#~ msgid "cached_block"
#~ msgstr ""

#~ msgid "The block of the cache stage"
#~ msgstr ""

#~ msgid "write_buffer_index: int"
#~ msgstr ""

#~ msgid ""
#~ "The index of the buffer in block's"
#~ " write region, the unique name of "
#~ "a write buffer in the block, or"
#~ " a Buffer object that is within "
#~ "the blocks write region."
#~ msgstr ""

#~ msgid ""
#~ "An optional list of consumers that "
#~ "should read directly from the cache. "
#~ "If not specified, all consumers will "
#~ "read from the original buffer."
#~ msgstr ""

#~ msgid "loop: LoopRV"
#~ msgstr ""

#~ msgid "preserve_unit_loops: bool"
#~ msgstr ""

#~ msgid "index: int"
#~ msgstr ""

#~ msgid ""
#~ "The block index of the loop body"
#~ " subtree blocks: - `index = -1` "
#~ "means inserted into the last possible"
#~ " insertion point; - `index = -2` "
#~ "means inserted into the first possible"
#~ " insertion point; - Otherwise, `index` "
#~ "is a nonnegative number that indicates"
#~ " the insertion point"
#~ msgstr ""

#~ msgid "copy"
#~ msgstr ""

#~ msgid "Schedule"
#~ msgstr ""

#~ msgid "A new copy of the schedule"
#~ msgstr ""

#~ msgid "The block which fill const pad values into full write region;"
#~ msgstr ""

#~ msgid ""
#~ "The block which fill in-bound "
#~ "values into region where pad predicate"
#~ " is true."
#~ msgstr ""

#~ msgid "The pad value filling block is inserted right before the given loop."
#~ msgstr ""

#~ msgid "The input block is a complete block."
#~ msgstr ""

#~ msgid "The input block is a block which match padding pattern."
#~ msgstr ""

#~ msgid "The padding block to be decomposed."
#~ msgstr ""

#~ msgid "The loop above which the pad value filling block is inserted before."
#~ msgstr ""

#~ msgid "pad_value_block"
#~ msgstr ""

#~ msgid "The block filling const pad values."
#~ msgstr ""

#~ msgid "Before decompose-padding, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do decompose-padding with specified loop:"
#~ msgstr ""

#~ msgid "After applying decompose-padding, the IR becomes:"
#~ msgstr ""

#~ msgid "init_block"
#~ msgstr ""

#~ msgid "The init block"
#~ msgstr ""

#~ msgid "seed"
#~ msgstr ""

#~ msgid "The forked random state, not the same as the current random state"
#~ msgstr ""

#~ msgid "*loops"
#~ msgstr ""

#~ msgid "List[LoopRV]"
#~ msgstr ""

#~ msgid "fused_loop"
#~ msgstr ""

#~ msgid "The new loop after fusion"
#~ msgstr ""

#~ msgid "rand_var_or_sref"
#~ msgstr ""

#~ msgid "Union[ExprRV, BlockRV, LoopRV, StmtSRef]"
#~ msgstr ""

#~ msgid "Optional[Union[int, Block, For]]"
#~ msgstr ""

#~ msgid "The corresponding result"
#~ msgstr ""

#~ msgid ""
#~ "By default, if `func_name` is not "
#~ "specified, the schedule will search for"
#~ " the block in the function that "
#~ "is currently being \"worked on\". To "
#~ "switch the function to be worked "
#~ "on, use `work_on` before calling this"
#~ " method."
#~ msgstr ""

#~ msgid "name"
#~ msgstr ""

#~ msgid "func_name"
#~ msgstr ""

#~ msgid "Optional[str] = None"
#~ msgstr ""

#~ msgid ""
#~ "The block retrieved IndexError is raised"
#~ " if 0 or multiple blocks exist "
#~ "with the specific name."
#~ msgstr ""

#~ msgid "Union[BlockRV, LoopRV]"
#~ msgstr ""

#~ msgid "blocks"
#~ msgstr ""

#~ msgid "A list of leaf blocks inside a specific block/loop"
#~ msgstr ""

#~ msgid "consumers"
#~ msgstr ""

#~ msgid "A list of consumers of the given block"
#~ msgstr ""

#~ msgid "loops"
#~ msgstr ""

#~ msgid "A list of loops above the given block in its scope, from outer to inner"
#~ msgstr ""

#~ msgid "scope_block"
#~ msgstr ""

#~ msgid "Union[BlockRV, str],"
#~ msgstr ""

#~ msgid "The scope block from which output blocks are collected"
#~ msgstr ""

#~ msgid "output_blocks"
#~ msgstr ""

#~ msgid "A list of all blocks that write to some output buffer"
#~ msgstr ""

#~ msgid "producers"
#~ msgstr ""

#~ msgid "A list of producers of the given block"
#~ msgstr ""

#~ msgid "rand_var_or_stmt"
#~ msgstr ""

#~ msgid "Union[BlockRV, LoopRV, Block, For]"
#~ msgstr ""

#~ msgid "Optional[StmtSRef]"
#~ msgstr ""

#~ msgid ""
#~ "Merge a list of loops into one."
#~ " The loops under their LCA requires:"
#~ " 1) Under the same scope. 2) "
#~ "Can't have annotations or thread "
#~ "bindings. 3) Start with 0 and have"
#~ " same extent and same nesting depth."
#~ " 4) From target loop to their "
#~ "LCA, The inner loop must be the"
#~ " only child of the outer loop."
#~ msgstr ""

#~ msgid "The loops to be merged"
#~ msgstr ""

#~ msgid "The new loop after merge"
#~ msgstr ""

#~ msgid "Before applying merge, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid ""
#~ "On a block with trivial binding, "
#~ "this primitive pads the iteration domain"
#~ " of the block by the given "
#~ "padding factors, for example, 127 -> "
#~ "128, 132 -> 144 when padding "
#~ "factor is 16. Extra producer and "
#~ "consumer padding blocks will be "
#~ "generated to avoid out-of-bound "
#~ "buffer access."
#~ msgstr ""

#~ msgid ""
#~ "Einsum pattern means all the indices "
#~ "on the buffer access are either by"
#~ " constants (e.g. B[0]) or by "
#~ "variables (e.g. B[i]), but not by "
#~ "composite expressions (e.g. B[i + 1])."
#~ msgstr ""

#~ msgid "The block that matches the Einsum pattern."
#~ msgstr ""

#~ msgid "padding"
#~ msgstr ""

#~ msgid "List[int]"
#~ msgstr ""

#~ msgid "The padding for each block iter."
#~ msgstr ""

#~ msgid "Before applying pad-einsum, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do pad-einsum with specified block:"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that read/write a "
#~ "buffer region into a read/write cache"
#~ " with reindexing. The layout of the"
#~ " cache will be the same as by"
#~ " the iterators of the block that "
#~ "reads/writes the buffer. It requires: 1)"
#~ " There is only one block who "
#~ "reads/writes the target buffer 2) There"
#~ " is only one buffer load/store of "
#~ "this buffer in the block"
#~ msgstr ""

#~ msgid "block : Union[BlockRV, str]"
#~ msgstr ""

#~ msgid ""
#~ "The block that accesses the target "
#~ "buffer.  If a string, this must "
#~ "uniquely identify a block."
#~ msgstr ""

#~ msgid "buffer: Union[Tuple[str,int], Buffer, str]"
#~ msgstr ""

#~ msgid ""
#~ "The buffer to be transformed, or a"
#~ " specification of how to identify the"
#~ " buffer to be transformed."
#~ msgstr ""

#~ msgid ""
#~ "If `buffer` if a tuple of "
#~ "``(str,int)``, the first item should be"
#~ " either \"read\" or \"write\", and "
#~ "the second item is an index into"
#~ " the block's read or write regions."
#~ msgstr ""

#~ msgid ""
#~ "If `buffer` is a string, it is "
#~ "the name of the buffer, which must"
#~ " exist within the reads/writes of the"
#~ " block.  In addition, the reads/writes "
#~ "of the block may not contain more"
#~ " than one buffer with this name."
#~ msgstr ""

#~ msgid ""
#~ "If `buffer` is a Buffer object, it"
#~ " must exist within the reads/writes "
#~ "of the block."
#~ msgstr ""

#~ msgid "reindex_block"
#~ msgstr ""

#~ msgid "The block of the reindex stage"
#~ msgstr ""

#~ msgid "Before reindex, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reindex:"
#~ msgstr ""

#~ msgid "After applying reindex, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a read cache using "
#~ "customized indices specified by index "
#~ "map. The read region of the buffer"
#~ " must be a single point."
#~ msgstr ""

#~ msgid ""
#~ "The cache stage block follows the "
#~ "original order of loops and block "
#~ "itervars in the block. If a block"
#~ " itervar does not appear in the "
#~ "buffer access region, it and its "
#~ "corresponding loop variables will be "
#~ "omitted. User can then use "
#~ "`transform_block_layout` primitive to reorder "
#~ "the block itervars and surrounding loops"
#~ " of the cache read/write block."
#~ msgstr ""

#~ msgid ""
#~ "Unlike `cache_read`, `reindex_cache_read` only "
#~ "supports single consumer, please use "
#~ "`cache_read` when there are multiple "
#~ "consumers."
#~ msgstr ""

#~ msgid "index_map: Union[IndexMap, Callable]"
#~ msgstr ""

#~ msgid ""
#~ "User defined indices to access allocated"
#~ " cache buffer, maps from block iter"
#~ " vars."
#~ msgstr ""

#~ msgid "Before reindex_cache_read, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and reindex_cache_read:"
#~ msgstr ""

#~ msgid "After applying reindex_cache_read, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "reindex_cache_write transform_block_layout "
#~ "transform_layout cache_read reindex"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a write cache using "
#~ "customized indices specified by index "
#~ "map. The write region of the "
#~ "buffer must be a single point."
#~ msgstr ""

#~ msgid ""
#~ "Unlike `cache_write`, `reindex_cache_write` only "
#~ "supports single consumer, please use "
#~ "`cache_write` when there are multiple "
#~ "consumers."
#~ msgstr ""

#~ msgid "index_map: Union[Callable, IndexMap]"
#~ msgstr ""

#~ msgid "Before reindex_cache_write, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and reindex_cache_write:"
#~ msgstr ""

#~ msgid "After applying reindex_cache_write, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "reindex_cache_read transform_block_layout transform_layout"
#~ " cache_write reindex"
#~ msgstr ""

#~ msgid "rand_var"
#~ msgstr ""

#~ msgid "Union[BlockRV, LoopRV, ExprRV]"
#~ msgstr ""

#~ msgid "*ordered_loops"
#~ msgstr ""

#~ msgid "The block to be transformed."
#~ msgstr ""

#~ msgid "new_order"
#~ msgstr ""

#~ msgid "The new block itervar order."
#~ msgstr ""

#~ msgid "Before reorder_block_iter_var, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reorder_block_iter_var:"
#~ msgstr ""

#~ msgid "After applying reorder_block_iter_var, the IR becomes:"
#~ msgstr ""

#~ msgid "reorder"
#~ msgstr ""

#~ msgid "factor_axis"
#~ msgstr ""

#~ msgid "rf_block"
#~ msgstr ""

#~ msgid ""
#~ "The block which computes partial results"
#~ " over each slices (i.e., the first"
#~ " block as described in the above "
#~ "illustration)"
#~ msgstr ""

#~ msgid ""
#~ "Compute the target buffer via rolling"
#~ " buffering, select the outermost rollable"
#~ " axis with a positive bound overlap"
#~ " that appears in the block's ancestor"
#~ " loops as `rolling axis`, fold and"
#~ " circularize the buffer along the "
#~ "rolling dimension, append block predicate "
#~ "to avoid recomputing overlapping elements. "
#~ "It requires:"
#~ msgstr ""

#~ msgid "The block is not an output block and has only RAW dependencies."
#~ msgstr ""

#~ msgid "The buffer to be an intermediate buffer defined via `alloc_buffer`."
#~ msgstr ""

#~ msgid ""
#~ "3) The LCA of the producer and "
#~ "consumer of the buffer is a for"
#~ " loop, typically, the producer and "
#~ "consumer of the buffer are cascaded "
#~ "through compute_at."
#~ msgstr ""

#~ msgid ""
#~ "4) The access region of the buffer"
#~ " has at least one dimension that "
#~ "contains a positive bound overlap."
#~ msgstr ""

#~ msgid "write_buffer_index"
#~ msgstr ""

#~ msgid "Before rolling_buffer, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do rolling_buffer:"
#~ msgstr ""

#~ msgid "After applying rolling_buffer, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "The region_cover property of the "
#~ "consumer block of the target buffer "
#~ "will become false."
#~ msgstr ""

#~ msgid "candidates"
#~ msgstr ""

#~ msgid "probs"
#~ msgstr ""

#~ msgid "List[float]"
#~ msgstr ""

#~ msgid "decision"
#~ msgstr ""

#~ msgid "Optional[int]"
#~ msgstr ""

#~ msgid "ExprRV"
#~ msgstr ""

#~ msgid "The random variable sampled from candidates"
#~ msgstr ""

#~ msgid "The sampled loop where the input block is to be computed at"
#~ msgstr ""

#~ msgid "n"
#~ msgstr ""

#~ msgid "partition_pos"
#~ msgstr ""

#~ msgid "The position to partition tiles to two parts"
#~ msgstr ""

#~ msgid "innerpart_factor"
#~ msgstr ""

#~ msgid "The factor of the second part"
#~ msgstr ""

#~ msgid "decision: Optional[List[int]]"
#~ msgstr ""

#~ msgid "List[ExprRV]"
#~ msgstr ""

#~ msgid "A list of length `n`, the random partitioned tile sizes sampled"
#~ msgstr ""

#~ msgid "max_innermost_factor"
#~ msgstr ""

#~ msgid "A list of length `n`, the random perfect tile sizes sampled"
#~ msgstr ""

#~ msgid "axis_separators : Optional[List[int]]"
#~ msgstr ""

#~ msgid "The axis separators."
#~ msgstr ""

#~ msgid "Before set_axis_separator, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do set_axis_separator:"
#~ msgstr ""

#~ msgid "After applying set_axis_separator, the IR becomes:"
#~ msgstr ""

#~ msgid "buffer_index"
#~ msgstr ""

#~ msgid ""
#~ "`set_scope` requires the buffer to be"
#~ " an intermediate buffer defined via "
#~ "`alloc_buffer`."
#~ msgstr ""

#~ msgid ""
#~ "All parameters are forwarded to the "
#~ "underlying `Module.show` and `Trace.show` "
#~ "methods."
#~ msgstr ""

#~ msgid "factors: List[Union[int, ExprRV, None]]"
#~ msgstr ""

#~ msgid "split_loops"
#~ msgstr ""

#~ msgid "The new loops after split"
#~ msgstr ""

#~ msgid "offset"
#~ msgstr ""

#~ msgid "tensor_intrin"
#~ msgstr ""

#~ msgid "The block to be transformed"
#~ msgstr ""

#~ msgid "index_map"
#~ msgstr ""

#~ msgid "Union[IndexMap, Callable]"
#~ msgstr ""

#~ msgid "The transformation to apply."
#~ msgstr ""

#~ msgid "Before transform_block_layout, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do transform_block_layout:"
#~ msgstr ""

#~ msgid "After applying transform_block_layout, the IR becomes:"
#~ msgstr ""

#~ msgid "index_map : Union[IndexMap, Callable]"
#~ msgstr ""

#~ msgid ""
#~ "If `index_map` is a callable, and "
#~ "the returned list contains "
#~ "IndexMap.AXIS_SEPARATOR, the SetAxisSeparators "
#~ "primitive will be called in addition "
#~ "to the TransformLayout primitive."
#~ msgstr ""

#~ msgid "pad_value: Optional[Union[int, float, PrimExpr, IndexMap, Callable]]"
#~ msgstr ""

#~ msgid ""
#~ "The value to be used for any "
#~ "padding introduced by the transformation.  "
#~ "If the schedule contains a producer "
#~ "block for the specified buffer, the "
#~ "pad value will be written as part"
#~ " of the producer block if possible,"
#~ " or after the producer block "
#~ "otherwise.  Otherwise, if the buffer is"
#~ " an input, will insert an annotation"
#~ " block to state that the padding "
#~ "contains the known value."
#~ msgstr ""

#~ msgid ""
#~ "The pad value may not contain "
#~ "instances of BufferLoad, except where it"
#~ " loads a value from the buffer "
#~ "being transformed (e.g. to create a "
#~ "circular buffer with padding that "
#~ "consists of repeated elements)."
#~ msgstr ""

#~ msgid ""
#~ "Note: If applied to an input "
#~ "buffer, the calling scope is responsible"
#~ " for ensuring that the pad_value is"
#~ " present. Algebraic symplifications, branch "
#~ "elimination, and other optimizations may "
#~ "assume that this precondition is met,"
#~ " and may result in incorrect results"
#~ " being returned."
#~ msgstr ""

#~ msgid "If None, the transformation may not introduce padding."
#~ msgstr ""

#~ msgid ""
#~ "If an int, float or PrimExpr, the"
#~ " transformation is the specific value "
#~ "to be present in the padding."
#~ msgstr ""

#~ msgid ""
#~ "If an IndexMap or Callable, the "
#~ "transformation is the value to be "
#~ "present in the padding in terms of"
#~ " the transformed index."
#~ msgstr ""

#~ msgid "assume_injective_transform : bool"
#~ msgstr ""

#~ msgid ""
#~ "If set to true, the schedule  "
#~ "primitive will assume the index_map is"
#~ " injective and skip checking overlapping"
#~ " of the mapped indices. This can "
#~ "be useful for complicated index_map that"
#~ " the analysis does not cover. It "
#~ "is the callers' responsibility to ensure"
#~ " the index map is injective, "
#~ "otherwise, the correctness of the "
#~ "schedule is not guaranteed."
#~ msgstr ""

#~ msgid "Before transform_layout, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do transform_layout:"
#~ msgstr ""

#~ msgid "After applying transform_layout, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Hide some buffer access in a given"
#~ " block. This is an unsafe schedule"
#~ " primitive."
#~ msgstr ""

#~ msgid "The block where we hide read access."
#~ msgstr ""

#~ msgid "buf_type"
#~ msgstr ""

#~ msgid "The buffer type: \"read\"/\"write\"."
#~ msgstr ""

#~ msgid "buf_index_array"
#~ msgstr ""

#~ msgid "The array of buffer indices we hide access."
#~ msgstr ""

#~ msgid ""
#~ "This schedule primitive is unsafe, and"
#~ " may fail dependency analysis. One "
#~ "use case of `unsafe_hide_buffer_access` is "
#~ "to hide the buffer access to "
#~ "indices buffers (e.g. in sparse "
#~ "computation) so that we can further "
#~ "tensorize the block (the indices buffers"
#~ " appeared in read/write regions may "
#~ "fail the pattern matching in `tensorize`"
#~ " primitive, and hide the access to"
#~ " these buffers could address the "
#~ "issue)."
#~ msgstr ""

#~ msgid ""
#~ "This schedule primitive is unsafe and"
#~ " may change the correctness of "
#~ "program because of type conversion, "
#~ "please use with caution."
#~ msgstr ""

#~ msgid "The data type to be set"
#~ msgstr ""

#~ msgid "Before unsafe_set_dtype, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do unsafe_set_dtype:"
#~ msgstr ""

#~ msgid "After applying set_dtype, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "`unsafe_set_dtype` requires the buffer to "
#~ "be an intermediate buffer defined via"
#~ " `alloc_buffer`."
#~ msgstr ""

#~ msgid ""
#~ "By default, the schedule works on "
#~ "the function with the name \"main\", "
#~ "or the only function in the "
#~ "IRModule if there is only one. If"
#~ " there is multiple functions in the"
#~ " IRModule, and none of their names"
#~ " are \"main\", users will have to "
#~ "call this method to explicitly specify"
#~ " which function to work on."
#~ msgstr ""

#~ msgid ""
#~ "This sugar function will guide the "
#~ "`GetBlock` method if its `func_name` is"
#~ " not specified."
#~ msgstr ""

#~ msgid "The name of the function to work on."
#~ msgstr ""

#~ msgid ""
#~ "The data structure contains the "
#~ "following information 1) The AST being"
#~ " scheduled (mod) 2) The sref tree "
#~ "of schedulable statements (indicated by "
#~ "the srefs) 3) The dependency information"
#~ " of each block scope (block_info) 4)"
#~ " A reverse mapping from the AST "
#~ "nodes to that in the sref tree "
#~ "(get_sref) 5) A debug flag, if "
#~ "set, extra checking is enabled "
#~ "(debug_mask) 6) A enable check flag, "
#~ "if False, some prerequisite checks are"
#~ " disabled."
#~ msgstr ""

#~ msgid "IRModule"
#~ msgstr ""

#~ msgid ""
#~ "Indicates whether we enable prerequisite "
#~ "checks for some schedule primitives or"
#~ " not, defaults to `True`."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`__init__ <tvm.tir.ScheduleState.__init__>`\\ "
#~ "\\(mod\\, \\*\\[\\, debug\\_mask\\, "
#~ "enable\\_check\\]\\)"
#~ msgstr ""

#~ msgid "Construct a schedule state from an IRModule or a PrimFunc"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`_get_cached_flags "
#~ "<tvm.tir.ScheduleState._get_cached_flags>`\\ \\(block\\_sref\\)"
#~ msgstr ""

#~ msgid "Get the cached flags of the corresponding block"
#~ msgstr ""

#~ msgid "flags"
#~ msgstr ""

#~ msgid "CachedFlags"
#~ msgstr ""

#~ msgid "Three flags: affine_binding, region_cover, stage_pipeline"
#~ msgstr ""

#~ msgid "It is an API intended for internal testing use."
#~ msgstr ""

#~ msgid "Union[Block, For]"
#~ msgstr ""

#~ msgid "src_sref"
#~ msgstr ""

#~ msgid "tgt_stmt"
#~ msgstr ""

#~ msgid "Union[Block, For, BlockRealize]"
#~ msgstr ""

#~ msgid "block_sref_reuse"
#~ msgstr ""

#~ msgid "Optional[Dict[Block, Block]] = None"
#~ msgstr ""

#~ msgid "true_value"
#~ msgstr ""

#~ msgid "false_value"
#~ msgstr ""

#~ msgid "seq"
#~ msgstr ""

#~ msgid "List[Stmt]"
#~ msgstr ""

#~ msgid "vectors"
#~ msgstr ""

#~ msgid "Array of indices"
#~ msgstr ""

#~ msgid "device_type"
#~ msgstr ""

#~ msgid "The device type which the space will be allocated."
#~ msgstr ""

#~ msgid "device_id"
#~ msgstr ""

#~ msgid "The device id which the space will be allocated."
#~ msgstr ""

#~ msgid "nbytes"
#~ msgstr ""

#~ msgid "The size of the space requested."
#~ msgstr ""

#~ msgid "dtype_code_hint"
#~ msgstr ""

#~ msgid ""
#~ "The type code of the array "
#~ "elements. Only used in certain backends"
#~ " such as OpenGL."
#~ msgstr ""

#~ msgid "dtype_bits_hint"
#~ msgstr ""

#~ msgid ""
#~ "The type bits of the array "
#~ "elements. Only used in certain backends"
#~ " such as OpenGL."
#~ msgstr ""

#~ msgid "call"
#~ msgstr ""

#~ msgid "The call expression."
#~ msgstr ""

#~ msgid "ptr"
#~ msgstr ""

#~ msgid "The result allocated space pointer."
#~ msgstr ""

#~ msgid "desc"
#~ msgstr ""

#~ msgid "impl"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get <tvm.tir.TensorIntrin.get>`\\ \\(name\\[\\,"
#~ " allow\\_missing\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`register <tvm.tir.TensorIntrin.register>`\\ "
#~ "\\(name\\, desc\\, impl\\[\\, override\\]\\)"
#~ msgstr ""

#~ msgid "allow_missing"
#~ msgstr ""

#~ msgid ""
#~ "Whether to allow missing tensor intrin."
#~ " If False, raise an error if "
#~ "the tensor intrin"
#~ msgstr ""

#~ msgid "doesn't exist."
#~ msgstr ""

#~ msgid "Optional[TensorIntrin]"
#~ msgstr ""

#~ msgid "The TensorIntrin with the specified name, or None if not found."
#~ msgstr ""

#~ msgid "override: bool"
#~ msgstr ""

#~ msgid "Whether override existing intrinsic."
#~ msgstr ""

#~ msgid "Union[str, tvm.irType]"
#~ msgstr ""

#~ msgid "x"
#~ msgstr ""

#~ msgid "y"
#~ msgstr ""

#~ msgid "The result."
#~ msgstr ""

#~ msgid "object"
#~ msgstr ""

#~ msgid "tvm.Expr"
#~ msgstr ""

#~ msgid "The result Expr of add operaton."
#~ msgstr ""

#~ msgid "buffer_load: BufferLoad"
#~ msgstr ""

#~ msgid "The buffer load."
#~ msgstr ""

#~ msgid "list"
#~ msgstr ""

#~ msgid "expr: Expr"
#~ msgstr ""

#~ msgid "Expression"
#~ msgstr ""

#~ msgid "cond"
#~ msgstr ""

#~ msgid "The constraint condition."
#~ msgstr ""

#~ msgid "x1"
#~ msgstr ""

#~ msgid "x2"
#~ msgstr ""

#~ msgid "bijective_layout"
#~ msgstr ""

#~ msgid "BijectiveLayout"
#~ msgstr ""

#~ msgid "The created bijective layout"
#~ msgstr ""

#~ msgid "Left operand"
#~ msgstr ""

#~ msgid "Right operand"
#~ msgstr ""

#~ msgid "res"
#~ msgstr ""

#~ msgid "Input operand"
#~ msgstr ""

#~ msgid ""
#~ "Same as call_packed, except that the "
#~ "first argument is the function name "
#~ "(as in call_extern), and the last "
#~ "argument is the resource handle."
#~ msgstr ""

#~ msgid "list of Expr or Buffer."
#~ msgstr ""

#~ msgid "te.extern : Create tensor with extern function call."
#~ msgstr ""

#~ msgid ""
#~ "Lowered version of call c-packed. Same"
#~ " as call_packed, except that the "
#~ "first argument is the function name "
#~ "(as in call_extern), and the last "
#~ "argument is the resource handle."
#~ msgstr ""

#~ msgid "func_name: str"
#~ msgstr ""

#~ msgid ""
#~ "When the argument is Buffer, the "
#~ "corresponding PackedFunc will receive an "
#~ "TVMArrayHandle whose content is valid "
#~ "during the callback period. If the "
#~ "PackedFunc is a python callback, then"
#~ " the corresponding argument is NDArray."
#~ msgstr ""

#~ msgid ""
#~ "Lowered version of call packed. The "
#~ "argument to packed function can be "
#~ "Expr or Buffer. The argument is "
#~ "the corresponding POD type when Expr "
#~ "is presented. When the argument is "
#~ "Buffer, the corresponding PackedFunc will "
#~ "recieve an TVMArrayHandle whose content "
#~ "is valid during the callback period. "
#~ "If the PackedFunc is a python "
#~ "callback, then the corresponding argument "
#~ "is NDArray."
#~ msgstr ""

#~ msgid "The result Expr of ceildiv operaton."
#~ msgstr ""

#~ msgid "fcombine"
#~ msgstr ""

#~ msgid "function(Expr -> Expr -> Expr)"
#~ msgstr ""

#~ msgid "fidentity"
#~ msgstr ""

#~ msgid "function(str -> Expr)"
#~ msgstr ""

#~ msgid "reducer"
#~ msgstr ""

#~ msgid "function"
#~ msgstr ""

#~ msgid ""
#~ "A function which creates a reduce "
#~ "expression over axis. There are two "
#~ "ways to use it:"
#~ msgstr ""

#~ msgid "Example"
#~ msgstr ""

#~ msgid "barrier_count"
#~ msgstr ""

#~ msgid "The number of barriers to create."
#~ msgstr ""

#~ msgid "tuple of Expr"
#~ msgstr ""

#~ msgid "data"
#~ msgstr ""

#~ msgid "Var, optional"
#~ msgstr ""

#~ msgid "strides: array of Expr"
#~ msgstr ""

#~ msgid "elem_offset: Expr, optional"
#~ msgstr ""

#~ msgid "scope: str, optional"
#~ msgstr ""

#~ msgid "data_alignment: int, optional"
#~ msgstr ""

#~ msgid "offset_factor: int, optional"
#~ msgstr ""

#~ msgid "buffer_type: str, optional, {\"\", \"auto_broadcast\"}"
#~ msgstr ""

#~ msgid "axis_separators"
#~ msgstr ""

#~ msgid "list of int, optional"
#~ msgstr ""

#~ msgid ""
#~ "If passed, a list of separators "
#~ "between groups of axes, each of "
#~ "which is flattened to an output "
#~ "axis.  For flat memory spaces, should"
#~ " either be None, or an empty "
#~ "list."
#~ msgstr ""

#~ msgid "tvm.tir.Buffer"
#~ msgstr ""

#~ msgid "The created buffer"
#~ msgstr ""

#~ msgid "The result expression."
#~ msgstr ""

#~ msgid "End profile intrinsic. Parameters ---------- id : int"
#~ msgstr ""

#~ msgid "The intrinsic id."
#~ msgstr ""

#~ msgid "z"
#~ msgstr ""

#~ msgid "t"
#~ msgstr ""

#~ msgid "f"
#~ msgstr ""

#~ msgid "The result of conditional expression."
#~ msgstr ""

#~ msgid "The infinity value of dtype."
#~ msgstr ""

#~ msgid "layout_str"
#~ msgstr ""

#~ msgid ""
#~ "The dtype of generated axes vars "
#~ "in the returned layout. It is "
#~ "required to be integer type."
#~ msgstr ""

#~ msgid "layout"
#~ msgstr ""

#~ msgid "Layout"
#~ msgstr ""

#~ msgid "The created layout"
#~ msgstr ""

#~ msgid "The marked expression."
#~ msgstr ""

#~ msgid "param_name"
#~ msgstr ""

#~ msgid "The name of param."
#~ msgstr ""

#~ msgid "expr"
#~ msgstr ""

#~ msgid "IterVar"
#~ msgstr ""

#~ msgid "where"
#~ msgstr ""

#~ msgid "optional, Expr"
#~ msgstr ""

#~ msgid "The result value."
#~ msgstr ""

#~ msgid "The maximum value of dtype."
#~ msgstr ""

#~ msgid "The minimum value of dtype."
#~ msgstr ""

#~ msgid "local_size"
#~ msgstr ""

#~ msgid "IntImm"
#~ msgstr ""

#~ msgid "The number of elements."
#~ msgstr ""

#~ msgid "local_ptr"
#~ msgstr ""

#~ msgid "The destination pointer variable."
#~ msgstr ""

#~ msgid "The destination offset."
#~ msgstr ""

#~ msgid "m"
#~ msgstr ""

#~ msgid "The shape of mma fragment."
#~ msgstr ""

#~ msgid "dst_ptr"
#~ msgstr ""

#~ msgid "src_ptr"
#~ msgstr ""

#~ msgid "The source pointer variable."
#~ msgstr ""

#~ msgid "src_offset"
#~ msgstr ""

#~ msgid "The source offset."
#~ msgstr ""

#~ msgid "dst_stride"
#~ msgstr ""

#~ msgid "The destination stride."
#~ msgstr ""

#~ msgid "The result Expr of multiply operaton."
#~ msgstr ""

#~ msgid "barrier_id"
#~ msgstr ""

#~ msgid "The ID of the barrier shared memory pointer."
#~ msgstr ""

#~ msgid "byte_count"
#~ msgstr ""

#~ msgid ""
#~ "Increases the tx count of the "
#~ "mbarrier object to track completion of"
#~ " addtional async transactions."
#~ msgstr ""

#~ msgid "shared_ptr"
#~ msgstr ""

#~ msgid "The shared memory pointer variable."
#~ msgstr ""

#~ msgid "shared_offset"
#~ msgstr ""

#~ msgid "The offset of shared memory pointer."
#~ msgstr ""

#~ msgid "global_ptr"
#~ msgstr ""

#~ msgid "The global memory pointer variable."
#~ msgstr ""

#~ msgid "global_offset"
#~ msgstr ""

#~ msgid "The offset of global memory pointer."
#~ msgstr ""

#~ msgid "bytes"
#~ msgstr ""

#~ msgid "The data size to copy."
#~ msgstr ""

#~ msgid "thread_count"
#~ msgstr ""

#~ msgid "Number of threads expected to arrive at the barrier."
#~ msgstr ""

#~ msgid "trans"
#~ msgstr ""

#~ msgid "The matrix is loaded in column-major format."
#~ msgstr ""

#~ msgid "num"
#~ msgstr ""

#~ msgid "The number of matrices."
#~ msgstr ""

#~ msgid "type"
#~ msgstr ""

#~ msgid "Literal[\".b16\"]"
#~ msgstr ""

#~ msgid "The data type of the matrices."
#~ msgstr ""

#~ msgid "The local pointer variable."
#~ msgstr ""

#~ msgid "local_offset"
#~ msgstr ""

#~ msgid "The offset of local pointer."
#~ msgstr ""

#~ msgid "smem_ptr"
#~ msgstr ""

#~ msgid "smem_offset"
#~ msgstr ""

#~ msgid "The offset of shared memort pointer."
#~ msgstr ""

#~ msgid "A_layout"
#~ msgstr ""

#~ msgid "Literal[\"row\", \"col\"]"
#~ msgstr ""

#~ msgid "The layout of multiplicand fragment A."
#~ msgstr ""

#~ msgid "B_layout"
#~ msgstr ""

#~ msgid "The layout of multiplicand fragment B."
#~ msgstr ""

#~ msgid "A_dtype"
#~ msgstr ""

#~ msgid "The data type of multiplicand fragment A."
#~ msgstr ""

#~ msgid "B_dtype"
#~ msgstr ""

#~ msgid "The data type of multiplicand fragment B."
#~ msgstr ""

#~ msgid "C_dtype"
#~ msgstr ""

#~ msgid "The data type of accumulator fragment C."
#~ msgstr ""

#~ msgid "multiplicand_a"
#~ msgstr ""

#~ msgid "The multiplicand fragment A variable."
#~ msgstr ""

#~ msgid "a_index"
#~ msgstr ""

#~ msgid "The index of multiplicand fragment A."
#~ msgstr ""

#~ msgid "multiplicand_b"
#~ msgstr ""

#~ msgid "The multiplicand fragment B variable."
#~ msgstr ""

#~ msgid "b_index"
#~ msgstr ""

#~ msgid "accumulator"
#~ msgstr ""

#~ msgid "The accumulator fragment C variable."
#~ msgstr ""

#~ msgid "c_index"
#~ msgstr ""

#~ msgid "The index of accumulator fragment C."
#~ msgstr ""

#~ msgid "saturate"
#~ msgstr ""

#~ msgid "The optional saturation at the output."
#~ msgstr ""

#~ msgid "operator"
#~ msgstr ""

#~ msgid "Optional[Literal[\"xor\", \"and\"]]"
#~ msgstr ""

#~ msgid "The 1-bit operator."
#~ msgstr ""

#~ msgid "The data type of multiplicand fragment C."
#~ msgstr ""

#~ msgid "The index of multiplicand fragment B."
#~ msgstr ""

#~ msgid "metadata"
#~ msgstr ""

#~ msgid "The metadata of operand."
#~ msgstr ""

#~ msgid "meta_index"
#~ msgstr ""

#~ msgid "The metadata index of operand."
#~ msgstr ""

#~ msgid "sparse_selector"
#~ msgstr ""

#~ msgid "The sparse selector indicating the thread that stores the metadata."
#~ msgstr ""

#~ msgid ""
#~ "The number of the most recent "
#~ "uncommitted pending cp.async groups to "
#~ "wait."
#~ msgstr ""

#~ msgid "q"
#~ msgstr ""

#~ msgid "s"
#~ msgstr ""

#~ msgid "First Q-number."
#~ msgstr ""

#~ msgid "Second Q-number."
#~ msgstr ""

#~ msgid "ls"
#~ msgstr ""

#~ msgid "Integer left shift."
#~ msgstr ""

#~ msgid "rs"
#~ msgstr ""

#~ msgid "Integer right shift."
#~ msgstr ""

#~ msgid "Number of fractional bits in x and y. Needs to be > 0."
#~ msgstr ""

#~ msgid "is_lshift_required"
#~ msgstr ""

#~ msgid "Whether we need to do left shift or not."
#~ msgstr ""

#~ msgid "is_rshift_required"
#~ msgstr ""

#~ msgid "Whether we need to do right shift or not."
#~ msgstr ""

#~ msgid "The input value."
#~ msgstr ""

#~ msgid "The reinterpret cast value of dtype."
#~ msgstr ""

#~ msgid "val"
#~ msgstr ""

#~ msgid "ret"
#~ msgstr ""

#~ msgid "The return expression"
#~ msgstr ""

#~ msgid "Start profile intrinsic. Parameters ---------- id : int"
#~ msgstr ""

#~ msgid "stmt : A block statement"
#~ msgstr ""

#~ msgid "stmt_list"
#~ msgstr ""

#~ msgid "list of Stmt"
#~ msgstr ""

#~ msgid "The unpacked list of statements"
#~ msgstr ""

#~ msgid "list of Expr or Var"
#~ msgstr ""

#~ msgid "The combined statement."
#~ msgstr ""

#~ msgid "The result Expr of subtract operaton."
#~ msgstr ""

#~ msgid "list of Expr or Buffers."
#~ msgstr ""

#~ msgid "trace_action"
#~ msgstr ""

#~ msgid "str."
#~ msgstr ""

#~ msgid "tvm.tir.call_packed : Creates packed function."
#~ msgstr ""

#~ msgid "ptype"
#~ msgstr ""

#~ msgid "The data type of pointer."
#~ msgstr ""

#~ msgid "DType*"
#~ msgstr ""

#~ msgid "The data of pointer."
#~ msgstr ""

#~ msgid "The offset of pointer."
#~ msgstr ""

#~ msgid "rw_mask"
#~ msgstr ""

#~ msgid "The read write mask."
#~ msgstr ""

#~ msgid "fragment_d"
#~ msgstr ""

#~ msgid "The bwmma fragment_d."
#~ msgstr ""

#~ msgid "index_d"
#~ msgstr ""

#~ msgid "The fragment_d index."
#~ msgstr ""

#~ msgid "fragment_a"
#~ msgstr ""

#~ msgid "The bwmma fragment_a."
#~ msgstr ""

#~ msgid "index_a"
#~ msgstr ""

#~ msgid "The fragment_a index."
#~ msgstr ""

#~ msgid "fragment_b"
#~ msgstr ""

#~ msgid "The bwmma fragment_b."
#~ msgstr ""

#~ msgid "index_b"
#~ msgstr ""

#~ msgid "The fragment_b index."
#~ msgstr ""

#~ msgid "fragment_c"
#~ msgstr ""

#~ msgid "The bwmma fragment_c."
#~ msgstr ""

#~ msgid "index_c"
#~ msgstr ""

#~ msgid "The fragment_c index."
#~ msgstr ""

#~ msgid "Return new on stack dtype[num] Parameters ---------- expected : int"
#~ msgstr ""

#~ msgid "The expected return code."
#~ msgstr ""

#~ msgid "return_unexpected"
#~ msgstr ""

#~ msgid "The unexpected return code."
#~ msgstr ""

#~ msgid "nested_call"
#~ msgstr ""

#~ msgid "The call expression to check return."
#~ msgstr ""

#~ msgid "fragment"
#~ msgstr ""

#~ msgid "The wmma fragment"
#~ msgstr ""

#~ msgid "UIntImm"
#~ msgstr ""

#~ msgid "The shape of wmma fragment."
#~ msgstr ""

#~ msgid "k"
#~ msgstr ""

#~ msgid "The fragment index."
#~ msgstr ""

#~ msgid "The value to be filled in fragment."
#~ msgstr ""

#~ msgid "The wmma fragment."
#~ msgstr ""

#~ msgid "buffer_ptr"
#~ msgstr ""

#~ msgid "The fragment buffer pointer."
#~ msgstr ""

#~ msgid "The fragment stride."
#~ msgstr ""

#~ msgid "Literal[\"row_major\", \"column_major\"]"
#~ msgstr ""

#~ msgid "The fragment layout."
#~ msgstr ""

#~ msgid "The wmma fragment_d."
#~ msgstr ""

#~ msgid "The wmma fragment_a."
#~ msgstr ""

#~ msgid "The wmma fragment_b."
#~ msgstr ""

#~ msgid "The wmma fragment_c."
#~ msgstr ""

#~ msgid "dtype_str"
#~ msgstr ""

#~ msgid "The data type of array."
#~ msgstr ""

#~ msgid "The size of array."
#~ msgstr ""

#~ msgid "The data of array."
#~ msgstr ""

#~ msgid "The shape of array."
#~ msgstr ""

#~ msgid "strides"
#~ msgstr ""

#~ msgid "The strides of array."
#~ msgstr ""

#~ msgid "ndim"
#~ msgstr ""

#~ msgid "The dimensions of array."
#~ msgstr ""

#~ msgid "arr_dtype"
#~ msgstr ""

#~ msgid "elem_offse"
#~ msgstr ""

#~ msgid "The element offset of array."
#~ msgstr ""

#~ msgid "The tuple shape."
#~ msgstr ""

#~ msgid "The date type of the result."
#~ msgstr ""

#~ msgid "arr"
#~ msgstr ""

#~ msgid "StructType*"
#~ msgstr ""

#~ msgid "The array of struct."
#~ msgstr ""

#~ msgid "The index of struct."
#~ msgstr ""

#~ msgid "field"
#~ msgstr ""

#~ msgid "The field of struct."
#~ msgstr ""

#~ msgid "The value to be set in field."
#~ msgstr ""

#~ msgid "freduce_args"
#~ msgstr ""

#~ msgid "The args."
#~ msgstr ""

#~ msgid "The value in tuple."
#~ msgstr ""

#~ msgid "vec1"
#~ msgstr ""

#~ msgid "The input vector."
#~ msgstr ""

#~ msgid "vec2"
#~ msgstr ""

#~ msgid "vec"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AnnotateDeviceRegions "
#~ "<tvm.tir.transform.AnnotateDeviceRegions>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Annotate locations that should be run on the device"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AnnotateEntryFunc "
#~ "<tvm.tir.transform.AnnotateEntryFunc>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Set a PrimFunc as the entry point if it is only function in IRModule."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ApplyLayoutTransforms "
#~ "<tvm.tir.transform.ApplyLayoutTransforms>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Reshape buffers that appear in the "
#~ "\"layout_transform_map\" fucntion attribute."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BF16ComputeLegalize "
#~ "<tvm.tir.transform.BF16ComputeLegalize>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Legalize bf16 compute Ops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BF16StorageLegalize "
#~ "<tvm.tir.transform.BF16StorageLegalize>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Legalize bf16 storage types to u16."
#~ msgstr ""

#~ msgid ":py:obj:`BindTarget <tvm.tir.transform.BindTarget>`\\ \\(target\\)"
#~ msgstr ""

#~ msgid ""
#~ "Annotate a PrimFunc with a given "
#~ "target. Parameters ------- target : "
#~ "tvm.target.Target     target."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CommonSubexprElimTIR "
#~ "<tvm.tir.transform.CommonSubexprElimTIR>`\\ "
#~ "\\(\\[enable\\_cse\\_tir\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CompactBufferAllocation "
#~ "<tvm.tir.transform.CompactBufferAllocation>`\\ "
#~ "\\(\\[is\\_strict\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`ConvertSSA <tvm.tir.transform.ConvertSSA>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Convert an IRModule to be SSA form."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DefaultGPUSchedule "
#~ "<tvm.tir.transform.DefaultGPUSchedule>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "The pass sets default thread bindings"
#~ " for PrimFuncs, including symbolic shape"
#~ " functions, allowing their build and "
#~ "execution on GPU devices."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ExtractPrimFuncConstants "
#~ "<tvm.tir.transform.ExtractPrimFuncConstants>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Collects and unificates tir non-scalar"
#~ " constants to module's attr 'Constants' "
#~ "array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FP8ComputeLegalize "
#~ "<tvm.tir.transform.FP8ComputeLegalize>`\\ "
#~ "\\(\\[promote\\_dtype\\_str\\]\\)"
#~ msgstr ""

#~ msgid "Legalize fp8 compute Ops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FP8StorageLegalize "
#~ "<tvm.tir.transform.FP8StorageLegalize>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Legalize fp8 storage types to u8."
#~ msgstr ""

#~ msgid "Filter out PrimFuncs that does not satisfy the given condition."
#~ msgstr ""

#~ msgid ""
#~ "Flatten the multi-dimensional BufferLoad "
#~ "and BufferStore to single dimensional "
#~ "BufferLoad/BufferStore for the TIR not "
#~ "contains opaque block."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ForceNarrowIndexToInt32 "
#~ "<tvm.tir.transform.ForceNarrowIndexToInt32>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Force narrow down indexing expressions "
#~ "and integer buffers to int32 dtype."
#~ msgstr ""

#~ msgid ":py:obj:`HoistExpression <tvm.tir.transform.HoistExpression>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Generalized verison of HoistIfThenElse."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectPTXAsyncCopy "
#~ "<tvm.tir.transform.InjectPTXAsyncCopy>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Rewrite global to shared memory copy on CUDA with asyncronous copy."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectPermutedLayout "
#~ "<tvm.tir.transform.InjectPermutedLayout>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Inject permuted layout in mma"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectSoftwarePipeline "
#~ "<tvm.tir.transform.InjectSoftwarePipeline>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Transform annotated loops into pipelined "
#~ "one that parallelize producers and "
#~ "consumers"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InstallDebugSpans "
#~ "<tvm.tir.transform.InstallDebugSpans>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Add line information from the TIR "
#~ "printer as spans on each statement "
#~ "and expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InstrumentProfileIntrinsics "
#~ "<tvm.tir.transform.InstrumentProfileIntrinsics>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Insert intrinsic calls to instrument function and loop level profiling."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LiftThreadBinding "
#~ "<tvm.tir.transform.LiftThreadBinding>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lift the same thread bindings to their LCA loops."
#~ msgstr ""

#~ msgid ":py:obj:`LowerAutoCopy <tvm.tir.transform.LowerAutoCopy>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Automatically do memory optimizations for auto copy blocks"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerDeviceKernelLaunch "
#~ "<tvm.tir.transform.LowerDeviceKernelLaunch>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower cross-device function calls."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerOpaqueBlock "
#~ "<tvm.tir.transform.LowerOpaqueBlock>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Remove the block to ensure that the TIR can not be scheduled again."
#~ msgstr ""

#~ msgid ":py:obj:`MakePackedAPI <tvm.tir.transform.MakePackedAPI>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ManifestSharedMemoryLocalStage "
#~ "<tvm.tir.transform.ManifestSharedMemoryLocalStage>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Add the explicit local stage for the shared memory access on GPU."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PointerValueTypeRewrite "
#~ "<tvm.tir.transform.PointerValueTypeRewrite>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Rewrite the pointer content type of "
#~ "arguments, as well as Alloc internal "
#~ "to the function to use the most"
#~ " frequently accessed type for load/store"
#~ " to avoid pointer casting in backend"
#~ " when possible."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ReduceBranchingThroughOvercompute "
#~ "<tvm.tir.transform.ReduceBranchingThroughOvercompute>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Reduce branching by introducing overcompute"
#~ msgstr ""

#~ msgid ":py:obj:`RemoveAssume <tvm.tir.transform.RemoveAssume>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Remove all instances of builtin::assume"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RemoveStoreUndef "
#~ "<tvm.tir.transform.RemoveStoreUndef>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Remove stores of undefined values from the Stmt."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RemoveWeightLayoutRewriteBlock "
#~ "<tvm.tir.transform.RemoveWeightLayoutRewriteBlock>`\\ "
#~ "\\(\\[...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Remove weight layout rewrite block "
#~ "before benchmarking during tuning stage."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RenormalizeSplitPattern "
#~ "<tvm.tir.transform.RenormalizeSplitPattern>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Renormalize the split pattern from "
#~ "floordiv(floormod()) to floormod(floordiv())"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`TransformMmaBufferLayout "
#~ "<tvm.tir.transform.TransformMmaBufferLayout>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Transform mma buffer layout"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`VerifyVTCMLimit <tvm.tir.transform.VerifyVTCMLimit>`\\"
#~ " \\(limit\\)"
#~ msgstr ""

#~ msgid "Verify if the size of the allocated vtcm memory satisfies the limit."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`HoistedConditionals "
#~ "<tvm.tir.transform.HoistedConditionals>`\\ \\(value\\)"
#~ msgstr ""

#~ msgid "Flags for use in HoistExpressionConfig.conditional_types"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`HoistedLetBindings "
#~ "<tvm.tir.transform.HoistedLetBindings>`\\ \\(value\\)"
#~ msgstr ""

#~ msgid "Flags for use in HoistExpressionConfig.let_binding_types"
#~ msgstr ""

#~ msgid ""
#~ "Insert `AttrStmt` nodes specifying a "
#~ "target on which regions within the "
#~ "PrimFunc should be executed.  Only "
#~ "modifies functions that have a "
#~ "`tvm::attr::kTarget` attribute, and where that"
#~ " target defines a host."
#~ msgstr ""

#~ msgid "fpass"
#~ msgstr ""

#~ msgid "tvm.transform.Pass"
#~ msgstr ""

#~ msgid "The result pass"
#~ msgstr ""

#~ msgid "ftransform: tvm.tir.PrimFunc -> tvm.tir.PrimFunc"
#~ msgstr ""

#~ msgid ""
#~ "Annotate a PrimFunc with a given "
#~ "target. Parameters ------- target : "
#~ "tvm.target.Target"
#~ msgstr ""

#~ msgid "is_strict"
#~ msgstr ""

#~ msgid ""
#~ "Ensure the compacted shape to be "
#~ "always smaller than the original shape."
#~ " Otherwise it allows to grow the "
#~ "shape to match actual accessed buffer"
#~ " regions."
#~ msgstr ""

#~ msgid ""
#~ "This pass handles cases where the "
#~ "same `tir.Var` appears in multiple "
#~ "functions within the same module.  For"
#~ " example, after extracting a fragment "
#~ "from one function into another, where"
#~ " the same `tir.Var` may be defined"
#~ " both as within the body of the"
#~ " original function, and as a "
#~ "parameter within the hoisted function."
#~ msgstr ""

#~ msgid ""
#~ "The pass sets default thread bindings"
#~ " for PrimFuncs, including symbolic shape"
#~ " functions, allowing their build and "
#~ "execution on GPU devices. It examines"
#~ " all the blocks within the PrimFunc"
#~ " and conducts loop fusion, splitting, "
#~ "and reordering operation based on the"
#~ " loop extent and target information, "
#~ "such as the maximum thread block "
#~ "number and maximum thread per block."
#~ msgstr ""

#~ msgid ""
#~ "The primary objective of this pass "
#~ "is not to optimize performance, but "
#~ "rather to generate a valid GPU "
#~ "kernel for unscheduled or symbolic shape"
#~ " PrimFuncs. The pass is currently "
#~ "only working for CUDA targets."
#~ msgstr ""

#~ msgid "ret: tvm.transform.Pass"
#~ msgstr ""

#~ msgid "promote_dtype"
#~ msgstr ""

#~ msgid "The data type we promote fp8 to, options: float16/float32."
#~ msgstr ""

#~ msgid ""
#~ "Filter out PrimFuncs that does not "
#~ "satisfy the given condition. `fcond` "
#~ "should be a function that takes a"
#~ " primfunc and returns boolean."
#~ msgstr ""

#~ msgid "This pass should not be used in default cases."
#~ msgstr ""

#~ msgid ""
#~ "Hoist loop-invariant expressions to "
#~ "outside the eligible loops. Searches for"
#~ " expressions in:"
#~ msgstr ""

#~ msgid "LetStmt bindings"
#~ msgstr ""

#~ msgid "IfThenElse conditions"
#~ msgstr ""

#~ msgid "Boolean operators"
#~ msgstr ""

#~ msgid "variant"
#~ msgstr ""

#~ msgid "Optional[String]"
#~ msgstr ""

#~ msgid ""
#~ "Each bitflag represents a type of "
#~ "expression that should be hoisted to "
#~ "the outermost loop possible."
#~ msgstr ""

#~ msgid ":py:obj:`All <tvm.tir.transform.HoistedConditionals.All>`\\"
#~ msgstr ""

#~ msgid "Enable all hoisting of conditionals"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BooleanExpression "
#~ "<tvm.tir.transform.HoistedConditionals.BooleanExpression>`\\"
#~ msgstr ""

#~ msgid "If set, look for hoist candidates in all boolean expressions"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`IfElseExpr "
#~ "<tvm.tir.transform.HoistedConditionals.IfElseExpr>`\\"
#~ msgstr ""

#~ msgid "If set, look for hoist candidates in tir.if_then_else"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`IfElseStmt "
#~ "<tvm.tir.transform.HoistedConditionals.IfElseStmt>`\\"
#~ msgstr ""

#~ msgid "If set, look for hoist candidates in IfElseStmt"
#~ msgstr ""

#~ msgid ":py:obj:`Never <tvm.tir.transform.HoistedConditionals.Never>`\\"
#~ msgstr ""

#~ msgid "No hoisting of conditionals"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`UsingBlockVar "
#~ "<tvm.tir.transform.HoistedConditionals.UsingBlockVar>`\\"
#~ msgstr ""

#~ msgid "If set, allow hoisting of conditionals that use a block variable (e.g."
#~ msgstr ""

#~ msgid ""
#~ "If set, allow hoisting of conditionals"
#~ " that use a block variable (e.g. "
#~ "threadIdx.x)"
#~ msgstr ""

#~ msgid ""
#~ "Each bitflag represents a type of "
#~ "let binding expression that should be"
#~ " hoisted to the outermost loop "
#~ "possible."
#~ msgstr ""

#~ msgid ":py:obj:`All <tvm.tir.transform.HoistedLetBindings.All>`\\"
#~ msgstr ""

#~ msgid "Enable all hoisting of let bindings"
#~ msgstr ""

#~ msgid ":py:obj:`LetExpr <tvm.tir.transform.HoistedLetBindings.LetExpr>`\\"
#~ msgstr ""

#~ msgid "Bindings occuring in Let expressions"
#~ msgstr ""

#~ msgid ":py:obj:`LetStmt <tvm.tir.transform.HoistedLetBindings.LetStmt>`\\"
#~ msgstr ""

#~ msgid "Bindings occuring in LetStmt"
#~ msgstr ""

#~ msgid ":py:obj:`Never <tvm.tir.transform.HoistedLetBindings.Never>`\\"
#~ msgstr ""

#~ msgid "No hoisting of let bindings"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RequiredByConditional "
#~ "<tvm.tir.transform.HoistedLetBindings.RequiredByConditional>`\\"
#~ msgstr ""

#~ msgid "Bindings that are used by a hoisted conditional"
#~ msgstr ""

#~ msgid "pragma_key"
#~ msgstr ""

#~ msgid "fintrin"
#~ msgstr ""

#~ msgid ""
#~ "Prior to this pass, host to device"
#~ " calls are represented as subroutine "
#~ "calls, with environment parameters (e.g. "
#~ "env_thread) specified internally.  The device"
#~ " function is an internal function, "
#~ "without a `tvm::attr::kGlobalSymbol` attribute."
#~ msgstr ""

#~ msgid ""
#~ "After this pass, host to device "
#~ "calls are represented as tvm_call_packed "
#~ "built-in.  The device function is "
#~ "an externally-exposed function, with a"
#~ " non-empty `tvm::attr::kGlobalSymbol` attribute."
#~ msgstr ""

#~ msgid ""
#~ "Prior to this pass, the PrimFunc "
#~ "may have Buffer arguments defined in "
#~ "the `PrimFuncNode::buffer_map`.  This pass "
#~ "consumes the `buffer_map`, using it to"
#~ " generate `TVMArgs` and `TVMRetValue*` "
#~ "arguments that implement the `PackedFunc` "
#~ "API."
#~ msgstr ""

#~ msgid ""
#~ "For static shapes, the `BufferNode::shape`,"
#~ " `BufferNode::strides`, and `BufferNode::elem_offset`"
#~ " member variables are used to "
#~ "generate runtime checks on the "
#~ "corresponding member variables in the "
#~ "user-provided `DLTensor*` or `tvm.nd.array` "
#~ "argument.  (e.g. A PrimFunc that accepts"
#~ " a buffer of shape `[16,32]` "
#~ "validates that the `DLTensor::shape` array "
#~ "is `[16,32]`.)"
#~ msgstr ""

#~ msgid ""
#~ "For dynamic Buffers, in which one "
#~ "or more of these `BufferNode` member "
#~ "variables use `tir.Var` that are not "
#~ "defined by other PrimFunc parameters, "
#~ "these are instead used to define "
#~ "the variables based on the corresponding"
#~ " `DLTensor` members.  (e.g. A PrimFunc "
#~ "that accepts a buffer of shape "
#~ "`[tir.Var(\"n\"), tir.Var(\"m\")]`, when passed "
#~ "a `DLTensor` of shape `[16,32]`, will"
#~ " define `n = 16` and `n=32`, "
#~ "based on the argument's shape."
#~ msgstr ""

#~ msgid ""
#~ "Prior to this pass, the PrimFunc "
#~ "may have Buffer arguments defined in "
#~ "the `PrimFuncNode::buffer_map`.  This pass "
#~ "consumes the `buffer_map`, using it to"
#~ " generate `T*` arguments (e.g. `float32*`)"
#~ " that can be directly called by "
#~ "a C API."
#~ msgstr ""

#~ msgid ""
#~ "For static shapes, no runtime validation"
#~ " is performed to confirm that the "
#~ "argument buffer's shape matches the "
#~ "expected shape.  For dynamic shapes, "
#~ "`MakeUnpackedAPI` requires that the dynamic"
#~ " parameters be passed as separate "
#~ "`tir.Var` parameters."
#~ msgstr ""

#~ msgid "target_bits"
#~ msgstr ""

#~ msgid "skip_ndarray_rewrite"
#~ msgstr ""

#~ msgid ""
#~ "If True, exact rewrite of NDArray, "
#~ "according to the given index map, "
#~ "will be skipped. Only the shape of"
#~ " the NDArray is transformed correctly, "
#~ "and the content of the destination "
#~ "array will be filled with random "
#~ "values."
#~ msgstr ""

#~ msgid ""
#~ "When this pass is called many "
#~ "times during MetaSchedule tuning, the "
#~ "raw data of NDArray, before and "
#~ "after rewrite, does not matter. Since"
#~ " NDArray layout rewrite, using IndexMap's"
#~ " MapNDArray, is currently slow, skipping"
#~ " the exact rewrite is sometimes "
#~ "necessary."
#~ msgstr ""

#~ msgid "cache_line_size: int"
#~ msgstr ""

#~ msgid "create_bound_attribute:"
#~ msgstr ""

#~ msgid "enable_vectorize"
#~ msgstr ""

#~ msgid "pass_func"
#~ msgstr ""

#~ msgid ""
#~ "Optional[Callable[(tvm.tir.PrimFunc, IRModule, "
#~ "PassContext) -> tvm.tir.PrimFunc]]"
#~ msgstr ""

#~ msgid "opt_level"
#~ msgstr ""

#~ msgid "Optional[str]"
#~ msgstr ""

#~ msgid "required"
#~ msgstr ""

#~ msgid "Optional[List[str]]"
#~ msgstr ""

#~ msgid "create_function_pass : Union[Callable, FunctionPass]"
#~ msgstr ""

#~ msgid ""
#~ "A decorator will be returned if "
#~ "pass_func is not provided, otherwise "
#~ "return the decorated result. The "
#~ "returned decorator has two behaviors "
#~ "depending on the input: A new "
#~ "FunctionPass will be returned when we"
#~ " decorate a pass function. A new "
#~ "FunctionPass class will be returned when"
#~ " we decorate a class type."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`IRModule <tvm.tir.analysis.IRModule>`\\ "
#~ "\\(\\[functions\\, type\\_definitions\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "IRModule that holds functions and type definitions."
#~ msgstr ""

#~ msgid ":py:obj:`OOBChecker <tvm.tir.analysis.OOBChecker>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Detect out of bounds memory access in arrays."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`calculate_allocated_bytes "
#~ "<tvm.tir.analysis.calculate_allocated_bytes>`\\ "
#~ "\\(func\\_or\\_mod\\)"
#~ msgstr ""

#~ msgid "Calculate allocated memory per memory scope required by TIR PrimFuncs."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`calculate_constant_bytes "
#~ "<tvm.tir.analysis.calculate_constant_bytes>`\\ \\(func\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Calculate the constant size in bytes "
#~ "needed by the TIR allocates inside "
#~ "the TIR PrimFunc."
#~ msgstr ""

#~ msgid ""
#~ "Detect the lowest common ancestor(LCA) "
#~ "of buffer access, including both "
#~ "high-level access (BufferLoad, BufferStore) "
#~ "and low-level access (BufferLoad, "
#~ "BufferStore and opaque access)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`estimate_tir_flops "
#~ "<tvm.tir.analysis.estimate_tir_flops>`\\ \\(stmt\\_or\\_mod\\)"
#~ msgstr ""

#~ msgid "Estimate the FLOPs of a TIR fragment."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`find_anchor_block "
#~ "<tvm.tir.analysis.find_anchor_block>`\\ \\(mod\\)"
#~ msgstr ""

#~ msgid "Find the \"anchor block\" of the given module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_vtcm_compaction_passes "
#~ "<tvm.tir.analysis.get_vtcm_compaction_passes>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Utility function to get the list "
#~ "of lowering passes to be applied "
#~ "to calculate the compacted VTCM "
#~ "allocation size"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`undefined_vars <tvm.tir.analysis.undefined_vars>`\\ "
#~ "\\(node\\[\\, defs\\]\\)"
#~ msgstr ""

#~ msgid "Find undefined vars in a TIR statement or expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`verify_well_formed "
#~ "<tvm.tir.analysis.verify_well_formed>`\\ \\(obj\\[\\, "
#~ "assert\\_mode\\]\\)"
#~ msgstr ""

#~ msgid "Verify if the given TIR is well-formed. The verification includes:"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_flattened_buffer "
#~ "<tvm.tir.analysis.Buffer.get_flattened_buffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`offset_of <tvm.tir.analysis.Buffer.offset_of>`\\ \\(indices\\)"
#~ msgstr ""

#~ msgid "IRModule is the basic unit for all IR transformations across the stack."
#~ msgstr ""

#~ msgid "functions: Optional[dict]."
#~ msgstr ""

#~ msgid "Map of global var to BaseFunc"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`__getitem__ <tvm.tir.analysis.IRModule.__getitem__>`\\"
#~ " \\(var\\)"
#~ msgstr ""

#~ msgid "Lookup a global definition by name or by variable."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`__setitem__ <tvm.tir.analysis.IRModule.__setitem__>`\\"
#~ " \\(var\\, val\\)"
#~ msgstr ""

#~ msgid "Add a mapping to the module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`astext <tvm.tir.analysis.IRModule.astext>`\\ "
#~ "\\(\\[show\\_meta\\_data\\, annotate\\]\\)"
#~ msgstr ""

#~ msgid "Get the text format of the expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`from_expr <tvm.tir.analysis.IRModule.from_expr>`\\ "
#~ "\\(expr\\[\\, functions\\, type\\_defs\\]\\)"
#~ msgstr ""

#~ msgid "Construct a module from a standalone expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_attr <tvm.tir.analysis.IRModule.get_attr>`\\ "
#~ "\\(attr\\_key\\)"
#~ msgstr ""

#~ msgid "Get the IRModule attribute."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_constructor "
#~ "<tvm.tir.analysis.IRModule.get_constructor>`\\ \\(tag\\)"
#~ msgstr ""

#~ msgid "Look up an ADT constructor by tag."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_global_type_var "
#~ "<tvm.tir.analysis.IRModule.get_global_type_var>`\\ \\(name\\)"
#~ msgstr ""

#~ msgid "Get a global type variable in the function by name."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_global_type_vars "
#~ "<tvm.tir.analysis.IRModule.get_global_type_vars>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Collect all global type vars defined in this module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_global_var "
#~ "<tvm.tir.analysis.IRModule.get_global_var>`\\ \\(name\\)"
#~ msgstr ""

#~ msgid "Get a global variable in the function by name."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_global_vars "
#~ "<tvm.tir.analysis.IRModule.get_global_vars>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Collect all global vars defined in this module."
#~ msgstr ""

#~ msgid ":py:obj:`update <tvm.tir.analysis.IRModule.update>`\\ \\(other\\)"
#~ msgstr ""

#~ msgid "Insert functions in another Module to current one."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`update_func <tvm.tir.analysis.IRModule.update_func>`\\"
#~ " \\(var\\, func\\)"
#~ msgstr ""

#~ msgid "Update the function corresponding to a global variable in the module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`update_global_info "
#~ "<tvm.tir.analysis.IRModule.update_global_info>`\\ \\(name\\, "
#~ "global\\_info\\)"
#~ msgstr ""

#~ msgid "Update global info in the module"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`with_attr <tvm.tir.analysis.IRModule.with_attr>`\\ "
#~ "\\(attr\\_key\\, attr\\_value\\)"
#~ msgstr ""

#~ msgid "Copy the IRModule and add an attribute to it."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`with_attrs <tvm.tir.analysis.IRModule.with_attrs>`\\"
#~ " \\(attr\\_map\\)"
#~ msgstr ""

#~ msgid ""
#~ "Copy the IRModule and add the "
#~ "given attribute map to it. Parameters"
#~ " ---------- attr_map: Union[DictAttrs, Dict[str,"
#~ " Object]]     The attribute map Returns "
#~ "------- mod : IRModule     A new "
#~ "copy of the IRModule with the "
#~ "attribute."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`without_attr "
#~ "<tvm.tir.analysis.IRModule.without_attr>`\\ \\(attr\\_key\\)"
#~ msgstr ""

#~ msgid ""
#~ "Copy the IRModule and remove an "
#~ "attribute key and its associated value."
#~ " Parameters ---------- attr_key : str"
#~ "     The attribute key. Returns ------- "
#~ "mod : IRModule     A new copy of"
#~ " the IRModule without the attribute."
#~ msgstr ""

#~ msgid "var: Union[String, GlobalVar, GlobalTypeVar]"
#~ msgstr ""

#~ msgid "The name or global variable."
#~ msgstr ""

#~ msgid "val: Union[Function, Type]"
#~ msgstr ""

#~ msgid "The definition referenced by :code:`var` (either a function or type)."
#~ msgstr ""

#~ msgid "var: GlobalVar"
#~ msgstr ""

#~ msgid "The global variable."
#~ msgstr ""

#~ msgid "The value."
#~ msgstr ""

#~ msgid "show_meta_data"
#~ msgstr ""

#~ msgid "Whether to include meta data section in the text if there is meta data."
#~ msgstr ""

#~ msgid "annotate: Optional[Object->str]"
#~ msgstr ""

#~ msgid ""
#~ "Optionally annotate function to provide "
#~ "additional information in the comment "
#~ "block."
#~ msgstr ""

#~ msgid "text"
#~ msgstr ""

#~ msgid "The text format of the expression."
#~ msgstr ""

#~ msgid "Notes"
#~ msgstr ""

#~ msgid ""
#~ "The meta data section is necessary "
#~ "to fully parse the text format. "
#~ "However, it can contain dumps that "
#~ "are big (e.g constant weights), so "
#~ "it can be helpful to skip printing"
#~ " the meta data section."
#~ msgstr ""

#~ msgid "expr: RelayExpr"
#~ msgstr ""

#~ msgid "The starting expression"
#~ msgstr ""

#~ msgid "global_funcs: Optional[dict]"
#~ msgstr ""

#~ msgid "Map of global vars to function definitions"
#~ msgstr ""

#~ msgid "type_defs: Optional[dict]"
#~ msgstr ""

#~ msgid "Map of global type vars to type definitions"
#~ msgstr ""

#~ msgid "mod: Module"
#~ msgstr ""

#~ msgid ""
#~ "A module containing the passed "
#~ "definitions, where expr is set as "
#~ "the entry point (wrapped in a "
#~ "function if necessary)"
#~ msgstr ""

#~ msgid "The attribute key."
#~ msgstr ""

#~ msgid "attr_value"
#~ msgstr ""

#~ msgid "Any"
#~ msgstr ""

#~ msgid "Attribute value"
#~ msgstr ""

#~ msgid "tag: int"
#~ msgstr ""

#~ msgid "The tag for a constructor."
#~ msgstr ""

#~ msgid "constructor: Constructor"
#~ msgstr ""

#~ msgid "The constructor associated with the given tag,"
#~ msgstr ""

#~ msgid "Raises"
#~ msgstr ""

#~ msgid "tvm.error.TVMError if the corresponding constructor cannot be found."
#~ msgstr ""

#~ msgid "name: str"
#~ msgstr ""

#~ msgid "The name of the global type variable."
#~ msgstr ""

#~ msgid "global_type_var: GlobalTypeVar"
#~ msgstr ""

#~ msgid "The global variable mapped to :code:`name`."
#~ msgstr ""

#~ msgid "tvm.error.TVMError if we cannot find corresponding global type var."
#~ msgstr ""

#~ msgid "global_type_vars: Array[GlobalTypeVar]"
#~ msgstr ""

#~ msgid "An array of global type vars."
#~ msgstr ""

#~ msgid "The name of the global variable."
#~ msgstr ""

#~ msgid "global_var: GlobalVar"
#~ msgstr ""

#~ msgid "tvm.error.TVMError if we cannot find corresponding global var."
#~ msgstr ""

#~ msgid "global_vars: Array[GlobalVar]"
#~ msgstr ""

#~ msgid "An array of global vars."
#~ msgstr ""

#~ msgid "other: IRModule"
#~ msgstr ""

#~ msgid "The module to merge into the current Module."
#~ msgstr ""

#~ msgid "func: tvm.relay.Function"
#~ msgstr ""

#~ msgid "The function to be inserted."
#~ msgstr ""

#~ msgid "The name for the global info."
#~ msgstr ""

#~ msgid "global_info: List[GlobalInfo]"
#~ msgstr ""

#~ msgid "The global info to be updated."
#~ msgstr ""

#~ msgid "Object"
#~ msgstr ""

#~ msgid "The new attribute value."
#~ msgstr ""

#~ msgid "A new copy of the IRModule with the attribute"
#~ msgstr ""

#~ msgid ""
#~ "Copy the IRModule and add the "
#~ "given attribute map to it. Parameters"
#~ " ---------- attr_map: Union[DictAttrs, Dict[str,"
#~ " Object]]"
#~ msgstr ""

#~ msgid "The attribute map"
#~ msgstr ""

#~ msgid ""
#~ "Copy the IRModule and remove an "
#~ "attribute key and its associated value."
#~ " Parameters ---------- attr_key : str"
#~ msgstr ""

#~ msgid "A new copy of the IRModule without the attribute"
#~ msgstr ""

#~ msgid ":py:obj:`_move <tvm.tir.analysis.Object._move>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Create an RValue reference to the object and mark the object as moved."
#~ msgstr ""

#~ msgid ""
#~ "This is a advanced developer API "
#~ "that can be useful when passing an"
#~ " unique reference to an Object that"
#~ " you no longer needed to a "
#~ "function."
#~ msgstr ""

#~ msgid ""
#~ "A unique reference can trigger copy "
#~ "on write optimization that avoids copy"
#~ " when we transform an object."
#~ msgstr ""

#~ msgid ""
#~ "All the reference of the object "
#~ "becomes invalid after it is moved. "
#~ "Be very careful when using this "
#~ "feature."
#~ msgstr ""

#~ msgid "rvalue : The rvalue reference."
#~ msgstr ""

#~ msgid "func: tvm.tir.PrimFunc"
#~ msgstr ""

#~ msgid "relay_func_type: tvm.relay.FuncType"
#~ msgstr ""

#~ msgid "arg_and_result_memory_scopes: Array[AnyStr]"
#~ msgstr ""

#~ msgid "result: tvm.tir.PrimFunc"
#~ msgstr ""

#~ msgid "The rewritten func."
#~ msgstr ""

#~ msgid "func_or_mod: Union[PrimFunc, IRModule]"
#~ msgstr ""

#~ msgid ""
#~ "The function or module to be "
#~ "detected. If a module is passed, "
#~ "allocated memory is calculated for all"
#~ " PrimFuncs inside the module"
#~ msgstr ""

#~ msgid "Union[Dict[str, int], Dict[str, Dict[str, int]]]"
#~ msgstr ""

#~ msgid ""
#~ "Allocated memory size per scope in "
#~ "bytes for each function in the "
#~ "IRModule returned as a dict with "
#~ "function names as keys and a dict"
#~ " of allocated sizes as values. If "
#~ "a single PrimFunc is passed, the "
#~ "function name is returned as \"main\""
#~ msgstr ""

#~ msgid "constant_byte_alignment"
#~ msgstr ""

#~ msgid "Workspace size in bytes."
#~ msgstr ""

#~ msgid "workspace_byte_alignment"
#~ msgstr ""

#~ msgid ""
#~ "Detect the lowest common ancestor(LCA) "
#~ "of buffer access, including both "
#~ "high-level access (BufferLoad, BufferStore) "
#~ "and low-level access (BufferLoad, "
#~ "BufferStore and opaque access). The LCA"
#~ " may be a For loop or a "
#~ "Block."
#~ msgstr ""

#~ msgid "Dict[Buffer, Stmt]"
#~ msgstr ""

#~ msgid "Map from buffer to the LCA of all access to it."
#~ msgstr ""

#~ msgid "stmt_or_mod: Union[Stmt, IRModule]"
#~ msgstr ""

#~ msgid "The TIR fragment or IRModule to be estimated."
#~ msgstr ""

#~ msgid "flops: float"
#~ msgstr ""

#~ msgid "The estimated FLOPs."
#~ msgstr ""

#~ msgid "The comparison result"
#~ msgstr ""

#~ msgid "tvm.ir.structural_equal"
#~ msgstr ""

#~ msgid ""
#~ "We define the anchor block to be"
#~ " the block with (1) an init "
#~ "statement and (2) having the biggest "
#~ "flops count. The latter condition is "
#~ "only used when there are multiple "
#~ "blocks with an init statement."
#~ msgstr ""

#~ msgid ""
#~ "For example, if the input module "
#~ "is conv2d + fused spatial blocks, "
#~ "conv2d is the anchor block. The "
#~ "input module may not contain more "
#~ "than one such block. For example, "
#~ "a module having two conv2d is not"
#~ " allowed as an input."
#~ msgstr ""

#~ msgid ""
#~ "However, a module created from winograd"
#~ " convolution has multiple blocks with "
#~ "an init statement (input transform, "
#~ "batched GEMM, and output transform). We"
#~ " use the second condition, the flops"
#~ " count, to determine that the batched"
#~ " GEMM block is the anchor block."
#~ msgstr ""

#~ msgid "mod: tvm.ir.IRModule"
#~ msgstr ""

#~ msgid "The input TIR module."
#~ msgstr ""

#~ msgid "anchor_block: Block"
#~ msgstr ""

#~ msgid "The anchor block if found, None otherwise."
#~ msgstr ""

#~ msgid "block: tvm.tir.Block"
#~ msgstr ""

#~ msgid "buffer_var_map"
#~ msgstr ""

#~ msgid "Dict[Var, Buffer]"
#~ msgstr ""

#~ msgid "List[List[BufferRegion]]"
#~ msgstr ""

#~ msgid ""
#~ "An array only consisting of the "
#~ "read regions and write regions of "
#~ "the input block"
#~ msgstr ""

#~ msgid "result: List[AnyStr]"
#~ msgstr ""

#~ msgid ""
#~ "Memory scope constraints for funcs args"
#~ " and result in Relay form. The "
#~ "empty string denotes 'no constraint'."
#~ msgstr ""

#~ msgid "List[tvm.transform.Pass]"
#~ msgstr ""

#~ msgid "returns list of passes"
#~ msgstr ""

#~ msgid "node: Union[Stmt, PrimExpr]"
#~ msgstr ""

#~ msgid "The TIR statement or expression to be checked."
#~ msgstr ""

#~ msgid "defs: Optional[List[Var]]"
#~ msgstr ""

#~ msgid "The vars that is defined"
#~ msgstr ""

#~ msgid "The undefined vars."
#~ msgstr ""

#~ msgid "constraints"
#~ msgstr ""

#~ msgid "Dict[str, int]"
#~ msgstr ""

#~ msgid "The result of verification."
#~ msgstr ""

#~ msgid ""
#~ "Check if expressions not contain vars"
#~ " that is defined outside the block."
#~ msgstr ""

#~ msgid "obj: Union[tvm.tir.PrimFunc, tvm.ir.IRModule]"
#~ msgstr ""

#~ msgid "The function or module to be verified."
#~ msgstr ""

#~ msgid "assert_mode: bool"
#~ msgstr ""

#~ msgid ""
#~ "The indicator if it raises an "
#~ "error when the function is not "
#~ "well-formed."
#~ msgstr ""

#~ msgid "result: bool"
#~ msgstr ""

#~ msgid "Whether it is a well-formed TIR function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pre_order_visit "
#~ "<tvm.tir.stmt_functor.pre_order_visit>`\\ \\(stmt\\, "
#~ "fvisit\\)"
#~ msgstr ""

#~ msgid "Recursive pre-order visit on stmt AST, applying fvisit on each node."
#~ msgstr ""

#~ msgid ":py:obj:`renew_defs <tvm.tir.stmt_functor.renew_defs>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid ""
#~ "Re-generate the definition nodes for "
#~ "a TIR, including VarDef, BufferDef."
#~ msgstr ""

#~ msgid "preorder: function"
#~ msgstr ""

#~ msgid "postorder"
#~ msgstr ""

#~ msgid "only_enable"
#~ msgstr ""

#~ msgid "fvisit: function"
#~ msgstr ""

#~ msgid "If fvisit returns False, it won't visit the children of the node."
#~ msgstr ""

#~ msgid "fvisit: function of the signature Object -> bool"
#~ msgstr ""

#~ msgid ""
#~ "Re-generate the definition nodes for "
#~ "a TIR, including VarDef, BufferDef. This"
#~ " pass works as a simple DeepCopy "
#~ "to duplicate a function with different"
#~ " Vars and Buffers but the same "
#~ "behavior"
#~ msgstr ""

#~ msgid "func: PrimFunc"
#~ msgstr ""

#~ msgid "The input function"
#~ msgstr ""

#~ msgid "The new generated func."
#~ msgstr ""

#~ msgid "node: ObjectRef"
#~ msgstr ""

#~ msgid "vmap"
#~ msgstr ""

#~ msgid "Dict[Var, PrimExpr]"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferLoad <tvm.tir.BufferLoad>`\\ \\(buffer\\,"
#~ " indices\\[\\, predicate\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferStore <tvm.tir.BufferStore>`\\ \\(buffer\\,"
#~ " value\\, indices\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`For <tvm.tir.For>`\\ \\(loop\\_var\\, "
#~ "min\\, extent\\, kind\\, body\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ForKind <tvm.tir.ForKind>`\\ \\(value\\[\\, "
#~ "names\\, module\\, qualname\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`dp4a <tvm.tir.dp4a>`\\ \\(vec1\\, vec2\\[\\, acc\\]\\)"
#~ msgstr ""

#~ msgid "Dot product of two int8x4 vectors and add an optional accumulator"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_active_lane_mask <tvm.tir.get_active_lane_mask>`\\"
#~ " \\(dtype\\, base\\, limit\\)"
#~ msgstr ""

#~ msgid ""
#~ "Calculate a predicate mask given an "
#~ "upper bound (limit) and a current "
#~ "value (base)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_vscale_expr <tvm.tir.get_vscale_expr>`\\ "
#~ "\\(dtype\\[\\, min\\_size\\]\\)"
#~ msgstr ""

#~ msgid "Create a datatype dependent scalable expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`make_filled_simdgroup_matrix "
#~ "<tvm.tir.make_filled_simdgroup_matrix>`\\ \\(d\\, index\\,"
#~ " value\\)"
#~ msgstr ""

#~ msgid "Create a filled SIMDGroup matrix"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reinterpret <tvm.tir.reinterpret>`\\ \\(dtype\\,"
#~ " value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`simdgroup_load <tvm.tir.simdgroup_load>`\\ "
#~ "\\(d\\, index\\, ptr\\, stride\\[\\, col\\,"
#~ " ...\\]\\)"
#~ msgstr ""

#~ msgid "Load data from device memory or threadgroup memory to simdgroup"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`simdgroup_multiply_accumulate "
#~ "<tvm.tir.simdgroup_multiply_accumulate>`\\ \\(d\\, "
#~ "index\\_d\\, a\\, ...\\)"
#~ msgstr ""

#~ msgid "Multiply and accumulate two matrices in simdgroup i.e. d = a * b + c."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`simdgroup_store <tvm.tir.simdgroup_store>`\\ "
#~ "\\(d\\, index\\, ptr\\, stride\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Store data from simdgroup to device memory or threadgroup memory"
#~ msgstr ""

#~ msgid ":py:obj:`vscale <tvm.tir.vscale>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Get the target's vscale value. It "
#~ "will be lowered to llvm.vscale intrinsic"
#~ " (https://llvm.org/docs/LangRef.html#llvm-vscale-"
#~ "intrinsic) Returns ------- call : "
#~ "PrimExpr     Call to the vscale "
#~ "intrinsic."
#~ msgstr ""

#~ msgid "The location of this expression in the source code."
#~ msgstr ""

#~ msgid "The location of the stmt in the source code."
#~ msgstr ""

#~ msgid "Optional[Mapping[str, Object]]"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`vload <tvm.tir.Buffer.vload>`\\ \\(begin\\[\\,"
#~ " dtype\\, predicate\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`vstore <tvm.tir.Buffer.vstore>`\\ \\(begin\\, "
#~ "value\\[\\, predicate\\]\\)"
#~ msgstr ""

#~ msgid "Optional[PrimExpr]"
#~ msgstr ""

#~ msgid ""
#~ "A vector mask of boolean values "
#~ "indicating which lanes of a vector "
#~ "are to be loaded. The number lanes"
#~ " of the mask must be equal to"
#~ " the number of lanes being loaded."
#~ msgstr ""

#~ msgid ""
#~ "A vector mask of boolean values "
#~ "indicating which lanes of a vector "
#~ "are to be stored. The number lanes"
#~ " of the mask must be equal to"
#~ " the number of lanes in value."
#~ msgstr ""

#~ msgid "The buffer indices to load values from."
#~ msgstr ""

#~ msgid "Union[Op, str]"
#~ msgstr ""

#~ msgid "The expression to be evaluated."
#~ msgstr ""

#~ msgid "min"
#~ msgstr ""

#~ msgid "Optional[Stmt]"
#~ msgstr ""

#~ msgid "The value in to be bound."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`loop_partition <tvm.tir.Schedule.loop_partition>`\\ "
#~ "\\(loop\\, factors\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Partition a loop into a list of consecutive loops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`split <tvm.tir.Schedule.split>`\\ \\(loop\\, "
#~ "factors\\[\\, preserve\\_unit\\_iters\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Partition a loop into a list of"
#~ " consecutive loops. It requires: 1) "
#~ "The loop can't have annotation or "
#~ "thread binding. Predicates may be added"
#~ " to ensure the total loop numbers "
#~ "keeps unchanged. In `factors`, at most"
#~ " one of the factors can be "
#~ "None, which will be automatically "
#~ "inferred."
#~ msgstr ""

#~ msgid "The loop to be partition"
#~ msgstr ""

#~ msgid ""
#~ "The partitioning factors Potential inputs "
#~ "are: - None - ExprRV - Positive"
#~ " constant integers"
#~ msgstr ""

#~ msgid "partition_loops"
#~ msgstr ""

#~ msgid "The new loops after partition"
#~ msgstr ""

#~ msgid "Before partition, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do partition:"
#~ msgstr ""

#~ msgid "After applying partition, the IR becomes:"
#~ msgstr ""

#~ msgid "disable_predication"
#~ msgstr ""

#~ msgid ""
#~ "If enabled, don't create a predicate "
#~ "for guarding the loop. This can be"
#~ " useful when splitting with scalable "
#~ "factors that the schedule writer knows"
#~ " are divisible by the loop bound."
#~ msgstr ""

#~ msgid ""
#~ "Warning: enabling this feature may "
#~ "result in incorrect code generation if"
#~ " not used carefully."
#~ msgstr ""

#~ msgid "Union[str, ir.Type]"
#~ msgstr ""

#~ msgid "int8x4"
#~ msgstr ""

#~ msgid "acc"
#~ msgstr ""

#~ msgid "int32"
#~ msgstr ""

#~ msgid "The accumulator."
#~ msgstr ""

#~ msgid ""
#~ "It will be lowered to the "
#~ "llvm.get.active.lane.mask intrinsic. "
#~ "(https://llvm.org/docs/LangRef.html#llvm-get-active-"
#~ "lane-mask-intrinsics)"
#~ msgstr ""

#~ msgid "An expression reprsenting the base."
#~ msgstr ""

#~ msgid "limit"
#~ msgstr ""

#~ msgid "An expression representing the limit."
#~ msgstr ""

#~ msgid "Union[str, tvm.DataType]"
#~ msgstr ""

#~ msgid "Element data type."
#~ msgstr ""

#~ msgid "min_size"
#~ msgstr ""

#~ msgid "The minimum size of the scalable vector in bits."
#~ msgstr ""

#~ msgid "d"
#~ msgstr ""

#~ msgid "The simdgroup var"
#~ msgstr ""

#~ msgid "The index of the matrix."
#~ msgstr ""

#~ msgid "The value to fill."
#~ msgstr ""

#~ msgid "col"
#~ msgstr ""

#~ msgid "The number of columns."
#~ msgstr ""

#~ msgid "row"
#~ msgstr ""

#~ msgid "The number of rows."
#~ msgstr ""

#~ msgid "The pointer."
#~ msgstr ""

#~ msgid "The stride."
#~ msgstr ""

#~ msgid "transpose_matrix"
#~ msgstr ""

#~ msgid "Whether to transpose the matrix."
#~ msgstr ""

#~ msgid "Multiply and accumulate two matrices in simdgroup i.e. d = a * b + c"
#~ msgstr ""

#~ msgid "The destination matrix."
#~ msgstr ""

#~ msgid "The index of the destination matrix."
#~ msgstr ""

#~ msgid "The first matrix."
#~ msgstr ""

#~ msgid "The index of the first matrix."
#~ msgstr ""

#~ msgid "The second matrix."
#~ msgstr ""

#~ msgid "The index of the second matrix."
#~ msgstr ""

#~ msgid "c"
#~ msgstr ""

#~ msgid "The third matrix."
#~ msgstr ""

#~ msgid "The index of the third matrix."
#~ msgstr ""

#~ msgid "The SIMDGroup."
#~ msgstr ""

#~ msgid "The input statement."
#~ msgstr ""

#~ msgid "*args"
#~ msgstr ""

#~ msgid "Union[PrimExpr, Stmt]"
#~ msgstr ""

#~ msgid ""
#~ "Get the target's vscale value. It "
#~ "will be lowered to llvm.vscale intrinsic"
#~ " (https://llvm.org/docs/LangRef.html#llvm-vscale-"
#~ "intrinsic) Returns ------- call : "
#~ "PrimExpr"
#~ msgstr ""

#~ msgid "Call to the vscale intrinsic"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`HoistedConditionals "
#~ "<tvm.tir.transform.HoistedConditionals>`\\ \\(value\\[\\, "
#~ "names\\, module\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`HoistedLetBindings "
#~ "<tvm.tir.transform.HoistedLetBindings>`\\ \\(value\\[\\, "
#~ "names\\, module\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InlinePrivateFunctions "
#~ "<tvm.tir.transform.InlinePrivateFunctions>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Inline calls to private functions"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MergeSharedMemoryAllocations "
#~ "<tvm.tir.transform.MergeSharedMemoryAllocations>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "This pass merges multiple TIR-level "
#~ "shared memory allocations into one "
#~ "allocation."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`_generate_next_value_ "
#~ "<tvm.tir.transform.HoistedConditionals._generate_next_value_>`\\ "
#~ "\\(name\\, start\\, count\\, ...\\)"
#~ msgstr ""

#~ msgid "Generate the next value when not given."
#~ msgstr ""

#~ msgid ""
#~ "If set, allow hoisting of conditionals"
#~ " that use a block variable (e.g. "
#~ "threadIdx.x)."
#~ msgstr ""

#~ msgid ""
#~ "name: the name of the member "
#~ "start: the initial start value or "
#~ "None count: the number of existing "
#~ "members last_values: the last value "
#~ "assigned or None"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`_generate_next_value_ "
#~ "<tvm.tir.transform.HoistedLetBindings._generate_next_value_>`\\ "
#~ "\\(name\\, start\\, count\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`assert_pure_function "
#~ "<tvm.tir.analysis.assert_pure_function>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid "Asserts that the function is a pure function"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`is_pure_function <tvm.tir.analysis.is_pure_function>`\\"
#~ " \\(func\\)"
#~ msgstr ""

#~ msgid "Checks if the function is a pure function"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`vload <tvm.tir.analysis.Buffer.vload>`\\ "
#~ "\\(begin\\[\\, dtype\\, predicate\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`vstore <tvm.tir.analysis.Buffer.vstore>`\\ "
#~ "\\(begin\\, value\\[\\, predicate\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`functions_items "
#~ "<tvm.tir.analysis.IRModule.functions_items>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Get items in self.functions.items() in alphabetical order."
#~ msgstr ""

#~ msgid "items: List[Tuple[GlobalVar, Function]]"
#~ msgstr ""

#~ msgid "The functions items."
#~ msgstr ""

