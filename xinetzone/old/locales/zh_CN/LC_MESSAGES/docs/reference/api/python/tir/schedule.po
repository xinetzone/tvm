# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-09-05 09:32+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/reference/api/python/tir/schedule.rst:19
msgid "tvm.tir.schedule"
msgstr ""

#: of tvm.tir.schedule:1
msgid "Namespace for the TensorIR schedule API."
msgstr ""

#: of tvm.tir.schedule.schedule.ScheduleError:1
msgid "Error that happens during TensorIR scheduling."
msgstr ""

#: of tvm.tir.schedule.schedule.BlockRV:1
msgid "A random variable that refers to a block"
msgstr ""

#: of tvm.tir.schedule.schedule.BlockRV.__init__:1
msgid "Construct a new BlockRV."
msgstr ""

#: ../../doc/docs/reference/api/python/tir/schedule.rst of
#: tvm.tir.block_scope.StmtSRef.inline_mark
#: tvm.tir.block_scope.StmtSRef.root_mark
#: tvm.tir.schedule.instruction.Instruction
#: tvm.tir.schedule.instruction.InstructionKind
#: tvm.tir.schedule.schedule.BlockRV.__init__
#: tvm.tir.schedule.schedule.LoopRV.__init__
#: tvm.tir.schedule.schedule.Schedule._create_non_traced
#: tvm.tir.schedule.schedule.Schedule.can_decompose_padding
#: tvm.tir.schedule.schedule.Schedule.enter_postproc
#: tvm.tir.schedule.schedule.Schedule.show tvm.tir.schedule.state.ScheduleState
#: tvm.tir.schedule.trace.Trace
msgid "返回类型"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:1
msgid ""
"An object corresponds to each block sref in the sref tree, which tracks "
"the producer-consumer dependency between blocks."
msgstr ""

#: of tvm.tir.block_scope.BlockScope:4
msgid "Glossary:"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:6
msgid ""
"Block scope: A contiguous subtree of the sref tree, rooted at each block "
"sref, whose components are:"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:9
msgid "scope root: a block sref"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:10
msgid "internal srefs: loop srefs"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:11
msgid "scope leaves: block srefs"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:13
msgid ""
"Child block: The scope leaf blocks under the scope root or a specific "
"internal sref"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:1
msgid "Get all dependencies whose `dst` is the target `block`."
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:4
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:4
#: tvm.tir.block_scope.Dependency:6
#: tvm.tir.schedule.instruction.Instruction.__init__:4
#: tvm.tir.schedule.instruction.InstructionKind.get:4
#: tvm.tir.schedule.schedule.Schedule.__init__:4
#: tvm.tir.schedule.schedule.Schedule.add_unit_loop:4
#: tvm.tir.schedule.schedule.Schedule.annotate:4
#: tvm.tir.schedule.schedule.Schedule.bind:10
#: tvm.tir.schedule.schedule.Schedule.blockize:4
#: tvm.tir.schedule.schedule.Schedule.cache_index:5
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:6
#: tvm.tir.schedule.schedule.Schedule.cache_read:8
#: tvm.tir.schedule.schedule.Schedule.cache_write:8
#: tvm.tir.schedule.schedule.Schedule.compute_at:19
#: tvm.tir.schedule.schedule.Schedule.compute_inline:14
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:18
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:18
#: tvm.tir.schedule.schedule.Schedule.fuse:8
#: tvm.tir.schedule.schedule.Schedule.get:9
#: tvm.tir.schedule.schedule.Schedule.get_block:8
#: tvm.tir.schedule.schedule.Schedule.get_child_blocks:4
#: tvm.tir.schedule.schedule.Schedule.get_consumers:4
#: tvm.tir.schedule.schedule.Schedule.get_loops:4
#: tvm.tir.schedule.schedule.Schedule.get_output_blocks:6
#: tvm.tir.schedule.schedule.Schedule.get_producers:4
#: tvm.tir.schedule.schedule.Schedule.get_sref:8
#: tvm.tir.schedule.schedule.Schedule.loop_partition:8
#: tvm.tir.schedule.schedule.Schedule.merge:8
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:12
#: tvm.tir.schedule.schedule.Schedule.parallel:9
#: tvm.tir.schedule.schedule.Schedule.reindex:8
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:13
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:13
#: tvm.tir.schedule.schedule.Schedule.remove_rv:4
#: tvm.tir.schedule.schedule.Schedule.reorder:12
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:4
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:16
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:17
#: tvm.tir.schedule.schedule.Schedule.rfactor:61
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:17
#: tvm.tir.schedule.schedule.Schedule.sample_categorical:4
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:4
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:4
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:4
#: tvm.tir.schedule.schedule.Schedule.seed:4
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:5
#: tvm.tir.schedule.schedule.Schedule.set_scope:5
#: tvm.tir.schedule.schedule.Schedule.split:9
#: tvm.tir.schedule.schedule.Schedule.storage_align:7
#: tvm.tir.schedule.schedule.Schedule.tensorize:4
#: tvm.tir.schedule.schedule.Schedule.transform_block_layout:4
#: tvm.tir.schedule.schedule.Schedule.transform_layout:4
#: tvm.tir.schedule.schedule.Schedule.unannotate:4
#: tvm.tir.schedule.schedule.Schedule.unroll:4
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:4
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:8
#: tvm.tir.schedule.schedule.Schedule.vectorize:9
#: tvm.tir.schedule.schedule.Schedule.work_on:11
#: tvm.tir.schedule.state.ScheduleState:13
#: tvm.tir.schedule.state.ScheduleState.__init__:4
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:4
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:4
#: tvm.tir.schedule.state.ScheduleState.get_sref:4
#: tvm.tir.schedule.state.ScheduleState.replace:12
#: tvm.tir.schedule.trace.Trace.__init__:4
#: tvm.tir.schedule.trace.Trace.append:4
#: tvm.tir.schedule.trace.Trace.apply_json_to_schedule:4
#: tvm.tir.schedule.trace.Trace.apply_to_schedule:4
#: tvm.tir.schedule.trace.Trace.as_json:4
#: tvm.tir.schedule.trace.Trace.as_python:4
#: tvm.tir.schedule.trace.Trace.get_decision:4
#: tvm.tir.schedule.trace.Trace.show:4
#: tvm.tir.schedule.trace.Trace.simplified:4
#: tvm.tir.schedule.trace.Trace.with_decision:5
msgid "Parameters"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:5
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:5
msgid "block: StmtSRef"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:6
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:6
msgid "The queried block"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:9
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:9
#: tvm.tir.schedule.InstructionKind.is_pure:7
#: tvm.tir.schedule.instruction.InstructionKind.get:9
#: tvm.tir.schedule.schedule.Schedule.add_unit_loop:9
#: tvm.tir.schedule.schedule.Schedule.blockize:11
#: tvm.tir.schedule.schedule.Schedule.cache_index:18
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:20
#: tvm.tir.schedule.schedule.Schedule.cache_read:25
#: tvm.tir.schedule.schedule.Schedule.cache_write:25
#: tvm.tir.schedule.schedule.Schedule.copy:9
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:25
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:25
#: tvm.tir.schedule.schedule.Schedule.fork_seed:4
#: tvm.tir.schedule.schedule.Schedule.fuse:13
#: tvm.tir.schedule.schedule.Schedule.get:14
#: tvm.tir.schedule.schedule.Schedule.get_block:15
#: tvm.tir.schedule.schedule.Schedule.get_child_blocks:9
#: tvm.tir.schedule.schedule.Schedule.get_consumers:9
#: tvm.tir.schedule.schedule.Schedule.get_loops:9
#: tvm.tir.schedule.schedule.Schedule.get_output_blocks:11
#: tvm.tir.schedule.schedule.Schedule.get_producers:9
#: tvm.tir.schedule.schedule.Schedule.get_sref:13
#: tvm.tir.schedule.schedule.Schedule.loop_partition:23
#: tvm.tir.schedule.schedule.Schedule.merge:13
#: tvm.tir.schedule.schedule.Schedule.reindex:32
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:24
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:27
#: tvm.tir.schedule.schedule.Schedule.rfactor:68
#: tvm.tir.schedule.schedule.Schedule.sample_categorical:13
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:11
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:17
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:15
#: tvm.tir.schedule.schedule.Schedule.split:32
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:9
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:9
#: tvm.tir.schedule.state.ScheduleState.get_sref:9
#: tvm.tir.schedule.trace.Trace.as_json:9
#: tvm.tir.schedule.trace.Trace.as_python:9
#: tvm.tir.schedule.trace.Trace.get_decision:9
#: tvm.tir.schedule.trace.Trace.pop:4 tvm.tir.schedule.trace.Trace.simplified:9
#: tvm.tir.schedule.trace.Trace.with_decision:14
msgid "Returns"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:10
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:10
msgid "blocks: List[Dependency]"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:11
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:11
msgid "The dependencies"
msgstr ""

#: ../../doc/docs/reference/api/python/tir/schedule.rst of
#: tvm.tir.schedule.instruction.Instruction
#: tvm.tir.schedule.instruction.InstructionKind
#: tvm.tir.schedule.schedule.Schedule._create_non_traced
#: tvm.tir.schedule.schedule.Schedule.can_decompose_padding
#: tvm.tir.schedule.state.ScheduleState tvm.tir.schedule.trace.Trace
msgid "参数"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_src:1
msgid "Get all dependencies whose `src` is the target`block`."
msgstr ""

#: of tvm.tir.block_scope.DepKind:1
msgid "Type of dependency."
msgstr ""

#: of tvm.tir.block_scope.DepKind:4 tvm.tir.schedule.instruction.Instruction:4
#: tvm.tir.schedule.instruction.InstructionKind:13
#: tvm.tir.schedule.state.ScheduleDebugMask:8 tvm.tir.schedule.trace.Trace:16
msgid "Attributes"
msgstr ""

#: of tvm.tir.block_scope.DepKind:5
msgid "RAW"
msgstr ""

#: of tvm.tir.block_scope.DepKind:-1
msgid "int = 0"
msgstr ""

#: of tvm.tir.block_scope.DepKind:6
msgid "Read-after-write dependency"
msgstr ""

#: of tvm.tir.block_scope.DepKind:7
msgid "WAW"
msgstr ""

#: of tvm.tir.block_scope.DepKind:-1
#: tvm.tir.schedule.state.ScheduleDebugMask:-1
msgid "int = 1"
msgstr ""

#: of tvm.tir.block_scope.DepKind:8
msgid "Write-after-write dependency"
msgstr ""

#: of tvm.tir.block_scope.DepKind:9
msgid "WAR"
msgstr ""

#: of tvm.tir.block_scope.DepKind:-1
#: tvm.tir.schedule.state.ScheduleDebugMask:-1
msgid "int = 2"
msgstr ""

#: of tvm.tir.block_scope.DepKind:10
msgid "Write-after-read dependency. Not supported in TensorIR for now."
msgstr ""

#: of tvm.tir.block_scope.DepKind:11
msgid "OPAQUE: int = 3"
msgstr ""

#: of tvm.tir.block_scope.DepKind:12
msgid "Opaque dependency"
msgstr ""

#: of tvm.tir.block_scope.Dependency:1
msgid ""
"A tuple (src, dst, kind) representing certain types of dependency. For "
"example, (A, B, kRAW) means block B depends on block A, and the "
"dependency kind is read-after-write, which means block B reads the result"
" written by block A."
msgstr ""

#: of tvm.tir.block_scope.Dependency:7
msgid "src"
msgstr ""

#: of tvm.tir.block_scope.Dependency:-1
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:-1
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:-1
#: tvm.tir.schedule.state.ScheduleState.get_sref:-1
#: tvm.tir.schedule.state.ScheduleState.replace:-1
msgid "StmtSRef"
msgstr ""

#: of tvm.tir.block_scope.Dependency:8
msgid "The source of the dependency relation"
msgstr ""

#: of tvm.tir.block_scope.Dependency:9
msgid "dst"
msgstr ""

#: of tvm.tir.block_scope.Dependency:10
msgid "The destination of the dependency relation"
msgstr ""

#: of tvm.tir.block_scope.Dependency:11
#: tvm.tir.schedule.instruction.Instruction:5
#: tvm.tir.schedule.instruction.Instruction.__init__:5
#: tvm.tir.schedule.instruction.InstructionKind.get:10
msgid "kind"
msgstr ""

#: of tvm.tir.block_scope.Dependency:-1
msgid "DepKind"
msgstr ""

#: of tvm.tir.block_scope.Dependency:12
msgid "The dependency kind"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:1
msgid "Schedule instructions each corresponds to a schedule primitive"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:-1
#: tvm.tir.schedule.instruction.Instruction.__init__:-1
#: tvm.tir.schedule.instruction.InstructionKind.get:-1
msgid "InstructionKind"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:6
#: tvm.tir.schedule.instruction.Instruction.__init__:6
msgid "The kind of the instruction"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:7
#: tvm.tir.schedule.instruction.Instruction.__init__:7
msgid "inputs"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:-1
#: tvm.tir.schedule.instruction.Instruction.__init__:-1
msgid "List[INPUT_RV_TYPE]"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:8
#: tvm.tir.schedule.instruction.Instruction.__init__:8
msgid ""
"The input random variables of the instruction, and the type of each "
"element can be one of the following: - BlockRV - LoopRV - ExprRV - float "
"- int - str - None"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:17
#: tvm.tir.schedule.instruction.Instruction.__init__:17
msgid "attrs"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:-1
#: tvm.tir.schedule.instruction.Instruction.__init__:-1
msgid "List[ATTR_TYPE]"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:18
#: tvm.tir.schedule.instruction.Instruction.__init__:18
msgid ""
"The attributes of the instruction. Similar to attributes of an operator, "
"attributes of an instruction are arbitrary constant metadata required by "
"the instructions. For example, the name of the block to be retrieved in "
"`GetBlock`."
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:21
#: tvm.tir.schedule.instruction.Instruction.__init__:21
msgid "outputs"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:-1
#: tvm.tir.schedule.instruction.Instruction.__init__:-1
msgid "List[OUTPUT_RV_TYPE]"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction:22
#: tvm.tir.schedule.instruction.Instruction.__init__:22
msgid ""
"The output random variables of the instruction, and the type of each "
"element can be one of the following: - BlockRV - LoopRV - ExprRV, atomic "
"variables only, won't be constants or composite PrimExpr"
msgstr ""

#: of tvm.tir.schedule.instruction.Instruction.__init__:1
#: tvm.tir.schedule.trace.Trace.__init__:1
msgid "Constructor"
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind:1
msgid ""
"Kind of an instruction, e.g. Split, Reorder, etc. Besides the name, every"
" kind of instruction has its own properties, including: 1) A boolean "
"indicating if the instruction is pure, i.e. change nothing in the "
"schedule state 2) A functor that applies the instruction to a TensorIR "
"schedule 3) A functor that converts the instruction to a statement in "
"python syntax 4) A functor that serialize its attributes to JSON 5) A "
"functor that deserialize its attributes from JSON"
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind:9
msgid ""
"Unlike `tvm.ir.op`, `InstructionKind` doesn't support unstructured "
"properties, mainly because there is no such usecase yet to add any other "
"property."
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind:14
#: tvm.tir.schedule.instruction.InstructionKind.get:5
#: tvm.tir.schedule.schedule.Schedule.get_block:9
msgid "name"
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind:-1
#: tvm.tir.schedule.instruction.InstructionKind.get:-1
#: tvm.tir.schedule.schedule.Schedule.annotate:-1
#: tvm.tir.schedule.schedule.Schedule.bind:-1
#: tvm.tir.schedule.schedule.Schedule.get_block:-1
#: tvm.tir.schedule.schedule.Schedule.set_scope:-1
#: tvm.tir.schedule.schedule.Schedule.tensorize:-1
#: tvm.tir.schedule.schedule.Schedule.unannotate:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:-1
#: tvm.tir.schedule.schedule.Schedule.work_on:-1
msgid "str"
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind:15
msgid "The name of a kind of instructions"
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind:18
#: tvm.tir.schedule.schedule.Schedule.__init__:34
#: tvm.tir.schedule.schedule.Schedule.blockize:67
#: tvm.tir.schedule.schedule.Schedule.rfactor:124
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:95
#: tvm.tir.schedule.schedule.Schedule.set_scope:63
#: tvm.tir.schedule.schedule.Schedule.storage_align:70
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:13
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:66
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:14
#: tvm.tir.schedule.state.ScheduleState.replace:26
msgid "Note"
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind:19
msgid "The functor properties are not exposed on python side at the moment"
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind.get:1
msgid "Retrieve an InstructionKind using its name"
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind.get:6
msgid "The registered name of the InstructionKind"
msgstr ""

#: of tvm.tir.schedule.instruction.InstructionKind.get:11
msgid "The InstructionKind retrieved"
msgstr ""

#: of tvm.tir.schedule.InstructionKind.is_pure:1
msgid ""
"Indicates if the instruction is pure, i.e. removing it alone doesn't "
"mutate the schedule state. For example, the instruction `GetBlock` is "
"pure because it changes nothing, while `ComputeInline` is not because "
"removing it leads to a different resulting schedule."
msgstr ""

#: of tvm.tir.schedule.InstructionKind.is_pure:8
msgid "pure"
msgstr ""

#: of tvm.tir.schedule.InstructionKind.is_pure:-1
#: tvm.tir.schedule.schedule.Schedule.blockize:-1
#: tvm.tir.schedule.schedule.Schedule.loop_partition:-1
#: tvm.tir.schedule.schedule.Schedule.split:-1
#: tvm.tir.schedule.schedule.Schedule.tensorize:-1
#: tvm.tir.schedule.state.ScheduleState:-1
#: tvm.tir.schedule.trace.Trace.apply_to_schedule:-1
#: tvm.tir.schedule.trace.Trace.simplified:-1
#: tvm.tir.schedule.trace.Trace.with_decision:-1
msgid "bool"
msgstr ""

#: of tvm.tir.schedule.InstructionKind.is_pure:9
msgid "The boolean flag indicating if the instruction is pure"
msgstr ""

#: of tvm.tir.schedule.schedule.LoopRV:1
msgid "A random variable that refers to a loop"
msgstr ""

#: of tvm.tir.schedule.schedule.LoopRV.__init__:1
msgid "Construct a new LoopRV."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1
msgid "The user-facing schedule class"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:3
msgid ""
"A schedule is a set of transformations that change the order of "
"computation but preserve the semantics of computation. Some example of "
"schedules: 1) Split a loop into two; 2) Reorder two loops; 3) Inline the "
"computation of a specific buffer into its consumer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:9
msgid ""
"The schedule class stores auxiliary information to schedule correctly and"
" efficiently."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:11
msgid ""
"Link to tutorial: "
"https://tvm.apache.org/docs/tutorials/language/schedule_primitives.html"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:1
msgid "Construct a TensorIR schedule class from an IRModule"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:5
#: tvm.tir.schedule.state.ScheduleState:14
#: tvm.tir.schedule.state.ScheduleState.__init__:5
msgid "mod"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:-1
#: tvm.tir.schedule.state.ScheduleState.__init__:-1
msgid "Union[PrimFunc, IRModule]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:6
#: tvm.tir.schedule.state.ScheduleState.__init__:6
msgid "The IRModule or PrimFunc to be scheduled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:7
msgid "seed: Optional[int]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:8
msgid ""
"The seed value for schedule's random state Note that None and -1 means "
"use device random, otherwise only integer between 1 and 2147483647 is "
"allowed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:11
#: tvm.tir.schedule.state.ScheduleState:16
#: tvm.tir.schedule.state.ScheduleState.__init__:7
msgid "debug_mask"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:-1
#: tvm.tir.schedule.state.ScheduleState.__init__:-1
msgid "Union[str, int]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:12
#: tvm.tir.schedule.state.ScheduleState.__init__:8
msgid ""
"Do extra correctness checking after the class creation and each time "
"after calling the Replace method. Possible choices of `debug_mask`: 1) "
"\"all\" - Turn on all the checks 2) \"none\" - Turn off all the checks 3)"
" An integer - Turn on checks according to the bitmasks provided in "
"ScheduleDebugMask"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:18
msgid "error_render_level"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:-1
msgid "str = \"detail\""
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:19
msgid ""
"The level of error rendering. Choices: \"detail\", \"fast\", \"none\". - "
"\"detail\": Render a detailed error message, with the TIR and error "
"locations printed - \"fast: Show a simple error message without rendering"
" or string manipulation - \"none\": Do not show any error message."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:23
#: tvm.tir.schedule.state.ScheduleState:19
msgid "enable_check"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:-1
msgid "bool = True"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:24
msgid ""
"The default schedule checks are too strict and might prevent us "
"performing some valid schedules. `enable_check` is an argument to control"
" whether we enable prerequisite checks for some schedule primitives or "
"not: - true: perform prerequisite check before applying some schedules. -"
" false: do not perform some check before applying schedules, but still "
"raise error if schedule fails."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:31
msgid ""
"It's user duty to guarantee schedule correctness if `enable_check` is set"
" to `False`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:35
msgid "The checks performed includes: 1) VerifySRefTree 2) VerifyCachedFlags"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule._create_non_traced:1
msgid "Construct a non-traced TensorIR schedule class from an IRModule."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:1
msgid "Create a new unit loop on top of the specific block or loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:5
#: tvm.tir.schedule.schedule.Schedule.get_child_blocks:5
#: tvm.tir.schedule.schedule.Schedule.tensorize:5
msgid "block_or_loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:-1
msgid "Union[LoopRV, BlockRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:6
msgid "The block above which the new loop is created"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:10
msgid "new_loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:-1
#: tvm.tir.schedule.schedule.Schedule.bind:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:-1
#: tvm.tir.schedule.schedule.Schedule.fuse:-1
#: tvm.tir.schedule.schedule.Schedule.loop_partition:-1
#: tvm.tir.schedule.schedule.Schedule.merge:-1
#: tvm.tir.schedule.schedule.Schedule.parallel:-1
#: tvm.tir.schedule.schedule.Schedule.rfactor:-1
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:-1
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:-1
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:-1
#: tvm.tir.schedule.schedule.Schedule.split:-1
#: tvm.tir.schedule.schedule.Schedule.unroll:-1
#: tvm.tir.schedule.schedule.Schedule.vectorize:-1
msgid "LoopRV"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:11
msgid "The new unit loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:14
#: tvm.tir.schedule.schedule.Schedule.annotate:13
#: tvm.tir.schedule.schedule.Schedule.bind:22
#: tvm.tir.schedule.schedule.Schedule.blockize:16
#: tvm.tir.schedule.schedule.Schedule.cache_index:23
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:25
#: tvm.tir.schedule.schedule.Schedule.cache_read:30
#: tvm.tir.schedule.schedule.Schedule.cache_write:30
#: tvm.tir.schedule.schedule.Schedule.compute_at:36
#: tvm.tir.schedule.schedule.Schedule.compute_inline:19
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:30
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:30
#: tvm.tir.schedule.schedule.Schedule.fuse:18
#: tvm.tir.schedule.schedule.Schedule.loop_partition:28
#: tvm.tir.schedule.schedule.Schedule.merge:18
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:20
#: tvm.tir.schedule.schedule.Schedule.parallel:14
#: tvm.tir.schedule.schedule.Schedule.reindex:37
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:29
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:32
#: tvm.tir.schedule.schedule.Schedule.reorder:17
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:11
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:33
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:22
#: tvm.tir.schedule.schedule.Schedule.rfactor:74
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:24
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:33
#: tvm.tir.schedule.schedule.Schedule.set_scope:14
#: tvm.tir.schedule.schedule.Schedule.split:37
#: tvm.tir.schedule.schedule.Schedule.storage_align:20
#: tvm.tir.schedule.schedule.Schedule.tensorize:13
#: tvm.tir.schedule.schedule.Schedule.transform_block_layout:12
#: tvm.tir.schedule.schedule.Schedule.transform_layout:74
#: tvm.tir.schedule.schedule.Schedule.unannotate:11
#: tvm.tir.schedule.schedule.Schedule.unroll:9
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:17
#: tvm.tir.schedule.schedule.Schedule.vectorize:14
msgid "Examples"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:16
msgid "Before add_unit_loop, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:30
msgid "Create the schedule and do add-unit-loop:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:38
msgid "After applying add-unit-loop, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:1
msgid "Annotate a block/loop with a key value pair"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:5
#: tvm.tir.schedule.schedule.Schedule.unannotate:5
msgid "block_or_loop: Union[BlockRV, LoopRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:6
msgid "The block/loop to be annotated"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:7
#: tvm.tir.schedule.schedule.Schedule.unannotate:7
msgid "ann_key"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:8
#: tvm.tir.schedule.schedule.Schedule.unannotate:8
msgid "The annotation key"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:9
msgid "ann_val"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:-1
msgid "AnnotationValueT"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:10
msgid "The annotation value"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:15
msgid "Before annotate, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:28
#: tvm.tir.schedule.schedule.Schedule.unannotate:27
msgid "Create the schedule and do annotate:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:36
msgid "After applying annotate, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:1
msgid ""
"Bind the input loop to the given thread axis. It requires: 1) The scope "
"block that the loop is in should have stage-pipeline property 2) All the "
"blocks under the loop are complete blocks or reduction blocks, and have "
"affine bindings 3) For each block under the loop, if the thread axis "
"starts with \"threadIdx`, the loop can only be contained in data-parallel"
" block iter and reduction block iters' bindings. Otherwise the loop can "
"only be contained in data-parallel block iters' bindings"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:11
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:21
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:21
#: tvm.tir.schedule.schedule.Schedule.loop_partition:9
#: tvm.tir.schedule.schedule.Schedule.parallel:10
#: tvm.tir.schedule.schedule.Schedule.rfactor:62
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:5
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:5
#: tvm.tir.schedule.schedule.Schedule.split:10
#: tvm.tir.schedule.schedule.Schedule.unroll:5
#: tvm.tir.schedule.schedule.Schedule.vectorize:10
msgid "loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:12
msgid "The loop to be bound to the thread axis"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:13
msgid "thread_axis"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:14
msgid ""
"The thread axis to be bound to the loop. Possible candidates: - "
"blockIdx.x/y/z - threadIdx.x/y/z - vthread.x/y/z - vthread (It is a "
"legacy behavior that will be deprecated. Please use `vthread.x/y/z` "
"instead.)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:24
msgid "Before bind, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:37
msgid "Create the schedule and do bind:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:46
msgid "After applying bind, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:1
msgid ""
"Convert multiple blocks or the subtree rooted at a specific loop into a "
"block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:5
msgid "target"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:-1
msgid "LoopRV or List[BlockRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:6
msgid "The root of the subtree or the specified blocks."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:7
#: tvm.tir.schedule.schedule.Schedule.loop_partition:19
#: tvm.tir.schedule.schedule.Schedule.split:20
#: tvm.tir.schedule.schedule.Schedule.tensorize:9
msgid "preserve_unit_iters"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:8
#: tvm.tir.schedule.schedule.Schedule.loop_partition:20
#: tvm.tir.schedule.schedule.Schedule.split:21
#: tvm.tir.schedule.schedule.Schedule.tensorize:10
msgid "Whether or not to preserve unit iterators in block bindings"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:12
#: tvm.tir.schedule.schedule.Schedule.get:15
#: tvm.tir.schedule.schedule.Schedule.get_sref:14
#: tvm.tir.schedule.schedule.Schedule.sample_categorical:14
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:12
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:18
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:16
msgid "result"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:-1
#: tvm.tir.schedule.schedule.Schedule.cache_read:-1
#: tvm.tir.schedule.schedule.Schedule.cache_write:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:-1
#: tvm.tir.schedule.schedule.Schedule.get_block:-1
#: tvm.tir.schedule.schedule.Schedule.reindex:-1
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:-1
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:-1
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:-1
#: tvm.tir.schedule.schedule.Schedule.rfactor:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:-1
msgid "BlockRV"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:13
msgid "The new block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:18
msgid "Before blockize, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:35
#: tvm.tir.schedule.schedule.Schedule.set_scope:35
msgid "Create the schedule and do set_scope:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:45
msgid "After applying blockize, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:68
msgid ""
"blockize requires there is exactly one block under the given loop and the"
" bindings of the block are divisible by the subspace represented by the "
"loops starting at the given loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:1
msgid ""
"Create a block to cache precomputed index for later use. if there is no "
"index computation, keep unchanged."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:6
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:7
#: tvm.tir.schedule.schedule.Schedule.cache_read:9
#: tvm.tir.schedule.schedule.Schedule.cache_write:9
#: tvm.tir.schedule.schedule.Schedule.compute_at:20
#: tvm.tir.schedule.schedule.Schedule.compute_inline:15
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:19
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:19
#: tvm.tir.schedule.schedule.Schedule.get_block:16
#: tvm.tir.schedule.schedule.Schedule.get_consumers:5
#: tvm.tir.schedule.schedule.Schedule.get_loops:5
#: tvm.tir.schedule.schedule.Schedule.get_producers:5
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:13
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:14
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:14
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:5
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:17
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:18
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:18
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:5
#: tvm.tir.schedule.schedule.Schedule.set_scope:6
#: tvm.tir.schedule.schedule.Schedule.storage_align:8
#: tvm.tir.schedule.schedule.Schedule.transform_block_layout:5
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:5
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:9
msgid "block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:-1
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:-1
#: tvm.tir.schedule.schedule.Schedule.cache_read:-1
#: tvm.tir.schedule.schedule.Schedule.cache_write:-1
#: tvm.tir.schedule.schedule.Schedule.compute_at:-1
#: tvm.tir.schedule.schedule.Schedule.compute_inline:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:-1
#: tvm.tir.schedule.schedule.Schedule.get_consumers:-1
#: tvm.tir.schedule.schedule.Schedule.get_loops:-1
#: tvm.tir.schedule.schedule.Schedule.get_producers:-1
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:-1
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:-1
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:-1
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:-1
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:-1
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:-1
#: tvm.tir.schedule.schedule.Schedule.set_scope:-1
#: tvm.tir.schedule.schedule.Schedule.storage_align:-1
#: tvm.tir.schedule.schedule.Schedule.transform_block_layout:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:-1
msgid "Union[BlockRV, str]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:7
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:8
msgid "The target block operates on the target buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:9
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:15
#: tvm.tir.schedule.schedule.Schedule.cache_read:17
#: tvm.tir.schedule.schedule.Schedule.cache_write:17
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:18
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:18
msgid "storage_scope: str"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:10
msgid "The storage scope of cached block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:12
msgid "cse_thresh: int"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:13
msgid ""
"The repeat threshold that determines a common sub expr, default 0 means "
"cache all index computation."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:19
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:21
msgid "cached_blocks"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:-1
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:-1
#: tvm.tir.schedule.schedule.Schedule.get_consumers:-1
#: tvm.tir.schedule.schedule.Schedule.get_output_blocks:-1
#: tvm.tir.schedule.schedule.Schedule.get_producers:-1
msgid "List[BlockRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:20
msgid "The blocks of the stage writing the cache buffers"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:24
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:26
msgid "Before cache_inplace, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:37
msgid "Create the schedule and cache_index:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:46
msgid "After applying cache_index, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:1
msgid ""
"Create blocks that reads & write a buffer region into a cache block. It "
"requires the target block both read & write the target buffer. Mainly for"
" inplace operation."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:10
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:16
msgid "read_buffer_index: int"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:11
#: tvm.tir.schedule.schedule.Schedule.cache_read:13
msgid ""
"The index of the buffer in block's read region, the unique name of a read"
" buffer in the block, or a Buffer object that is within the blocks read "
"region."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:16
#: tvm.tir.schedule.schedule.Schedule.cache_read:18
#: tvm.tir.schedule.schedule.Schedule.cache_write:18
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:19
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:19
msgid "The target storage scope."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:22
msgid "The blocks of the cache stage, read cache first, write cache second"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:38
msgid "Create the schedule and cache_inplace:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:47
msgid "After applying cache_inplace, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:1
msgid "Create a block that reads a buffer region into a read cache. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:3
msgid "There is at most one block who write the buffer in the scope."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:5
#: tvm.tir.schedule.schedule.Schedule.cache_write:5
msgid "The scope block have stage-pipeline property."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:10
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:15
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:15
msgid "The consumer block of the target buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:12
msgid "buffer: Union[int, str, Buffer]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:20
#: tvm.tir.schedule.schedule.Schedule.cache_write:20
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:22
msgid "consumer_blocks: Optional[List[Union[BlockRV, str]]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:21
msgid ""
"An optional list of consumers that should read from the cache. If not "
"specified, all consumers will use the cache."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:26
#: tvm.tir.schedule.schedule.Schedule.cache_write:26
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:25
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:28
msgid "cached_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:27
#: tvm.tir.schedule.schedule.Schedule.cache_write:27
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:26
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:29
msgid "The block of the cache stage"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:31
msgid "Before cache_read, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:44
msgid "Create the schedule and cache_read:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:53
msgid "After applying cache_read, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:1
msgid "Create a block that reads a buffer region into a write cache. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:3
msgid "There is only one block who write the buffer in the scope."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:10
msgid "The producer block of the target buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:12
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:16
msgid "write_buffer_index: int"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:13
msgid ""
"The index of the buffer in block's write region, the unique name of a "
"write buffer in the block, or a Buffer object that is within the blocks "
"write region."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:21
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:23
msgid ""
"An optional list of consumers that should read directly from the cache. "
"If not specified, all consumers will read from the original buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:31
msgid "Before cache_write, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:44
msgid "Create the schedule and cache_write:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:53
msgid "After applying cache_write, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.can_decompose_padding:1
msgid "Check whether the block match padding pattern and can be decomposed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:1
msgid ""
"Compute-At. Move a producer block under the specific loop, and regenerate"
" the loops induced by the block so that the buffer region produced by the"
" producer block could cover those regions consumed by its consumer blocks"
" under the given loop. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:5
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:5
msgid ""
"`block` and `loop` are under the same scope, `loop` is not the ancestor "
"of `block`"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:7
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:7
msgid "The scope block has stage-pipeline property"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:9
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:9
msgid ""
"3) The subtree of the scope block, where the given block is in, satisfies"
" the compact dataflow condition. i.e. all the blocks in the scope block's"
" subtree must be either complete block or reduction block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:13
msgid ""
"4) The block is not an output block with regard to the scope block, i.e. "
"the buffers written by the block are allocated under the scope block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:16
msgid "All the consumers of the block are under the given loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:21
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:18
msgid "The block to be moved"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:23
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:20
msgid "loop: LoopRV"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:24
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:21
msgid "The loop where the block to be moved under"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:26
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:23
msgid "preserve_unit_loops: bool"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:27
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:24
msgid "Whether to keep the trivial loops whose extents are 1"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:29
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:26
msgid "index: int"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:30
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:27
msgid ""
"The block index of the loop body subtree blocks: - `index = -1` means "
"inserted into the last possible insertion point; - `index = -2` means "
"inserted into the first possible insertion point; - Otherwise, `index` is"
" a nonnegative number that indicates the insertion point"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:38
msgid "Before compute-at, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:56
msgid "Create the schedule and do compute-at:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:66
msgid "After applying compute-at, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:1
msgid "Inline a block into its consumer(s). It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:3
msgid "The block is a complete non-root block, which only produces one buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:5
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:5
msgid "The block must not be the only leaf in the scope."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:7
msgid ""
"The body of the block must be a BufferStore statement in the form of, "
"``A[i, j, k, ...] = ...`` where the indices of the LHS are all distinct "
"atomic variables, and no variables other than those indexing variables "
"are allowed in the statement."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:16
msgid "The block to be inlined to its consumer(s)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:21
msgid "Before compute-inline, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:39
msgid "Create the schedule and do compute-inline:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:47
msgid "After applying compute-inline, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.copy:1
msgid ""
"Returns a copy of the schedule, including both the state and the symbol "
"table, * guaranteeing that * 1) SRef tree is completely reconstructed; * "
"2) The IRModule being scheduled is untouched; * 3) All the random "
"variables are valid in the copy, pointing to the corresponding sref * "
"reconstructed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.copy:10
msgid "copy"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.copy:-1
#: tvm.tir.schedule.trace.Trace.apply_json_to_schedule:-1
#: tvm.tir.schedule.trace.Trace.apply_to_schedule:-1
msgid "Schedule"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.copy:11
msgid "A new copy of the schedule"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:1
msgid "Decompose a block of padding computation pattern into two separate blocks."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:3
msgid "The block which fill const pad values into full write region;"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:5
msgid ""
"The block which fill in-bound values into region where pad predicate is "
"true."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:7
msgid "The pad value filling block is inserted right before the given loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:9
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:9
msgid "The schedule primitive requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:11
msgid "The input block is a complete block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:13
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:13
msgid "The input loop is the ancestor of the block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:15
msgid "The input block is a block which match padding pattern."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:20
msgid "The padding block to be decomposed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:22
msgid "The loop above which the pad value filling block is inserted before."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:26
msgid "pad_value_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:27
msgid "The block filling const pad values."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:31
msgid "Before decompose-padding, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:42
msgid "Create the schedule and do decompose-padding with specified loop:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:51
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:48
msgid "After applying decompose-padding, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:1
msgid "Decompose a reduction block into two separate blocks."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:3
msgid ""
"The init block, which is translated from the init statement of the "
"reduction block;"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:5
msgid "The update block, which is the original block without init statement."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:7
msgid "The init block is inserted right before the given loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:11
msgid "The input block is a reduction block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:15
msgid ""
"The input loop is not lower than all the loops related to reduce block "
"var."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:20
msgid "The reduction block to be decomposed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:22
msgid "The loop above which the init block is inserted before."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:26
msgid "init_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:27
msgid "The init block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:31
msgid "Before decompose-reduction, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:46
msgid "Create the schedule and do decompose-reduction with specified loop:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:56
msgid "After applying decompose-reduction, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.enter_postproc:1
msgid "A no-op that marks the start of postprocessing phase of scheduling"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fork_seed:1
msgid "Returns a forked random state as seed for new schedules"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fork_seed:5
#: tvm.tir.schedule.schedule.Schedule.seed:5
msgid "seed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fork_seed:-1
#: tvm.tir.schedule.schedule.Schedule.rfactor:-1
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:-1
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:-1
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:-1
#: tvm.tir.schedule.schedule.Schedule.seed:-1
#: tvm.tir.schedule.schedule.Schedule.set_scope:-1
#: tvm.tir.schedule.schedule.Schedule.storage_align:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:-1
#: tvm.tir.schedule.state.ScheduleState:-1
msgid "int"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fork_seed:6
msgid "The forked random state, not the same as the current random state"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:1
msgid ""
"Fuse a list of consecutive loops into one. It requires: 1) The loops "
"can't have annotations or thread bindings. 2) The (i+1)-th loop must be "
"the only child of the i-th loop. 3) All loops must start with 0. 4) The "
"domain of a loop to be fused cannot depend on another loop to be fused."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:9
#: tvm.tir.schedule.schedule.Schedule.merge:9
msgid "*loops"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:-1
#: tvm.tir.schedule.schedule.Schedule.get_child_blocks:-1
#: tvm.tir.schedule.schedule.Schedule.get_loops:-1
#: tvm.tir.schedule.schedule.Schedule.loop_partition:-1
#: tvm.tir.schedule.schedule.Schedule.merge:-1
#: tvm.tir.schedule.schedule.Schedule.reorder:-1
#: tvm.tir.schedule.schedule.Schedule.split:-1
msgid "List[LoopRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:10
msgid "The loops to be fused"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:14
#: tvm.tir.schedule.schedule.Schedule.merge:14
msgid "fused_loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:15
msgid "The new loop after fusion"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:20
msgid "Before applying fuse, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:33
#: tvm.tir.schedule.schedule.Schedule.merge:38
msgid "Create the schedule and do fuse:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:42
#: tvm.tir.schedule.schedule.Schedule.merge:48
msgid "After applying fuse, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:1
msgid ""
"Returns: - the corresponding Block that a BlockRV evaluates to; - the "
"corresponding For that a LoopRV evaluates to; - the corresponding integer"
" that a ExprRV evaluates to; - the corresponding Block that a block sref "
"points to; - the corresponding For that a loop sref points to;"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:10
msgid "rand_var_or_sref"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:-1
msgid "Union[ExprRV, BlockRV, LoopRV, StmtSRef]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:11
#: tvm.tir.schedule.schedule.Schedule.get_sref:10
msgid "The random variable / sref to be evaluated"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:-1
msgid "Optional[Union[int, Block, For]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:16
#: tvm.tir.schedule.schedule.Schedule.get_sref:15
msgid "The corresponding result"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:1
msgid "Retrieve a block in a specific function with its name"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:3
msgid ""
"By default, if `func_name` is not specified, the schedule will search for"
" the block in the function that is currently being \"worked on\". To "
"switch the function to be worked on, use `work_on` before calling this "
"method."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:10
msgid "The name of the block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:11
#: tvm.tir.schedule.schedule.Schedule.work_on:12
msgid "func_name"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:-1
msgid "Optional[str] = None"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:12
msgid "The name of the function"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:17
msgid ""
"The block retrieved IndexError is raised if 0 or multiple blocks exist "
"with the specific name."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:1
msgid "Get the leaf blocks of a specific block/loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:-1
#: tvm.tir.schedule.schedule.Schedule.tensorize:-1
msgid "Union[BlockRV, LoopRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:6
msgid "The query block/loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:10
msgid "blocks"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:11
msgid "A list of leaf blocks inside a specific block/loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_consumers:1
msgid "Get the consumers of a specific block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_consumers:6
#: tvm.tir.schedule.schedule.Schedule.get_producers:6
msgid "The block in the query"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_consumers:10
msgid "consumers"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_consumers:11
msgid "A list of consumers of the given block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_loops:1
msgid "Get the parent loops of the block in its scope, from outer to inner"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_loops:6
msgid "The query block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_loops:10
msgid "loops"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_loops:11
msgid "A list of loops above the given block in its scope, from outer to inner"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:1
msgid ""
"Get the list of output blocks within the given scope An output block is a"
" block which has atleast one buffer being written to, but is not "
"allocated within the PrimFunc"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:7
msgid "scope_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:-1
msgid "Union[BlockRV, str],"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:8
msgid "The scope block from which output blocks are collected"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:12
msgid "output_blocks"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:13
msgid "A list of all blocks that write to some output buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_producers:1
msgid "Get the producers of a specific block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_producers:10
msgid "producers"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_producers:11
msgid "A list of producers of the given block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_sref:1
msgid ""
"Returns the corresponding sref to the given 1) LoopRV 2) BlockRV 3) Block"
" 4) For"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_sref:9
msgid "rand_var_or_stmt"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_sref:-1
msgid "Union[BlockRV, LoopRV, Block, For]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_sref:-1
msgid "Optional[StmtSRef]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.loop_partition:1
msgid ""
"Partition a loop into a list of consecutive loops. It requires: 1) The "
"loop can't have annotation or thread binding. Predicates may be added to "
"ensure the total loop numbers keeps unchanged. In `factors`, at most one "
"of the factors can be None, which will be automatically inferred."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.loop_partition:10
msgid "The loop to be partition"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.loop_partition:12
#: tvm.tir.schedule.schedule.Schedule.split:13
msgid "factors: List[Union[int, ExprRV, None]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.loop_partition:13
msgid ""
"The partitioning factors Potential inputs are: - None - ExprRV - Positive"
" constant integers"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.loop_partition:24
msgid "partition_loops"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.loop_partition:25
msgid "The new loops after partition"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.loop_partition:30
msgid "Before partition, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.loop_partition:43
msgid "Create the schedule and do partition:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.loop_partition:52
msgid "After applying partition, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.merge:1
msgid ""
"Merge a list of loops into one. The loops under their LCA requires: 1) "
"Under the same scope. 2) Can't have annotations or thread bindings. 3) "
"Start with 0 and have same extent and same nesting depth. 4) From target "
"loop to their LCA, The inner loop must be the only child of the outer "
"loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.merge:10
msgid "The loops to be merged"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.merge:15
msgid "The new loop after merge"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.merge:20
msgid "Before applying merge, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:1
msgid "Pad the computation of Einsum."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:3
msgid ""
"On a block with trivial binding, this primitive pads the iteration domain"
" of the block by the given padding factors, for example, 127 -> 128, 132 "
"-> 144 when padding factor is 16. Extra producer and consumer padding "
"blocks will be generated to avoid out-of-bound buffer access."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:8
msgid ""
"Einsum pattern means all the indices on the buffer access are either by "
"constants (e.g. B[0]) or by variables (e.g. B[i]), but not by composite "
"expressions (e.g. B[i + 1])."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:14
msgid "The block that matches the Einsum pattern."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:16
msgid "padding"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:-1
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:-1
#: tvm.tir.schedule.schedule.Schedule.sample_categorical:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:-1
msgid "List[int]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:17
msgid "The padding for each block iter."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:22
msgid "Before applying pad-einsum, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:39
msgid "Create the schedule and do pad-einsum with specified block:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:1
msgid ""
"Parallelize the input loop. It requires: 1) The scope block that the loop"
" is in should have stage-pipeline property 2) All the blocks under the "
"loop are complete blocks or reduction blocks, and have affine bindings 3)"
" For each block under the loop, the loop can only be contained in data-"
"parallel block iters' bindings"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:11
msgid "The loop to be parallelized"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:16
msgid "Before parallel, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:29
msgid "Create the schedule and do parallel:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:37
msgid "After applying parallel, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:1
msgid ""
"Create a block that read/write a buffer region into a read/write cache "
"with reindexing. The layout of the cache will be the same as by the "
"iterators of the block that reads/writes the buffer. It requires: 1) "
"There is only one block who reads/writes the target buffer 2) There is "
"only one buffer load/store of this buffer in the block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:9
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:6
#: tvm.tir.schedule.schedule.Schedule.transform_layout:5
msgid "block : Union[BlockRV, str]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:11
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:8
#: tvm.tir.schedule.schedule.Schedule.transform_layout:7
msgid ""
"The block that accesses the target buffer.  If a string, this must "
"uniquely identify a block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:14
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:11
#: tvm.tir.schedule.schedule.Schedule.transform_layout:10
msgid "buffer: Union[Tuple[str,int], Buffer, str]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:16
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:13
#: tvm.tir.schedule.schedule.Schedule.transform_layout:12
msgid ""
"The buffer to be transformed, or a specification of how to identify the "
"buffer to be transformed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:19
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:16
#: tvm.tir.schedule.schedule.Schedule.transform_layout:15
msgid ""
"If `buffer` if a tuple of ``(str,int)``, the first item should be either "
"\"read\" or \"write\", and the second item is an index into the block's "
"read or write regions."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:23
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:20
#: tvm.tir.schedule.schedule.Schedule.transform_layout:19
msgid ""
"If `buffer` is a string, it is the name of the buffer, which must exist "
"within the reads/writes of the block.  In addition, the reads/writes of "
"the block may not contain more than one buffer with this name."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:28
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:25
#: tvm.tir.schedule.schedule.Schedule.transform_layout:24
msgid ""
"If `buffer` is a Buffer object, it must exist within the reads/writes of "
"the block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:33
msgid "reindex_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:34
msgid "The block of the reindex stage"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:39
msgid "Before reindex, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:53
msgid "Create the schedule and do reindex:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:61
msgid "After applying reindex, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:1
msgid ""
"Create a block that reads a buffer region into a read cache using "
"customized indices specified by index map. The read region of the buffer "
"must be a single point."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:4
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:4
msgid ""
"The cache stage block follows the original order of loops and block "
"itervars in the block. If a block itervar does not appear in the buffer "
"access region, it and its corresponding loop variables will be omitted. "
"User can then use `transform_block_layout` primitive to reorder the block"
" itervars and surrounding loops of the cache read/write block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:9
msgid ""
"Unlike `cache_read`, `reindex_cache_read` only supports single consumer, "
"please use `cache_read` when there are multiple consumers."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:17
msgid "The index of the buffer in block's read region."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:20
msgid "index_map: Union[IndexMap, Callable]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:21
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:21
msgid ""
"User defined indices to access allocated cache buffer, maps from block "
"iter vars."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:30
msgid "Before reindex_cache_read, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:43
msgid "Create the schedule and reindex_cache_read:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:52
msgid "After applying reindex_cache_read, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:71
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:74
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:58
msgid "See Also"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:72
msgid ""
"reindex_cache_write transform_block_layout transform_layout cache_read "
"reindex"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:1
msgid ""
"Create a block that reads a buffer region into a write cache using "
"customized indices specified by index map. The write region of the buffer"
" must be a single point."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:9
msgid ""
"Unlike `cache_write`, `reindex_cache_write` only supports single "
"consumer, please use `cache_write` when there are multiple consumers."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:17
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:21
#: tvm.tir.schedule.schedule.Schedule.storage_align:11
msgid "The index of the buffer in block's write region."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:20
msgid "index_map: Union[Callable, IndexMap]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:33
msgid "Before reindex_cache_write, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:46
msgid "Create the schedule and reindex_cache_write:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:55
msgid "After applying reindex_cache_write, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:75
msgid ""
"reindex_cache_read transform_block_layout transform_layout cache_write "
"reindex"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.remove_rv:1
msgid "Remove a random variable from the symbol table"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.remove_rv:5
msgid "rand_var"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.remove_rv:-1
msgid "Union[BlockRV, LoopRV, ExprRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.remove_rv:6
msgid "The random variable to be removed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:1
msgid ""
"Reorder a list of loops. It doesn't require the loops to be consecutive. "
"It requires: 1) The loops are in the same chain. That means: the loops "
"can be ordered to [l_1, l_2, ... , l_n] where l_i is an ancestor of "
"l_{i+1} and there are only single-branch loops between l_1 and l_n (which"
" also indicates they are under the same scope). 2) After reordering, the "
"domain of an outer loop cannot depend on any of the inner loops. 3) For "
"every block under the loop nests, its block binding must be affine, and "
"the block variables must be either data parallel or reduction. 4) No "
"duplicated loops are allowed in the arguments."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:13
msgid "*ordered_loops"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:14
msgid "The loops in the new order"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:19
msgid "Before reorder, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:32
msgid "Create the schedule and do reorder:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:41
msgid "After applying reorder, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:1
msgid "Reorder the itervars inside a given block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:6
msgid "The block to be transformed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:7
msgid "new_order"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:8
msgid "The new block itervar order."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:13
msgid "Before reorder_block_iter_var, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:30
msgid "Create the schedule and do reorder_block_iter_var:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:38
msgid "After applying reorder_block_iter_var, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:59
msgid "reorder"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:1
msgid ""
"Reverse-Compute-At. Move a consumer block under the specific loop, and "
"regenerate the loops induced by the block so that the buffer region "
"consumed by the consumer block could cover those regions produced by its "
"producer blocks under the given loop. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:13
msgid "All the producers of the block are under the given loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:35
msgid "Before reverse-compute-at, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:53
msgid "Create the schedule and do reverse-compute-at:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:63
msgid "After applying reverse-compute-at, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:1
msgid "Inline a block into its only producer. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:3
msgid ""
"The block is a complete non-root block, which only produces and consumes "
"one buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:7
msgid ""
"The only producer of the block is a read-after-write producer and a "
"complete non-root block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:10
msgid ""
"The body of the block must be a BufferStore statement in the form of, "
"``B[f(i, j, k, ...)] = g(i, j, k, A[i, j, k, ...] ...)`` where the "
"indices of each `BufferLoad` on the RHS are all distinct atomic "
"variables, and no variables other than those indexing variables are "
"allowed in the statement."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:19
msgid "The block to be inlined to its producer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:24
msgid "Before reverse-compute-inline, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:42
msgid "Create the schedule and do reverse-compute-inline:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:50
msgid "After applying reverse-compute-inline, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:1
msgid "Factorize an associative reduction block by the specified loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:3
msgid ""
"An associative reduction cannot be parallelized directly, because it "
"leads to potential race condition during accumulation. Alternatively, the"
" reduction could be factorized on a loop with the following steps: - Step"
" 1: evenly slice the reduction into `n` separate chunks, where `n` is the"
" loop extent - Step 2: compute the chunks separately and write the result"
" into `n` intermediate buffers; - Step 3: accumulate the `n` separate "
"buffer into the result buffer. Note that the Step 2 above introduces "
"opportunities for parallelization."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:11
msgid ""
"RFactor is a schedule primitive that implements the transformation "
"described above: Given a block that writes to buffer `B`, it factorizes a"
" loop of extent `n`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:14
msgid "For example, the pseudocode below accumulates `B[i] = sum(A[i, : , : ])`:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:23
msgid ""
"Suppose RFactor is applied on the innermost loop `k` and `factor_axis = "
"1`. RFactor then creates an intermediate buffer and two blocks."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:26
msgid ""
"1. The intermediate buffer, or \"rf-buffer\" is a buffer of rank `ndim(B)"
" + 1` and size `size(B) * n`, whose shape expands from `shape(B)` by "
"adding an axis of `n` at the position specified by `factor_axis`. For "
"example,"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:30
msgid "shape(B) = [1, 2, 3], factor_axis = 0  => shape(B_rf) = [n, 1, 2, 3]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:31
msgid "shape(B) = [1, 2, 3], factor_axis = 1  => shape(B_rf) = [1, n, 2, 3]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:32
msgid "shape(B) = [1, 2, 3], factor_axis = 2  => shape(B_rf) = [1, 2, n, 3]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:33
msgid "shape(B) = [1, 2, 3], factor_axis = 3  => shape(B_rf) = [1, 2, 3, n]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:35
msgid ""
"2. The rfactor block, or \"rf-block\", is a block that writes to the `rf-"
"buffer` without accumulating over the loop `k`, i.e. the loop `k` is "
"converted from a reduction loop to a data parallel loop. In our example, "
"the rf-block is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:48
msgid ""
"3. The write-back block, or `wb-block`, is a block that accumulates the "
"rf-buffer into the result buffer. All the reduction loops are removed "
"except the loop `k` for accumulation. In our example, the wb-block is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:63
msgid "The loop outside block for which we want to do rfactor"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:64
msgid "factor_axis"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:65
msgid ""
"The position where the new dimension is placed in the new introduced "
"rfactor buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:69
msgid "rf_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:70
msgid ""
"The block which computes partial results over each slices (i.e., the "
"first block as described in the above illustration)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:76
msgid "Before rfactor, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:91
msgid "Create the schedule and do rfactor:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:100
msgid "After applying rfactor, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:126
msgid ""
"Rfactor requires: 1) `loop` has only one child block, and it is a "
"reduction block; 2) `loop` is a reduction loop, i.e. the loop variable is"
" bound to only reduction variables in the block binding; 3) `loop` is not"
" parallelized, vectorized, unrolled or bound to any thread axis; 4) The "
"block scope that `loop` is in is a staged-pipeline; 5) The outermost loop"
" outside the reduction block should has the reduction block as its first "
"child block; 6) The outermost reduction loop should have only one child "
"block; 7) An unary extent loop that is not bound to any reduction or data"
" parallel variables in the block binding should not appear under some "
"reduction loop; 8) The reduction block should write to only one buffer, "
"and its init and body are both simple `BufferStore`s, and the pattern is "
"registered as an associative reducer. The pre-defined patterns include: "
"plus, multiplication, min and max; 9) Each of the loops on top of the "
"block cannot be bound to a data parallel and a reduction block binding at"
" the same time; 10) `factor_axis` should be in range `[-ndim(B) - 1, "
"ndim(B)]`, where `B` is the buffer that the reduction block writes to. "
"Negative indexing is normalized according to numpy convention."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:1
msgid ""
"Compute the target buffer via rolling buffering, select the outermost "
"rollable axis with a positive bound overlap that appears in the block's "
"ancestor loops as `rolling axis`, fold and circularize the buffer along "
"the rolling dimension, append block predicate to avoid recomputing "
"overlapping elements. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:6
msgid "The block is not an output block and has only RAW dependencies."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:8
msgid "The buffer to be an intermediate buffer defined via `alloc_buffer`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:10
msgid ""
"3) The LCA of the producer and consumer of the buffer is a for loop, "
"typically, the producer and consumer of the buffer are cascaded through "
"compute_at."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:13
msgid ""
"4) The access region of the buffer has at least one dimension that "
"contains a positive bound overlap."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:19
#: tvm.tir.schedule.schedule.Schedule.storage_align:9
msgid "The producer block of the buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:20
msgid "write_buffer_index"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:26
msgid "Before rolling_buffer, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:55
msgid "Create the schedule and do rolling_buffer:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:63
msgid "After applying rolling_buffer, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:96
msgid ""
"The region_cover property of the consumer block of the target buffer will"
" become false."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:1
msgid "Sample an integer given the probability distribution"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:5
msgid "candidates"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:6
msgid "The candidates to be sampled from"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:7
msgid "probs"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:-1
msgid "List[float]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:8
msgid "The probability of each candidate"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:9
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:7
#: tvm.tir.schedule.trace.Trace.append:7
#: tvm.tir.schedule.trace.Trace.get_decision:10
#: tvm.tir.schedule.trace.Trace.with_decision:8
msgid "decision"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:-1
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:-1
msgid "Optional[int]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:10
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:14
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:12
msgid "The sampling decision, if any"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:-1
msgid "ExprRV"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:15
msgid "The random variable sampled from candidates"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_compute_location:1
msgid "Sample a compute-at location of the given block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_compute_location:6
msgid "The block whose compute-at location is to be sampled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_compute_location:8
msgid "The sampling decision"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_compute_location:13
msgid "The sampled loop where the input block is to be computed at"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:1
msgid "Sample the factors to a partitioned tile for a specific loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:6
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:6
msgid "The loop to be tiled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:7
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:7
msgid "n"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:8
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:8
msgid "The number of tiles to be sampled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:9
msgid "partition_pos"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:10
msgid "The position to partition tiles to two parts"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:11
msgid "innerpart_factor"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:12
msgid "The factor of the second part"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:13
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:11
msgid "decision: Optional[List[int]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:-1
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:-1
msgid "List[ExprRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:19
msgid "A list of length `n`, the random partitioned tile sizes sampled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:1
msgid "Sample the factors to perfect tile a specific loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:9
msgid "max_innermost_factor"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:10
msgid "The maximum tile size allowed to be sampled in the innermost loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:17
msgid "A list of length `n`, the random perfect tile sizes sampled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.seed:1
msgid "Seed the randomness"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.seed:6
msgid "The new random seed, -1 if use device random, otherwise non-negative"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:1
msgid ""
"Set the axis separator of a buffer, where the buffer is specified by a "
"block and a read or write index."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:28
msgid "axis_separators : Optional[List[int]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:30
msgid "The axis separators."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:35
msgid "Before set_axis_separator, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:54
msgid "Create the schedule and do set_axis_separator:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:63
msgid "After applying set_axis_separator, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:1
msgid ""
"Set the storage scope of a buffer, where the buffer is specified by the a"
" block and a write-index."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:7
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:10
msgid "The producer block of the buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:8
#: tvm.tir.schedule.schedule.Schedule.storage_align:10
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:11
msgid "buffer_index"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:9
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:12
msgid "The index of the buffer in block's write region"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:10
msgid "storage_scope"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:11
msgid "The storage scope to be set"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:16
msgid "Before set_scope, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:43
msgid "After applying set_scope, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:64
msgid ""
"`set_scope` requires the buffer to be an intermediate buffer defined via "
"`alloc_buffer`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.show:1
#: tvm.tir.schedule.trace.Trace.show:1
msgid "A sugar for print highlighted TVM script."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.show:3
msgid ""
"All parameters are forwarded to the underlying `Module.show` and "
"`Trace.show` methods."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:1
msgid ""
"Split a loop into a list of consecutive loops. It requires: 1) The loop "
"can't have annotation or thread binding. 2) The loop must start with 0. "
"Predicates may be added to ensure the total loop numbers keeps unchanged."
" In `factors`, at most one of the factors can be None, which will be "
"automatically inferred."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:11
msgid "The loop to be split"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:14
msgid ""
"The splitting factors Potential inputs are: - None - ExprRV - Positive "
"constant integers"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:23
msgid "disable_predication"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:24
msgid ""
"If enabled, don't create a predicate for guarding the loop. This can be "
"useful when splitting with scalable factors that the schedule writer "
"knows are divisible by the loop bound."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:28
msgid ""
"Warning: enabling this feature may result in incorrect code generation if"
" not used carefully."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:33
msgid "split_loops"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:34
msgid "The new loops after split"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:39
msgid "Before split, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:52
msgid "Create the schedule and do split:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:61
msgid "After applying split, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:1
msgid ""
"Set alignment requirement for specific dimension such that stride[axis] "
"== k * factor + offset for some k. This is useful to set memory layout "
"for more friendly memory access pattern. For example, we can set "
"alignment to be factor=2, offset=1 to avoid bank conflict for thread "
"access on higher dimension in GPU shared memory."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:12
msgid "axis"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:13
msgid "The dimension to be specified for alignment."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:14
msgid "factor"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:15
msgid "The factor multiple of alignment."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:16
msgid "offset"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:17
msgid "The required offset factor."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:22
msgid "Before storage_align, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:40
msgid "Create the schedule and do storage_align:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:48
msgid "After applying storage_align, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:67
msgid "After lowering passes, buffer B will have strides as [129, 1]."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:71
msgid ""
"Storage_align requires the buffer to be an intermediate buffer defined "
"via `alloc_buffer`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:1
msgid "Tensorize the computation enclosed by loop with the tensor intrinsic."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:6
msgid "The loop to be tensorized."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:7
msgid "tensor_intrin"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:8
msgid "The tensor intrin or the name of the tensor intrin."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:15
msgid "Before tensorize, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:36
msgid "Declare and register the tensor intrinsic:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:80
msgid "Create the schedule and do tensorize:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:90
msgid "After applying tensorize, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:1
msgid "Apply a transformation represented by IndexMap to block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:6
msgid "The block to be transformed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:8
msgid "index_map"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:-1
msgid "Union[IndexMap, Callable]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:9
#: tvm.tir.schedule.schedule.Schedule.transform_layout:29
msgid "The transformation to apply."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:14
msgid "Before transform_block_layout, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:28
msgid "Create the schedule and do transform_block_layout:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:36
msgid "After applying transform_block_layout, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:1
msgid "Apply a transformation represented by IndexMap to buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:27
msgid "index_map : Union[IndexMap, Callable]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:31
msgid ""
"If `index_map` is a callable, and the returned list contains "
"IndexMap.AXIS_SEPARATOR, the SetAxisSeparators primitive will be called "
"in addition to the TransformLayout primitive."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:36
msgid "pad_value: Optional[Union[int, float, PrimExpr, IndexMap, Callable]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:38
msgid ""
"The value to be used for any padding introduced by the transformation.  "
"If the schedule contains a producer block for the specified buffer, the "
"pad value will be written as part of the producer block if possible, or "
"after the producer block otherwise.  Otherwise, if the buffer is an "
"input, will insert an annotation block to state that the padding contains"
" the known value."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:46
msgid ""
"The pad value may not contain instances of BufferLoad, except where it "
"loads a value from the buffer being transformed (e.g. to create a "
"circular buffer with padding that consists of repeated elements)."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:51
msgid ""
"Note: If applied to an input buffer, the calling scope is responsible for"
" ensuring that the pad_value is present. Algebraic symplifications, "
"branch elimination, and other optimizations may assume that this "
"precondition is met, and may result in incorrect results being returned."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:57
msgid "If None, the transformation may not introduce padding."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:59
msgid ""
"If an int, float or PrimExpr, the transformation is the specific value to"
" be present in the padding."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:62
msgid ""
"If an IndexMap or Callable, the transformation is the value to be present"
" in the padding in terms of the transformed index."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:66
msgid "assume_injective_transform : bool"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:68
msgid ""
"If set to true, the schedule  primitive will assume the index_map is "
"injective and skip checking overlapping of the mapped indices. This can "
"be useful for complicated index_map that the analysis does not cover. It "
"is the callers' responsibility to ensure the index map is injective, "
"otherwise, the correctness of the schedule is not guaranteed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:75
msgid "Before transform_layout, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:93
msgid "Create the schedule and do transform_layout:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:102
msgid "After applying transform_layout, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unannotate:1
msgid "Unannotate a block/loop's annotation with key ann_key"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unannotate:6
msgid "The block/loop to be unannotated"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unannotate:13
msgid "Before unannotate, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unannotate:35
msgid "After applying unannotate, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:1
msgid "Unroll the input loop. It requires nothing"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:6
msgid "The loop to be unrolled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:11
msgid "Before unroll, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:24
msgid "Create the schedule and do unroll:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:32
msgid "After applying unroll, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:1
msgid ""
"Hide some buffer access in a given block. This is an unsafe schedule "
"primitive."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:6
msgid "The block where we hide read access."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:7
msgid "buf_type"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:8
msgid "The buffer type: \"read\"/\"write\"."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:9
msgid "buf_index_array"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:10
msgid "The array of buffer indices we hide access."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:14
msgid ""
"This schedule primitive is unsafe, and may fail dependency analysis. One "
"use case of `unsafe_hide_buffer_access` is to hide the buffer access to "
"indices buffers (e.g. in sparse computation) so that we can further "
"tensorize the block (the indices buffers appeared in read/write regions "
"may fail the pattern matching in `tensorize` primitive, and hide the "
"access to these buffers could address the issue)."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:1
msgid ""
"Set the data type of a buffer, where the buffer is specified by the a "
"block and write-index."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:4
msgid ""
"This schedule primitive is unsafe and may change the correctness of "
"program because of type conversion, please use with caution."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:13
msgid "dtype"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:14
msgid "The data type to be set"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:19
msgid "Before unsafe_set_dtype, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:38
msgid "Create the schedule and do unsafe_set_dtype:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:46
msgid "After applying set_dtype, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:67
msgid ""
"`unsafe_set_dtype` requires the buffer to be an intermediate buffer "
"defined via `alloc_buffer`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:1
msgid ""
"Vectorize the input loop. It requires: 1) The scope block that the loop "
"is in should have stage-pipeline property 2) All the blocks under the "
"loop are complete blocks or reduction blocks, and have affine bindings 3)"
" For each block under the loop, the loop can only be contained in data-"
"parallel block iters' bindings"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:11
msgid "The loop to be vectorized"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:16
msgid "Before vectorize, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:29
msgid "Create the schedule and do vectorize:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:37
msgid "After applying vectorize, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.work_on:1
msgid "Instruct the schedule to work on a function in the IRModule."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.work_on:3
msgid ""
"By default, the schedule works on the function with the name \"main\", or"
" the only function in the IRModule if there is only one. If there is "
"multiple functions in the IRModule, and none of their names are \"main\","
" users will have to call this method to explicitly specify which function"
" to work on."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.work_on:8
msgid ""
"This sugar function will guide the `GetBlock` method if its `func_name` "
"is not specified."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.work_on:13
msgid "The name of the function to work on."
msgstr ""

#: of tvm.tir.schedule.Schedule.func_working_on:1
msgid ""
"Returns the GlobalVar of the func that the schedule is currently working "
"on"
msgstr ""

#: of tvm.tir.schedule.Schedule.mod:1
msgid "Returns the AST of the module being scheduled"
msgstr ""

#: of tvm.tir.schedule.Schedule.state:1
msgid "Returns the ScheduleState in the current schedule class"
msgstr ""

#: of tvm.tir.schedule.Schedule.trace:1
msgid "Returns the internally maintained trace of scheduling program execution"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleDebugMask:1
msgid "The bitmask of the `debug_mask` flag in the ScheduleState class."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleDebugMask:3
msgid ""
"If the `debug_mask` flag has a certain bit on, then the correpsonding "
"verification pass will be conducted. For example, if `(debug_mask & "
"VERIFY_SREF_TREE) != 0`, then the correctness of the sref tree will be "
"verified after each schedule instruction."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleDebugMask:9
msgid "VERIFY_SREF_TREE"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleDebugMask:10
msgid "Verify the correctness of the sref tree"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleDebugMask:11
msgid "VERIFY_CACHED_FLAGS"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleDebugMask:12
msgid "Verify the correctness of affine_binding, region_cover and stage_pipeline"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:1
msgid ""
"The state of scheduling, which exposes a `Replace` method as the primary "
"resort for all the scheduling primitives to manipulate the TensorIR."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:4
msgid ""
"The data structure contains the following information 1) The AST being "
"scheduled (mod) 2) The sref tree of schedulable statements (indicated by "
"the srefs) 3) The dependency information of each block scope (block_info)"
" 4) A reverse mapping from the AST nodes to that in the sref tree "
"(get_sref) 5) A debug flag, if set, extra checking is enabled "
"(debug_mask) 6) A enable check flag, if False, some prerequisite checks "
"are disabled."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:-1
msgid "IRModule"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:15
msgid "The AST of the module being scheduled"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:17
msgid ""
"Do extra correctness checking after the object construction and each time"
" after calling the Replace method."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:20
msgid ""
"Indicates whether we enable prerequisite checks for some schedule "
"primitives or not, defaults to `True`."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1
msgid "Construct a schedule state from an IRModule or a PrimFunc"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:1
msgid "Get the cached flags of the corresponding block"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:5
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:5
msgid "block_sref"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:6
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:6
msgid "The block sref to be retrieved"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:10
msgid "flags"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:-1
msgid "CachedFlags"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:11
msgid "Three flags: affine_binding, region_cover, stage_pipeline"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:15
msgid "It is an API intended for internal testing use."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.get_block_scope:1
msgid "Get the BlockScope correpsonding to the block sref"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.get_block_scope:10
#: tvm.tir.schedule.state.ScheduleState.get_sref:10
msgid "sref"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.get_block_scope:11
#: tvm.tir.schedule.state.ScheduleState.get_sref:11
msgid "The corresponding sref"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.get_sref:1
msgid "Return the corresponding sref that points to the stmt"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.get_sref:5
msgid "stmt"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.get_sref:-1
msgid "Union[Block, For]"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.get_sref:6
msgid "The schedulable statement in the TensorIR to be retrieved for its sref"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:1
msgid ""
"Replace the part of the AST, as being pointed to by `src_sref`, with a "
"specific statement `tgt_stmt`, and maintain the sref tree accordingly. "
"Replace will try to perform copy on write as much as possible when the "
"ScheduleState holds the only copy to the IRModule and IR nodes."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:6
msgid ""
"Only 3 types of replacements are allowed: from `src_sref->stmt` to "
"`tgt_stmt`. 1) Block -> Block 2) Loop -> Loop 3) Loop -> BlockRealize"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:13
msgid "src_sref"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:14
msgid "The sref to the statement to be replaced in the TensorIR AST"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:16
msgid "tgt_stmt"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:-1
msgid "Union[Block, For, BlockRealize]"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:17
msgid "The statement to be replaced to"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:19
msgid "block_sref_reuse"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:-1
msgid "Optional[Dict[Block, Block]] = None"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:20
msgid ""
"Maps an old block (to be replaced in the subtree under `src_sref->stmt`) "
"to a new block (replaced to, in the subtree under `tgt_stmt`), and "
"enforces reuse of srefs between them (rather than create new srefs) i.e. "
"after being replaced, the sref that points to the old block will point to"
" the new one"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:27
msgid ""
"The reuse of loop srefs are detected automatically according to the reuse"
" of loop vars."
msgstr ""

#: of tvm.tir.block_scope.StmtSRef:1
msgid ""
"An object that refers to schedulable elements in the TensorIR, aka "
"\"sref\"."
msgstr ""

#: of tvm.tir.block_scope.StmtSRef:3
msgid ""
"Glossary - Block sref: An StmtSref that points to a TensorIR block. - "
"Loop sref: An StmtSRef that points to a TensorIR for loop. - Parent sref:"
" The parent sref of an sref is the block/loop sref that points to its "
"closest schedulable statement of its ancestors on the TensorIR AST. - "
"Root sref: Sref to the root block. Every sref has exactly one parent sref"
" except for root sref. - Sref tree: The parent-children-relationship of "
"srefs that forms a tree, uniquely determined by the TensorIR AST."
msgstr ""

#: of tvm.tir.block_scope.StmtSRef.inline_mark:1
msgid ""
"A special StmtSRef, which doesn't point to any stmt in the AST, only "
"serving as a \"mark\" to hint compute-at to do the work of compute-inline"
msgstr ""

#: of tvm.tir.block_scope.StmtSRef.root_mark:1
msgid ""
"A special StmtSRef, which doesn't point to any stmt in the AST, only "
"serving as a \"mark\" to hint compute-at to do nothing"
msgstr ""

#: of tvm.tir.schedule.StmtSRef.parent:1
msgid "The parent sref"
msgstr ""

#: of tvm.tir.schedule.StmtSRef.stmt:1
msgid "The block/for stmt the object refers to"
msgstr ""

#: of tvm.tir.schedule.trace.Trace:1
msgid "An execution trace of a scheduling program."
msgstr ""

#: of tvm.tir.schedule.trace.Trace:3
msgid ""
"A trace has two parts: 1) The instructions invoked so far 2) The random "
"decisions made upon those instructions, if any"
msgstr ""

#: of tvm.tir.schedule.trace.Trace:7
msgid ""
"A trace can be serialized to: 1) Roundtrippable JSON format: can be saved"
" to file and loaded back 2) Python syntax: allows users to copy-paste the"
" trace to reproduce the scheduling process"
msgstr ""

#: of tvm.tir.schedule.trace.Trace:11
msgid ""
"A trace can be applied to a TensorIR schedule by re-applying all its "
"instructions possibly with their decisions accordingly. Re-sampling is "
"invoked if a sampling instruction doesn't have its corresponding "
"decision; Otherwise the existing decision will be reused accordingly."
msgstr ""

#: of tvm.tir.schedule.trace.Trace:17 tvm.tir.schedule.trace.Trace.__init__:5
#: tvm.tir.schedule.trace.Trace.append:5
#: tvm.tir.schedule.trace.Trace.get_decision:5
msgid "insts"
msgstr ""

#: of tvm.tir.schedule.trace.Trace:-1 tvm.tir.schedule.trace.Trace.__init__:-1
msgid "List[Instruction]"
msgstr ""

#: of tvm.tir.schedule.trace.Trace:18 tvm.tir.schedule.trace.Trace.__init__:6
msgid "The instructions invoked so far in the program execution"
msgstr ""

#: of tvm.tir.schedule.trace.Trace:19 tvm.tir.schedule.trace.Trace.__init__:7
msgid "decisions"
msgstr ""

#: of tvm.tir.schedule.trace.Trace:-1 tvm.tir.schedule.trace.Trace.__init__:-1
msgid "Dict[Instruction, DECISION_TYPE]"
msgstr ""

#: of tvm.tir.schedule.trace.Trace:20 tvm.tir.schedule.trace.Trace.__init__:8
msgid "The random decisions made upon those instructions"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.append:1
msgid "Append a new instruction to the trace"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.append:-1
#: tvm.tir.schedule.trace.Trace.get_decision:-1
#: tvm.tir.schedule.trace.Trace.pop:-1
#: tvm.tir.schedule.trace.Trace.with_decision:-1
msgid "Instruction"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.append:6
msgid "The new instruction to be appended"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.append:-1
msgid "Optional[DECISION_TYPE] = None"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.append:8
msgid "The random decision made on this instruction"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_json_to_schedule:1
msgid "Apply a JSON-serialized trace to a TensorIR schedule"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_json_to_schedule:5
msgid "json_obj"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_json_to_schedule:-1
msgid "JSON_TYPE"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_json_to_schedule:6
msgid "The JSON-serialized trace"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_json_to_schedule:7
#: tvm.tir.schedule.trace.Trace.apply_to_schedule:5
msgid "sch"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_json_to_schedule:8
msgid "The TensorIR schedule"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_to_schedule:1
msgid "Apply the trace to a TensorIR schedule"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_to_schedule:6
msgid "The schedule to be applied onto"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_to_schedule:7
#: tvm.tir.schedule.trace.Trace.as_json:5
#: tvm.tir.schedule.trace.Trace.as_python:5
#: tvm.tir.schedule.trace.Trace.simplified:5
#: tvm.tir.schedule.trace.Trace.with_decision:10
msgid "remove_postproc"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_to_schedule:8
#: tvm.tir.schedule.trace.Trace.as_json:6
#: tvm.tir.schedule.trace.Trace.as_python:6
#: tvm.tir.schedule.trace.Trace.simplified:6
#: tvm.tir.schedule.trace.Trace.with_decision:11
msgid "If postprocessing instructions are removed"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_to_schedule:9
msgid "decision_provider: Optional[Callable] = None"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.apply_to_schedule:10
msgid ""
"A callback that allows users to mutate decisions on the fly when applying"
" instructions. The signature of the callback is: - The 1st argument: The "
"instruction - The 2nd argument: The input random variables - The 3rd "
"argument: The attributes - The 4th argument: The decision - Return: A new"
" decision"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.as_json:1
msgid "Serialize the trace as a JSON-style object"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.as_json:-1
#: tvm.tir.schedule.trace.Trace.as_python:-1
msgid "bool = False"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.as_json:10
msgid "json: JSON_TYPE"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.as_json:11
msgid "The JSON-style object"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.as_python:1
msgid "Serialize the trace as a sequence of python statements"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.as_python:10
msgid "py_stmts: List[str]"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.as_python:11
msgid "A sequence of python statements"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.get_decision:1
msgid "Retrieve the decision made on a specific instruction"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.get_decision:6
msgid "The instruction whose decision is to be retrieved"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.get_decision:-1
msgid "Optional[DECISION_TYPE]"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.get_decision:11
msgid ""
"The corresponding decision; None if there is no decision made on the "
"instruction"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.pop:1
msgid ""
"Remove the last instruction, along with the decision made on that "
"instruction, if any"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.pop:5
msgid "popped_inst"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.pop:6
msgid "Returns the instruction removed; NullOpt if the trace is empty"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.show:5
msgid "style : str, optional"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.show:7
msgid ""
"Pygmentize printing style, auto-detected if None.  See "
"`tvm.script.highlight.cprint` for more details."
msgstr ""

#: of tvm.tir.schedule.trace.Trace.show:10
msgid "black_format: bool"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.show:12
msgid ""
"If true, use the formatter Black to format the TVMScript. If None, "
"determine based on the \"TVM_BLACK_FORMAT\" environment variable."
msgstr ""

#: of tvm.tir.schedule.trace.Trace.simplified:1
msgid "Simplify the trace with dead-code elimination"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.simplified:10
#: tvm.tir.schedule.trace.Trace.with_decision:15
msgid "trace: Trace"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.simplified:11
msgid "A simplified trace"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.with_decision:1
msgid ""
"Create a new trace with an instruction whose decision is changed, "
"assuming this instruction exists in the resulting trace"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.with_decision:6
msgid "inst"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.with_decision:7
msgid "The instruction whose decision is to be changed"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.with_decision:-1
msgid "DECISION_TYPE"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.with_decision:9
msgid "The decision to be changed to"
msgstr ""

#: of tvm.tir.schedule.trace.Trace.with_decision:16
msgid "The new trace with the decision changed"
msgstr ""

