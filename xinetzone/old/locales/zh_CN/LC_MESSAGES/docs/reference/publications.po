# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-04-27 19:13+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../../xin/docs/reference/publications.rst:19
msgid "Publications"
msgstr "出版物"

#: ../../../xin/docs/reference/publications.rst:21
msgid ""
"TVM is developed as part of peer-reviewed research in machine learning "
"compiler framework for CPUs, GPUs, and machine learning accelerators."
msgstr "TVM 是作为 CPU、GPU 和机器学习加速器的机器学习编译器框架的同行评审研究的一部分而开发的。"

#: ../../../xin/docs/reference/publications.rst:24
#, fuzzy
msgid ""
"This document includes references to publications describing the "
"research, results, and design that use or built on top of TVM."
msgstr "本文档包括对描述 TVM 基础研究、结果和设计的出版物的参考。"

#: ../../../xin/docs/reference/publications.rst:27
msgid "2018"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:29
#, fuzzy
msgid ""
"`TVM: An Automated End-to-End Optimizing Compiler for Deep Learning`__, "
"[Slides_]"
msgstr "`TVM: 用于深度学习的自动端到端优化编译器 <https://arxiv.org/abs/1802.04799>`_"

#: ../../../xin/docs/reference/publications.rst:34
msgid "`Learning to Optimize Tensor Programs`__, [Slides]"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:38
msgid "2020"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:40
#, fuzzy
msgid ""
"`Ansor: Generating High-Performance Tensor Programs for Deep Learning`__,"
" [Slides__] [Tutorial__]"
msgstr "`Ansor: 为深度学习生成高性能的张量程序 <https://arxiv.org/abs/2006.06762>`_"

#: ../../../xin/docs/reference/publications.rst:46
msgid "2021"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:48
#, fuzzy
msgid ""
"`Nimble: Efficiently Compiling Dynamic Neural Networks for Model "
"Inference`__, [Slides__]"
msgstr "`Nimble: 高效地编译动态神经网络的模型推理 <https://arxiv.org/abs/2006.03031>`_"

#: ../../../xin/docs/reference/publications.rst:53
msgid "`Cortex: A Compiler for Recursive Deep Learning Models`__, [Slides__]"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:58
msgid "`UNIT: Unifying Tensorized Instruction Compilation`__, [Slides]"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:62
msgid "`Lorien: Efficient Deep Learning Workloads Delivery`__, [Slides]"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:67
msgid ""
"`Bring Your Own Codegen to Deep Learning Compiler`__, [Slides] "
"[Tutorial__]"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:72
msgid "2022"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:74
msgid "`DietCode: Automatic optimization for dynamic tensor program`__, [Slides]"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:78
msgid ""
"`Bolt: Bridging the Gap between Auto-tuners and Hardware-native "
"Performance`__, [Slides]"
msgstr ""

#: ../../../xin/docs/reference/publications.rst:82
msgid ""
"`The CoRa Tensor Compiler: Compilation for Ragged Tensors with Minimal "
"Padding`__, [Slides]"
msgstr ""

#~ msgid ""
#~ "`Learning to Optimize Tensor Programs "
#~ "<https://arxiv.org/pdf/1805.08166.pdf>`_"
#~ msgstr "`学习优化张量程序 <https://arxiv.org/pdf/1805.08166.pdf>`_"

