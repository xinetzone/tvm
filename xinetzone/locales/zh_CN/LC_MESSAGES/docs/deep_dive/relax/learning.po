# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-10-09 21:52+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/deep_dive/relax/learning.rst:21
msgid "Understand Relax Abstraction"
msgstr "理解 Relax 抽象"

#: ../../doc/docs/deep_dive/relax/learning.rst:22
msgid ""
"Relax is a graph abstraction used in Apache TVM Unity strategy, which "
"helps to end-to-end optimize ML models. The principal objective of Relax "
"is to depict the structure and data flow of ML models, including the "
"dependencies and relationships between different parts of the model, as "
"well as how to execute the model on hardware."
msgstr ""
"Relax 是 Apache TVM Unity 策略中使用的一种计算图抽象工具，它有助于对 ML 模型进行端到端的优化。"
"Relax 的主要目标是描述 ML 模型的结构和数据流，包括模型各部分之间的依赖关系和相互关系，以及如何在硬件上执行模型。"

#: ../../doc/docs/deep_dive/relax/learning.rst:29
msgid "End to End Model Execution"
msgstr "端到端模型执行"

#: ../../doc/docs/deep_dive/relax/learning.rst:31
msgid ""
"In this chapter, we will use the following model as an example. This is a"
" two-layer neural network that consists of two linear operations with "
"relu activation."
msgstr ""
"在本章中，将使用以下模型作为示例。这是包含两个线性算子的两层神经网络，并采用了 ReLU 激活函数。"

#: ../../doc/docs/deep_dive/relax/learning.rst:41
msgid "High-Level Operations Representation"
msgstr "高级运算表示"

#: ../../doc/docs/deep_dive/relax/learning.rst:43
msgid "Let us begin by reviewing a Numpy implementation of the model."
msgstr "首先回顾该模型的 Numpy 实现。"

#: ../../doc/docs/deep_dive/relax/learning.rst:53
msgid ""
"The above example code shows the high-level array operations to perform "
"the end-to-end model execution. Of course, we can rewrite the above code "
"using Relax as follows:"
msgstr ""
"上述示例代码展示了执行端到端模型运算的高级数组运算。当然，可以使用 Relax 按照以下方式重写上述代码："

#: ../../doc/docs/deep_dive/relax/learning.rst:76
msgid "Low-Level Integration"
msgstr "低层次集成"

#: ../../doc/docs/deep_dive/relax/learning.rst:78
msgid ""
"However, again from the pov of machine learning compilation (MLC), we "
"would like to see through the details under the hood of these array "
"computations."
msgstr ""
"然而，从机器学习编译（machine learning compilation，MLC）的角度来看，希望能够深入了解这些数组计算背后的细节。"

#: ../../doc/docs/deep_dive/relax/learning.rst:81
msgid ""
"For the purpose of illustrating details under the hood, we will again "
"write examples in low-level numpy:"
msgstr ""
"为了详细说明底层细节，将再次以低级 numpy 为例进行编写。"

#: ../../doc/docs/deep_dive/relax/learning.rst:83
msgid ""
"We will use a loop instead of array functions when necessary to "
"demonstrate the possible loop computations. When possible, we always "
"explicitly allocate arrays via numpy.empty and pass them around. The code"
" block below shows a low-level numpy implementation of the same model."
msgstr ""
"在必要时，将使用循环而非数组函数来演示可能的循环计算。"
"只要有可能，总是通过 numpy.empty 显式地分配数组并传递它们。"
"下面的代码块展示了同一模型的低级 numpy 实现。"

#: ../../doc/docs/deep_dive/relax/learning.rst:122
msgid ""
"With the low-level NumPy example in mind, now we are ready to introduce "
"an Relax abstraction for the end-to-end model execution. The code block "
"below shows a TVMScript implementation of the model."
msgstr ""
"考虑到低级的 NumPy 示例，现在准备介绍针对端到端模型执行的 Relax 抽象。下面的代码块展示了该模型的 TVMScript 实现。"

#: ../../doc/docs/deep_dive/relax/learning.rst:175
msgid ""
"The above code contains kinds of functions: the primitive tensor "
"functions (``T.prim_func``) and a ``R.function`` (relax function). Relax "
"function is a new type of abstraction representing high-level neural "
"network executions."
msgstr ""
"以上代码包含多种函数：元张量函数 (``T.prim_func``) 和 ``R.function`` （relax 函数）。"
"Relax 函数是一种新的抽象类型，代表高层次的神经网络执行。"

#: ../../doc/docs/deep_dive/relax/learning.rst:179
msgid ""
"Note that the above relax module natively supports symbolic shapes, see "
"the ``\"n\"`` in the tensor shapes in ``main`` function and ``M``, ``N``,"
" ``K`` in the ``linear`` function. This is a key feature of Relax "
"abstraction, which enables the compiler to track dynamic shape relations "
"globally across tensor operators and function calls."
msgstr ""
"请注意，上述 Relax 模块原生支持符号形状，例如在 ``main`` 函数中的张量形状中看到的 ``\"n\"``，"
"以及 ``linear`` 函数中的 ``M``、``N`` 和 ``K``。"
"这是 Relax 抽象的关键特性，它使得编译器能够全局追踪张量运算和函数调用之间的动态形状关系。"

#: ../../doc/docs/deep_dive/relax/learning.rst:184
msgid ""
"Again it is helpful to see the TVMScript code and low-level numpy code "
"side-by-side and check the corresponding elements, and we are going to "
"walk through each of them in detail. Since we already learned about "
"primitive tensor functions, we are going to focus on the high-level "
"execution part."
msgstr ""
"再次查看 TVMScript 代码和底层 numpy 代码的并列对比，并检查相应的元素是非常有帮助的，将逐一详细地分析它们。"
"由于已经学习了元张量函数，将专注于高级执行部分。"

#: ../../doc/docs/deep_dive/relax/learning.rst:189
msgid "Key Elements of Relax"
msgstr "Relax 的关键要素"

#: ../../doc/docs/deep_dive/relax/learning.rst:190
msgid ""
"This section will introduce the key elements of Relax abstraction and how"
" it enables optimization in ML compilers."
msgstr ""
"本节将介绍 Relax 抽象化的关键要素以及它如何实现 ML 编译器中的优化。"

#: ../../doc/docs/deep_dive/relax/learning.rst:194
msgid "Structure Info"
msgstr "结构信息"

#: ../../doc/docs/deep_dive/relax/learning.rst:195
msgid ""
"Structure info is a new concept in Relax that represents the type of "
"relax expressions. It can be ``TensorStructInfo``, ``TupleStructInfo``, "
"etc. In the above example, we use ``TensorStructInfo`` (short in "
"``R.Tensor`` in TVMScript) to represent the shape and dtype of the tensor"
" of the inputs, outputs, and intermediate results."
msgstr ""
"在 Relax 中，结构信息是新概念，它表示不同类型的 relax 表达式。"
"这些类型可以是 ``TensorStructInfo``、``TupleStructInfo`` 等。"
"在上面的例子中，使用 ``TensorStructInfo`` （在 TVMScript 中简称为 ``R.Tensor``）来表示输入、输出以及中间结果的张量的形状和数据类型。"

#: ../../doc/docs/deep_dive/relax/learning.rst:201
msgid "R.call_tir"
msgstr ""

#: ../../doc/docs/deep_dive/relax/learning.rst:202
msgid ""
"The ``R.call_tir`` function is a new abstraction in Relax that allows "
"calling primitive tensor functions in the same IRModule. This is a key "
"feature of Relax that enables cross-level abstractions, from high-level "
"neural network layers to low-level tensor operations. Taking one line "
"from the above code as an example:"
msgstr ""
"``R.call_tir`` 函数是 Relax 中的新抽象概念，允许在同一 IRModule 中调用元张量函数。"
"这是 Relax 的关键特性，它支持从高层神经网络层到低层张量运算的跨级抽象。以上述代码中的一行为例："

#: ../../doc/docs/deep_dive/relax/learning.rst:211
msgid ""
"To explain what does ``R.call_tir`` work, let us review an equivalent "
"low-level numpy implementation of the operation, as follows:"
msgstr ""
"要解释 ``R.call_tir`` 是如何工作的，回顾一下其等效的低级 NumPy 实现，如下所示："

#: ../../doc/docs/deep_dive/relax/learning.rst:219
msgid ""
"Specifically, ``call_tir`` allocates an output tensor res, then pass the "
"inputs and the output to the prim_func. After executing prim_func the "
"result is populated in res, then we can return the result."
msgstr ""
"具体来说，``call_tir`` 函数首先分配输出张量 res，然后将 inputs 和 输出传递给 prim_func。"
"执行 prim_func 后，结果会被填充到 res 中，随后便可以返回该结果。"

#: ../../doc/docs/deep_dive/relax/learning.rst:223
msgid ""
"This convention is called **destination passing**, The idea is that input"
" and output are explicitly allocated outside and passed to the low-level "
"primitive function. This style is commonly used in low-level library "
"designs, so higher-level frameworks can handle that memory allocation "
"decision. Note that not all tensor operations can be presented in this "
"style (specifically, there are operations whose output shape depends on "
"the input). Nevertheless, in common practice, it is usually helpful to "
"write the low-level function in this style when possible."
msgstr ""
"这种约定被称为 **目标传递** (destination passing)，其核心思想是输入和输出在外部显式分配并传递给低级的元函数。"
"这种风格通常用于低级库设计中，以便更高级的框架可以处理内存分配的决定。"
"需要注意的是，并非所有的张量运算都可以以这种方式呈现（特别是，有些算子的输出形状依赖于输入）。"
"然而，在实践中，当可能时，以这种方式编写低级函数通常是有益的。"

#: ../../doc/docs/deep_dive/relax/learning.rst:231
msgid "Dataflow Block"
msgstr "数据流程块"

#: ../../doc/docs/deep_dive/relax/learning.rst:232
msgid ""
"Another important element in a relax function is the R.dataflow() scope "
"annotation."
msgstr ""
"在 relax 函数中另一个重要的元素是 R.dataflow() 范围注释。"

#: ../../doc/docs/deep_dive/relax/learning.rst:242
msgid ""
"Before we talk about the dataflow block, let us first introduce the "
"concept of **pure** and **side-effect**. A function is **pure** or "
"**side-effect free** if:"
msgstr ""
"在讨论数据流块之前，首先介绍“纯函数”和“副作用”的概念。函数是“纯的”或者说是“无副作用的”，如果它满足以下条件："

#: ../../doc/docs/deep_dive/relax/learning.rst:245
msgid "it only reads from its inputs and returns the result via its output"
msgstr "它仅从输入中读取数据，并通过输出返回结果。"

#: ../../doc/docs/deep_dive/relax/learning.rst:246
msgid ""
"it will not change other parts of the program (such as incrementing a "
"global counter)."
msgstr ""
"它不会改变程序的其他部分（比如增加 “全局计数器”）。"

#: ../../doc/docs/deep_dive/relax/learning.rst:248
msgid ""
"For example, all ``R.call_tir`` functions are pure functions, as they "
"only read from their inputs and write the output to another new allocated"
" tensor. However, the **inplace operations** are not pure functions, in "
"other words, they are side-effect functions, because they will change the"
" existing intermediate or input tensors."
msgstr ""
"例如，所有的 ``R.call_tir`` 函数都是纯函数，因为它们仅从输入读取数据并将输出写入另一个新分配的张量。"
"然而，原地操作不是纯函数，换句话说，它们是有副作用的函数，因为它们会改变现有的中间或输入张量。"

#: ../../doc/docs/deep_dive/relax/learning.rst:253
msgid ""
"A dataflow block is a way for us to mark the computational graph regions "
"of the program. Specifically, within a dataflow block, all the operations"
" need to be **side-effect free**. Outside a dataflow block, the "
"operations can contain side-effect."
msgstr ""
"数据流块是一种方法，用于标记程序的计算图区域。具体来说，在数据流块内部，所有运算都需要是无副作用的。在数据流块外部，运算可以包含副作用。"

#: ../../doc/docs/deep_dive/relax/learning.rst:259
msgid ""
"A common question that arises is why we need to manually mark dataflow "
"blocks instead of automatically inferring them. There are two main "
"reasons for this approach:"
msgstr ""
"常见的问题是，为什么需要手动标记数据流块而不是自动推断它们。采取这种方法有两个主要理由："

#: ../../doc/docs/deep_dive/relax/learning.rst:262
msgid ""
"Automatic inference of dataflow blocks can be challenging and imprecise, "
"particularly when dealing with calls to packed functions (such as cuBLAS "
"integrations). By manually marking dataflow blocks, we enable the "
"compiler to accurately understand and optimize the program's dataflow."
msgstr ""
"数据流块的自动推断可能会面临挑战且不够精确，尤其是在处理对打包函数（如 cuBLAS 集成）的调用时。"
"通过手动标记数据流块，可以使得编译器能够准确理解并优化程序的数据流。"

#: ../../doc/docs/deep_dive/relax/learning.rst:266
msgid ""
"Many optimizations can only be applied within dataflow blocks. For "
"instance, fusion optimization is limited to operations within a single "
"dataflow block. If the compiler were to incorrectly infer dataflow "
"boundaries, it might miss crucial optimization opportunities, potentially"
" impacting the program's performance."
msgstr ""
"许多优化只能在数据流块内应用。"
"例如，融合优化仅限于单个数据流块内的运算。如果编译器错误地推断数据流边界，可能会错过关键的优化机会，从而可能影响程序的性能。"

#: ../../doc/docs/deep_dive/relax/learning.rst:271
msgid ""
"By allowing manual marking of dataflow blocks, we ensure that the "
"compiler has the most accurate information to work with, leading to more "
"effective optimizations."
msgstr ""
"通过允许手动标记数据流块，确保编译器拥有最准确的信息进行处理，从而带来更有效的优化。"
