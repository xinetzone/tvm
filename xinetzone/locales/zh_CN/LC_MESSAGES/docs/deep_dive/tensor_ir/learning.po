# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-01-17 09:58+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.16.0\n"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:21
msgid "Understand TensorIR Abstraction"
msgstr "理解 TensorIR 抽象"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:22
msgid ""
"TensorIR is the tensor program abstraction in Apache TVM, which is one of"
" the standard machine learning compilation frameworks. The principal "
"objective of tensor program abstraction is to depict loops and associated"
" hardware acceleration options, including threading, the application of "
"specialized hardware instructions, and memory access."
msgstr ""
"TensorIR 是 Apache TVM "
"中的张量程序抽象，它是标准机器学习编译框架之一。张量程序抽象的主要目标是描述循环及其相关的硬件加速选项，包括线程化、应用专用硬件指令以及内存访问。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:27
msgid ""
"To help our explanations, let us use the following sequence of tensor "
"computations as a motivating example. Specifically, for two :math:`128 "
"\\times 128` matrices ``A`` and ``B``, let us perform the following two "
"steps of tensor computations."
msgstr ""
"为了帮助我们的解释，使用以下张量计算序列作为启发性的例子。具体来说，对于两个 :math:`128 \\times 128` 的矩阵 ``A`` "
"和 ``B``，执行以下两步张量计算。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:31
msgid ""
"Y_{i, j} &= \\sum_k A_{i, k} \\times B_{k, j} \\\\\n"
"C_{i, j} &= \\mathbb{relu}(Y_{i, j}) = \\mathbb{max}(Y_{i, j}, 0)"
msgstr ""

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:37
msgid ""
"The above computations resemble a typical primitive tensor function "
"commonly seen in neural networks, a linear layer with relu activation. We"
" use TensorIR to depict the above computations as follows."
msgstr "上述计算与神经网络中常见的基本张量函数相似，即带有 ReLU 激活的线性层。使用 TensorIR 来描述上述计算，如下所示。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:40
msgid ""
"Before we invoke TensorIR, let's use native Python codes with NumPy to "
"show the computation:"
msgstr "在调用 TensorIR 之前，先用原生的 Python 代码结合 NumPy 来展示计算过程："

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:56
msgid ""
"With the low-level NumPy example in mind, now we are ready to introduce "
"TensorIR. The code block below shows a TensorIR implementation of "
"``mm_relu``. The particular code is implemented in a language called "
"TVMScript, which is a domain-specific dialect embedded in python AST."
msgstr ""
"在记住了低级别的 NumPy 示例之后，现在准备介绍 TensorIR。下面的代码块展示了 ``mm_relu`` "
"的TensorIR实现。这段特定的代码是用一种名为 TVMScript 的语言编写的，这是一种嵌入在 Python AST 中的特定领域方言。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:84
msgid "Next, let's invest the elements in the above TensorIR program."
msgstr "接下来，将分析上述 TensorIR 程序中的各元素。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:87
msgid "Function Parameters and Buffers"
msgstr "函数参数与缓冲区"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:88
msgid ""
"**The function parameters correspond to the same set of parameters on the"
" numpy function.**"
msgstr "函数参数与 numpy 函数上的同一组参数相对应。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:101
msgid ""
"Here ``A``, ``B``, and ``C`` takes a type named ``T.Buffer``, which with "
"shape argument ``(128, 128)`` and data type ``float32``. This additional "
"information helps possible MLC process to generate code that specializes "
"in the shape and data type."
msgstr ""
"在此，变量 ``A``、``B`` 和 ``C`` 采用了名为 ``T.Buffer`` 的类型，该类型具有形状参数 ``(128, 128)``"
" 和数据类型 ``float32``。这些额外的信息有助于可能的 MLC 处理过程生成专门针对该形状和数据类型的代码。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:106
msgid ""
"**Similarly, TensorIR also uses a buffer type in intermediate result "
"allocation.**"
msgstr "同样，TensorIR 在中间结果的分配中也采用了缓冲区类型。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:116
msgid "Loop Iterations"
msgstr "循环迭代"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:117
msgid "**There are also direct correspondence of loop iterations.**"
msgstr "**循环迭代也存在直接的对应关系。**"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:119
msgid ""
"``T.grid`` is a syntactic sugar in TensorIR for us to write multiple "
"nested iterators."
msgstr "``T.grid`` 是 TensorIR 中的语法糖，它允许编写多个嵌套迭代器。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:138
msgid "Computational Block"
msgstr "计算块"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:139
msgid ""
"A significant distinction lies in computational statements: **TensorIR "
"incorporates an additional construct termed** ``T.block``."
msgstr "显著的区别在于计算语句：**TensorIR 引入了额外的构造**，称为 ``T.block`` 。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:158
msgid ""
"A **block** represents a fundamental computation unit within TensorIR. "
"Importantly, a block encompasses more information than standard NumPy "
"code. It comprises a set of block axes ``(vi, vj, vk)`` and the "
"computations delineated around them."
msgstr ""
"在 TensorIR 中，**块** 表示基础的计算单元。重要的是，块包含的信息比标准的 NumPy 代码多。它包括一组块轴 ``(vi, vj,"
" vk)`` 和围绕它们的计算。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:168
msgid ""
"The above three lines declare the **key properties** about block axes in "
"the following syntax."
msgstr "上述三行声明了关于块轴的 **关键属性**，在以下语法中。\""

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:174
msgid "These three lines convey the following details:"
msgstr "这三行文字传达了以下细节："

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:176
msgid ""
"They specify the binding of ``vi``, ``vj``, ``vk`` (in this instance, to "
"``i``, ``j``, ``k``)."
msgstr "它们指定了 ``vi``、``vj`` 和 ``vk`` （在本例中分别对应于 ``i``、``j`` 和 ``k``）的绑定关系。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:177
msgid ""
"They declare the original range intended for ``vi``, ``vj``, ``vk`` (the "
"128 in ``T.axis.spatial(128, i)``)."
msgstr "他们声明最初的范围是为 ``vi``，``vj``，``vk`` （即 ``T.axis.spatial(128, i)`` 中的 128）准备的。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:179
msgid "They announce the properties of the iterators (spatial, reduce)."
msgstr "他们声明了迭代器的属性（spatial, reduce）。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:182
msgid "Block Axis Properties"
msgstr "块轴属性"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:183
#, fuzzy
msgid ""
"Let's delve deeper into the properties of the block axis. These "
"properties signify the axis's relationship to the computation in "
"progress. The block comprises three axes ``vi``, ``vj``, and ``vk``, "
"meanwhile the block reads the buffer ``A[vi, vk]``, ``B[vk, vj]`` and "
"writes the buffer ``Y[vi, vj]``. Strictly speaking, the block performs "
"(reduction) updates to Y, which we label as write for the time being, as "
"we don't require the value of Y from another block."
msgstr ""
"更深入地探讨块轴的属性。这些属性代表了轴与进行中的计算之间的关系。该块包含三个轴 ``vi``、``vj`` 和 ``vk``，同时块读取缓冲区 "
"``A[vi, vk]`` 和 ``B[vk, vj]``，并写入缓冲区 ``Y[vi, vj]``。严格来说，块对 Y "
"执行（归约）更新操作，暂时将其标记为写操作，因为不需要来自另一个块的 Y 的值。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:189
msgid ""
"Significantly, for a fixed value of ``vi`` and ``vj``, the computation "
"block yields a point value at a spatial location of ``Y`` (``Y[vi, vj]``)"
" that is independent of other locations in ``Y`` (with different ``vi``, "
"``vj`` values). We can refer to ``vi``, ``vj`` as **spatial axes** since "
"they directly correspond to the start of a spatial region of buffers that"
" the block writes to. The axes involved in reduction (``vk``) are "
"designated as **reduce axes**."
msgstr ""
"重要的是，对于固定的 ``vi`` 和 ``vj`` 值，计算块会在空间位置 ``Y`` （即 ``Y[vi, "
"vj]``）处产生点值，这个点值与其他位置在“Y”上的值（具有不同的 ``vi``，``vj`` 值）是独立的。可以将 ``vi``、``vj``"
" 称为 **空间轴**，因为它们直接对应于该块写入的缓冲区的空间区域的起始位置。参与归约操作的轴（``vk``）被指定为 **归约轴**。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:196
msgid "Why Extra Information in Block"
msgstr "为什么 Block 中需要额外信息"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:197
msgid ""
"One crucial observation is that the additional information (block axis "
"range and their properties) makes the block to be **self-contained** when"
" it comes to the iterations that it is supposed to carry out independent "
"from the external loop-nest ``i, j, k``."
msgstr "一个关键观察是，额外的信息（块轴的范围及其属性）使得该块在它需要执行的迭代中 **自包含**，独立于外部循环嵌套 ``i, j, k``。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:201
msgid ""
"The block axis information also provides additional properties that help "
"us to validate the correctness of the external loops that are used to "
"carry out the computation. For example, the above code block will result "
"in an error because the loop expects an iterator of size 128, but we only"
" bound it to a for loop of size 127."
msgstr ""
"块轴信息还提供了其他属性，帮助验证用于执行计算的外部循环是否正确。例如，上述代码块会导致错误，因为循环期望大小为128的迭代器，而只将其绑定到大小为"
" 127 的 for 循环上。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:216
msgid "Sugars for Block Axes Binding"
msgstr "块轴绑定的语法糖"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:217
msgid ""
"In situations where each of the block axes is directly mapped to an outer"
" loop iterator, we can use ``T.axis.remap`` to declare the block axis in "
"a single line."
msgstr "在每个块轴直接映射到外部循环迭代器的情况下，可以使用 ``T.axis.remap`` 来在一行中声明块轴。"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:225
msgid "which is equivalent to"
msgstr "等同于"

#: ../../doc/docs/deep_dive/tensor_ir/learning.rst:233
msgid "So we can also write the programs as follows."
msgstr "因此，也可以如下形式编写程序。"

