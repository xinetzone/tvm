# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-09 00:02+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:20003
msgid "Reduction"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:20004
msgid "**Author**: [Tianqi Chen](https://tqchen.github.io)"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:20006
msgid ""
"This is an introduction material on how to do reduction in TVM. "
"Associative reduction operators like sum/max/min are typical construction"
" blocks of linear algebra operations."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:20010
msgid "In this tutorial, we will demonstrate how to do reduction in TVM."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:40002
msgid "Describe Sum of Rows"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:40003
msgid ""
"Assume we want to compute sum of rows as our example. In numpy semantics "
"this can be written as :code:`B = numpy.sum(A, axis=1)`"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:40006
msgid ""
"The following lines describe the row sum operation. To create a reduction"
" formula, we declare a reduction axis using :any:`te.reduce_axis`. "
":any:`te.reduce_axis` takes in the range of reductions. :any:`te.sum` "
"takes in the expression to be reduced as well as the reduction axis and "
"compute the sum of value over all k in the declared range."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:40012
msgid "The equivalent C code is as follows:"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:60002
msgid "Schedule the Reduction"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:60003
msgid ""
"There are several ways to schedule a reduction. Before doing anything, "
"let us print out the IR code of default schedule."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:80002
msgid ""
"You can find that the IR code is quite like the C code. The reduction "
"axis is similar to a normal axis, it can be splitted."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:80005
msgid ""
"In the following code we split both the row axis of B as well axis by "
"different factors. The result is a nested reduction."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:100002
msgid "If we are building a GPU kernel, we can bind the rows of B to GPU threads."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:120002
msgid "Reduction Factoring and Parallelization"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:120003
msgid ""
"One problem of building a reduction is that we cannot simply parallelize "
"over the reduction axis. We need to divide the computation of the "
"reduction, store the local reduction result in a temporal array before "
"doing a reduction over the temp array."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:120008
msgid ""
"The rfactor primitive does such rewrite of the computation. In the "
"following schedule, the result of B is written to a temporary result "
"B.rf. The factored dimension becomes the first dimension of B.rf."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:140002
msgid ""
"The scheduled operator of B also get rewritten to be sum over the first "
"axis of reduced result of B.f"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:160002
msgid "Cross Thread Reduction"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:160003
msgid ""
"We can now parallelize over the factored axis. Here the reduction axis of"
" B is marked to be a thread. TVM allows reduction axis to be marked as "
"thread if it is the only axis in reduction and cross thread reduction is "
"possible in the device."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:160008
msgid ""
"This is indeed the case after the factoring. We can directly compute BF "
"at the reduction axis as well. The final generated kernel will divide the"
" rows by blockIdx.x and threadIdx.y columns by threadIdx.x and finally do"
" a cross thread reduction over threadIdx.x"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:180002
msgid "Verify the correctness of result kernel by comparing it to numpy."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:200002
msgid "Describe Convolution via 2D Reduction"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:200003
msgid ""
"In TVM, we can describe convolution via 2D reduction in a simple way. "
"Here is an example for 2D convolution with filter size = [3, 3] and "
"strides = [1, 1]."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:220003
msgid "Define General Commutative Reduction Operation"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:220004
msgid ""
"Besides the built-in reduction operations like :any:`te.sum`, "
":any:`tvm.te.min` and :any:`tvm.te.max`, you can also define your "
"commutative reduction operation by :any:`te.comm_reducer`."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:250002
msgid "Summary"
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:250003
msgid "This tutorial provides a walk through of reduction schedule."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:250005
msgid "Describe reduction with reduce_axis."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:250006
msgid "Use rfactor to factor out axis if we need parallelism."
msgstr ""

#: ../../xin/docs/how_to/work_with_schedules/reduction.ipynb:250007
msgid "Define new reduction operation by :any:`te.comm_reducer`"
msgstr ""

#~ msgid ":download:`Download Python source code: reduction.py <reduction.py>`"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Jupyter notebook: "
#~ "reduction.ipynb <reduction.ipynb>`"
#~ msgstr ""

#~ msgid ""
#~ "`Gallery generated by Sphinx-Gallery "
#~ "<https://sphinx-gallery.github.io>`_"
#~ msgstr ""

#~ msgid ""
#~ "Click :ref:`here "
#~ "<sphx_glr_download_how_to_work_with_schedules_reduction.py>` to"
#~ " download the full example code"
#~ msgstr ""

#~ msgid "**Author**: `Tianqi Chen <https://tqchen.github.io>`_"
#~ msgstr ""

#~ msgid ""
#~ "Sometimes we would like to perform "
#~ "reduction that involves multiple values "
#~ "like :code:`argmax`, which can be done"
#~ " by tuple inputs. See :ref:`reduction-"
#~ "with-tuple-inputs` for more detail."
#~ msgstr ""

