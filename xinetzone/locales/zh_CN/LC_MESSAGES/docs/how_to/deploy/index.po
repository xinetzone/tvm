# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020 - 2021, Apache Software Foundation
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm 0.9.dev282+gf54634c5d\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-06-10 19:41+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../../xin/docs/how_to/deploy/index.rst:5
msgid "部署模型并集成到 TVM"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:7
msgid "本页面是如何将 TVM 部署到各种平台以及如何将其与您的项目集成的指南。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:12
msgid "构建 TVM 运行时库"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:16
msgid "与传统的深度学习框架不同。TVM 堆栈分为两个主要组件："
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:18
msgid "TVM 编译器（compiler）：完成模型的所有编译和优化"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:19
msgid "TVM 运行时（runtime）：在目标设备上运行"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:21
msgid ""
"为了集成已编译的模块，**不需要** 在目标设备上构建整个 TVM。 您只需要在 desktop 上构建 TVM "
"编译器堆栈，并使用它来交叉编译部署在目标设备上的模块。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:24
msgid "只需要使用轻量级的 runtime API，它可以集成到各种平台中。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:26
msgid "例如，在基于 Linux 的嵌入式系统（如 Raspberry Pi）上，可以通过以下命令构建运行时 API："
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:38
msgid "注意，输入 ``make runtime`` 来只构建运行时库。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:40
msgid "还可以交叉编译运行时。运行时库的交叉编译不应与嵌入式设备的交叉编译模型混淆。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:42
msgid ""
"如果你想包含额外的运行时，如 OpenCL，你可以修改 ``config.cmake`` 来启用这些选项。 在获得 TVM "
"运行时库之后，您可以链接已编译的库"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:49
msgid ""
"模型（TVM 优化或未优化）可以由 TVM 针对不同的架构进行交叉编译，例如在 ``x64_64`` host 上的 ``aarch64``。 "
"一旦模型被交叉编译，为了能够运行交叉编译的模型，就必须有与目标架构兼容的运行时。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:53
msgid "交叉编译其他架构的 TVM 运行时"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:55
msgid ""
"在 :ref:`上面的 <build-tvm-runtime-on-target-device>` 例子中，运行时库是在树莓派上编译的。 "
"与树莓派（Raspberry Pi）等目标设备相比，在拥有高性能处理器和充足资源的主机（即 "
"host，如笔记本电脑、工作站）上生成运行时库的速度要快得多。 为了交叉编译运行时，必须安装目标设备的工具链（toolchain）。 "
"在安装了正确的工具链之后，与原生编译的主要区别是向 cmake 传递一些额外的命令行参数，以指定要使用的工具链。 作为参照，在现代笔记本电脑（使用"
" 8 个线程）上为 ``aarch64`` 构建 TVM 运行时库大约需要 20 秒，而在树莓派4 上构建运行时需要 10 分钟。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:62
msgid "aarch64 的交叉编译"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:83
msgid "对于 bare metal ARM 设备，可以使用以下工具链代替 gcc-aarch64-linux-* 进行安装"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:91
msgid "RISC-V 的交叉编译"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:114
msgid "``file`` 命令可用于查询生成的运行时的体系结构。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:123
msgid "针对目标设备优化和调优模型"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:125
msgid "在嵌入式设备上测试、调优和 benchmark TVM kernel 的最简单和推荐的方法是通过 TVM 的 RPC API。相关教程的链接如下："
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:127
msgid ":ref:`tutorial-cross-compilation-and-rpc`"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:128
msgid ":ref:`tutorial-deploy-model-on-rasp`"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:131
msgid "在目标设备上部署已优化的模型"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:133
msgid "在完成调优和基准测试之后，可能需要在目标设备上部署模型，而不需要依赖 RPC。关于如何这样做，请参阅以下参考资料。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:148
msgid "额外的部署指南"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:150
msgid ""
"还开发了一些针对特定设备的操作指南，可以在 Jupyter 笔记本中查看可用的 Python "
"代码。这些“如何操作”描述了如何准备模型并将其部署到许多受支持的后端。"
msgstr ""

#: ../../../xin/docs/how_to/deploy/index.rst:152
msgid ":ref:`../deploy_models/index`"
msgstr ""

#~ msgid "Deploy Models and Integrate TVM"
#~ msgstr "部署模型，集成 TVM"

#~ msgid ""
#~ "This page contains guidelines on how "
#~ "to deploy TVM to various platforms "
#~ "as well as how to integrate it "
#~ "with your project."
#~ msgstr ""

#~ msgid "Build the TVM runtime library"
#~ msgstr ""

#~ msgid ""
#~ "Unlike traditional deep learning frameworks."
#~ " TVM stack is divided into two "
#~ "major components:"
#~ msgstr ""

#~ msgid ""
#~ "TVM compiler, which does all the "
#~ "compilation and optimizations of the "
#~ "model"
#~ msgstr ""

#~ msgid "TVM runtime, which runs on the target devices."
#~ msgstr ""

#~ msgid ""
#~ "In order to integrate the compiled "
#~ "module, we **do not** need to "
#~ "build entire TVM on the target "
#~ "device. You only need to build the"
#~ " TVM compiler stack on your desktop"
#~ " and use that to cross-compile "
#~ "modules that are deployed on the "
#~ "target device."
#~ msgstr ""

#~ msgid ""
#~ "We only need to use a light-"
#~ "weight runtime API that can be "
#~ "integrated into various platforms."
#~ msgstr ""

#~ msgid ""
#~ "For example, you can run the "
#~ "following commands to build the runtime"
#~ " API on a Linux based embedded "
#~ "system such as Raspberry Pi:"
#~ msgstr ""

#~ msgid "Note that we type ``make runtime`` to only build the runtime library."
#~ msgstr ""

#~ msgid ""
#~ "It is also possible to cross "
#~ "compile the runtime. Cross compiling the"
#~ " runtime library should not be "
#~ "confused with cross compiling models for"
#~ " embedded devices."
#~ msgstr ""

#~ msgid ""
#~ "If you want to include additional "
#~ "runtime such as OpenCL, you can "
#~ "modify ``config.cmake`` to enable these "
#~ "options. After you get the TVM "
#~ "runtime library, you can link the "
#~ "compiled library"
#~ msgstr ""

#~ msgid ""
#~ "A model (optimized or not by TVM)"
#~ " can be cross compiled by TVM "
#~ "for different architectures such as "
#~ "``aarch64`` on a ``x64_64`` host. Once"
#~ " the model is cross compiled it "
#~ "is neccessary to have a runtime "
#~ "compatible with the target architecture "
#~ "to be able to run the cross "
#~ "compiled model."
#~ msgstr ""

#~ msgid "Cross compile the TVM runtime for other architectures"
#~ msgstr ""

#~ msgid ""
#~ "In the example :ref:`above <build-"
#~ "tvm-runtime-on-target-device>` the "
#~ "runtime library was compiled on a "
#~ "Raspberry Pi. Producing the runtime "
#~ "library can be done much faster on"
#~ " hosts that have high performace "
#~ "processors with ample resources (such as"
#~ " laptops, workstation) compared to a "
#~ "target devices such as a Raspberry "
#~ "Pi. In-order to cross compile the"
#~ " runtime the toolchain for the target"
#~ " device must be installed. After "
#~ "installing the correct toolchain, the "
#~ "main difference compared to compiling "
#~ "natively is to pass some additional "
#~ "command line argument to cmake that "
#~ "specify a toolchain to be used. "
#~ "For reference building the TVM runtime"
#~ " library on a modern laptop (using"
#~ " 8 threads) for ``aarch64`` takes "
#~ "around 20 seconds vs ~10 min to"
#~ " build the runtime on a Raspberry "
#~ "Pi 4."
#~ msgstr ""

#~ msgid "cross-compile for aarch64"
#~ msgstr ""

#~ msgid ""
#~ "For bare metal ARM devices the "
#~ "following toolchain is quite handy to"
#~ " install instead of gcc-aarch64-linux-*"
#~ msgstr ""

#~ msgid "cross-compile for RISC-V"
#~ msgstr ""

#~ msgid ""
#~ "The ``file`` command can be used "
#~ "to query the architecture of the "
#~ "produced runtime."
#~ msgstr ""

#~ msgid "Optimize and tune models for target devices"
#~ msgstr ""

#~ msgid ""
#~ "The easiest and recommended way to "
#~ "test, tune and benchmark TVM kernels "
#~ "on embedded devices is through TVM's "
#~ "RPC API. Here are the links to "
#~ "the related tutorials."
#~ msgstr ""

#~ msgid "Deploy optimized model on target devices"
#~ msgstr ""

#~ msgid ""
#~ "After you finished tuning and "
#~ "benchmarking, you might need to deploy"
#~ " the model on the target device "
#~ "without relying on RPC. See the "
#~ "following resources on how to do "
#~ "so."
#~ msgstr ""

#~ msgid "Additional Deployment How-Tos"
#~ msgstr "额外的部署指南"

#~ msgid ""
#~ "We have also developed a number of"
#~ " how-tos targeting specific devices, "
#~ "with working Python code that can "
#~ "be viewed in a Jupyter notebook. "
#~ "These how-tos describe how to "
#~ "prepare and deploy models to many "
#~ "of the supported backends."
#~ msgstr ""
#~ "我们还开发了一些针对特定设备的操作指南，使用可在 Jupyter 笔记本上查看的工作 Python"
#~ " 代码。这些指南描述了如何准备模型并将其部署到许多受支持的后端。"

