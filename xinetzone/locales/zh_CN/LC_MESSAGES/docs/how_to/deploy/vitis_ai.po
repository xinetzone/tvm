# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-10-09 21:52+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:20
msgid "Vitis AI Integration"
msgstr "Vitis AI 集成"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:22
msgid ""
"`Vitis AI <https://github.com/Xilinx/Vitis-AI>`__ is Xilinx's development"
" stack for hardware-accelerated AI inference on Xilinx platforms, "
"including both edge devices and Alveo cards. It consists of optimized IP,"
" tools, libraries, models, and example designs. It is designed with high "
"efficiency and ease of use in mind, unleashing the full potential of AI "
"acceleration on Xilinx FPGA and ACAP."
msgstr ""
"`Vitis AI <https://github.com/Xilinx/Vitis-AI>`__ 是赛灵思（Xilinx）为在其平台上进行硬件加速的 AI 推理而开发的工具栈，适用于边缘设备和 Alveo 加速卡。"
"它包含了优化的 IP 核、工具、库、模型以及示例设计。Vitis AI 以高效和易用为核心设计理念，旨在充分发挥赛灵思 FPGA 和自适应计算加速平台（ACAP）在 AI 加速方面的全部潜力。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:29
msgid ""
"The current Vitis AI flow inside TVM enables acceleration of Neural "
"Network model inference on edge and cloud with the `Zynq Ultrascale+ "
"MPSoc <https://www.xilinx.com/products/silicon-devices/soc/zynq-"
"ultrascale-mpsoc.html>`__, `Alveo <https://www.xilinx.com/products"
"/boards-and-kits/alveo.html>`__ and `Versal "
"<https://www.xilinx.com/products/silicon-devices/acap/versal.html>`__ "
"platforms. The identifiers for the supported edge and cloud Deep Learning"
" Processor Units (DPU's) are:"
msgstr ""
"当前 TVM 中的 Vitis AI 流程支持在边缘和云端加速神经网络模型推理，"
"适用于以下平台：`Zynq Ultrascale+ MPSoc <https://www.xilinx.com/products/silicon-devices/soc/zynq-ultrascale-mpsoc.html>`__、"
"`Alveo <https://www.xilinx.com/products/boards-and-kits/alveo.html>`__ 和 "
"`Versal <https://www.xilinx.com/products/silicon-devices/acap/versal.html>`__。"
"支持的边缘和云端深度学习处理器单元（DPU）的标识符为："

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:37
msgid "**Target Board**"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:37
msgid "**DPU ID**"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:37
msgid "**TVM Target ID**"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:39
msgid "`ZCU104 <https://www.xilinx.com/products/boards-and-kits/zcu104.html>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:39
#: ../../doc/docs/how_to/deploy/vitis_ai.rst:41
#: ../../doc/docs/how_to/deploy/vitis_ai.rst:43
msgid "DPUCZDX8G"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:39
msgid "DPUCZDX8G-zcu104"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:41
msgid ""
"`ZCU102 <https://www.xilinx.com/products/boards-and-"
"kits/ek-u1-zcu102-g.html>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:41
msgid "DPUCZDX8G-zcu102"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:43
msgid ""
"`Kria KV260 <https://www.xilinx.com/products/som/kria/kv260-vision-"
"starter-kit.html>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:43
msgid "DPUCZDX8G-kv260"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:45
msgid "`VCK190 <https://www.xilinx.com/products/boards-and-kits/vck190.html>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:45
msgid "DPUCVDX8G"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:47
msgid "`VCK5000 <https://www.xilinx.com/products/boards-and-kits/vck5000.html>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:47
msgid "DPUCVDX8H"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:49
msgid "`U200 <https://www.xilinx.com/products/boards-and-kits/alveo/u200.html>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:49
#: ../../doc/docs/how_to/deploy/vitis_ai.rst:51
msgid "DPUCADF8H"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:51
msgid "`U250 <https://www.xilinx.com/products/boards-and-kits/alveo/u250.html>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:53
msgid "`U50 <https://www.xilinx.com/products/boards-and-kits/alveo/u50.html>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:53
#: ../../doc/docs/how_to/deploy/vitis_ai.rst:55
msgid "DPUCAHX8H / DPUCAHX8L"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:53
msgid "DPUCAHX8H-u50 / DPUCAHX8L"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:55
msgid "`U280 <https://www.xilinx.com/products/boards-and-kits/alveo/u280.html>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:55
msgid "DPUCAHX8H-u280 / DPUCAHX8L"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:58
msgid "For more information about the DPU identifiers see following table:"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:61
msgid "DPU"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:61
msgid "Application"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:61
msgid "HW Platform"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:61
msgid "Quantization Method"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:61
msgid "Quantization Bitwidth"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:61
msgid "Design Target"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "Deep Learning"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "Processing Unit"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "C: CNN"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "R: RNN"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "AD: Alveo DDR"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "AH: Alveo HBM"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "VD: Versal DDR with AIE & PL"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "ZD: Zynq DDR"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "X: DECENT"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "I: Integer threshold"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "F: Float threshold"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "4: 4-bit"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "8: 8-bit"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "16: 16-bit"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "M: Mixed Precision"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "G: General purpose"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "H: High throughput"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "L: Low latency"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst
msgid "C: Cost optimized"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:69
msgid ""
"On this page you will find information on how to `setup <#setup-"
"instructions>`__ TVM with Vitis AI on different platforms (Zynq, Alveo, "
"Versal) and on how to get started with `Compiling a Model "
"<#compiling-a-model>`__ and executing on different platforms: `Inference "
"<#inference>`__."
msgstr ""
"在本页中，您将找到有关如何在不同平台（Zynq、Alveo、Versal）上 `设置 <#setup-instructions>`__ TVM 与 Vitis AI 的信息，"
"以及如何开始 `编译模型 <#compiling-a-model>`__ 并在不同平台上执行推理：`推理 <#inference>`__。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:74
msgid "System Requirements"
msgstr "系统要求"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:76
msgid ""
"The `Vitis AI System Requirements page <https://github.com/Xilinx/Vitis-"
"AI/blob/master/docs/learn/system_requirements.md>`__ lists the system "
"requirements for running docker containers as well as doing executing on "
"Alveo cards. For edge devices (e.g. Zynq), deploying models requires a "
"host machine for compiling models using the TVM with Vitis AI flow, and "
"an edge device for running the compiled models. The host system "
"requirements are the same as specified in the link above."
msgstr ""
"`Vitis AI 系统要求页面 <https://github.com/Xilinx/Vitis-AI/blob/master/docs/learn/system_requirements.md>`__ 列出了运行 Docker 容器以及在 Alveo 加速卡上执行任务的系统要求。"
"对于边缘设备（例如 Zynq），部署模型需要一台主机用于通过 TVM 与 Vitis AI 流程编译模型，以及一台边缘设备用于运行编译后的模型。主机的系统要求与上述链接中的要求相同。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:82
msgid "Setup instructions"
msgstr "安装说明"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:84
msgid ""
"This section provide the instructions for setting up the TVM with Vitis "
"AI flow for both cloud and edge. TVM with Vitis AI support is provided "
"through a docker container. The provided scripts and Dockerfile compiles "
"TVM and Vitis AI into a single image."
msgstr ""
"本节提供了为云端和边缘设备设置 TVM 与 Vitis AI 流程的说明。TVM 对 Vitis AI 的支持通过 Docker 容器提供。所提供的脚本和 Dockerfile 将 TVM 和 Vitis AI 编译到镜像中。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:88
msgid "Clone TVM repo"
msgstr "克隆 TVM 仓库"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:95
msgid "Build and start the TVM - Vitis AI docker container."
msgstr "构建并启动 TVM - Vitis AI Docker 容器。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:105
msgid "Build TVM inside the container with Vitis AI (inside tvm directory)"
msgstr "在容器内使用 Vitis AI 构建 TVM（在 tvm 目录内）"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:117
msgid "Install TVM"
msgstr "安装 TVM"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:124
msgid ""
"Inside this docker container you can now compile models for both cloud "
"and edge targets. To run on cloud Alveo or Versal VCK5000 cards inside "
"the docker container, please follow the `Alveo <#alveo-setup>`__ "
"respectively  `Versal VCK5000 <#versal-vck5000-setup>`__ setup "
"instructions. To setup your Zynq or Versal VCK190 evaluation board for "
"inference, please follow the `Zynq <#zynq-setup>`__ respectively `Versal "
"VCK190 <#versal-vck190-setup>`__ instructions."
msgstr ""
"在此 Docker 容器内，您现在可以为云端和边缘目标编译模型。若要在容器内运行于云端 Alveo 或 Versal VCK5000 加速卡上，请分别遵循 `Alveo <#alveo-setup>`__ 和 `Versal VCK5000 <#versal-vck5000-setup>`__ 的设置说明。"
"若要为推理设置您的 Zynq 或 Versal VCK190 评估板，请分别遵循 `Zynq <#zynq-setup>`__ 和 `Versal VCK190 <#versal-vck190-setup>`__ 的说明。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:131
msgid "Alveo Setup"
msgstr "Alveo 设置"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:133
msgid ""
"Check out following page for setup information: `Alveo Setup "
"<https://github.com/Xilinx/Vitis-AI/blob/v1.4/setup/alveo/README.md>`__."
msgstr ""
"请查看以下页面以获取设置信息：`Alveo 设置 <https://github.com/Xilinx/Vitis-AI/blob/v1.4/setup/alveo/README.md>`__。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:135
#: ../../doc/docs/how_to/deploy/vitis_ai.rst:151
msgid ""
"After setup, you can select the right DPU inside the docker container in "
"the following way:"
msgstr ""
"设置完成后，您可以在 Docker 容器内通过以下方式选择正确的 DPU："

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:144
msgid ""
"The DPU identifier for this can be found in the second column of the DPU "
"Targets table at the top of this page."
msgstr ""
"其 DPU 标识符可以在本页顶部的 DPU 目标表格的第二列中找到。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:147
msgid "Versal VCK5000 Setup"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:149
msgid ""
"Check out following page for setup information: `VCK5000 Setup "
"<https://github.com/Xilinx/Vitis-"
"AI/blob/v1.4/setup/vck5000/README.md>`__."
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:161
msgid "Zynq Setup"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:163
msgid ""
"For the Zynq target (DPUCZDX8G) the compilation stage will run inside the"
" docker on a host machine. This doesn't require any specific setup except"
" for building the TVM - Vitis AI docker. For executing the model, the "
"Zynq board will first have to be set up and more information on that can "
"be found here."
msgstr ""
"对于 Zynq 目标（DPUCZDX8G），编译阶段将在主机上的 Docker 容器内运行。除了构建 TVM - Vitis AI Docker 外，这不需要任何特定的设置。要执行模型，首先需要设置 Zynq 开发板，更多相关信息可以在这里找到。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:169
msgid "Download the Petalinux image for your target:"
msgstr "下载适用于您目标的 Petalinux 镜像："

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:168
msgid ""
"`ZCU104 <https://www.xilinx.com/member/forms/download/design-license-"
"xef.html?filename=xilinx-zcu104-dpu-v2021.1-v1.4.0.img.gz>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:169
msgid ""
"`ZCU102 <https://www.xilinx.com/member/forms/download/design-license-"
"xef.html?filename=xilinx-zcu102-dpu-v2021.1-v1.4.0.img.gz>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:170
msgid ""
"`Kria KV260 <https://www.xilinx.com/member/forms/download/design-license-"
"xef.html?filename=xilinx-kv260-dpu-v2020.2-v1.4.0.img.gz>`__"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:171
msgid "Use Etcher software to burn the image file onto the SD card."
msgstr "使用 Etcher 软件将镜像文件烧录到 SD 卡上。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:172
msgid "Insert the SD card with the image into the destination board."
msgstr "将带有镜像的 SD 卡插入目标开发板。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:173
msgid ""
"Plug in the power and boot the board using the serial port to operate on "
"the system."
msgstr ""
"插入电源并使用串口启动开发板以操作系统。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:174
msgid ""
"Set up the IP information of the board using the serial port. For more "
"details on step 1 to 5, please refer to `Setting Up The Evaluation Board "
"<https://www.xilinx.com/html_docs/vitis_ai/1_4/installation.html#ariaid-"
"title8>`__."
msgstr ""
"使用串口设置开发板的 IP 信息。有关步骤 1 到 5 的更多详细信息，请参阅 `设置评估板 <https://www.xilinx.com/html_docs/vitis_ai/1_4/installation.html#ariaid-title8>`__。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:175
msgid "Create 4GB of swap space on the board"
msgstr "在开发板上创建 4GB 的交换空间"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:185
msgid "Install hdf5 dependency (will take between 30 min and 1 hour to finish)"
msgstr "安装 hdf5 依赖项（需要 30 分钟到 1 小时完成）"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:198
msgid "Install Python dependencies"
msgstr "安装 Python 依赖项"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:204
msgid "Install PyXIR"
msgstr "安装 PyXIR"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:212
msgid "Build and install TVM with Vitis AI"
msgstr "构建并安装带有 Vitis AI 的 TVM"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:228
msgid "Check whether the setup was successful in the Python shell:"
msgstr "在 Python shell 中检查设置是否成功："

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:236
msgid ""
"You might see a warning about the 'cpu-tf' runtime not being found. This "
"warning is expected on the board and can be ignored."
msgstr ""
"您可能会看到一条关于未找到 'cpu-tf' 运行时的警告。此警告在开发板上是预期的，可以忽略。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:241
msgid "Versal VCK190 Setup"
msgstr ""

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:243
msgid ""
"For the Versal VCK190 setup, please follow the instructions for `Zynq "
"Setup <#zynq-setup>`__, but now use the `VCK190 image "
"<https://www.xilinx.com/member/forms/download/design-license-"
"xef.html?filename=xilinx-vck190-dpu-v2020.2-v1.4.0.img.gz>`__ in step 1. "
"The other steps are the same."
msgstr ""
"对于 Versal VCK190 的设置，请遵循 `Zynq 设置 <#zynq-setup>`__ 的说明，但在步骤 1 中使用 `VCK190 镜像 <https://www.xilinx.com/member/forms/download/design-license-xef.html?filename=xilinx-vck190-dpu-v2020.2-v1.4.0.img.gz>`__。其他步骤相同。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:249
msgid "Compiling a Model"
msgstr "编译模型"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:251
msgid ""
"The TVM with Vitis AI flow contains two stages: Compilation and "
"Inference. During the compilation a user can choose a model to compile "
"for the cloud or edge target devices that are currently supported. Once a"
" model is compiled, the generated files can be used to run the model on a"
" the specified target device during the `Inference <#inference>`__ stage."
" Currently, the TVM with Vitis AI flow supported a selected number of "
"Xilinx data center and edge devices."
msgstr ""
"带有 Vitis AI 的 TVM 流程包含两个阶段：编译和推理。"
"在编译期间，用户可以选择为当前支持的云端或边缘目标设备编译模型。"
"编译完成后，生成的文件可用于在 `推理 <#inference>`__ 阶段在指定的目标设备上运行模型。目前，带有 Vitis AI 的 TVM 流程支持部分赛灵思数据中心和边缘设备。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:258
msgid ""
"In this section we walk through the typical flow for compiling models "
"with Vitis AI inside TVM."
msgstr ""
"在本节中，将介绍在 TVM 中使用 Vitis AI 编译模型的典型流程。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:261
msgid "**Imports**"
msgstr "**导入**"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:263
msgid ""
"Make sure to import PyXIR and the DPU target (``import "
"pyxir.contrib.target.DPUCADF8H`` for DPUCADF8H):"
msgstr ""
"确保导入 PyXIR 和 DPU 目标（对于 DPUCADF8H，使用 ``import pyxir.contrib.target.DPUCADF8H``）："

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:276
msgid "**Declare the Target**"
msgstr "**声明目标**"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:283
msgid ""
"The TVM with Vitis AI flow currently supports the DPU targets listed in "
"the table at the top of this page. Once the appropriate targets are "
"defined, we invoke the TVM compiler to build the graph for the specified "
"target."
msgstr ""
"带有 Vitis AI 的 TVM 流程目前支持本页顶部表格中列出的 DPU 目标。一旦定义了适当的目标，调用 TVM 编译器为指定的目标构建图。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:287
msgid "**Import the Model**"
msgstr "**导入模型**"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:289
msgid "Example code to import an MXNet model:"
msgstr "导入 MXNet 模型的示例代码："

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:296
msgid "**Partition the Model**"
msgstr "**模型分区**"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:298
msgid ""
"After importing the model, we utilize the Relay API to annotate the Relay"
" expression for the provided DPU target and partition the graph."
msgstr ""
"导入模型后，利用 Relay API 为提供的 DPU 目标注释 Relay 表达式并对图进行分区。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:305
msgid "**Build the Model**"
msgstr "**构建模型**"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:307
msgid ""
"The partitioned model is passed to the TVM compiler to generate the "
"runtime libraries for the TVM Runtime."
msgstr ""
"将分区后的模型传递给 TVM 编译器，以生成 TVM 运行时的运行时库。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:319
msgid "**Quantize the Model**"
msgstr "**量化模型**"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:321
msgid ""
"Usually, to be able to accelerate inference of Neural Network models with"
" Vitis AI DPU accelerators, those models need to quantized upfront. In "
"TVM - Vitis AI flow, we make use of on-the-fly quantization to remove "
"this additional preprocessing step. In this flow, one doesn't need to "
"quantize his/her model upfront but can make use of the typical inference "
"execution calls (module.run) to quantize the model on-the-fly using the "
"first N inputs that are provided (see more information below). This will "
"set up and calibrate the Vitis-AI DPU and from that point onwards "
"inference will be accelerated for all next inputs. Note that the edge "
"flow deviates slightly from the explained flow in that inference won't be"
" accelerated after the first N inputs but the model will have been "
"quantized and compiled and can be moved to the edge device for "
"deployment. Please check out the `Running on Zynq <#running-on-zynq>`__ "
"section below for more information."
msgstr ""
"通常，为了能够使用 Vitis AI DPU 加速器加速神经网络模型的推理，这些模型需要提前量化。"
"在 TVM - Vitis AI 流程中，利用即时量化来消除这一额外的预处理步骤。在此流程中，用户无需提前量化模型，而是可以使用典型的推理执行调用（module.run）通过提供的前 N 个输入即时量化模型（详见下文）。"
"这将设置并校准 Vitis-AI DPU，从那时起，所有后续输入的推理都将被加速。"
"请注意，边缘流程与上述流程略有不同，因为在提供前 N 个输入后，推理不会立即加速，但模型将被量化和编译，并可以移动到边缘设备进行部署。更多信息请查看下面的 `在 Zynq 上运行 <#running-on-zynq>`__ 部分。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:347
msgid ""
"By default, the number of images used for quantization is set to 128. You"
" could change the number of images used for On-The-Fly Quantization with "
"the PX_QUANT_SIZE environment variable. For example, execute the "
"following line in the terminal before calling the compilation script to "
"reduce the quantization calibration dataset to eight images. This can be "
"used for quick testing."
msgstr ""
"默认情况下，用于量化的图像数量设置为 128。您可以使用 PX_QUANT_SIZE 环境变量更改用于即时量化的图像数量。例如，在调用编译脚本之前，在终端中执行以下行，将量化校准数据集减少到八张图像。这可以用于快速测试。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:358
msgid ""
"Lastly, we store the compiled output from the TVM compiler on disk for "
"running the model on the target device. This happens as follows for cloud"
" DPU's (Alveo, VCK5000):"
msgstr ""
"最后，将 TVM 编译器的编译输出存储在磁盘上，以便在目标设备上运行模型。对于云端 DPU（Alveo、VCK5000），操作如下："

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:368
msgid ""
"For edge targets (Zynq, VCK190) we have to rebuild for aarch64. To do "
"this we first have to normally export the module to also serialize the "
"Vitis AI runtime module (vitis_ai.rtmod). We will load this runtime "
"module again afterwards to rebuild and export for aarch64."
msgstr ""
"对于边缘目标（Zynq、VCK190），需要为 aarch64 重新构建。为此，首先需要正常导出模块以序列化 Vitis AI 运行时模块（vitis_ai.rtmod）。之后，将再次加载此运行时模块以重新构建并导出为 aarch64。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:394
msgid ""
"This concludes the tutorial to compile a model using TVM with Vitis AI. "
"For instructions on how to run a compiled model please refer to the next "
"section."
msgstr ""
"本教程到此结束，介绍了如何使用 TVM 与 Vitis AI 编译模型。有关如何运行编译模型的说明，请参阅下一节。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:398
msgid "Inference"
msgstr "推理"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:400
msgid ""
"The TVM with Vitis AI flow contains two stages: Compilation and "
"Inference. During the compilation a user can choose to compile a model "
"for any of the target devices that are currently supported. Once a model "
"is compiled, the generated files can be used to run the model on a target"
" device during the Inference stage."
msgstr ""
"带有 Vitis AI 的 TVM 流程包含两个阶段：编译和推理。在编译期间，用户可以选择为当前支持的任何目标设备编译模型。编译完成后，生成的文件可用于在推理阶段在目标设备上运行模型。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:406
msgid ""
"Check out the `Running on Alveo and VCK5000 <#running-on-alveo-and-"
"vck5000>`__ and `Running on Zynq and VCK190 <#running-on-zynq-and-"
"vck190>`__ sections for doing inference on cloud accelerator cards "
"respectively edge boards."
msgstr ""
"请查看 `在 Alveo 和 VCK5000 上运行 <#running-on-alveo-and-vck5000>`__ 和 `在 Zynq 和 VCK190 上运行 <#running-on-zynq-and-vck190>`__ 部分，了解如何在云端加速卡和边缘开发板上进行推理。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:411
msgid "Running on Alveo and VCK5000"
msgstr "在 Alveo 和 VCK5000 上运行"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:413
msgid ""
"After having followed the steps in the `Compiling a Model "
"<#compiling-a-model>`__ section, you can continue running on new inputs "
"inside the docker for accelerated inference:"
msgstr ""
"在按照 `编译模型 <#compiling-a-model>`__ 部分的步骤操作后，您可以继续在 Docker 内对新输入进行加速推理："

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:422
msgid ""
"Alternatively, you can load the exported runtime module (the "
"deploy_lib.so exported in  `Compiling a Model <#compiling-a-model>`__):"
msgstr ""
"或者，您可以加载导出的运行时模块（在 `编译模型 <#compiling-a-model>`__ 中导出的 deploy_lib.so）："

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:444
msgid "Running on Zynq and VCK190"
msgstr "在 Zynq 和 VCK190 上运行"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:446
msgid ""
"Before proceeding, please follow the  `Zynq <#zynq-setup>`__ or `Versal "
"VCK190 <#versal-vck190-setup>`__ setup instructions."
msgstr ""
"在继续之前，请遵循 `Zynq <#zynq-setup>`__ 或 `Versal VCK190 <#versal-vck190-setup>`__ 的设置说明。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:449
msgid ""
"Prior to running a model on the board, you need to compile the model for "
"your target evaluation board and transfer the compiled model on to the "
"board. Please refer to the `Compiling a Model <#compiling-a-model>`__ "
"section for information on how to compile a model."
msgstr ""
"在开发板上运行模型之前，您需要为目标评估板编译模型并将编译后的模型传输到开发板上。有关如何编译模型的信息，请参阅 `编译模型 <#compiling-a-model>`__ 部分。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:454
msgid ""
"Afterwards, you will have to transfer the compiled model "
"(deploy_lib_edge.so) to the evaluation board. Then, on the board you can "
"use the typical \"load_module\" and \"module.run\" APIs to execute. For "
"this, please make sure to run the script as root (execute ``su`` in "
"terminal to log into root)."
msgstr ""
"之后，您需要将编译后的模型（deploy_lib_edge.so）传输到评估板上。然后，在开发板上，您可以使用典型的 \"load_module\" 和 \"module.run\" API 来执行。为此，请确保以 root 身份运行脚本（在终端中执行 ``su`` 以登录 root）。"

#: ../../doc/docs/how_to/deploy/vitis_ai.rst:461
msgid ""
"Note also that you **shouldn't** import the PyXIR DPU targets in the run "
"script (``import pyxir.contrib.target.DPUCZDX8G``)."
msgstr ""
"还请注意，您**不应**在运行脚本中导入 PyXIR DPU 目标（``import pyxir.contrib.target.DPUCZDX8G``）。"
