# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-09 00:02+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20003
msgid "Deploy a Hugging Face Pruned Model on CPU"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20004
msgid "**Author**: [Josh Fromm](https://github.com/jwfromm)"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20006
msgid ""
"This tutorial demonstrates how to take any pruned model, in this case "
"[PruneBert from Hugging Face](https://huggingface.co/huggingface"
"/prunebert-base-uncased-6-finepruned-w-distil-squad), and use TVM to "
"leverage the model's sparsity support to produce real speedups. Although "
"the primary purpose of this tutorial is to realize speedups on already "
"pruned models, it may also be useful to estimate how fast a model would "
"be *if* it were pruned. To this end, we also provide a function that "
"takes an unpruned model and replaces its weights with random and pruned "
"weights at a specified sparsity. This may be a useful feature when trying"
" to decide if a model is worth pruning or not."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20016
msgid ""
"Before we get into the code, it's useful to discuss sparsity and pruning "
"and dig into the two different types of sparsity: **structured** and "
"**unstructured**."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20020
#, python-format
msgid ""
"Pruning is a technique primarily used to reduce the parameter size of a "
"model by replacing weight values with 0s. Although many methods exist for"
" choosing which weights should be set to 0, the most straight forward is "
"by picking the weights with the smallest value. Typically, weights are "
"pruned to a desired sparsity percentage. For example, a 95% sparse model "
"would have only 5% of its weights non-zero. Pruning to very high "
"sparsities often requires fine-tuning or full retraining as it tends to "
"be a lossy approximation. Although parameter size benefits are quite easy"
" to obtain from a pruned model through simple compression, leveraging "
"sparsity to yield runtime speedups is more complicated."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20031
msgid ""
"In structured sparsity weights are pruned with the goal of clustering "
"pruned weights together. In other words, they are pruned using both their"
" value and location. The benefit of bunching up pruned weights is that it"
" allows an algorithm such as matrix multiplication to skip entire blocks."
" It turns out that some degree of *block sparsity* is very important to "
"realizing significant speedups on most hardware available today. This is "
"because when loading memory in most CPUs or GPUs, it doesn't save any "
"work to skip reading a single value at a time, instead an entire chunk or"
" tile is read in and executed using something like vectorized "
"instructions."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20041
msgid ""
"Unstructured sparse weights are those that are pruned only on the value "
"of the original weights. They may appear to be scattered randomly "
"throughout a tensor rather than in chunks like we'd see in block sparse "
"weights. At low sparsities, unstructured pruning techniques are difficult"
" to accelerate. However, at high sparsities many blocks of all 0 values "
"will naturally appear, making it possible to accelerate."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:20048
#, python-format
msgid ""
"This tutorial interacts with both structured and unstructured sparsity. "
"Hugging Face's PruneBert model is unstructured but 95% sparse, allowing "
"us to apply TVM's block sparse optimizations to it, even if not "
"optimally. When generating random sparse weights for an unpruned model, "
"we do so with structured sparsity. A fun exercise is comparing the real "
"speed of PruneBert with the block sparse speed using fake weights to see "
"the benefit of structured sparsity."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:40002
msgid "Load Required Modules"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:40003
msgid ""
"Other than TVM, scipy, the latest transformers, and tensorflow 2.2+ are "
"required."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:60002
msgid "Configure Settings"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:60003
msgid ""
"Let's start by defining some parameters that define the type of model and"
" sparsity to run."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:80002
msgid "Download and Convert Transformers Model"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:80003
msgid ""
"Now we'll grab a model from the transformers module, download it, convert"
" it into a TensorFlow graphdef in preperation for converting that "
"graphdef into a relay graph that we can optimize and deploy."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:100002
msgid "Convert to Relay Graph"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:100003
msgid ""
"We now have all the tooling to get a transformers model in the right "
"format for relay conversion. Let's import it! In the following function "
"we save the imported graph in relay's json format so that we dont have to"
" reimport from tensorflow each time this script is run."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:120002
msgid "Run the Dense Graph"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:120003
msgid ""
"Let's run the default version of the imported model. Note that even if "
"the weights are sparse, we won't see any speedup because we are using "
"regular dense matrix multiplications on these dense (but mostly zero) "
"tensors instead of sparse aware kernels."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:140002
msgid "Run the Sparse Graph"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:140003
msgid ""
"Next we'll convert the graph into a sparse representation and generate "
"fake sparse weights if needed. Then we'll use the same benchmarking "
"script as dense to see how much faster we go! We apply a few relay passes"
" to the graph to get it leveraging sparsity. First we use "
"`simplify_fc_transpose` to use transposes on the weights of dense layers "
"into the parameters. This makes it easier to convert to matrix multiplies"
" to sparse versions. Next we apply `bsr_dense.convert` to identify all "
"weight matrices that can be sparse, and automatically replace them."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:140012
msgid ""
"The `bsr_dense.convert` call below is doing the heavy lifting of "
"identifying which weights in the model can be made sparse by checking if "
"they are at least `sparsity_threshold` percent sparse. If so, it converts"
" those weights into *Block Compressed Row Format (BSR)*. BSR is "
"essentially a representation that indexes into the nonzero chunks of the "
"tensor, making it easy for an algorithm to load those non-zero chunks and"
" ignore the rest of the tensor. Once the sparse weights are in BSR "
"format, `relay.transform.DenseToSparse` is applied to actually replace "
"`relay.dense` operations with `relay.sparse_dense` calls that can be run "
"faster."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:160002
msgid "Run All the Code!"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:160003
msgid ""
"And that's it! Now we'll simply call all the needed function to benchmark"
" the model according to the set parameters. Note that to run this code "
"you'll need to uncomment the last line first."
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:180002
msgid "Sample Output"
msgstr ""

#: ../../xin/docs/how_to/deploy_models/deploy_sparse.ipynb:180003
msgid ""
"For reference, below is the output of the script when run on an AMD CPU "
"and shows about a 2.5X speedup from using sparsity."
msgstr ""

#~ msgid ""
#~ ":download:`Download Python source code: "
#~ "deploy_sparse.py <deploy_sparse.py>`"
#~ msgstr ""

#~ msgid ""
#~ ":download:`Download Jupyter notebook: "
#~ "deploy_sparse.ipynb <deploy_sparse.ipynb>`"
#~ msgstr ""

#~ msgid ""
#~ "`Gallery generated by Sphinx-Gallery "
#~ "<https://sphinx-gallery.github.io>`_"
#~ msgstr ""

#~ msgid ""
#~ "Click :ref:`here "
#~ "<sphx_glr_download_how_to_deploy_models_deploy_sparse.py>` to"
#~ " download the full example code"
#~ msgstr ""

#~ msgid "**Author**: `Josh Fromm <https://github.com/jwfromm>`_"
#~ msgstr ""

#~ msgid ""
#~ "This tutorial demonstrates how to take"
#~ " any pruned model, in this case "
#~ "`PruneBert from Hugging Face "
#~ "<https://huggingface.co/huggingface/prunebert-base-"
#~ "uncased-6-finepruned-w-distil-squad>`_, and use "
#~ "TVM to leverage the model's sparsity "
#~ "support to produce real speedups. "
#~ "Although the primary purpose of this "
#~ "tutorial is to realize speedups on "
#~ "already pruned models, it may also "
#~ "be useful to estimate how fast a"
#~ " model would be *if* it were "
#~ "pruned. To this end, we also "
#~ "provide a function that takes an "
#~ "unpruned model and replaces its weights"
#~ " with random and pruned weights at"
#~ " a specified sparsity. This may be"
#~ " a useful feature when trying to "
#~ "decide if a model is worth pruning"
#~ " or not."
#~ msgstr ""

#~ msgid ""
#~ "Pruning is a technique primarily used"
#~ " to reduce the parameter size of "
#~ "a model by replacing weight values "
#~ "with 0s. Although many methods exist "
#~ "for choosing which weights should be "
#~ "set to 0, the most straight "
#~ "forward is by picking the weights "
#~ "with the smallest value. Typically, "
#~ "weights are pruned to a desired "
#~ "sparsity percentage. For example, a 95%"
#~ " sparse model would have only 5% "
#~ "of its weights non-zero. Pruning "
#~ "to very high sparsities often requires"
#~ " finetuning or full retraining as it"
#~ " tends to be a lossy approximation."
#~ " Although parameter size benefits are "
#~ "quite easy to obtain from a pruned"
#~ " model through simple compression, "
#~ "leveraging sparsity to yield runtime "
#~ "speedups is more complicated."
#~ msgstr ""

