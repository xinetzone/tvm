# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-06-05 11:22+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../doc/docs/reference/api/python/tir/schedule.rst:19
msgid "tvm.tir.schedule"
msgstr ""

#~ msgid "Namespace for the TensorIR schedule API."
#~ msgstr ""

#~ msgid "A random variable that refers to a block"
#~ msgstr ""

#~ msgid ""
#~ "An object corresponds to each block "
#~ "sref in the sref tree, which "
#~ "tracks the producer-consumer dependency "
#~ "between blocks."
#~ msgstr ""

#~ msgid "Glossary:"
#~ msgstr ""

#~ msgid ""
#~ "Block scope: A contiguous subtree of "
#~ "the sref tree, rooted at each "
#~ "block sref, whose components are:"
#~ msgstr ""

#~ msgid "scope root: a block sref"
#~ msgstr ""

#~ msgid "internal srefs: loop srefs"
#~ msgstr ""

#~ msgid "scope leaves: block srefs"
#~ msgstr ""

#~ msgid ""
#~ "Child block: The scope leaf blocks "
#~ "under the scope root or a specific"
#~ " internal sref"
#~ msgstr ""

#~ msgid "Get all dependencies whose `dst` is the target `block`."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "The queried block"
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "**blocks** -- The dependencies"
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "Get all dependencies whose `src` is the target`block`."
#~ msgstr ""

#~ msgid "Type of dependency."
#~ msgstr ""

#~ msgid "Read-after-write dependency"
#~ msgstr ""

#~ msgid "type"
#~ msgstr ""

#~ msgid "int = 0"
#~ msgstr ""

#~ msgid "Write-after-write dependency"
#~ msgstr ""

#~ msgid "int = 1"
#~ msgstr ""

#~ msgid "Write-after-read dependency. Not supported in TensorIR for now."
#~ msgstr ""

#~ msgid "int = 2"
#~ msgstr ""

#~ msgid "Opaque dependency"
#~ msgstr ""

#~ msgid "int = 3"
#~ msgstr ""

#~ msgid ""
#~ "A tuple (src, dst, kind) representing"
#~ " certain types of dependency. For "
#~ "example, (A, B, kRAW) means block "
#~ "B depends on block A, and the "
#~ "dependency kind is read-after-write, "
#~ "which means block B reads the "
#~ "result written by block A."
#~ msgstr ""

#~ msgid "The source of the dependency relation"
#~ msgstr ""

#~ msgid "The destination of the dependency relation"
#~ msgstr ""

#~ msgid "The dependency kind"
#~ msgstr ""

#~ msgid "Schedule instructions each corresponds to a schedule primitive"
#~ msgstr ""

#~ msgid "The kind of the instruction"
#~ msgstr ""

#~ msgid "InstructionKind"
#~ msgstr ""

#~ msgid ""
#~ "The input random variables of the "
#~ "instruction, and the type of each "
#~ "element can be one of the "
#~ "following: - BlockRV - LoopRV - "
#~ "ExprRV - float - int - str -"
#~ " None"
#~ msgstr ""

#~ msgid "List[INPUT_RV_TYPE]"
#~ msgstr ""

#~ msgid ""
#~ "The attributes of the instruction. "
#~ "Similar to attributes of an operator,"
#~ " attributes of an instruction are "
#~ "arbitrary constant metadata required by "
#~ "the instructions. For example, the name"
#~ " of the block to be retrieved "
#~ "in `GetBlock`."
#~ msgstr ""

#~ msgid "List[ATTR_TYPE]"
#~ msgstr ""

#~ msgid ""
#~ "The output random variables of the "
#~ "instruction, and the type of each "
#~ "element can be one of the "
#~ "following: - BlockRV - LoopRV - "
#~ "ExprRV, atomic variables only, won't be"
#~ " constants or composite PrimExpr"
#~ msgstr ""

#~ msgid "List[OUTPUT_RV_TYPE]"
#~ msgstr ""

#~ msgid ""
#~ "Kind of an instruction, e.g. Split, "
#~ "Reorder, etc. Besides the name, every"
#~ " kind of instruction has its own "
#~ "properties, including: 1) A boolean "
#~ "indicating if the instruction is pure,"
#~ " i.e. change nothing in the schedule"
#~ " state 2) A functor that applies "
#~ "the instruction to a TensorIR schedule"
#~ " 3) A functor that converts the "
#~ "instruction to a statement in python "
#~ "syntax 4) A functor that serialize "
#~ "its attributes to JSON 5) A "
#~ "functor that deserialize its attributes "
#~ "from JSON"
#~ msgstr ""

#~ msgid ""
#~ "Unlike `tvm.ir.op`, `InstructionKind` doesn't "
#~ "support unstructured properties, mainly "
#~ "because there is no such usecase "
#~ "yet to add any other property."
#~ msgstr ""

#~ msgid "The name of a kind of instructions"
#~ msgstr ""

#~ msgid "str"
#~ msgstr ""

#~ msgid "The functor properties are not exposed on python side at the moment"
#~ msgstr ""

#~ msgid "Retrieve an InstructionKind using its name"
#~ msgstr ""

#~ msgid "The registered name of the InstructionKind"
#~ msgstr ""

#~ msgid "**kind** -- The InstructionKind retrieved"
#~ msgstr ""

#~ msgid ""
#~ "Indicates if the instruction is pure,"
#~ " i.e. removing it alone doesn't "
#~ "mutate the schedule state. For example,"
#~ " the instruction `GetBlock` is pure "
#~ "because it changes nothing, while "
#~ "`ComputeInline` is not because removing "
#~ "it leads to a different resulting "
#~ "schedule."
#~ msgstr ""

#~ msgid "**pure** -- The boolean flag indicating if the instruction is pure"
#~ msgstr ""

#~ msgid "A random variable that refers to a loop"
#~ msgstr ""

#~ msgid "The user-facing schedule class"
#~ msgstr ""

#~ msgid ""
#~ "A schedule is a set of "
#~ "transformations that change the order of"
#~ " computation but preserve the semantics "
#~ "of computation. Some example of "
#~ "schedules: 1) Split a loop into "
#~ "two; 2) Reorder two loops; 3) "
#~ "Inline the computation of a specific "
#~ "buffer into its consumer"
#~ msgstr ""

#~ msgid ""
#~ "The schedule class stores auxiliary "
#~ "information to schedule correctly and "
#~ "efficiently."
#~ msgstr ""

#~ msgid ""
#~ "Link to tutorial: "
#~ "https://tvm.apache.org/docs/tutorials/language/schedule_primitives.html"
#~ msgstr ""

#~ msgid "Create a new unit loop on top of the specific block or loop."
#~ msgstr ""

#~ msgid "The block above which the new loop is created"
#~ msgstr ""

#~ msgid "**new_loop** -- The new unit loop"
#~ msgstr ""

#~ msgid "示例"
#~ msgstr ""

#~ msgid "Before add_unit_loop, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do add-unit-loop:"
#~ msgstr ""

#~ msgid "After applying add-unit-loop, the IR becomes:"
#~ msgstr ""

#~ msgid "Annotate a block/loop with a key value pair"
#~ msgstr ""

#~ msgid "The block/loop to be annotated"
#~ msgstr ""

#~ msgid "The annotation key"
#~ msgstr ""

#~ msgid "The annotation value"
#~ msgstr ""

#~ msgid "Before annotate, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do annotate:"
#~ msgstr ""

#~ msgid "After applying annotate, the IR becomes:"
#~ msgstr ""

#~ msgid "Annotate the read or write region of a block"
#~ msgstr ""

#~ msgid "The block to be annotated"
#~ msgstr ""

#~ msgid "The index of the buffer in block's read or write region"
#~ msgstr ""

#~ msgid "The buffer type: \"read\" or \"write\""
#~ msgstr ""

#~ msgid ""
#~ "A function that takes the block's "
#~ "iter_vars and returns a Tuple[Union[PrimExpr,"
#~ " Tuple[PrimExpr, PrimExpr]], ...] which "
#~ "defines the new read or write "
#~ "region for the buffer. Each element "
#~ "in the tuple can be: - A "
#~ "single PrimExpr representing the iter_var "
#~ "itself - A tuple of two PrimExprs"
#~ " representing the range (begin, end)"
#~ msgstr ""

#~ msgid ""
#~ "Annotate a 2D read region for a"
#~ " buffer. Before annotate_buffer_access, in "
#~ "TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do annotate_buffer_access:"
#~ msgstr ""

#~ msgid "After applying annotate_buffer_access, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "This annotates the read region for "
#~ "buffer A (index 0) in block \"B\""
#~ " to be [vi-1:vi+1, vj-1:vj+1] for "
#~ "each (vi, vj) in the block's "
#~ "iteration domain."
#~ msgstr ""

#~ msgid ""
#~ "This function allows manual specification "
#~ "of read or write regions, which "
#~ "can be useful in cases where the"
#~ " compiler cannot accurately infer the "
#~ "access pattern, such as complex data-"
#~ "dependent accesses. It overrides the "
#~ "automatically inferred region for the "
#~ "specified buffer. The function adds an"
#~ " annotation to the block, indicating "
#~ "that an explicit region has been "
#~ "provided for the buffer at the "
#~ "given index. This annotation is used "
#~ "in the CompactBufferAllocation pass to "
#~ "respect the manually specified region "
#~ "instead of relying on automatic "
#~ "inference."
#~ msgstr ""

#~ msgid ""
#~ "Caution should be exercised when using"
#~ " this function, as incorrect annotations"
#~ " may lead to incorrect code "
#~ "generation or runtime errors. It's "
#~ "crucial to ensure that the specified "
#~ "region covers all actual reads or "
#~ "writes performed by the block for "
#~ "the given buffer."
#~ msgstr ""

#~ msgid ""
#~ "Bind the input loop to the given"
#~ " thread axis. It requires: 1) The "
#~ "scope block that the loop is in"
#~ " should have stage-pipeline property "
#~ "2) All the blocks under the loop"
#~ " are complete blocks or reduction "
#~ "blocks, and have affine bindings 3) "
#~ "For each block under the loop, if"
#~ " the thread axis starts with "
#~ "\"threadIdx`, the loop can only be "
#~ "contained in data-parallel block iter"
#~ " and reduction block iters' bindings. "
#~ "Otherwise the loop can only be "
#~ "contained in data-parallel block iters'"
#~ " bindings"
#~ msgstr ""

#~ msgid "The loop to be bound to the thread axis"
#~ msgstr ""

#~ msgid ""
#~ "The thread axis to be bound to "
#~ "the loop. Possible candidates: - "
#~ "blockIdx.x/y/z - threadIdx.x/y/z - "
#~ "vthread.x/y/z - vthread (It is a "
#~ "legacy behavior that will be deprecated."
#~ " Please use `vthread.x/y/z` instead.)"
#~ msgstr ""

#~ msgid "Before bind, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do bind:"
#~ msgstr ""

#~ msgid "After applying bind, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Convert multiple blocks or the subtree"
#~ " rooted at a specific loop into "
#~ "a block."
#~ msgstr ""

#~ msgid "The root of the subtree or the specified blocks."
#~ msgstr ""

#~ msgid "Whether or not to preserve unit iterators in block bindings"
#~ msgstr ""

#~ msgid "**result** -- The new block."
#~ msgstr ""

#~ msgid "Before blockize, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do set_scope:"
#~ msgstr ""

#~ msgid "After applying blockize, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "blockize requires there is exactly one"
#~ " block under the given loop and "
#~ "the bindings of the block are "
#~ "divisible by the subspace represented by"
#~ " the loops starting at the given "
#~ "loop."
#~ msgstr ""

#~ msgid ""
#~ "Create a block to cache precomputed "
#~ "index for later use. if there is"
#~ " no index computation, keep unchanged."
#~ msgstr ""

#~ msgid "The target block operates on the target buffer."
#~ msgstr ""

#~ msgid "The storage scope of cached block."
#~ msgstr ""

#~ msgid ""
#~ "The repeat threshold that determines a"
#~ " common sub expr, default 0 means "
#~ "cache all index computation."
#~ msgstr ""

#~ msgid "**cached_blocks** -- The blocks of the stage writing the cache buffers"
#~ msgstr ""

#~ msgid "Before cache_inplace, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and cache_index:"
#~ msgstr ""

#~ msgid "After applying cache_index, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Create blocks that reads & write a"
#~ " buffer region into a cache block."
#~ " It requires the target block both"
#~ " read & write the target buffer. "
#~ "Mainly for inplace operation."
#~ msgstr ""

#~ msgid ""
#~ "The index of the buffer in block's"
#~ " read region, the unique name of "
#~ "a read buffer in the block, or "
#~ "a Buffer object that is within the"
#~ " blocks read region."
#~ msgstr ""

#~ msgid "The target storage scope."
#~ msgstr ""

#~ msgid ""
#~ "**cached_blocks** -- The blocks of the"
#~ " cache stage, read cache first, write"
#~ " cache second"
#~ msgstr ""

#~ msgid "Create the schedule and cache_inplace:"
#~ msgstr ""

#~ msgid "After applying cache_inplace, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a read cache. It "
#~ "requires:"
#~ msgstr ""

#~ msgid "There is at most one block who write the buffer in the scope."
#~ msgstr ""

#~ msgid "The scope block have stage-pipeline property."
#~ msgstr ""

#~ msgid "The consumer block of the target buffer."
#~ msgstr ""

#~ msgid ""
#~ "An optional list of consumers that "
#~ "should read from the cache. If not"
#~ " specified, all consumers will use "
#~ "the cache."
#~ msgstr ""

#~ msgid "**cached_block** -- The block of the cache stage"
#~ msgstr ""

#~ msgid "Before cache_read, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and cache_read:"
#~ msgstr ""

#~ msgid "After applying cache_read, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a write cache. It "
#~ "requires:"
#~ msgstr ""

#~ msgid "There is only one block who write the buffer in the scope."
#~ msgstr ""

#~ msgid "The producer block of the target buffer."
#~ msgstr ""

#~ msgid ""
#~ "The index of the buffer in block's"
#~ " write region, the unique name of "
#~ "a write buffer in the block, or"
#~ " a Buffer object that is within "
#~ "the blocks write region."
#~ msgstr ""

#~ msgid ""
#~ "An optional list of consumers that "
#~ "should read directly from the cache. "
#~ "If not specified, all consumers will "
#~ "read from the original buffer."
#~ msgstr ""

#~ msgid "Before cache_write, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and cache_write:"
#~ msgstr ""

#~ msgid "After applying cache_write, the IR becomes:"
#~ msgstr ""

#~ msgid "Check whether the block match padding pattern and can be decomposed."
#~ msgstr ""

#~ msgid ""
#~ "Compute-At. Move a producer block "
#~ "under the specific loop, and regenerate"
#~ " the loops induced by the block "
#~ "so that the buffer region produced "
#~ "by the producer block could cover "
#~ "those regions consumed by its consumer"
#~ " blocks under the given loop. It "
#~ "requires:"
#~ msgstr ""

#~ msgid ""
#~ "`block` and `loop` are under the "
#~ "same scope, `loop` is not the "
#~ "ancestor of `block`"
#~ msgstr ""

#~ msgid "The scope block has stage-pipeline property"
#~ msgstr ""

#~ msgid ""
#~ "3) The subtree of the scope block,"
#~ " where the given block is in, "
#~ "satisfies the compact dataflow condition. "
#~ "i.e. all the blocks in the scope"
#~ " block's subtree must be either "
#~ "complete block or reduction block"
#~ msgstr ""

#~ msgid ""
#~ "4) The block is not an output "
#~ "block with regard to the scope "
#~ "block, i.e. the buffers written by "
#~ "the block are allocated under the "
#~ "scope block"
#~ msgstr ""

#~ msgid "All the consumers of the block are under the given loop"
#~ msgstr ""

#~ msgid "The block to be moved"
#~ msgstr ""

#~ msgid "The loop where the block to be moved under"
#~ msgstr ""

#~ msgid "Whether to keep the trivial loops whose extents are 1"
#~ msgstr ""

#~ msgid ""
#~ "The block index of the loop body"
#~ " subtree blocks: - `index = -1` "
#~ "means inserted into the last possible"
#~ " insertion point; - `index = -2` "
#~ "means inserted into the first possible"
#~ " insertion point; - Otherwise, `index` "
#~ "is a nonnegative number that indicates"
#~ " the insertion point"
#~ msgstr ""

#~ msgid "Before compute-at, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do compute-at:"
#~ msgstr ""

#~ msgid "After applying compute-at, the IR becomes:"
#~ msgstr ""

#~ msgid "Inline a block into its consumer(s). It requires:"
#~ msgstr ""

#~ msgid "The block is a complete non-root block, which only produces one buffer"
#~ msgstr ""

#~ msgid "The block must not be the only leaf in the scope."
#~ msgstr ""

#~ msgid ""
#~ "The body of the block must be "
#~ "a BufferStore statement in the form "
#~ "of, ``A[i, j, k, ...] = ...`` "
#~ "where the indices of the LHS are"
#~ " all distinct atomic variables, and "
#~ "no variables other than those indexing"
#~ " variables are allowed in the "
#~ "statement."
#~ msgstr ""

#~ msgid "The block to be inlined to its consumer(s)"
#~ msgstr ""

#~ msgid "Before compute-inline, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do compute-inline:"
#~ msgstr ""

#~ msgid "After applying compute-inline, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Returns a copy of the schedule, "
#~ "including both the state and the "
#~ "symbol table, * guaranteeing that * "
#~ "1) SRef tree is completely "
#~ "reconstructed; * 2) The IRModule being"
#~ " scheduled is untouched; * 3) All "
#~ "the random variables are valid in "
#~ "the copy, pointing to the corresponding"
#~ " sref * reconstructed"
#~ msgstr ""

#~ msgid "**copy** -- A new copy of the schedule"
#~ msgstr ""

#~ msgid ""
#~ "Decompose a block of padding computation"
#~ " pattern into two separate blocks."
#~ msgstr ""

#~ msgid "The block which fill const pad values into full write region;"
#~ msgstr ""

#~ msgid ""
#~ "The block which fill in-bound "
#~ "values into region where pad predicate"
#~ " is true."
#~ msgstr ""

#~ msgid "The pad value filling block is inserted right before the given loop."
#~ msgstr ""

#~ msgid "The schedule primitive requires:"
#~ msgstr ""

#~ msgid "The input block is a complete block."
#~ msgstr ""

#~ msgid "The input loop is the ancestor of the block."
#~ msgstr ""

#~ msgid "The input block is a block which match padding pattern."
#~ msgstr ""

#~ msgid "The padding block to be decomposed."
#~ msgstr ""

#~ msgid "The loop above which the pad value filling block is inserted before."
#~ msgstr ""

#~ msgid "**pad_value_block** -- The block filling const pad values."
#~ msgstr ""

#~ msgid "Before decompose-padding, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do decompose-padding with specified loop:"
#~ msgstr ""

#~ msgid "After applying decompose-padding, the IR becomes:"
#~ msgstr ""

#~ msgid "Decompose a reduction block into two separate blocks."
#~ msgstr ""

#~ msgid ""
#~ "The init block, which is translated "
#~ "from the init statement of the "
#~ "reduction block;"
#~ msgstr ""

#~ msgid "The update block, which is the original block without init statement."
#~ msgstr ""

#~ msgid "The init block is inserted right before the given loop."
#~ msgstr ""

#~ msgid "The input block is a reduction block."
#~ msgstr ""

#~ msgid ""
#~ "The input loop is not lower than"
#~ " all the loops related to reduce "
#~ "block var."
#~ msgstr ""

#~ msgid "The reduction block to be decomposed"
#~ msgstr ""

#~ msgid "The loop above which the init block is inserted before."
#~ msgstr ""

#~ msgid "**init_block** -- The init block"
#~ msgstr ""

#~ msgid "Before decompose-reduction, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do decompose-reduction with specified loop:"
#~ msgstr ""

#~ msgid "After applying decompose-reduction, the IR becomes:"
#~ msgstr ""

#~ msgid "A no-op that marks the start of postprocessing phase of scheduling"
#~ msgstr ""

#~ msgid "Returns a forked random state as seed for new schedules"
#~ msgstr ""

#~ msgid ""
#~ "**seed** -- The forked random state, "
#~ "not the same as the current random"
#~ " state"
#~ msgstr ""

#~ msgid ""
#~ "Returns the GlobalVar of the func "
#~ "that the schedule is currently working"
#~ " on"
#~ msgstr ""

#~ msgid ""
#~ "Fuse a list of consecutive loops "
#~ "into one. It requires: 1) The "
#~ "loops can't have annotations or thread"
#~ " bindings. 2) The (i+1)-th loop must"
#~ " be the only child of the i-th"
#~ " loop. 3) All loops must start "
#~ "with 0. 4) The domain of a "
#~ "loop to be fused cannot depend on"
#~ " another loop to be fused."
#~ msgstr ""

#~ msgid "The loops to be fused"
#~ msgstr ""

#~ msgid "**fused_loop** -- The new loop after fusion"
#~ msgstr ""

#~ msgid "Before applying fuse, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do fuse:"
#~ msgstr ""

#~ msgid "After applying fuse, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Returns: - the corresponding Block that"
#~ " a BlockRV evaluates to; - the "
#~ "corresponding For that a LoopRV "
#~ "evaluates to; - the corresponding "
#~ "integer that a ExprRV evaluates to; "
#~ "- the corresponding Block that a "
#~ "block sref points to; - the "
#~ "corresponding For that a loop sref "
#~ "points to;"
#~ msgstr ""

#~ msgid "The random variable / sref to be evaluated"
#~ msgstr ""

#~ msgid "**result** -- The corresponding result"
#~ msgstr ""

#~ msgid "Retrieve a block in a specific function with its name"
#~ msgstr ""

#~ msgid ""
#~ "By default, if `func_name` is not "
#~ "specified, the schedule will search for"
#~ " the block in the function that "
#~ "is currently being \"worked on\". To "
#~ "switch the function to be worked "
#~ "on, use `work_on` before calling this"
#~ " method."
#~ msgstr ""

#~ msgid "The name of the block"
#~ msgstr ""

#~ msgid "The name of the function"
#~ msgstr ""

#~ msgid ""
#~ "**block** -- The block retrieved "
#~ "IndexError is raised if 0 or "
#~ "multiple blocks exist with the specific"
#~ " name."
#~ msgstr ""

#~ msgid "Get the leaf blocks of a specific block/loop"
#~ msgstr ""

#~ msgid "The query block/loop"
#~ msgstr ""

#~ msgid "**blocks** -- A list of leaf blocks inside a specific block/loop"
#~ msgstr ""

#~ msgid "Get the consumers of a specific block"
#~ msgstr ""

#~ msgid "The block in the query"
#~ msgstr ""

#~ msgid "**consumers** -- A list of consumers of the given block"
#~ msgstr ""

#~ msgid "Get the parent loops of the block in its scope, from outer to inner"
#~ msgstr ""

#~ msgid "The query block"
#~ msgstr ""

#~ msgid ""
#~ "**loops** -- A list of loops above"
#~ " the given block in its scope, "
#~ "from outer to inner"
#~ msgstr ""

#~ msgid ""
#~ "Get the list of output blocks "
#~ "within the given scope An output "
#~ "block is a block which has atleast"
#~ " one buffer being written to, but "
#~ "is not allocated within the PrimFunc"
#~ msgstr ""

#~ msgid "The scope block from which output blocks are collected"
#~ msgstr ""

#~ msgid ""
#~ "**output_blocks** -- A list of all "
#~ "blocks that write to some output "
#~ "buffer"
#~ msgstr ""

#~ msgid "Get the producers of a specific block"
#~ msgstr ""

#~ msgid "**producers** -- A list of producers of the given block"
#~ msgstr ""

#~ msgid ""
#~ "Returns the corresponding sref to the"
#~ " given 1) LoopRV 2) BlockRV 3) "
#~ "Block 4) For"
#~ msgstr ""

#~ msgid ""
#~ "Partition a loop into a list of"
#~ " consecutive loops. It requires: 1) "
#~ "The loop can't have annotation or "
#~ "thread binding. Predicates may be added"
#~ " to ensure the total loop numbers "
#~ "keeps unchanged. In `factors`, at most"
#~ " one of the factors can be "
#~ "None, which will be automatically "
#~ "inferred."
#~ msgstr ""

#~ msgid "The loop to be partition"
#~ msgstr ""

#~ msgid ""
#~ "The partitioning factors Potential inputs "
#~ "are: - None - ExprRV - Positive"
#~ " constant integers"
#~ msgstr ""

#~ msgid "**partition_loops** -- The new loops after partition"
#~ msgstr ""

#~ msgid "Before partition, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do partition:"
#~ msgstr ""

#~ msgid "After applying partition, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Merge a list of loops into one."
#~ " The loops under their LCA requires:"
#~ " 1) Under the same scope. 2) "
#~ "Can't have annotations or thread "
#~ "bindings. 3) Start with 0 and have"
#~ " same extent and same nesting depth."
#~ " 4) From target loop to their "
#~ "LCA, The inner loop must be the"
#~ " only child of the outer loop."
#~ msgstr ""

#~ msgid "The loops to be merged"
#~ msgstr ""

#~ msgid "**fused_loop** -- The new loop after merge"
#~ msgstr ""

#~ msgid "Before applying merge, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Returns the AST of the module being scheduled"
#~ msgstr ""

#~ msgid "Pad the computation of Einsum."
#~ msgstr ""

#~ msgid ""
#~ "On a block with trivial binding, "
#~ "this primitive pads the iteration domain"
#~ " of the block by the given "
#~ "padding factors, for example, 127 -> "
#~ "128, 132 -> 144 when padding "
#~ "factor is 16. Extra producer and "
#~ "consumer padding blocks will be "
#~ "generated to avoid out-of-bound "
#~ "buffer access."
#~ msgstr ""

#~ msgid ""
#~ "Einsum pattern means all the indices "
#~ "on the buffer access are either by"
#~ " constants (e.g. B[0]) or by "
#~ "variables (e.g. B[i]), but not by "
#~ "composite expressions (e.g. B[i + 1])."
#~ msgstr ""

#~ msgid "The block that matches the Einsum pattern."
#~ msgstr ""

#~ msgid "The padding for each block iter."
#~ msgstr ""

#~ msgid "Before applying pad-einsum, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do pad-einsum with specified block:"
#~ msgstr ""

#~ msgid ""
#~ "Parallelize the input loop. It requires:"
#~ " 1) The scope block that the "
#~ "loop is in should have stage-"
#~ "pipeline property 2) All the blocks "
#~ "under the loop are complete blocks "
#~ "or reduction blocks, and have affine "
#~ "bindings 3) For each block under "
#~ "the loop, the loop can only be "
#~ "contained in data-parallel block iters'"
#~ " bindings"
#~ msgstr ""

#~ msgid "The loop to be parallelized"
#~ msgstr ""

#~ msgid "Before parallel, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do parallel:"
#~ msgstr ""

#~ msgid "After applying parallel, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that read/write a "
#~ "buffer region into a read/write cache"
#~ " with reindexing. The layout of the"
#~ " cache will be the same as by"
#~ " the iterators of the block that "
#~ "reads/writes the buffer. It requires: 1)"
#~ " There is only one block who "
#~ "reads/writes the target buffer 2) There"
#~ " is only one buffer load/store of "
#~ "this buffer in the block"
#~ msgstr ""

#~ msgid ""
#~ "The block that accesses the target "
#~ "buffer.  If a string, this must "
#~ "uniquely identify a block."
#~ msgstr ""

#~ msgid ""
#~ "The buffer to be transformed, or a"
#~ " specification of how to identify the"
#~ " buffer to be transformed.  If "
#~ "`buffer` if a tuple of ``(str,int)``,"
#~ " the first item should be either "
#~ "\"read\" or \"write\", and the second"
#~ " item is an index into the "
#~ "block's read or write regions.  If "
#~ "`buffer` is a string, it is the"
#~ " name of the buffer, which must "
#~ "exist within the reads/writes of the "
#~ "block.  In addition, the reads/writes of"
#~ " the block may not contain more "
#~ "than one buffer with this name.  "
#~ "If `buffer` is a Buffer object, it"
#~ " must exist within the reads/writes "
#~ "of the block."
#~ msgstr ""

#~ msgid ""
#~ "The buffer to be transformed, or a"
#~ " specification of how to identify the"
#~ " buffer to be transformed."
#~ msgstr ""

#~ msgid ""
#~ "If `buffer` if a tuple of "
#~ "``(str,int)``, the first item should be"
#~ " either \"read\" or \"write\", and "
#~ "the second item is an index into"
#~ " the block's read or write regions."
#~ msgstr ""

#~ msgid ""
#~ "If `buffer` is a string, it is "
#~ "the name of the buffer, which must"
#~ " exist within the reads/writes of the"
#~ " block.  In addition, the reads/writes "
#~ "of the block may not contain more"
#~ " than one buffer with this name."
#~ msgstr ""

#~ msgid ""
#~ "If `buffer` is a Buffer object, it"
#~ " must exist within the reads/writes "
#~ "of the block."
#~ msgstr ""

#~ msgid "**reindex_block** -- The block of the reindex stage"
#~ msgstr ""

#~ msgid "Before reindex, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reindex:"
#~ msgstr ""

#~ msgid "After applying reindex, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a read cache using "
#~ "customized indices specified by index "
#~ "map. The read region of the buffer"
#~ " must be a single point."
#~ msgstr ""

#~ msgid ""
#~ "The cache stage block follows the "
#~ "original order of loops and block "
#~ "itervars in the block. If a block"
#~ " itervar does not appear in the "
#~ "buffer access region, it and its "
#~ "corresponding loop variables will be "
#~ "omitted. User can then use "
#~ "`transform_block_layout` primitive to reorder "
#~ "the block itervars and surrounding loops"
#~ " of the cache read/write block."
#~ msgstr ""

#~ msgid ""
#~ "Unlike `cache_read`, `reindex_cache_read` only "
#~ "supports single consumer, please use "
#~ "`cache_read` when there are multiple "
#~ "consumers."
#~ msgstr ""

#~ msgid "The index of the buffer in block's read region."
#~ msgstr ""

#~ msgid ""
#~ "User defined indices to access allocated"
#~ " cache buffer, maps from block iter"
#~ " vars."
#~ msgstr ""

#~ msgid "Before reindex_cache_read, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and reindex_cache_read:"
#~ msgstr ""

#~ msgid "After applying reindex_cache_read, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ ":obj:`reindex_cache_write`, :obj:`transform_block_layout`, "
#~ ":obj:`transform_layout`, :obj:`cache_read`, :obj:`reindex`"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a write cache using "
#~ "customized indices specified by index "
#~ "map. The write region of the "
#~ "buffer must be a single point."
#~ msgstr ""

#~ msgid ""
#~ "Unlike `cache_write`, `reindex_cache_write` only "
#~ "supports single consumer, please use "
#~ "`cache_write` when there are multiple "
#~ "consumers."
#~ msgstr ""

#~ msgid "The index of the buffer in block's write region."
#~ msgstr ""

#~ msgid "Before reindex_cache_write, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and reindex_cache_write:"
#~ msgstr ""

#~ msgid "After applying reindex_cache_write, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ ":obj:`reindex_cache_read`, :obj:`transform_block_layout`, "
#~ ":obj:`transform_layout`, :obj:`cache_write`, "
#~ ":obj:`reindex`"
#~ msgstr ""

#~ msgid "Remove a random variable from the symbol table"
#~ msgstr ""

#~ msgid "The random variable to be removed"
#~ msgstr ""

#~ msgid ""
#~ "Reorder a list of loops. It "
#~ "doesn't require the loops to be "
#~ "consecutive. It requires: 1) The loops"
#~ " are in the same chain. That "
#~ "means: the loops can be ordered to"
#~ " [l_1, l_2, ... , l_n] where "
#~ "l_i is an ancestor of l_{i+1} and"
#~ " there are only single-branch loops"
#~ " between l_1 and l_n (which also "
#~ "indicates they are under the same "
#~ "scope). 2) After reordering, the domain"
#~ " of an outer loop cannot depend "
#~ "on any of the inner loops. 3) "
#~ "For every block under the loop "
#~ "nests, its block binding must be "
#~ "affine, and the block variables must "
#~ "be either data parallel or reduction."
#~ " 4) No duplicated loops are allowed"
#~ " in the arguments."
#~ msgstr ""

#~ msgid "The loops in the new order"
#~ msgstr ""

#~ msgid "Before reorder, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reorder:"
#~ msgstr ""

#~ msgid "After applying reorder, the IR becomes:"
#~ msgstr ""

#~ msgid "Reorder the itervars inside a given block."
#~ msgstr ""

#~ msgid "The block to be transformed."
#~ msgstr ""

#~ msgid "The new block itervar order."
#~ msgstr ""

#~ msgid "Before reorder_block_iter_var, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reorder_block_iter_var:"
#~ msgstr ""

#~ msgid "After applying reorder_block_iter_var, the IR becomes:"
#~ msgstr ""

#~ msgid ":obj:`reorder`"
#~ msgstr ""

#~ msgid ""
#~ "Reverse-Compute-At. Move a consumer "
#~ "block under the specific loop, and "
#~ "regenerate the loops induced by the "
#~ "block so that the buffer region "
#~ "consumed by the consumer block could "
#~ "cover those regions produced by its "
#~ "producer blocks under the given loop."
#~ " It requires:"
#~ msgstr ""

#~ msgid "All the producers of the block are under the given loop"
#~ msgstr ""

#~ msgid "Before reverse-compute-at, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reverse-compute-at:"
#~ msgstr ""

#~ msgid "After applying reverse-compute-at, the IR becomes:"
#~ msgstr ""

#~ msgid "Inline a block into its only producer. It requires:"
#~ msgstr ""

#~ msgid ""
#~ "The block is a complete non-root"
#~ " block, which only produces and "
#~ "consumes one buffer"
#~ msgstr ""

#~ msgid ""
#~ "The only producer of the block is"
#~ " a read-after-write producer and "
#~ "a complete non-root block"
#~ msgstr ""

#~ msgid ""
#~ "The body of the block must be "
#~ "a BufferStore statement in the form "
#~ "of, ``B[f(i, j, k, ...)] = g(i,"
#~ " j, k, A[i, j, k, ...] ...)``"
#~ " where the indices of each "
#~ "`BufferLoad` on the RHS are all "
#~ "distinct atomic variables, and no "
#~ "variables other than those indexing "
#~ "variables are allowed in the statement."
#~ msgstr ""

#~ msgid "The block to be inlined to its producer"
#~ msgstr ""

#~ msgid "Before reverse-compute-inline, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reverse-compute-inline:"
#~ msgstr ""

#~ msgid "After applying reverse-compute-inline, the IR becomes:"
#~ msgstr ""

#~ msgid "Factorize an associative reduction block by the specified loop."
#~ msgstr ""

#~ msgid ""
#~ "An associative reduction cannot be "
#~ "parallelized directly, because it leads "
#~ "to potential race condition during "
#~ "accumulation. Alternatively, the reduction "
#~ "could be factorized on a loop with"
#~ " the following steps: - Step 1: "
#~ "evenly slice the reduction into `n` "
#~ "separate chunks, where `n` is the "
#~ "loop extent - Step 2: compute the"
#~ " chunks separately and write the "
#~ "result into `n` intermediate buffers; -"
#~ " Step 3: accumulate the `n` separate"
#~ " buffer into the result buffer. Note"
#~ " that the Step 2 above introduces "
#~ "opportunities for parallelization."
#~ msgstr ""

#~ msgid ""
#~ "RFactor is a schedule primitive that "
#~ "implements the transformation described above:"
#~ " Given a block that writes to "
#~ "buffer `B`, it factorizes a loop "
#~ "of extent `n`."
#~ msgstr ""

#~ msgid ""
#~ "For example, the pseudocode below "
#~ "accumulates `B[i] = sum(A[i, : , :"
#~ " ])`:"
#~ msgstr ""

#~ msgid ""
#~ "Suppose RFactor is applied on the "
#~ "innermost loop `k` and `factor_axis ="
#~ " 1`. RFactor then creates an "
#~ "intermediate buffer and two blocks."
#~ msgstr ""

#~ msgid ""
#~ "1. The intermediate buffer, or \"rf-"
#~ "buffer\" is a buffer of rank "
#~ "`ndim(B) + 1` and size `size(B) *"
#~ " n`, whose shape expands from "
#~ "`shape(B)` by adding an axis of "
#~ "`n` at the position specified by "
#~ "`factor_axis`. For example,"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 0  => shape(B_rf) = [n, 1, 2, 3]"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 1  => shape(B_rf) = [1, n, 2, 3]"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 2  => shape(B_rf) = [1, 2, n, 3]"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 3  => shape(B_rf) = [1, 2, 3, n]"
#~ msgstr ""

#~ msgid ""
#~ "2. The rfactor block, or \"rf-"
#~ "block\", is a block that writes to"
#~ " the `rf-buffer` without accumulating "
#~ "over the loop `k`, i.e. the loop"
#~ " `k` is converted from a reduction"
#~ " loop to a data parallel loop. "
#~ "In our example, the rf-block is:"
#~ msgstr ""

#~ msgid ""
#~ "3. The write-back block, or "
#~ "`wb-block`, is a block that "
#~ "accumulates the rf-buffer into the "
#~ "result buffer. All the reduction loops"
#~ " are removed except the loop `k` "
#~ "for accumulation. In our example, the"
#~ " wb-block is:"
#~ msgstr ""

#~ msgid "The loop outside block for which we want to do rfactor"
#~ msgstr ""

#~ msgid ""
#~ "The position where the new dimension "
#~ "is placed in the new introduced "
#~ "rfactor buffer"
#~ msgstr ""

#~ msgid ""
#~ "**rf_block** -- The block which computes"
#~ " partial results over each slices "
#~ "(i.e., the first block as described "
#~ "in the above illustration)"
#~ msgstr ""

#~ msgid "Before rfactor, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do rfactor:"
#~ msgstr ""

#~ msgid "After applying rfactor, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Rfactor requires: 1) `loop` has only "
#~ "one child block, and it is a "
#~ "reduction block; 2) `loop` is a "
#~ "reduction loop, i.e. the loop variable"
#~ " is bound to only reduction variables"
#~ " in the block binding; 3) `loop` "
#~ "is not parallelized, vectorized, unrolled "
#~ "or bound to any thread axis; 4)"
#~ " The block scope that `loop` is "
#~ "in is a staged-pipeline; 5) The"
#~ " outermost loop outside the reduction "
#~ "block should has the reduction block "
#~ "as its first child block; 6) The"
#~ " outermost reduction loop should have "
#~ "only one child block; 7) An unary"
#~ " extent loop that is not bound "
#~ "to any reduction or data parallel "
#~ "variables in the block binding should"
#~ " not appear under some reduction "
#~ "loop; 8) The reduction block should "
#~ "write to only one buffer, and its"
#~ " init and body are both simple "
#~ "`BufferStore`s, and the pattern is "
#~ "registered as an associative reducer. "
#~ "The pre-defined patterns include: plus,"
#~ " multiplication, min and max; 9) Each"
#~ " of the loops on top of the "
#~ "block cannot be bound to a data"
#~ " parallel and a reduction block "
#~ "binding at the same time; 10) "
#~ "`factor_axis` should be in range "
#~ "`[-ndim(B) - 1, ndim(B)]`, where `B` "
#~ "is the buffer that the reduction "
#~ "block writes to. Negative indexing is"
#~ " normalized according to numpy convention."
#~ msgstr ""

#~ msgid ""
#~ "Compute the target buffer via rolling"
#~ " buffering, select the outermost rollable"
#~ " axis with a positive bound overlap"
#~ " that appears in the block's ancestor"
#~ " loops as `rolling axis`, fold and"
#~ " circularize the buffer along the "
#~ "rolling dimension, append block predicate "
#~ "to avoid recomputing overlapping elements. "
#~ "It requires:"
#~ msgstr ""

#~ msgid "The block is not an output block and has only RAW dependencies."
#~ msgstr ""

#~ msgid "The buffer to be an intermediate buffer defined via `alloc_buffer`."
#~ msgstr ""

#~ msgid ""
#~ "3) The LCA of the producer and "
#~ "consumer of the buffer is a for"
#~ " loop, typically, the producer and "
#~ "consumer of the buffer are cascaded "
#~ "through compute_at."
#~ msgstr ""

#~ msgid ""
#~ "4) The access region of the buffer"
#~ " has at least one dimension that "
#~ "contains a positive bound overlap."
#~ msgstr ""

#~ msgid "The producer block of the buffer."
#~ msgstr ""

#~ msgid "Before rolling_buffer, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do rolling_buffer:"
#~ msgstr ""

#~ msgid "After applying rolling_buffer, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "The region_cover property of the "
#~ "consumer block of the target buffer "
#~ "will become false."
#~ msgstr ""

#~ msgid "Sample an integer given the probability distribution"
#~ msgstr ""

#~ msgid "The candidates to be sampled from"
#~ msgstr ""

#~ msgid "The probability of each candidate"
#~ msgstr ""

#~ msgid "The sampling decision, if any"
#~ msgstr ""

#~ msgid "**result** -- The random variable sampled from candidates"
#~ msgstr ""

#~ msgid "Sample a compute-at location of the given block"
#~ msgstr ""

#~ msgid "The block whose compute-at location is to be sampled"
#~ msgstr ""

#~ msgid "The sampling decision"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The sampled loop where "
#~ "the input block is to be computed"
#~ " at"
#~ msgstr ""

#~ msgid "Sample the factors to a partitioned tile for a specific loop"
#~ msgstr ""

#~ msgid "The loop to be tiled"
#~ msgstr ""

#~ msgid "The number of tiles to be sampled"
#~ msgstr ""

#~ msgid "The position to partition tiles to two parts"
#~ msgstr ""

#~ msgid "The factor of the second part"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- A list of length "
#~ "`n`, the random partitioned tile sizes"
#~ " sampled"
#~ msgstr ""

#~ msgid "Sample the factors to perfect tile a specific loop"
#~ msgstr ""

#~ msgid "The maximum tile size allowed to be sampled in the innermost loop"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- A list of length "
#~ "`n`, the random perfect tile sizes "
#~ "sampled"
#~ msgstr ""

#~ msgid "Seed the randomness"
#~ msgstr ""

#~ msgid "The new random seed, -1 if use device random, otherwise non-negative"
#~ msgstr ""

#~ msgid ""
#~ "Set the axis separator of a "
#~ "buffer, where the buffer is specified"
#~ " by a block and a read or "
#~ "write index."
#~ msgstr ""

#~ msgid "The axis separators."
#~ msgstr ""

#~ msgid "Before set_axis_separator, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do set_axis_separator:"
#~ msgstr ""

#~ msgid "After applying set_axis_separator, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Set the storage scope of a buffer,"
#~ " where the buffer is specified by "
#~ "the a block and a write-index."
#~ msgstr ""

#~ msgid "The producer block of the buffer"
#~ msgstr ""

#~ msgid "The index of the buffer in block's write region"
#~ msgstr ""

#~ msgid "The storage scope to be set"
#~ msgstr ""

#~ msgid "Before set_scope, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "After applying set_scope, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "`set_scope` requires the buffer to be"
#~ " an intermediate buffer defined via "
#~ "`alloc_buffer`."
#~ msgstr ""

#~ msgid "A sugar for print highlighted TVM script."
#~ msgstr ""

#~ msgid ""
#~ "All parameters are forwarded to the "
#~ "underlying `Module.show` and `Trace.show` "
#~ "methods."
#~ msgstr ""

#~ msgid ""
#~ "Split a loop into a list of "
#~ "consecutive loops. It requires: 1) The"
#~ " loop can't have annotation or thread"
#~ " binding. 2) The loop must start "
#~ "with 0. Predicates may be added to"
#~ " ensure the total loop numbers keeps"
#~ " unchanged. In `factors`, at most one"
#~ " of the factors can be None, "
#~ "which will be automatically inferred."
#~ msgstr ""

#~ msgid "The loop to be split"
#~ msgstr ""

#~ msgid ""
#~ "The splitting factors Potential inputs "
#~ "are: - None - ExprRV - Positive"
#~ " constant integers"
#~ msgstr ""

#~ msgid ""
#~ "If enabled, don't create a predicate "
#~ "for guarding the loop. This can be"
#~ " useful when splitting with scalable "
#~ "factors that the schedule writer knows"
#~ " are divisible by the loop bound."
#~ "  Warning: enabling this feature may "
#~ "result in incorrect code generation if"
#~ " not used carefully."
#~ msgstr ""

#~ msgid ""
#~ "If enabled, don't create a predicate "
#~ "for guarding the loop. This can be"
#~ " useful when splitting with scalable "
#~ "factors that the schedule writer knows"
#~ " are divisible by the loop bound."
#~ msgstr ""

#~ msgid ""
#~ "Warning: enabling this feature may "
#~ "result in incorrect code generation if"
#~ " not used carefully."
#~ msgstr ""

#~ msgid "**split_loops** -- The new loops after split"
#~ msgstr ""

#~ msgid "Before split, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do split:"
#~ msgstr ""

#~ msgid "After applying split, the IR becomes:"
#~ msgstr ""

#~ msgid "Returns the ScheduleState in the current schedule class"
#~ msgstr ""

#~ msgid ""
#~ "Set alignment requirement for specific "
#~ "dimension such that stride[axis] == k"
#~ " * factor + offset for some k."
#~ " This is useful to set memory "
#~ "layout for more friendly memory access"
#~ " pattern. For example, we can set "
#~ "alignment to be factor=2, offset=1 to"
#~ " avoid bank conflict for thread "
#~ "access on higher dimension in GPU "
#~ "shared memory."
#~ msgstr ""

#~ msgid "The dimension to be specified for alignment."
#~ msgstr ""

#~ msgid "The factor multiple of alignment."
#~ msgstr ""

#~ msgid "The required offset factor."
#~ msgstr ""

#~ msgid "Before storage_align, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do storage_align:"
#~ msgstr ""

#~ msgid "After applying storage_align, the IR becomes:"
#~ msgstr ""

#~ msgid "After lowering passes, buffer B will have strides as [129, 1]."
#~ msgstr ""

#~ msgid ""
#~ "Storage_align requires the buffer to be"
#~ " an intermediate buffer defined via "
#~ "`alloc_buffer`."
#~ msgstr ""

#~ msgid "Tensorize the computation enclosed by loop with the tensor intrinsic."
#~ msgstr ""

#~ msgid "The loop to be tensorized."
#~ msgstr ""

#~ msgid "The tensor intrin or the name of the tensor intrin."
#~ msgstr ""

#~ msgid "Before tensorize, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Declare and register the tensor intrinsic:"
#~ msgstr ""

#~ msgid "Create the schedule and do tensorize:"
#~ msgstr ""

#~ msgid "After applying tensorize, the IR becomes:"
#~ msgstr ""

#~ msgid "Returns the internally maintained trace of scheduling program execution"
#~ msgstr ""

#~ msgid "Apply a transformation represented by IndexMap to block"
#~ msgstr ""

#~ msgid "The block to be transformed"
#~ msgstr ""

#~ msgid "The transformation to apply."
#~ msgstr ""

#~ msgid "Before transform_block_layout, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do transform_block_layout:"
#~ msgstr ""

#~ msgid "After applying transform_block_layout, the IR becomes:"
#~ msgstr ""

#~ msgid "Apply a transformation represented by IndexMap to buffer"
#~ msgstr ""

#~ msgid ""
#~ "The transformation to apply.  If "
#~ "`index_map` is a callable, and the "
#~ "returned list contains IndexMap.AXIS_SEPARATOR, "
#~ "the SetAxisSeparators primitive will be "
#~ "called in addition to the "
#~ "TransformLayout primitive."
#~ msgstr ""

#~ msgid ""
#~ "If `index_map` is a callable, and "
#~ "the returned list contains "
#~ "IndexMap.AXIS_SEPARATOR, the SetAxisSeparators "
#~ "primitive will be called in addition "
#~ "to the TransformLayout primitive."
#~ msgstr ""

#~ msgid ""
#~ "The value to be used for any "
#~ "padding introduced by the transformation.  "
#~ "If the schedule contains a producer "
#~ "block for the specified buffer, the "
#~ "pad value will be written as part"
#~ " of the producer block if possible,"
#~ " or after the producer block "
#~ "otherwise.  Otherwise, if the buffer is"
#~ " an input, will insert an annotation"
#~ " block to state that the padding "
#~ "contains the known value.  The pad "
#~ "value may not contain instances of "
#~ "BufferLoad, except where it loads a "
#~ "value from the buffer being transformed"
#~ " (e.g. to create a circular buffer"
#~ " with padding that consists of "
#~ "repeated elements).  Note: If applied to"
#~ " an input buffer, the calling scope"
#~ " is responsible for ensuring that the"
#~ " pad_value is present. Algebraic "
#~ "symplifications, branch elimination, and other"
#~ " optimizations may assume that this "
#~ "precondition is met, and may result "
#~ "in incorrect results being returned.  If"
#~ " None, the transformation may not "
#~ "introduce padding.  If an int, float "
#~ "or PrimExpr, the transformation is the"
#~ " specific value to be present in "
#~ "the padding.  If an IndexMap or "
#~ "Callable, the transformation is the "
#~ "value to be present in the padding"
#~ " in terms of the transformed index."
#~ msgstr ""

#~ msgid ""
#~ "The value to be used for any "
#~ "padding introduced by the transformation.  "
#~ "If the schedule contains a producer "
#~ "block for the specified buffer, the "
#~ "pad value will be written as part"
#~ " of the producer block if possible,"
#~ " or after the producer block "
#~ "otherwise.  Otherwise, if the buffer is"
#~ " an input, will insert an annotation"
#~ " block to state that the padding "
#~ "contains the known value."
#~ msgstr ""

#~ msgid ""
#~ "The pad value may not contain "
#~ "instances of BufferLoad, except where it"
#~ " loads a value from the buffer "
#~ "being transformed (e.g. to create a "
#~ "circular buffer with padding that "
#~ "consists of repeated elements)."
#~ msgstr ""

#~ msgid ""
#~ "Note: If applied to an input "
#~ "buffer, the calling scope is responsible"
#~ " for ensuring that the pad_value is"
#~ " present. Algebraic symplifications, branch "
#~ "elimination, and other optimizations may "
#~ "assume that this precondition is met,"
#~ " and may result in incorrect results"
#~ " being returned."
#~ msgstr ""

#~ msgid "If None, the transformation may not introduce padding."
#~ msgstr ""

#~ msgid ""
#~ "If an int, float or PrimExpr, the"
#~ " transformation is the specific value "
#~ "to be present in the padding."
#~ msgstr ""

#~ msgid ""
#~ "If an IndexMap or Callable, the "
#~ "transformation is the value to be "
#~ "present in the padding in terms of"
#~ " the transformed index."
#~ msgstr ""

#~ msgid ""
#~ "If set to true, the schedule  "
#~ "primitive will assume the index_map is"
#~ " injective and skip checking overlapping"
#~ " of the mapped indices. This can "
#~ "be useful for complicated index_map that"
#~ " the analysis does not cover. It "
#~ "is the callers' responsibility to ensure"
#~ " the index map is injective, "
#~ "otherwise, the correctness of the "
#~ "schedule is not guaranteed."
#~ msgstr ""

#~ msgid "Before transform_layout, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do transform_layout:"
#~ msgstr ""

#~ msgid "After applying transform_layout, the IR becomes:"
#~ msgstr ""

#~ msgid "Unannotate a block/loop's annotation with key ann_key"
#~ msgstr ""

#~ msgid "The block/loop to be unannotated"
#~ msgstr ""

#~ msgid "Before unannotate, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "After applying unannotate, the IR becomes:"
#~ msgstr ""

#~ msgid "Unroll the input loop. It requires nothing"
#~ msgstr ""

#~ msgid "The loop to be unrolled"
#~ msgstr ""

#~ msgid "Before unroll, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do unroll:"
#~ msgstr ""

#~ msgid "After applying unroll, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Hide some buffer access in a given"
#~ " block. This is an unsafe schedule"
#~ " primitive."
#~ msgstr ""

#~ msgid "The block where we hide read access."
#~ msgstr ""

#~ msgid "The buffer type: \"read\"/\"write\"."
#~ msgstr ""

#~ msgid "The array of buffer indices we hide access."
#~ msgstr ""

#~ msgid ""
#~ "This schedule primitive is unsafe, and"
#~ " may fail dependency analysis. One "
#~ "use case of `unsafe_hide_buffer_access` is "
#~ "to hide the buffer access to "
#~ "indices buffers (e.g. in sparse "
#~ "computation) so that we can further "
#~ "tensorize the block (the indices buffers"
#~ " appeared in read/write regions may "
#~ "fail the pattern matching in `tensorize`"
#~ " primitive, and hide the access to"
#~ " these buffers could address the "
#~ "issue)."
#~ msgstr ""

#~ msgid ""
#~ "Set the data type of a buffer, "
#~ "where the buffer is specified by "
#~ "the a block and write-index."
#~ msgstr ""

#~ msgid ""
#~ "This schedule primitive is unsafe and"
#~ " may change the correctness of "
#~ "program because of type conversion, "
#~ "please use with caution."
#~ msgstr ""

#~ msgid "The data type to be set"
#~ msgstr ""

#~ msgid "Before unsafe_set_dtype, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do unsafe_set_dtype:"
#~ msgstr ""

#~ msgid "After applying set_dtype, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "`unsafe_set_dtype` requires the buffer to "
#~ "be an intermediate buffer defined via"
#~ " `alloc_buffer`."
#~ msgstr ""

#~ msgid ""
#~ "Vectorize the input loop. It requires:"
#~ " 1) The scope block that the "
#~ "loop is in should have stage-"
#~ "pipeline property 2) All the blocks "
#~ "under the loop are complete blocks "
#~ "or reduction blocks, and have affine "
#~ "bindings 3) For each block under "
#~ "the loop, the loop can only be "
#~ "contained in data-parallel block iters'"
#~ " bindings"
#~ msgstr ""

#~ msgid "The loop to be vectorized"
#~ msgstr ""

#~ msgid "Before vectorize, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do vectorize:"
#~ msgstr ""

#~ msgid "After applying vectorize, the IR becomes:"
#~ msgstr ""

#~ msgid "Instruct the schedule to work on a function in the IRModule."
#~ msgstr ""

#~ msgid ""
#~ "By default, the schedule works on "
#~ "the function with the name \"main\", "
#~ "or the only function in the "
#~ "IRModule if there is only one. If"
#~ " there is multiple functions in the"
#~ " IRModule, and none of their names"
#~ " are \"main\", users will have to "
#~ "call this method to explicitly specify"
#~ " which function to work on."
#~ msgstr ""

#~ msgid ""
#~ "This sugar function will guide the "
#~ "`GetBlock` method if its `func_name` is"
#~ " not specified."
#~ msgstr ""

#~ msgid "The name of the function to work on."
#~ msgstr ""

#~ msgid "The bitmask of the `debug_mask` flag in the ScheduleState class."
#~ msgstr ""

#~ msgid ""
#~ "If the `debug_mask` flag has a "
#~ "certain bit on, then the correpsonding"
#~ " verification pass will be conducted. "
#~ "For example, if `(debug_mask & "
#~ "VERIFY_SREF_TREE) != 0`, then the "
#~ "correctness of the sref tree will "
#~ "be verified after each schedule "
#~ "instruction."
#~ msgstr ""

#~ msgid "Verify the correctness of the sref tree"
#~ msgstr ""

#~ msgid ""
#~ "Verify the correctness of affine_binding, "
#~ "region_cover and stage_pipeline"
#~ msgstr ""

#~ msgid "Error that happens during TensorIR scheduling."
#~ msgstr ""

#~ msgid ""
#~ "The state of scheduling, which exposes"
#~ " a `Replace` method as the primary"
#~ " resort for all the scheduling "
#~ "primitives to manipulate the TensorIR."
#~ msgstr ""

#~ msgid ""
#~ "The data structure contains the "
#~ "following information 1) The AST being"
#~ " scheduled (mod) 2) The sref tree "
#~ "of schedulable statements (indicated by "
#~ "the srefs) 3) The dependency information"
#~ " of each block scope (block_info) 4)"
#~ " A reverse mapping from the AST "
#~ "nodes to that in the sref tree "
#~ "(get_sref) 5) A debug flag, if "
#~ "set, extra checking is enabled "
#~ "(debug_mask) 6) A enable check flag, "
#~ "if False, some prerequisite checks are"
#~ " disabled."
#~ msgstr ""

#~ msgid "The AST of the module being scheduled"
#~ msgstr ""

#~ msgid ""
#~ "Do extra correctness checking after the"
#~ " object construction and each time "
#~ "after calling the Replace method."
#~ msgstr ""

#~ msgid ""
#~ "Indicates whether we enable prerequisite "
#~ "checks for some schedule primitives or"
#~ " not, defaults to `True`."
#~ msgstr ""

#~ msgid "Get the BlockScope correpsonding to the block sref"
#~ msgstr ""

#~ msgid "The block sref to be retrieved"
#~ msgstr ""

#~ msgid "**sref** -- The corresponding sref"
#~ msgstr ""

#~ msgid "Return the corresponding sref that points to the stmt"
#~ msgstr ""

#~ msgid "The schedulable statement in the TensorIR to be retrieved for its sref"
#~ msgstr ""

#~ msgid ""
#~ "Replace the part of the AST, as"
#~ " being pointed to by `src_sref`, with"
#~ " a specific statement `tgt_stmt`, and "
#~ "maintain the sref tree accordingly. "
#~ "Replace will try to perform copy "
#~ "on write as much as possible when"
#~ " the ScheduleState holds the only "
#~ "copy to the IRModule and IR nodes."
#~ msgstr ""

#~ msgid ""
#~ "Only 3 types of replacements are "
#~ "allowed: from `src_sref->stmt` to `tgt_stmt`."
#~ " 1) Block -> Block 2) Loop ->"
#~ " Loop 3) Loop -> BlockRealize"
#~ msgstr ""

#~ msgid "The sref to the statement to be replaced in the TensorIR AST"
#~ msgstr ""

#~ msgid "The statement to be replaced to"
#~ msgstr ""

#~ msgid ""
#~ "Maps an old block (to be replaced"
#~ " in the subtree under `src_sref->stmt`) "
#~ "to a new block (replaced to, in"
#~ " the subtree under `tgt_stmt`), and "
#~ "enforces reuse of srefs between them "
#~ "(rather than create new srefs) i.e. "
#~ "after being replaced, the sref that "
#~ "points to the old block will point"
#~ " to the new one"
#~ msgstr ""

#~ msgid ""
#~ "The reuse of loop srefs are "
#~ "detected automatically according to the "
#~ "reuse of loop vars."
#~ msgstr ""

#~ msgid ""
#~ "An object that refers to schedulable "
#~ "elements in the TensorIR, aka \"sref\"."
#~ msgstr ""

#~ msgid ""
#~ "Glossary - Block sref: An StmtSref "
#~ "that points to a TensorIR block. -"
#~ " Loop sref: An StmtSRef that points"
#~ " to a TensorIR for loop. - "
#~ "Parent sref: The parent sref of an"
#~ " sref is the block/loop sref that "
#~ "points to its closest schedulable "
#~ "statement of its ancestors on the "
#~ "TensorIR AST. - Root sref: Sref to"
#~ " the root block. Every sref has "
#~ "exactly one parent sref except for "
#~ "root sref. - Sref tree: The "
#~ "parent-children-relationship of srefs that"
#~ " forms a tree, uniquely determined by"
#~ " the TensorIR AST."
#~ msgstr ""

#~ msgid ""
#~ "A special StmtSRef, which doesn't point"
#~ " to any stmt in the AST, only"
#~ " serving as a \"mark\" to hint "
#~ "compute-at to do the work of "
#~ "compute-inline"
#~ msgstr ""

#~ msgid "The parent sref"
#~ msgstr ""

#~ msgid ""
#~ "A special StmtSRef, which doesn't point"
#~ " to any stmt in the AST, only"
#~ " serving as a \"mark\" to hint "
#~ "compute-at to do nothing"
#~ msgstr ""

#~ msgid "The block/for stmt the object refers to"
#~ msgstr ""

#~ msgid "An execution trace of a scheduling program."
#~ msgstr ""

#~ msgid ""
#~ "A trace has two parts: 1) The "
#~ "instructions invoked so far 2) The "
#~ "random decisions made upon those "
#~ "instructions, if any"
#~ msgstr ""

#~ msgid ""
#~ "A trace can be serialized to: 1)"
#~ " Roundtrippable JSON format: can be "
#~ "saved to file and loaded back 2)"
#~ " Python syntax: allows users to "
#~ "copy-paste the trace to reproduce the"
#~ " scheduling process"
#~ msgstr ""

#~ msgid ""
#~ "A trace can be applied to a "
#~ "TensorIR schedule by re-applying all "
#~ "its instructions possibly with their "
#~ "decisions accordingly. Re-sampling is "
#~ "invoked if a sampling instruction "
#~ "doesn't have its corresponding decision; "
#~ "Otherwise the existing decision will be"
#~ " reused accordingly."
#~ msgstr ""

#~ msgid "The instructions invoked so far in the program execution"
#~ msgstr ""

#~ msgid "List[Instruction]"
#~ msgstr ""

#~ msgid "The random decisions made upon those instructions"
#~ msgstr ""

#~ msgid "Dict[Instruction, DECISION_TYPE]"
#~ msgstr ""

#~ msgid "Append a new instruction to the trace"
#~ msgstr ""

#~ msgid "The new instruction to be appended"
#~ msgstr ""

#~ msgid "The random decision made on this instruction"
#~ msgstr ""

#~ msgid "Apply a JSON-serialized trace to a TensorIR schedule"
#~ msgstr ""

#~ msgid "The JSON-serialized trace"
#~ msgstr ""

#~ msgid "The TensorIR schedule"
#~ msgstr ""

#~ msgid "Apply the trace to a TensorIR schedule"
#~ msgstr ""

#~ msgid "The schedule to be applied onto"
#~ msgstr ""

#~ msgid "If postprocessing instructions are removed"
#~ msgstr ""

#~ msgid ""
#~ "A callback that allows users to "
#~ "mutate decisions on the fly when "
#~ "applying instructions. The signature of "
#~ "the callback is: - The 1st "
#~ "argument: The instruction - The 2nd "
#~ "argument: The input random variables -"
#~ " The 3rd argument: The attributes -"
#~ " The 4th argument: The decision - "
#~ "Return: A new decision"
#~ msgstr ""

#~ msgid "Serialize the trace as a JSON-style object"
#~ msgstr ""

#~ msgid "**json** -- The JSON-style object"
#~ msgstr ""

#~ msgid "Serialize the trace as a sequence of python statements"
#~ msgstr ""

#~ msgid "**py_stmts** -- A sequence of python statements"
#~ msgstr ""

#~ msgid "Retrieve the decision made on a specific instruction"
#~ msgstr ""

#~ msgid "The instruction whose decision is to be retrieved"
#~ msgstr ""

#~ msgid ""
#~ "**decision** -- The corresponding decision;"
#~ " None if there is no decision "
#~ "made on the instruction"
#~ msgstr ""

#~ msgid ""
#~ "Remove the last instruction, along with"
#~ " the decision made on that "
#~ "instruction, if any"
#~ msgstr ""

#~ msgid ""
#~ "**popped_inst** -- Returns the instruction "
#~ "removed; NullOpt if the trace is "
#~ "empty"
#~ msgstr ""

#~ msgid ""
#~ "Pygmentize printing style, auto-detected "
#~ "if None.  See `tvm.script.highlight.cprint` "
#~ "for more details."
#~ msgstr ""

#~ msgid ""
#~ "If true, use the formatter Black "
#~ "to format the TVMScript. If None, "
#~ "determine based on the \"TVM_BLACK_FORMAT\""
#~ " environment variable."
#~ msgstr ""

#~ msgid "Simplify the trace with dead-code elimination"
#~ msgstr ""

#~ msgid "**trace** -- A simplified trace"
#~ msgstr ""

#~ msgid ""
#~ "Create a new trace with an "
#~ "instruction whose decision is changed, "
#~ "assuming this instruction exists in the"
#~ " resulting trace"
#~ msgstr ""

#~ msgid "The instruction whose decision is to be changed"
#~ msgstr ""

#~ msgid "The decision to be changed to"
#~ msgstr ""

#~ msgid "**trace** -- The new trace with the decision changed"
#~ msgstr ""

