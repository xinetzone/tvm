# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-13 12:27+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../notebook/docs/reference/api/python/topi.rst:19
msgid "tvm.topi"
msgstr ""

#: of tvm.topi:1
msgid "TVM Operator Inventory."
msgstr ""

#: of tvm.topi:3
msgid ""
"TOPI is the operator collection library for TVM, to provide sugars for "
"constructing compute declaration as well as optimized schedules."
msgstr ""

#: of tvm.topi:6
msgid ""
"Some of the schedule function may have been specially optimized for a "
"specific workload."
msgstr ""

#: of tvm.topi:1 tvm.topi.nn:1
msgid "**Classes:**"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":py:obj:`Cast <tvm.topi.Cast>`\\ \\(dtype\\, value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1 tvm.topi:1:<autosummary>:1
msgid "Cast expression."
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":py:obj:`PrimExpr <tvm.topi.PrimExpr>`\\ \\(\\)"
msgstr ""

#: of tvm.ir.expr.PrimExpr:1 tvm.topi:1:<autosummary>:1
msgid "Base class of all primitive expressions."
msgstr ""

#: of tvm.topi:1
msgid "**Exceptions:**"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid ":py:obj:`InvalidShapeError <tvm.topi.InvalidShapeError>`\\"
msgstr ""

#: of tvm.topi:1:<autosummary>:1
msgid "Invalid shape for a topi function."
msgstr ""

#: of tvm.topi:1 tvm.topi.image:1 tvm.topi.nn:1 tvm.topi.sparse:1
msgid "**Functions:**"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`abs <tvm.topi.abs>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.abs:1
msgid "Take absolute value of the input of x, element-wise."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`acos <tvm.topi.acos>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.acos:1
msgid "Take arc cos of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`acosh <tvm.topi.acosh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.acosh:1
msgid "Take arc cosh of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`add <tvm.topi.add>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.add:1
msgid "Addition with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`adv_index <tvm.topi.adv_index>`\\ \\(data\\, indices\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.adv_index:1
msgid "Numpy style indexing with tensors."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`all <tvm.topi.all>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.reduction.all:1
msgid "Logical AND of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`any <tvm.topi.any>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.reduction.any:1
msgid "Logical OR of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`arange <tvm.topi.arange>`\\ \\(start\\[\\, stop\\, step\\, "
"dtype\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.arange:1
msgid "Creates a tensor with evenly spaced values within a given interval."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`argmax <tvm.topi.argmax>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"select\\_last\\_index\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.reduction.argmax:1
msgid "Returns the indices of the maximum values along an axis."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`argmin <tvm.topi.argmin>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"select\\_last\\_index\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.reduction.argmin:1
msgid "Returns the indices of the minimum values along an axis."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`argsort <tvm.topi.argsort>`\\ \\(data\\[\\, valid\\_count\\, "
"axis\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.sort.argsort:1
msgid ""
"Performs sorting along the given axis and returns an array of indices "
"having the same shape as an input array that index data in sorted order."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`argwhere <tvm.topi.argwhere>`\\ \\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.argwhere.argwhere:1
msgid "Find the indices of elements of a tensor that are non-zero."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`asin <tvm.topi.asin>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.asin:1
msgid "Take arc sin of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`asinh <tvm.topi.asinh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.asinh:1
msgid "Take arc sinh of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`atan <tvm.topi.atan>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.atan:1
msgid "Take atan of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`atanh <tvm.topi.atanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.atanh:1
msgid "Take atanh of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`binary_search <tvm.topi.binary_search>`\\ \\(ib\\, "
"sequence\\_offset\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.searchsorted.binary_search:1
msgid "Common IR generator for binary search used by CPU and GPU backends."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`bitwise_and <tvm.topi.bitwise_and>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.bitwise_and:1
msgid "Compute element-wise bitwise and of data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`bitwise_not <tvm.topi.bitwise_not>`\\ \\(data\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.bitwise_not:1
msgid "Compute element-wise bitwise not of data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`bitwise_or <tvm.topi.bitwise_or>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.bitwise_or:1
msgid "Compute element-wise bitwise or of data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`bitwise_xor <tvm.topi.bitwise_xor>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.bitwise_xor:1
msgid "Compute element-wise bitwise xor of data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`broadcast_to <tvm.topi.broadcast_to>`\\ \\(data\\, shape\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.broadcast_to:1
msgid "Broadcast the src to the target shape"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`cast <tvm.topi.cast>`\\ \\(x\\, dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.cast:1
msgid "Cast input to specified data type."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`ceil <tvm.topi.ceil>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.ceil:1
msgid "Take ceil of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`ceil_log2 <tvm.topi.ceil_log2>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
"Compute integer ceil log2 with a special code path for vulkan SPIR-V does"
" not support log2 on fp64."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`clip <tvm.topi.clip>`\\ \\(x\\, a\\_min\\, a\\_max\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid "Clip (limit) the values in an array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`collapse_sum <tvm.topi.collapse_sum>`\\ \\(data\\, "
"target\\_shape\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.reduction.collapse_sum:1
msgid "Return a summation of data to the given shape."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`concatenate <tvm.topi.concatenate>`\\ \\(a\\_tuple\\[\\, "
"axis\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.transform.concatenate:1
msgid "Join a sequence of arrays along an existing axis."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`const_vector <tvm.topi.const_vector>`\\ \\(vector\\[\\, "
"name\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.utils.const_vector:1
msgid "convert a const numpy 1-dimensional vector to tvm tensor"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`cos <tvm.topi.cos>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.cos:1
msgid "Take cos of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`cosh <tvm.topi.cosh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.cosh:1
msgid "Take cosh of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`cumprod <tvm.topi.cumprod>`\\ \\(data\\[\\, axis\\, dtype\\, "
"exclusive\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid "Numpy style cumprod op."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`cumsum <tvm.topi.cumsum>`\\ \\(data\\[\\, axis\\, dtype\\, "
"exclusive\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid "Numpy style cumsum op."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`decl_buffer <tvm.topi.decl_buffer>`\\ \\(shape\\[\\, dtype\\, "
"name\\, data\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:1 tvm.tir.expr.Cast:1:<autosummary>:1
msgid "Declare a new symbolic buffer."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`dft <tvm.topi.dft>`\\ \\(re\\_data\\, im\\_data\\, inverse\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
"Computes the discrete Fourier transform of input (calculation along the "
"last axis)."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`div <tvm.topi.div>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.tir.op.div:1
msgid "Compute a / b as in C/C++ semantics."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`divide <tvm.topi.divide>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.divide:1
msgid "Division with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`dynamic_strided_slice <tvm.topi.dynamic_strided_slice>`\\ "
"\\(a\\, begin\\, end\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.transform.dynamic_strided_slice:1
#: tvm.topi.transform.strided_slice:1
msgid "Slice of an array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`einsum <tvm.topi.einsum>`\\ \\(subscripts\\, \\*operand\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.einsum.einsum:1
msgid "Evaluates the Einstein summation convention on the operands."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`elemwise_sum <tvm.topi.elemwise_sum>`\\ \\(xs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.tensor.elemwise_sum:1
msgid "Perform element-wise sum on inputs"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`equal <tvm.topi.equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.equal:1
msgid "Compute (lhs==rhs) with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`erf <tvm.topi.erf>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.erf:1
msgid "Take gauss error function of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`erf_legalize <tvm.topi.erf_legalize>`\\ \\(attrs\\, inputs\\, "
"types\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.erf_legalize:1
msgid "Legalizes ERF op."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`exp <tvm.topi.exp>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.exp:1
msgid "Take exponential of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`expand_dims <tvm.topi.expand_dims>`\\ \\(a\\, axis\\[\\, "
"num\\_newaxis\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.expand_dims:1
msgid "Expand the shape of an array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`expand_like <tvm.topi.expand_like>`\\ \\(a\\, shape\\_like\\, "
"axis\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid "Expand an input array with the shape of second array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`extern <tvm.topi.extern>`\\ \\(shape\\, inputs\\, fcompute\\[\\,"
" name\\, ...\\]\\)"
msgstr ""

#: of tvm.te.operation.extern:1 tvm.tir.expr.Cast:1:<autosummary>:1
msgid "Compute several tensors via an extern function."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`fast_erf <tvm.topi.fast_erf>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.fast_erf:1
msgid "Take gauss error function of input x using fast_erf implementation."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`fast_exp <tvm.topi.fast_exp>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.fast_exp:1
msgid "Take exponential of input x using fast_exp implementation"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`fast_tanh <tvm.topi.fast_tanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.fast_tanh:1
msgid "Take hyperbolic tangent of input x using fast_tanh implementation"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`fixed_point_multiply <tvm.topi.fixed_point_multiply>`\\ \\(x\\, "
"multiplier\\, shift\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.fixed_point_multiply:1
#: tvm.topi.math.fixed_point_multiply_per_axis:1
msgid ""
"Fixed point multiplication between data and a fixed point constant "
"expressed as multiplier * 2^(-shift), where multiplier is a Q-number with"
" 31 fractional bits"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`fixed_point_multiply_per_axis "
"<tvm.topi.fixed_point_multiply_per_axis>`\\ \\(x\\, y\\, lshift\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`flip <tvm.topi.flip>`\\ \\(a\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.flip:1
msgid "Flip/reverse elements of an array in a particular axis."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`floor <tvm.topi.floor>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.floor:1
msgid "Take floor of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`floor_divide <tvm.topi.floor_divide>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.floor_divide:1
msgid "Floor division with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`floor_mod <tvm.topi.floor_mod>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.floor_mod:1
msgid "Floor modulus with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`floordiv <tvm.topi.floordiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.tir.op.floordiv:1
msgid "Compute the floordiv of two expressions."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`floormod <tvm.topi.floormod>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.tir.op.floormod:1
msgid "Compute the floormod of two expressions."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`full <tvm.topi.full>`\\ \\(shape\\, dtype\\, fill\\_value\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.tensor.full:1
msgid "Fill tensor with fill_value"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`full_like <tvm.topi.full_like>`\\ \\(x\\, fill\\_value\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.tensor.full_like:2
msgid "Construct a tensor with same shape as input tensor,"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`gather <tvm.topi.gather>`\\ \\(data\\, axis\\, indices\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.gather:1
msgid "Gather values along given axis from given indices."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`gather_nd <tvm.topi.gather_nd>`\\ \\(a\\, indices\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.gather_nd:1
msgid "Gather elements from a n-dimension array.."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`get_const_tuple <tvm.topi.get_const_tuple>`\\ \\(in\\_tuple\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.utils.get_const_tuple:1
msgid "Verifies input tuple is IntImm or Var, returns tuple of int or Var."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`greater <tvm.topi.greater>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.greater:1
msgid "Compute (lhs>rhs) with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`greater_equal <tvm.topi.greater_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.greater_equal:1
msgid "Compute (lhs>=rhs) with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`hybrid_argwhere_1d <tvm.topi.hybrid_argwhere_1d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.argwhere.hybrid_argwhere_1d:1
msgid "Find the indices of elements of a 1-D tensor that are non-zero."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`hybrid_argwhere_2d <tvm.topi.hybrid_argwhere_2d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.argwhere.hybrid_argwhere_2d:1
msgid "Find the indices of elements of a 2-D tensor that are non-zero."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`hybrid_argwhere_3d <tvm.topi.hybrid_argwhere_3d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.argwhere.hybrid_argwhere_3d:1
msgid "Find the indices of elements of a 3-D tensor that are non-zero."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`hybrid_argwhere_4d <tvm.topi.hybrid_argwhere_4d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.argwhere.hybrid_argwhere_4d:1
msgid "Find the indices of elements of a 4-D tensor that are non-zero."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`hybrid_argwhere_5d <tvm.topi.hybrid_argwhere_5d>`\\ "
"\\(output\\_shape\\, condition\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.argwhere.hybrid_argwhere_5d:1
msgid "Find the indices of elements of a 5-D tensor that are non-zero."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`identity <tvm.topi.identity>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.identity:1
msgid "Take identity of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`invert_permutation <tvm.topi.invert_permutation>`\\ \\(data\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.transform.invert_permutation:1
msgid "Computes the inverse permutation of data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`isfinite <tvm.topi.isfinite>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.isfinite:1
msgid "Check if value of x is finite, element-wise."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`isinf <tvm.topi.isinf>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.isinf:1
msgid "Check if value of x is infinite, element-wise."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`isnan <tvm.topi.isnan>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.isnan:1
msgid "Check if value of x is NaN, element-wise."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`layout_transform <tvm.topi.layout_transform>`\\ \\(array\\, "
"src\\_layout\\, dst\\_layout\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.layout_transform:1
msgid "Transform the layout according to src_layout and dst_layout"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`left_shift <tvm.topi.left_shift>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.left_shift:1
msgid "Left shift with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`less <tvm.topi.less>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.less:1
msgid "Compute (lhs<rhs) with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`less_equal <tvm.topi.less_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.less_equal:1
msgid "Compute (lhs<=rhs) with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`log <tvm.topi.log>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.log:1
msgid "Take logarithm of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`log10 <tvm.topi.log10>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.log10:1
msgid "Take logarithm to the base 10 of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`log2 <tvm.topi.log2>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.log2:1
msgid "Take logarithm to the base 2 of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`logical_and <tvm.topi.logical_and>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.logical_and:1
msgid "Compute element-wise logical and of data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`logical_not <tvm.topi.logical_not>`\\ \\(data\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.logical_not:1
msgid "Compute element-wise logical not of data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`logical_or <tvm.topi.logical_or>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.logical_or:1
msgid "Compute element-wise logical or of data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`logical_xor <tvm.topi.logical_xor>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.logical_xor:1
msgid "Compute element-wise logical xor of data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`make_idx <tvm.topi.make_idx>`\\ \\(b\\, e\\, s\\, z\\, i\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.utils.make_idx:1
msgid ""
"Return the array position in the selection that corresponds to an array "
"position in the full array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`matmul <tvm.topi.matmul>`\\ \\(a\\, b\\[\\, transp\\_a\\, "
"transp\\_b\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.matmul:1
msgid ""
"Creates an operation that calculates a matrix multiplication (row-major "
"notation): A(i, k) * B(k, j) if trans_a == trans_b, the usual transposed "
"combinations, otherwise"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`matrix_set_diag <tvm.topi.matrix_set_diag>`\\ \\(data\\, "
"diagonal\\[\\, k\\, align\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.matrix_set_diag:1
msgid ""
"Returns a tensor with the diagonals of input tensor replaced with the "
"provided diagonal values."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`max <tvm.topi.max>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.reduction.max:1
msgid "Maximum of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`maximum <tvm.topi.maximum>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.maximum:1
#: tvm.topi.broadcast.minimum:1
msgid "Take element-wise maximum of two tensors with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`meshgrid <tvm.topi.meshgrid>`\\ \\(a\\_tuple\\, indexing\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.meshgrid:1
msgid "Create coordinate matrices from coordinate vectors."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`min <tvm.topi.min>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.reduction.min:1
msgid "Minimum of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`minimum <tvm.topi.minimum>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`mod <tvm.topi.mod>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.mod:1
msgid "Modulus with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`multiply <tvm.topi.multiply>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.multiply:1
msgid "Multiplication with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`ndarray_size <tvm.topi.ndarray_size>`\\ \\(array\\[\\, "
"dtype\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.ndarray_size:1
msgid "Get the number of elements of input array"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`negative <tvm.topi.negative>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.negative:1
msgid "Take negation of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`not_equal <tvm.topi.not_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.not_equal:1
msgid "Compute (lhs!=rhs) with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`one_hot <tvm.topi.one_hot>`\\ \\(indices\\, on\\_value\\, "
"off\\_value\\, depth\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
"Returns a one-hot tensor where the locations repsented by indices take "
"value on_value, other locations take value off_value."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`power <tvm.topi.power>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.power:1
msgid "Power with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`prod <tvm.topi.prod>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.reduction.prod:1
msgid "Product of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`reinterpret <tvm.topi.reinterpret>`\\ \\(x\\, dtype\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.reinterpret:1
msgid "Reinterpret input to specified data type."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`repeat <tvm.topi.repeat>`\\ \\(a\\, repeats\\, axis\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.repeat:1
msgid "Repeats elements of an array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`reshape <tvm.topi.reshape>`\\ \\(a\\, newshape\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.reshape:1
msgid "Reshape the array"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`reverse_sequence <tvm.topi.reverse_sequence>`\\ \\(a\\, "
"seq\\_lengths\\[\\, seq\\_axis\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid "Reverse the tensor for variable length slices."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`right_shift <tvm.topi.right_shift>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.right_shift:1
msgid "Right shift with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`round <tvm.topi.round>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.round:1
msgid "Round elements of x to nearest integer."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`rsqrt <tvm.topi.rsqrt>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.rsqrt:1
msgid "Take inverse square root of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`scanop <tvm.topi.scanop>`\\ \\(data\\, binop\\, "
"identity\\_value\\, op\\_name\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.scan.scanop:1
msgid ""
"Cumulative binary operator (scan) with similar axis behavior as np.cumsum"
" and np.cumprod."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`scatter_elements <tvm.topi.scatter_elements>`\\ \\(data\\, "
"indices\\, updates\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.scatter_elements.scatter_elements:1
msgid "Scatter elements from updates to corresponding indices of copied data."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`scatter_nd <tvm.topi.scatter_nd>`\\ \\(data\\, indices\\, "
"updates\\, mode\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.scatter.scatter_nd:1
msgid "Scatter elements from a n-dimension array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`searchsorted <tvm.topi.searchsorted>`\\ \\(sorted\\_sequence\\, "
"values\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.searchsorted.searchsorted:3
msgid "Find indices where elements should be inserted to maintain order."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`sequence_mask <tvm.topi.sequence_mask>`\\ \\(data\\, "
"valid\\_length\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.sequence_mask:1
msgid ""
"Sets all elements outside the expected length of the sequence to a "
"constant value."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`shape <tvm.topi.shape>`\\ \\(array\\[\\, dtype\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.shape:1
msgid "Get the shape of input array"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`sigmoid <tvm.topi.sigmoid>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.sigmoid:1
msgid "Take sigmoid tanh of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`sign <tvm.topi.sign>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.sign:1
msgid "Returns -1, 0, 1 based on sign of x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`sin <tvm.topi.sin>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.sin:1
msgid "Take sin of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`sinh <tvm.topi.sinh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.sinh:1
msgid "Take sinh of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`sliding_window <tvm.topi.sliding_window>`\\ \\(data\\, axis\\, "
"window\\_shape\\, strides\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.sliding_window:1
msgid "Slide a window over the data tensor."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`sort <tvm.topi.sort>`\\ \\(data\\[\\, axis\\, is\\_ascend\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.sort.sort:1
msgid ""
"Performs sorting along the given axis and returns an array in sorted "
"order."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`sparse_reshape <tvm.topi.sparse_reshape>`\\ "
"\\(sparse\\_indices\\, prev\\_shape\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
#: tvm.topi.sparse_reshape.sparse_reshape:1
msgid "Reshape a Sparse Tensor"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`sparse_to_dense <tvm.topi.sparse_to_dense>`\\ "
"\\(sparse\\_indices\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.sparse_to_dense:1
msgid "Converts a sparse representation into a dense tensor."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`split <tvm.topi.split>`\\ \\(ary\\, "
"indices\\_or\\_sections\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.split:1
msgid "Split an array into multiple sub-arrays."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`sqrt <tvm.topi.sqrt>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.sqrt:1
msgid "Take square root of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`squeeze <tvm.topi.squeeze>`\\ \\(a\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.squeeze:1
msgid "Remove single-dimensional entries from the shape of an array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`stack <tvm.topi.stack>`\\ \\(a\\, axis\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.stack:1
#: tvm.topi.transform.tile:1
msgid "Repeats the whole array multiple times."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`stft <tvm.topi.stft>`\\ \\(data\\, n\\_fft\\, hop\\_length\\, "
"win\\_length\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
"The STFT computes the Fourier transform of short overlapping windows of "
"the input. This gives frequency components of the signal as they change "
"over time. Parameters ---------- data : relay.Expr     Either a 1-D "
"tensor or a 2-D batch tensor. n_fft : int     The size of Fourier "
"transform hop_length : int     The distance between neighboring sliding "
"window frames win_length : int     The size of window frame and STFT "
"filter window : relay.Expr     A 1-D tensor window frame normalized : "
"bool     Whether to return the normalized STFT results onesided : bool"
"     Whether to return onesided result or fill with conjugate symmetry "
"Returns ------- output : relay.Expr     Tensor containing the STFT result"
" Examples -------- .. code-block:: python."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`strided_set <tvm.topi.strided_set>`\\ \\(a\\, v\\, begin\\, "
"end\\[\\, strides\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.strided_set:1
msgid "Set slice of an array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`strided_slice <tvm.topi.strided_slice>`\\ \\(a\\, begin\\, "
"end\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`subtract <tvm.topi.subtract>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.broadcast.subtract:1
msgid "Subtraction with auto-broadcasting"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`sum <tvm.topi.sum>`\\ \\(data\\[\\, axis\\, keepdims\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.reduction.sum:1
msgid "Sum of array elements over a given axis or a list of axes"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`take <tvm.topi.take>`\\ \\(a\\, indices\\[\\, axis\\, "
"batch\\_dims\\, mode\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.take:1
msgid "Take elements from an array along an axis."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`take_legalize <tvm.topi.take_legalize>`\\ \\(attrs\\, inputs\\, "
"types\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.take_legalize:1
msgid "Legalizes dyn.topk op."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`tan <tvm.topi.tan>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.tan:1
msgid "Take tan of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`tanh <tvm.topi.tanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.tanh:1
msgid "Take hyperbolic tanh of input x."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`tensordot <tvm.topi.tensordot>`\\ \\(a\\, b\\, axes\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.tensordot:1
msgid "A generalization of matrix multiplication to tensor."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`tile <tvm.topi.tile>`\\ \\(a\\, reps\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`topk <tvm.topi.topk>`\\ \\(data\\[\\, k\\, axis\\, ret\\_type\\,"
" is\\_ascend\\, dtype\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.sort.topk:1
msgid "Get the top k elements in an input tensor along the given axis."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`transpose <tvm.topi.transpose>`\\ \\(a\\[\\, axes\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.transpose:1
msgid "Permute the dimensions of an array."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`trilu <tvm.topi.trilu>`\\ \\(data\\, k\\, upper\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.trilu:1
msgid ""
"Given a 2-D matrix or batches of 2-D matrices, returns the upper or lower"
" triangular part of the tensor."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`trunc <tvm.topi.trunc>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.math.trunc:1
msgid "Take truncated value of the input of x, element-wise."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ""
":py:obj:`unique <tvm.topi.unique>`\\ \\(data\\[\\, is\\_sorted\\, "
"return\\_counts\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid "Find the unique elements of a 1-D tensor."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`unravel_index <tvm.topi.unravel_index>`\\ \\(indices\\, shape\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.unravel_index:1
msgid ""
"Convert a flat index or array of flat indices into a tuple of coordinate "
"arrays."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`where <tvm.topi.where>`\\ \\(condition\\, x\\, y\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.transform.where:1
msgid "Get the elements, either from x or y, depending on the condition."
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1
msgid ":py:obj:`within_index <tvm.topi.within_index>`\\ \\(b\\, e\\, s\\, i\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1:<autosummary>:1 tvm.topi.utils.within_index:1
msgid "Return a boolean value that indicates if i is within the given index."
msgstr ""

#: of tvm.te.operation.extern:4 tvm.tir.buffer.decl_buffer:9
#: tvm.tir.expr.Cast:4 tvm.tir.op.div:4 tvm.tir.op.floordiv:4
#: tvm.tir.op.floormod:4 tvm.topi.argwhere.argwhere:4
#: tvm.topi.argwhere.hybrid_argwhere_1d:4
#: tvm.topi.argwhere.hybrid_argwhere_2d:4
#: tvm.topi.argwhere.hybrid_argwhere_3d:4
#: tvm.topi.argwhere.hybrid_argwhere_4d:4
#: tvm.topi.argwhere.hybrid_argwhere_5d:4 tvm.topi.broadcast.add:4
#: tvm.topi.broadcast.bitwise_and:4 tvm.topi.broadcast.bitwise_not:4
#: tvm.topi.broadcast.bitwise_or:4 tvm.topi.broadcast.bitwise_xor:4
#: tvm.topi.broadcast.broadcast_to:7 tvm.topi.broadcast.divide:4
#: tvm.topi.broadcast.equal:4 tvm.topi.broadcast.floor_divide:4
#: tvm.topi.broadcast.floor_mod:4 tvm.topi.broadcast.greater:4
#: tvm.topi.broadcast.greater_equal:4 tvm.topi.broadcast.left_shift:4
#: tvm.topi.broadcast.less:4 tvm.topi.broadcast.less_equal:4
#: tvm.topi.broadcast.logical_and:4 tvm.topi.broadcast.logical_not:4
#: tvm.topi.broadcast.logical_or:4 tvm.topi.broadcast.logical_xor:4
#: tvm.topi.broadcast.maximum:4 tvm.topi.broadcast.minimum:4
#: tvm.topi.broadcast.mod:4 tvm.topi.broadcast.multiply:4
#: tvm.topi.broadcast.not_equal:4 tvm.topi.broadcast.power:4
#: tvm.topi.broadcast.right_shift:4 tvm.topi.broadcast.subtract:4
#: tvm.topi.einsum.einsum:4 tvm.topi.image.dilation2d.dilation2d_nchw:4
#: tvm.topi.image.dilation2d.dilation2d_nhwc:4
#: tvm.topi.image.grid_sample.affine_grid:8
#: tvm.topi.image.grid_sample.grid_sample:31
#: tvm.topi.image.resize.crop_and_resize:4 tvm.topi.image.resize.resize1d:4
#: tvm.topi.image.resize.resize2d:4 tvm.topi.image.resize.resize3d:4
#: tvm.topi.math.abs:4 tvm.topi.math.acos:4 tvm.topi.math.acosh:4
#: tvm.topi.math.asin:4 tvm.topi.math.asinh:4 tvm.topi.math.atan:4
#: tvm.topi.math.atanh:4 tvm.topi.math.cast:4 tvm.topi.math.ceil:4
#: tvm.topi.math.ceil_log2:6 tvm.topi.math.clip:5 tvm.topi.math.cos:4
#: tvm.topi.math.cosh:4 tvm.topi.math.erf:4 tvm.topi.math.erf_legalize:4
#: tvm.topi.math.exp:4 tvm.topi.math.fast_erf:4 tvm.topi.math.fast_exp:4
#: tvm.topi.math.fast_tanh:4 tvm.topi.math.fixed_point_multiply:6
#: tvm.topi.math.fixed_point_multiply_per_axis:5 tvm.topi.math.floor:4
#: tvm.topi.math.identity:4 tvm.topi.math.isfinite:4 tvm.topi.math.isinf:4
#: tvm.topi.math.isnan:4 tvm.topi.math.log:4 tvm.topi.math.log10:4
#: tvm.topi.math.log2:4 tvm.topi.math.negative:4 tvm.topi.math.reinterpret:4
#: tvm.topi.math.round:4 tvm.topi.math.rsqrt:4 tvm.topi.math.sigmoid:4
#: tvm.topi.math.sign:4 tvm.topi.math.sin:4 tvm.topi.math.sinh:4
#: tvm.topi.math.sqrt:4 tvm.topi.math.tan:4 tvm.topi.math.tanh:4
#: tvm.topi.math.trunc:4 tvm.topi.nn.batch_matmul.batch_matmul:7
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:4
#: tvm.topi.nn.batch_norm.batch_norm:8
#: tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:4
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:4
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:4
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:4
#: tvm.topi.nn.bitserial_dense.bitserial_dense:4
#: tvm.topi.nn.bitserial_util.bitpack:4 tvm.topi.nn.bnn.binarize_pack:4
#: tvm.topi.nn.bnn.binary_dense:4 tvm.topi.nn.conv1d.conv1d:4
#: tvm.topi.nn.conv1d.group_conv1d_ncw:4 tvm.topi.nn.conv1d.group_conv1d_nwc:4
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:4
#: tvm.topi.nn.conv2d.conv:6 tvm.topi.nn.conv2d.conv2d:4
#: tvm.topi.nn.conv2d.conv2d_NCHWc:4 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:4
#: tvm.topi.nn.conv2d.conv2d_alter_layout:4
#: tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:4
#: tvm.topi.nn.conv2d.conv2d_hwcn:4 tvm.topi.nn.conv2d.conv2d_infer_layout:4
#: tvm.topi.nn.conv2d.conv2d_legalize:4 tvm.topi.nn.conv2d.conv2d_nchw:4
#: tvm.topi.nn.conv2d.conv2d_nhwc:4 tvm.topi.nn.conv2d.conv2d_winograd_nchw:5
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:5
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:5
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:5
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:4
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:4
#: tvm.topi.nn.conv2d.group_conv2d_nchw:4
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:4
#: tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:4
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:4
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:4
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:4
#: tvm.topi.nn.conv2d_transpose.layout_transform:6
#: tvm.topi.nn.conv3d.conv3d_alter_layout:4 tvm.topi.nn.conv3d.conv3d_ncdhw:4
#: tvm.topi.nn.conv3d.conv3d_ndhwc:4
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:4
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:4
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:4
#: tvm.topi.nn.correlation.correlation_nchw:4
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:6
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:6
#: tvm.topi.nn.dense.dense:6 tvm.topi.nn.dense.dense_alter_layout:4
#: tvm.topi.nn.dense.dense_legalize:4 tvm.topi.nn.dense.dense_pack:4
#: tvm.topi.nn.dense.matmul:4 tvm.topi.nn.dense.matmul_legalize:4
#: tvm.topi.nn.depth_to_space.depth_to_space:4
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:4
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:4
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:4
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:4
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:4
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:4
#: tvm.topi.nn.dilate.dilate:4 tvm.topi.nn.elemwise.leaky_relu:4
#: tvm.topi.nn.elemwise.prelu:8 tvm.topi.nn.elemwise.relu:4
#: tvm.topi.nn.fifo_buffer.fifo_buffer:18 tvm.topi.nn.flatten.flatten:4
#: tvm.topi.nn.group_norm.group_norm:6
#: tvm.topi.nn.instance_norm.instance_norm:4
#: tvm.topi.nn.layer_norm.layer_norm:6 tvm.topi.nn.local_response_norm.lrn:9
#: tvm.topi.nn.loss.nll_loss:11 tvm.topi.nn.lstm.lstm:4
#: tvm.topi.nn.mapping.scale_shift_nchw:4
#: tvm.topi.nn.mapping.scale_shift_nchwc:4
#: tvm.topi.nn.mapping.scale_shift_nhwc:4 tvm.topi.nn.pad.mirror_pad:4
#: tvm.topi.nn.pad.pad:4 tvm.topi.nn.pooling.adaptive_pool:12
#: tvm.topi.nn.pooling.global_pool:10 tvm.topi.nn.pooling.pool1d:10
#: tvm.topi.nn.pooling.pool2d:10 tvm.topi.nn.pooling.pool3d:10
#: tvm.topi.nn.pooling.pool_grad:10 tvm.topi.nn.qnn.add_alter_layout:8
#: tvm.topi.nn.qnn.bias_add_legalize:7
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:4
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:5
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:4
#: tvm.topi.nn.qnn.simulated_dequantize:7 tvm.topi.nn.qnn.simulated_quantize:7
#: tvm.topi.nn.rms_norm.rms_norm:4 tvm.topi.nn.softmax.fast_softmax:5
#: tvm.topi.nn.softmax.log_softmax:4 tvm.topi.nn.softmax.softmax:4
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:4
#: tvm.topi.nn.space_to_depth.space_to_depth:4 tvm.topi.nn.sparse.sparse_add:4
#: tvm.topi.nn.sparse.sparse_conv2d:5 tvm.topi.nn.sparse.sparse_dense:8
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:7
#: tvm.topi.nn.sparse.sparse_dense_sp_lhs:5
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:5
#: tvm.topi.nn.sparse.sparse_transpose:6
#: tvm.topi.nn.sparse.try_get_conv2d_sparse_input:4
#: tvm.topi.nn.sparse.try_get_sparse_input:4
#: tvm.topi.nn.upsampling.upsampling:5 tvm.topi.nn.upsampling.upsampling3d:5
#: tvm.topi.nn.utils.get_pad_tuple:4 tvm.topi.nn.utils.get_pad_tuple1d:4
#: tvm.topi.nn.utils.get_pad_tuple3d:4
#: tvm.topi.nn.utils.get_pad_tuple_generic:4 tvm.topi.reduction.all:4
#: tvm.topi.reduction.any:4 tvm.topi.reduction.argmax:4
#: tvm.topi.reduction.argmin:4 tvm.topi.reduction.collapse_sum:16
#: tvm.topi.reduction.max:4 tvm.topi.reduction.min:4 tvm.topi.reduction.prod:4
#: tvm.topi.reduction.sum:4 tvm.topi.scan.cumprod:4 tvm.topi.scan.cumsum:4
#: tvm.topi.scan.scanop:9 tvm.topi.scatter.scatter_nd:20
#: tvm.topi.scatter_elements.scatter_elements:16
#: tvm.topi.searchsorted.searchsorted:6 tvm.topi.signal.dft:5
#: tvm.topi.sort.argsort:6 tvm.topi.sort.sort:5 tvm.topi.sort.topk:4
#: tvm.topi.sparse.csrmm.csrmm:5 tvm.topi.sparse.csrmv.csrmv:5
#: tvm.topi.sparse.dense.dense:5 tvm.topi.sparse_reshape.sparse_reshape:4
#: tvm.topi.tensor.elemwise_sum:4 tvm.topi.tensor.full:4
#: tvm.topi.tensor.full_like:5 tvm.topi.transform.adv_index:4
#: tvm.topi.transform.arange:4 tvm.topi.transform.concatenate:4
#: tvm.topi.transform.dynamic_strided_slice:4 tvm.topi.transform.expand_dims:4
#: tvm.topi.transform.expand_like:23 tvm.topi.transform.flip:4
#: tvm.topi.transform.gather:15 tvm.topi.transform.gather_nd:4
#: tvm.topi.transform.invert_permutation:4
#: tvm.topi.transform.layout_transform:4 tvm.topi.transform.matmul:6
#: tvm.topi.transform.matrix_set_diag:4 tvm.topi.transform.meshgrid:4
#: tvm.topi.transform.ndarray_size:4 tvm.topi.transform.one_hot:6
#: tvm.topi.transform.repeat:4 tvm.topi.transform.reshape:4
#: tvm.topi.transform.reverse_sequence:5 tvm.topi.transform.sequence_mask:14
#: tvm.topi.transform.shape:4 tvm.topi.transform.sliding_window:4
#: tvm.topi.transform.sparse_to_dense:7 tvm.topi.transform.split:4
#: tvm.topi.transform.squeeze:4 tvm.topi.transform.stack:4
#: tvm.topi.transform.strided_set:4 tvm.topi.transform.strided_slice:4
#: tvm.topi.transform.take:4 tvm.topi.transform.take_legalize:4
#: tvm.topi.transform.tensordot:4 tvm.topi.transform.tile:4
#: tvm.topi.transform.transpose:4 tvm.topi.transform.trilu:5
#: tvm.topi.transform.unravel_index:7 tvm.topi.transform.where:4
#: tvm.topi.unique.unique:5 tvm.topi.utils.const_vector:4
#: tvm.topi.utils.equal_const_int:4 tvm.topi.utils.get_const_int:4
#: tvm.topi.utils.get_const_tuple:4 tvm.topi.utils.make_idx:8
#: tvm.topi.utils.simplify:4 tvm.topi.utils.within_index:4
msgid "Parameters"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:14 tvm.tir.expr.Cast:6 tvm.topi.math.cast:9
#: tvm.topi.math.reinterpret:9 tvm.topi.scan.cumprod:14 tvm.topi.scan.cumsum:14
#: tvm.topi.scan.scanop:28 tvm.topi.searchsorted.searchsorted:23
#: tvm.topi.sort.argsort:21 tvm.topi.sort.sort:17 tvm.topi.sort.topk:24
#: tvm.topi.tensor.full:7 tvm.topi.transform.arange:16
#: tvm.topi.transform.ndarray_size:9 tvm.topi.transform.one_hot:23
#: tvm.topi.transform.shape:9
msgid "dtype"
msgstr ""

#: of tvm.tir.expr.Cast:-1 tvm.topi.image.grid_sample.grid_sample:-1
#: tvm.topi.math.cast:-1 tvm.topi.math.reinterpret:-1
#: tvm.topi.nn.conv1d.conv1d:-1 tvm.topi.nn.conv1d.group_conv1d_ncw:-1
#: tvm.topi.nn.conv1d.group_conv1d_nwc:-1
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:-1
#: tvm.topi.nn.conv2d.conv:-1 tvm.topi.nn.conv2d.conv2d:-1
#: tvm.topi.nn.conv2d.conv2d_NCHWc:-1 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:-1
#: tvm.topi.nn.conv2d.group_conv2d_nchw:-1
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:-1
#: tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:-1
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:-1
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:-1
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:-1
#: tvm.topi.nn.pooling.adaptive_pool:-1 tvm.topi.nn.pooling.global_pool:-1
#: tvm.topi.nn.pooling.pool1d:-1 tvm.topi.nn.pooling.pool2d:-1
#: tvm.topi.nn.pooling.pool3d:-1 tvm.topi.nn.pooling.pool_grad:-1
#: tvm.topi.nn.sparse.sparse_conv2d:-1 tvm.topi.tensor.full:-1
#: tvm.topi.transform.layout_transform:-1 tvm.topi.transform.meshgrid:-1
msgid "str"
msgstr ""

#: of tvm.tir.expr.Cast:6
msgid "The data type"
msgstr ""

#: of tvm.tir.expr.Cast:9
msgid "value"
msgstr ""

#: of tvm.tir.expr.Cast:-1 tvm.tir.op.div:-1 tvm.tir.op.floordiv:-1
#: tvm.tir.op.floormod:-1
msgid "PrimExpr"
msgstr ""

#: of tvm.tir.expr.Cast:9
msgid "The value of the function."
msgstr ""

#: of tvm.tir.expr.Cast:11 tvm.tir.op.div:12 tvm.tir.op.floordiv:12
#: tvm.tir.op.floormod:12 tvm.topi.math.cast:12
msgid "span"
msgstr ""

#: of tvm.tir.expr.Cast:-1 tvm.tir.op.div:-1 tvm.tir.op.floordiv:-1
#: tvm.tir.op.floormod:-1 tvm.topi.math.cast:-1
msgid "Optional[Span]"
msgstr ""

#: of tvm.tir.expr.Cast:12
msgid "The location of this itervar in the source code."
msgstr ""

#: of tvm.topi.utils.InvalidShapeError:1
msgid ""
"Invalid shape for a topi function. i.e. call winograd template for non-"
"3x3 kernel)"
msgstr ""

#: of tvm.ir.expr.PrimExpr:3
msgid "PrimExpr is used in the low-level code optimizations and integer analysis."
msgstr ""

#: of tvm.topi.math.abs:6 tvm.topi.math.acos:6 tvm.topi.math.acosh:6
#: tvm.topi.math.asin:6 tvm.topi.math.asinh:6 tvm.topi.math.atan:6
#: tvm.topi.math.atanh:6 tvm.topi.math.cast:6 tvm.topi.math.ceil:6
#: tvm.topi.math.ceil_log2:8 tvm.topi.math.clip:6 tvm.topi.math.cos:6
#: tvm.topi.math.cosh:6 tvm.topi.math.erf:6 tvm.topi.math.exp:6
#: tvm.topi.math.fast_erf:6 tvm.topi.math.fast_exp:6 tvm.topi.math.fast_tanh:6
#: tvm.topi.math.fixed_point_multiply:7
#: tvm.topi.math.fixed_point_multiply_per_axis:6 tvm.topi.math.floor:6
#: tvm.topi.math.identity:6 tvm.topi.math.isfinite:6 tvm.topi.math.isinf:6
#: tvm.topi.math.isnan:6 tvm.topi.math.log:6 tvm.topi.math.log10:6
#: tvm.topi.math.log2:6 tvm.topi.math.negative:6 tvm.topi.math.reinterpret:6
#: tvm.topi.math.round:6 tvm.topi.math.rsqrt:6 tvm.topi.math.sigmoid:6
#: tvm.topi.math.sign:6 tvm.topi.math.sin:6 tvm.topi.math.sinh:6
#: tvm.topi.math.sqrt:6 tvm.topi.math.tan:6 tvm.topi.math.tanh:6
#: tvm.topi.math.trunc:6 tvm.topi.nn.elemwise.leaky_relu:6
#: tvm.topi.nn.elemwise.prelu:10 tvm.topi.nn.elemwise.relu:6
#: tvm.topi.sparse.csrmv.csrmv:10 tvm.topi.tensor.full_like:6
#: tvm.topi.transform.where:9
msgid "x"
msgstr ""

#: of tvm.topi.argwhere.argwhere:-1 tvm.topi.argwhere.hybrid_argwhere_1d:-1
#: tvm.topi.argwhere.hybrid_argwhere_2d:-1
#: tvm.topi.argwhere.hybrid_argwhere_3d:-1
#: tvm.topi.argwhere.hybrid_argwhere_4d:-1
#: tvm.topi.argwhere.hybrid_argwhere_5d:-1 tvm.topi.broadcast.broadcast_to:-1
#: tvm.topi.einsum.einsum:-1 tvm.topi.image.dilation2d.dilation2d_nchw:-1
#: tvm.topi.image.dilation2d.dilation2d_nhwc:-1
#: tvm.topi.image.resize.crop_and_resize:-1 tvm.topi.image.resize.resize1d:-1
#: tvm.topi.image.resize.resize2d:-1 tvm.topi.image.resize.resize3d:-1
#: tvm.topi.math.abs:-1 tvm.topi.math.acos:-1 tvm.topi.math.acosh:-1
#: tvm.topi.math.asin:-1 tvm.topi.math.asinh:-1 tvm.topi.math.atan:-1
#: tvm.topi.math.atanh:-1 tvm.topi.math.cast:-1 tvm.topi.math.ceil:-1
#: tvm.topi.math.ceil_log2:-1 tvm.topi.math.clip:-1 tvm.topi.math.cos:-1
#: tvm.topi.math.cosh:-1 tvm.topi.math.erf:-1 tvm.topi.math.exp:-1
#: tvm.topi.math.fast_erf:-1 tvm.topi.math.fast_exp:-1
#: tvm.topi.math.fast_tanh:-1 tvm.topi.math.fixed_point_multiply:-1
#: tvm.topi.math.fixed_point_multiply_per_axis:-1 tvm.topi.math.floor:-1
#: tvm.topi.math.identity:-1 tvm.topi.math.isfinite:-1 tvm.topi.math.isinf:-1
#: tvm.topi.math.isnan:-1 tvm.topi.math.log:-1 tvm.topi.math.log10:-1
#: tvm.topi.math.log2:-1 tvm.topi.math.negative:-1 tvm.topi.math.reinterpret:-1
#: tvm.topi.math.round:-1 tvm.topi.math.rsqrt:-1 tvm.topi.math.sigmoid:-1
#: tvm.topi.math.sign:-1 tvm.topi.math.sin:-1 tvm.topi.math.sinh:-1
#: tvm.topi.math.sqrt:-1 tvm.topi.math.tan:-1 tvm.topi.math.tanh:-1
#: tvm.topi.math.trunc:-1 tvm.topi.nn.batch_matmul.batch_matmul:-1
#: tvm.topi.nn.batch_norm.batch_norm:-1
#: tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:-1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:-1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:-1
#: tvm.topi.nn.bitserial_dense.bitserial_dense:-1
#: tvm.topi.nn.bnn.binarize_pack:-1 tvm.topi.nn.bnn.binary_dense:-1
#: tvm.topi.nn.conv1d.conv1d:-1 tvm.topi.nn.conv1d.group_conv1d_ncw:-1
#: tvm.topi.nn.conv1d.group_conv1d_nwc:-1
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:-1
#: tvm.topi.nn.conv2d.conv:-1 tvm.topi.nn.conv2d.conv2d:-1
#: tvm.topi.nn.conv2d.conv2d_NCHWc:-1 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:-1
#: tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:-1
#: tvm.topi.nn.conv2d.conv2d_hwcn:-1 tvm.topi.nn.conv2d.conv2d_nchw:-1
#: tvm.topi.nn.conv2d.conv2d_nhwc:-1 tvm.topi.nn.conv2d.conv2d_winograd_nchw:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:-1
#: tvm.topi.nn.conv2d.group_conv2d_nchw:-1
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:-1
#: tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:-1
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:-1
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:-1
#: tvm.topi.nn.conv3d.conv3d_ncdhw:-1 tvm.topi.nn.conv3d.conv3d_ndhwc:-1
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:-1
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:-1
#: tvm.topi.nn.correlation.correlation_nchw:-1
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:-1
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:-1
#: tvm.topi.nn.dense.dense:-1 tvm.topi.nn.dense.dense_pack:-1
#: tvm.topi.nn.dense.matmul:-1 tvm.topi.nn.depth_to_space.depth_to_space:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:-1
#: tvm.topi.nn.dilate.dilate:-1 tvm.topi.nn.elemwise.leaky_relu:-1
#: tvm.topi.nn.elemwise.prelu:-1 tvm.topi.nn.elemwise.relu:-1
#: tvm.topi.nn.fifo_buffer.fifo_buffer:-1 tvm.topi.nn.flatten.flatten:-1
#: tvm.topi.nn.group_norm.group_norm:-1
#: tvm.topi.nn.instance_norm.instance_norm:-1
#: tvm.topi.nn.layer_norm.layer_norm:-1 tvm.topi.nn.local_response_norm.lrn:-1
#: tvm.topi.nn.loss.nll_loss:-1 tvm.topi.nn.mapping.scale_shift_nchw:-1
#: tvm.topi.nn.mapping.scale_shift_nchwc:-1
#: tvm.topi.nn.mapping.scale_shift_nhwc:-1 tvm.topi.nn.pad.mirror_pad:-1
#: tvm.topi.nn.pad.pad:-1 tvm.topi.nn.pooling.adaptive_pool:-1
#: tvm.topi.nn.pooling.global_pool:-1 tvm.topi.nn.pooling.pool1d:-1
#: tvm.topi.nn.pooling.pool2d:-1 tvm.topi.nn.pooling.pool3d:-1
#: tvm.topi.nn.pooling.pool_grad:-1 tvm.topi.nn.rms_norm.rms_norm:-1
#: tvm.topi.nn.softmax.fast_softmax:-1 tvm.topi.nn.softmax.log_softmax:-1
#: tvm.topi.nn.softmax.softmax:-1
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:-1
#: tvm.topi.nn.space_to_depth.space_to_depth:-1
#: tvm.topi.nn.sparse.sparse_add:-1 tvm.topi.nn.sparse.sparse_conv2d:-1
#: tvm.topi.nn.sparse.sparse_dense:-1 tvm.topi.nn.sparse.sparse_dense_sp_lhs:-1
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:-1
#: tvm.topi.nn.sparse.sparse_transpose:-1 tvm.topi.nn.upsampling.upsampling:-1
#: tvm.topi.nn.upsampling.upsampling3d:-1 tvm.topi.reduction.all:-1
#: tvm.topi.reduction.any:-1 tvm.topi.reduction.argmax:-1
#: tvm.topi.reduction.argmin:-1 tvm.topi.reduction.collapse_sum:-1
#: tvm.topi.reduction.max:-1 tvm.topi.reduction.min:-1
#: tvm.topi.reduction.prod:-1 tvm.topi.reduction.sum:-1
#: tvm.topi.scan.cumprod:-1 tvm.topi.scan.cumsum:-1 tvm.topi.scan.scanop:-1
#: tvm.topi.scatter.scatter_nd:-1 tvm.topi.scatter_elements.scatter_elements:-1
#: tvm.topi.sort.argsort:-1 tvm.topi.sort.sort:-1 tvm.topi.sort.topk:-1
#: tvm.topi.sparse.csrmm.csrmm:-1 tvm.topi.sparse.csrmv.csrmv:-1
#: tvm.topi.sparse.dense.dense:-1 tvm.topi.tensor.elemwise_sum:-1
#: tvm.topi.tensor.full:-1 tvm.topi.tensor.full_like:-1
#: tvm.topi.transform.adv_index:-1 tvm.topi.transform.arange:-1
#: tvm.topi.transform.dynamic_strided_slice:-1
#: tvm.topi.transform.expand_dims:-1 tvm.topi.transform.expand_like:-1
#: tvm.topi.transform.flip:-1 tvm.topi.transform.gather:-1
#: tvm.topi.transform.gather_nd:-1 tvm.topi.transform.invert_permutation:-1
#: tvm.topi.transform.layout_transform:-1 tvm.topi.transform.ndarray_size:-1
#: tvm.topi.transform.one_hot:-1 tvm.topi.transform.repeat:-1
#: tvm.topi.transform.reshape:-1 tvm.topi.transform.reverse_sequence:-1
#: tvm.topi.transform.sequence_mask:-1 tvm.topi.transform.shape:-1
#: tvm.topi.transform.sparse_to_dense:-1 tvm.topi.transform.stack:-1
#: tvm.topi.transform.strided_set:-1 tvm.topi.transform.strided_slice:-1
#: tvm.topi.transform.take:-1 tvm.topi.transform.tile:-1
#: tvm.topi.transform.transpose:-1 tvm.topi.transform.where:-1
#: tvm.topi.unique.unique:-1
msgid "tvm.te.Tensor"
msgstr ""

#: of tvm.topi.math.abs:6 tvm.topi.math.acos:6 tvm.topi.math.acosh:6
#: tvm.topi.math.asin:6 tvm.topi.math.asinh:6 tvm.topi.math.atan:6
#: tvm.topi.math.atanh:6 tvm.topi.math.cast:6 tvm.topi.math.ceil:6
#: tvm.topi.math.ceil_log2:8 tvm.topi.math.clip:7 tvm.topi.math.cos:6
#: tvm.topi.math.cosh:6 tvm.topi.math.erf:6 tvm.topi.math.exp:6
#: tvm.topi.math.fast_erf:6 tvm.topi.math.fast_exp:6 tvm.topi.math.fast_tanh:6
#: tvm.topi.math.fixed_point_multiply:8
#: tvm.topi.math.fixed_point_multiply_per_axis:7 tvm.topi.math.floor:6
#: tvm.topi.math.identity:6 tvm.topi.math.isfinite:6 tvm.topi.math.isinf:6
#: tvm.topi.math.isnan:6 tvm.topi.math.log:6 tvm.topi.math.log10:6
#: tvm.topi.math.log2:6 tvm.topi.math.negative:6 tvm.topi.math.reinterpret:6
#: tvm.topi.math.round:6 tvm.topi.math.rsqrt:6 tvm.topi.math.sigmoid:6
#: tvm.topi.math.sign:6 tvm.topi.math.sin:6 tvm.topi.math.sinh:6
#: tvm.topi.math.sqrt:6 tvm.topi.math.tan:6 tvm.topi.math.tanh:6
#: tvm.topi.math.trunc:6 tvm.topi.nn.elemwise.leaky_relu:6
#: tvm.topi.nn.elemwise.prelu:10 tvm.topi.nn.elemwise.relu:6
#: tvm.topi.tensor.full_like:7
msgid "Input argument."
msgstr ""

#: of tvm.te.operation.extern:46 tvm.tir.buffer.decl_buffer:57
#: tvm.tir.op.div:15 tvm.tir.op.floordiv:15 tvm.tir.op.floormod:15
#: tvm.topi.argwhere.argwhere:9 tvm.topi.argwhere.hybrid_argwhere_1d:9
#: tvm.topi.argwhere.hybrid_argwhere_2d:9
#: tvm.topi.argwhere.hybrid_argwhere_3d:9
#: tvm.topi.argwhere.hybrid_argwhere_4d:9
#: tvm.topi.argwhere.hybrid_argwhere_5d:9 tvm.topi.broadcast.add:11
#: tvm.topi.broadcast.bitwise_and:11 tvm.topi.broadcast.bitwise_not:8
#: tvm.topi.broadcast.bitwise_or:11 tvm.topi.broadcast.bitwise_xor:11
#: tvm.topi.broadcast.broadcast_to:15 tvm.topi.broadcast.divide:11
#: tvm.topi.broadcast.equal:11 tvm.topi.broadcast.floor_divide:11
#: tvm.topi.broadcast.floor_mod:11 tvm.topi.broadcast.greater:11
#: tvm.topi.broadcast.greater_equal:11 tvm.topi.broadcast.left_shift:11
#: tvm.topi.broadcast.less:11 tvm.topi.broadcast.less_equal:11
#: tvm.topi.broadcast.logical_and:11 tvm.topi.broadcast.logical_not:8
#: tvm.topi.broadcast.logical_or:11 tvm.topi.broadcast.logical_xor:11
#: tvm.topi.broadcast.maximum:11 tvm.topi.broadcast.minimum:11
#: tvm.topi.broadcast.mod:11 tvm.topi.broadcast.multiply:11
#: tvm.topi.broadcast.not_equal:11 tvm.topi.broadcast.power:11
#: tvm.topi.broadcast.right_shift:11 tvm.topi.broadcast.subtract:11
#: tvm.topi.einsum.einsum:17 tvm.topi.image.dilation2d.dilation2d_nchw:24
#: tvm.topi.image.dilation2d.dilation2d_nhwc:24
#: tvm.topi.image.grid_sample.affine_grid:16
#: tvm.topi.image.grid_sample.grid_sample:58
#: tvm.topi.image.resize.crop_and_resize:34 tvm.topi.image.resize.resize1d:56
#: tvm.topi.image.resize.resize2d:50 tvm.topi.image.resize.resize3d:50
#: tvm.topi.math.abs:9 tvm.topi.math.acos:9 tvm.topi.math.acosh:9
#: tvm.topi.math.asin:9 tvm.topi.math.asinh:9 tvm.topi.math.atan:9
#: tvm.topi.math.atanh:9 tvm.topi.math.cast:15 tvm.topi.math.ceil:9
#: tvm.topi.math.ceil_log2:11 tvm.topi.math.clip:14 tvm.topi.math.cos:9
#: tvm.topi.math.cosh:9 tvm.topi.math.erf:9 tvm.topi.math.erf_legalize:13
#: tvm.topi.math.exp:9 tvm.topi.math.fast_erf:9 tvm.topi.math.fast_exp:9
#: tvm.topi.math.fast_tanh:9 tvm.topi.math.fixed_point_multiply:15
#: tvm.topi.math.fixed_point_multiply_per_axis:20 tvm.topi.math.floor:9
#: tvm.topi.math.identity:9 tvm.topi.math.isfinite:9 tvm.topi.math.isinf:9
#: tvm.topi.math.isnan:9 tvm.topi.math.log:9 tvm.topi.math.log10:9
#: tvm.topi.math.log2:9 tvm.topi.math.negative:9 tvm.topi.math.reinterpret:12
#: tvm.topi.math.round:9 tvm.topi.math.rsqrt:9 tvm.topi.math.sigmoid:9
#: tvm.topi.math.sign:9 tvm.topi.math.sin:9 tvm.topi.math.sinh:9
#: tvm.topi.math.sqrt:9 tvm.topi.math.tan:9 tvm.topi.math.tanh:9
#: tvm.topi.math.trunc:9 tvm.topi.nn.batch_matmul.batch_matmul:34
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:13
#: tvm.topi.nn.batch_norm.batch_norm:46
#: tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:22
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:13
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:33
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:33
#: tvm.topi.nn.bitserial_dense.bitserial_dense:11
#: tvm.topi.nn.bnn.binarize_pack:16 tvm.topi.nn.bnn.binary_dense:12
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:25
#: tvm.topi.nn.conv2d.conv:56 tvm.topi.nn.conv2d.conv2d:30
#: tvm.topi.nn.conv2d.conv2d_NCHWc:34 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:37
#: tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:13
#: tvm.topi.nn.conv2d.conv2d_hwcn:23 tvm.topi.nn.conv2d.conv2d_infer_layout:12
#: tvm.topi.nn.conv2d.conv2d_legalize:13 tvm.topi.nn.conv2d.conv2d_nchw:23
#: tvm.topi.nn.conv2d.conv2d_nhwc:32 tvm.topi.nn.conv2d.conv2d_winograd_nchw:26
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:24
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:26
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:24
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:11
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:11
#: tvm.topi.nn.conv2d.group_conv2d_nchw:29
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:29
#: tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:12
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:13
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:24
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:32
#: tvm.topi.nn.conv2d_transpose.layout_transform:17
#: tvm.topi.nn.conv3d.conv3d_ncdhw:24 tvm.topi.nn.conv3d.conv3d_ndhwc:33
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:11
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:13
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:24
#: tvm.topi.nn.correlation.correlation_nchw:32
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:33
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:33
#: tvm.topi.nn.dense.dense:26 tvm.topi.nn.dense.dense_legalize:13
#: tvm.topi.nn.dense.dense_pack:18 tvm.topi.nn.dense.matmul:30
#: tvm.topi.nn.dense.matmul_legalize:13
#: tvm.topi.nn.depth_to_space.depth_to_space:20
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:32
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:18
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:18
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:12
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:24
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:24
#: tvm.topi.nn.dilate.dilate:18 tvm.topi.nn.elemwise.leaky_relu:12
#: tvm.topi.nn.elemwise.prelu:19 tvm.topi.nn.elemwise.relu:9
#: tvm.topi.nn.fifo_buffer.fifo_buffer:27 tvm.topi.nn.flatten.flatten:9
#: tvm.topi.nn.group_norm.group_norm:29
#: tvm.topi.nn.instance_norm.instance_norm:22
#: tvm.topi.nn.layer_norm.layer_norm:23 tvm.topi.nn.local_response_norm.lrn:30
#: tvm.topi.nn.loss.nll_loss:32 tvm.topi.nn.lstm.lstm:33
#: tvm.topi.nn.mapping.scale_shift_nchw:15
#: tvm.topi.nn.mapping.scale_shift_nchwc:15
#: tvm.topi.nn.mapping.scale_shift_nhwc:15 tvm.topi.nn.pad.mirror_pad:21
#: tvm.topi.nn.pad.pad:21 tvm.topi.nn.pooling.adaptive_pool:32
#: tvm.topi.nn.pooling.global_pool:27 tvm.topi.nn.pooling.pool1d:45
#: tvm.topi.nn.pooling.pool2d:45 tvm.topi.nn.pooling.pool3d:45
#: tvm.topi.nn.pooling.pool_grad:45 tvm.topi.nn.rms_norm.rms_norm:18
#: tvm.topi.nn.softmax.fast_softmax:13 tvm.topi.nn.softmax.log_softmax:9
#: tvm.topi.nn.softmax.softmax:12
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:25
#: tvm.topi.nn.space_to_depth.space_to_depth:15
#: tvm.topi.nn.sparse.sparse_add:18 tvm.topi.nn.sparse.sparse_conv2d:26
#: tvm.topi.nn.sparse.sparse_dense:28 tvm.topi.nn.sparse.sparse_dense_sp_lhs:22
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:22
#: tvm.topi.nn.sparse.sparse_transpose:17
#: tvm.topi.nn.sparse.try_get_conv2d_sparse_input:9
#: tvm.topi.nn.sparse.try_get_sparse_input:9
#: tvm.topi.nn.upsampling.upsampling:28 tvm.topi.nn.upsampling.upsampling3d:37
#: tvm.topi.nn.utils.get_pad_tuple:12 tvm.topi.nn.utils.get_pad_tuple1d:12
#: tvm.topi.nn.utils.get_pad_tuple3d:12
#: tvm.topi.nn.utils.get_pad_tuple_generic:12 tvm.topi.reduction.all:19
#: tvm.topi.reduction.any:19 tvm.topi.reduction.argmax:23
#: tvm.topi.reduction.argmin:23 tvm.topi.reduction.collapse_sum:24
#: tvm.topi.reduction.max:19 tvm.topi.reduction.min:19
#: tvm.topi.reduction.prod:19 tvm.topi.reduction.sum:19
#: tvm.topi.scan.cumprod:23 tvm.topi.scan.cumsum:23 tvm.topi.scan.scanop:38
#: tvm.topi.scatter.scatter_nd:36 tvm.topi.scatter_elements.scatter_elements:40
#: tvm.topi.searchsorted.searchsorted:26 tvm.topi.signal.dft:17
#: tvm.topi.signal.stft:20 tvm.topi.sort.argsort:24 tvm.topi.sort.sort:20
#: tvm.topi.sort.topk:27 tvm.topi.sparse.csrmm.csrmm:16
#: tvm.topi.sparse.csrmv.csrmv:16 tvm.topi.sparse.dense.dense:16
#: tvm.topi.sparse_reshape.sparse_reshape:16 tvm.topi.tensor.elemwise_sum:9
#: tvm.topi.tensor.full:13 tvm.topi.tensor.full_like:12
#: tvm.topi.transform.adv_index:12 tvm.topi.transform.arange:19
#: tvm.topi.transform.concatenate:12
#: tvm.topi.transform.dynamic_strided_slice:23
#: tvm.topi.transform.expand_dims:12 tvm.topi.transform.expand_like:32
#: tvm.topi.transform.flip:12 tvm.topi.transform.gather:26
#: tvm.topi.transform.gather_nd:12 tvm.topi.transform.invert_permutation:9
#: tvm.topi.transform.matmul:13 tvm.topi.transform.matrix_set_diag:27
#: tvm.topi.transform.meshgrid:12 tvm.topi.transform.ndarray_size:12
#: tvm.topi.transform.one_hot:26 tvm.topi.transform.repeat:15
#: tvm.topi.transform.reshape:11 tvm.topi.transform.reverse_sequence:22
#: tvm.topi.transform.sequence_mask:29 tvm.topi.transform.shape:12
#: tvm.topi.transform.sliding_window:23 tvm.topi.transform.sparse_to_dense:22
#: tvm.topi.transform.split:12 tvm.topi.transform.squeeze:12
#: tvm.topi.transform.stack:13 tvm.topi.transform.strided_set:23
#: tvm.topi.transform.strided_slice:31 tvm.topi.transform.take:25
#: tvm.topi.transform.take_legalize:12 tvm.topi.transform.tensordot:10
#: tvm.topi.transform.tile:12 tvm.topi.transform.transpose:12
#: tvm.topi.transform.trilu:20 tvm.topi.transform.unravel_index:15
#: tvm.topi.transform.where:15 tvm.topi.unique.unique:16
#: tvm.topi.utils.const_vector:11 tvm.topi.utils.equal_const_int:9
#: tvm.topi.utils.get_const_int:9 tvm.topi.utils.get_const_tuple:9
#: tvm.topi.utils.make_idx:25 tvm.topi.utils.simplify:9
#: tvm.topi.utils.within_index:18
msgid "Returns"
msgstr ""

#: of tvm.topi.math.abs:10 tvm.topi.math.acos:10 tvm.topi.math.acosh:10
#: tvm.topi.math.asin:10 tvm.topi.math.asinh:10 tvm.topi.math.atan:10
#: tvm.topi.math.atanh:10 tvm.topi.math.cast:16 tvm.topi.math.ceil:10
#: tvm.topi.math.ceil_log2:12 tvm.topi.math.clip:15 tvm.topi.math.cos:10
#: tvm.topi.math.cosh:10 tvm.topi.math.erf:10 tvm.topi.math.exp:10
#: tvm.topi.math.fast_erf:10 tvm.topi.math.fast_exp:10
#: tvm.topi.math.fast_tanh:10 tvm.topi.math.fixed_point_multiply:16
#: tvm.topi.math.fixed_point_multiply_per_axis:8 tvm.topi.math.floor:10
#: tvm.topi.math.identity:10 tvm.topi.math.isfinite:10 tvm.topi.math.isinf:10
#: tvm.topi.math.isnan:10 tvm.topi.math.log:10 tvm.topi.math.log10:10
#: tvm.topi.math.log2:10 tvm.topi.math.negative:10 tvm.topi.math.reinterpret:13
#: tvm.topi.math.round:10 tvm.topi.math.rsqrt:10 tvm.topi.math.sigmoid:10
#: tvm.topi.math.sign:10 tvm.topi.math.sin:10 tvm.topi.math.sinh:10
#: tvm.topi.math.sqrt:10 tvm.topi.math.tan:10 tvm.topi.math.tanh:10
#: tvm.topi.math.trunc:10 tvm.topi.nn.elemwise.leaky_relu:13
#: tvm.topi.nn.elemwise.prelu:21 tvm.topi.nn.elemwise.relu:10
#: tvm.topi.sparse.csrmv.csrmv:13 tvm.topi.tensor.elemwise_sum:10
#: tvm.topi.tensor.full:14 tvm.topi.tensor.full_like:13
#: tvm.topi.transform.where:12
msgid "y"
msgstr ""

#: of tvm.topi.math.abs:11 tvm.topi.math.acos:11 tvm.topi.math.acosh:11
#: tvm.topi.math.asin:11 tvm.topi.math.asinh:11 tvm.topi.math.atan:11
#: tvm.topi.math.atanh:11 tvm.topi.math.cast:17 tvm.topi.math.ceil:11
#: tvm.topi.math.ceil_log2:13 tvm.topi.math.clip:16 tvm.topi.math.cos:11
#: tvm.topi.math.cosh:11 tvm.topi.math.erf:11 tvm.topi.math.exp:11
#: tvm.topi.math.fast_erf:11 tvm.topi.math.fast_exp:11
#: tvm.topi.math.fast_tanh:11 tvm.topi.math.fixed_point_multiply:17
#: tvm.topi.math.fixed_point_multiply_per_axis:22 tvm.topi.math.floor:11
#: tvm.topi.math.identity:11 tvm.topi.math.isfinite:11 tvm.topi.math.isinf:11
#: tvm.topi.math.isnan:11 tvm.topi.math.log:11 tvm.topi.math.log10:11
#: tvm.topi.math.log2:11 tvm.topi.math.negative:11 tvm.topi.math.reinterpret:14
#: tvm.topi.math.round:11 tvm.topi.math.rsqrt:11 tvm.topi.math.sigmoid:11
#: tvm.topi.math.sign:11 tvm.topi.math.sin:11 tvm.topi.math.sinh:11
#: tvm.topi.math.sqrt:11 tvm.topi.math.tan:11 tvm.topi.math.tanh:11
#: tvm.topi.math.trunc:11 tvm.topi.nn.elemwise.leaky_relu:14
#: tvm.topi.nn.elemwise.prelu:21 tvm.topi.nn.elemwise.relu:11
#: tvm.topi.tensor.elemwise_sum:11 tvm.topi.tensor.full:15
#: tvm.topi.tensor.full_like:14
msgid "The result."
msgstr ""

#: of tvm.topi.broadcast.add:5 tvm.topi.broadcast.bitwise_and:5
#: tvm.topi.broadcast.bitwise_or:5 tvm.topi.broadcast.bitwise_xor:5
#: tvm.topi.broadcast.divide:5 tvm.topi.broadcast.equal:5
#: tvm.topi.broadcast.floor_divide:5 tvm.topi.broadcast.floor_mod:5
#: tvm.topi.broadcast.greater:5 tvm.topi.broadcast.greater_equal:5
#: tvm.topi.broadcast.left_shift:5 tvm.topi.broadcast.less:5
#: tvm.topi.broadcast.less_equal:5 tvm.topi.broadcast.logical_and:5
#: tvm.topi.broadcast.logical_or:5 tvm.topi.broadcast.logical_xor:5
#: tvm.topi.broadcast.maximum:5 tvm.topi.broadcast.minimum:5
#: tvm.topi.broadcast.mod:5 tvm.topi.broadcast.multiply:5
#: tvm.topi.broadcast.not_equal:5 tvm.topi.broadcast.power:5
#: tvm.topi.broadcast.right_shift:5 tvm.topi.broadcast.subtract:5
msgid "lhs"
msgstr ""

#: of tvm.topi.broadcast.add:-1 tvm.topi.broadcast.bitwise_and:-1
#: tvm.topi.broadcast.bitwise_not:-1 tvm.topi.broadcast.bitwise_or:-1
#: tvm.topi.broadcast.bitwise_xor:-1 tvm.topi.broadcast.divide:-1
#: tvm.topi.broadcast.equal:-1 tvm.topi.broadcast.floor_divide:-1
#: tvm.topi.broadcast.floor_mod:-1 tvm.topi.broadcast.greater:-1
#: tvm.topi.broadcast.greater_equal:-1 tvm.topi.broadcast.left_shift:-1
#: tvm.topi.broadcast.less:-1 tvm.topi.broadcast.less_equal:-1
#: tvm.topi.broadcast.logical_and:-1 tvm.topi.broadcast.logical_not:-1
#: tvm.topi.broadcast.logical_or:-1 tvm.topi.broadcast.logical_xor:-1
#: tvm.topi.broadcast.maximum:-1 tvm.topi.broadcast.minimum:-1
#: tvm.topi.broadcast.mod:-1 tvm.topi.broadcast.multiply:-1
#: tvm.topi.broadcast.not_equal:-1 tvm.topi.broadcast.power:-1
#: tvm.topi.broadcast.right_shift:-1 tvm.topi.broadcast.subtract:-1
#: tvm.topi.math.cast:-1 tvm.topi.math.fixed_point_multiply:-1
msgid "tvm.te.Tensor or Expr"
msgstr ""

#: of tvm.topi.broadcast.add:6 tvm.topi.broadcast.bitwise_and:6
#: tvm.topi.broadcast.bitwise_or:6 tvm.topi.broadcast.bitwise_xor:6
#: tvm.topi.broadcast.divide:6 tvm.topi.broadcast.equal:6
#: tvm.topi.broadcast.floor_divide:6 tvm.topi.broadcast.floor_mod:6
#: tvm.topi.broadcast.greater:6 tvm.topi.broadcast.greater_equal:6
#: tvm.topi.broadcast.left_shift:6 tvm.topi.broadcast.less:6
#: tvm.topi.broadcast.less_equal:6 tvm.topi.broadcast.logical_and:6
#: tvm.topi.broadcast.logical_or:6 tvm.topi.broadcast.logical_xor:6
#: tvm.topi.broadcast.maximum:6 tvm.topi.broadcast.minimum:6
#: tvm.topi.broadcast.mod:6 tvm.topi.broadcast.multiply:6
#: tvm.topi.broadcast.not_equal:6 tvm.topi.broadcast.power:6
#: tvm.topi.broadcast.right_shift:6 tvm.topi.broadcast.subtract:6
msgid "The left operand"
msgstr ""

#: of tvm.topi.broadcast.add:8 tvm.topi.broadcast.bitwise_and:8
#: tvm.topi.broadcast.bitwise_or:8 tvm.topi.broadcast.bitwise_xor:8
#: tvm.topi.broadcast.divide:8 tvm.topi.broadcast.equal:8
#: tvm.topi.broadcast.floor_divide:8 tvm.topi.broadcast.floor_mod:8
#: tvm.topi.broadcast.greater:8 tvm.topi.broadcast.greater_equal:8
#: tvm.topi.broadcast.left_shift:8 tvm.topi.broadcast.less:8
#: tvm.topi.broadcast.less_equal:8 tvm.topi.broadcast.logical_and:8
#: tvm.topi.broadcast.logical_or:8 tvm.topi.broadcast.logical_xor:8
#: tvm.topi.broadcast.maximum:8 tvm.topi.broadcast.minimum:8
#: tvm.topi.broadcast.mod:8 tvm.topi.broadcast.multiply:8
#: tvm.topi.broadcast.not_equal:8 tvm.topi.broadcast.power:8
#: tvm.topi.broadcast.right_shift:8 tvm.topi.broadcast.subtract:8
msgid "rhs"
msgstr ""

#: of tvm.topi.broadcast.add:8 tvm.topi.broadcast.bitwise_and:8
#: tvm.topi.broadcast.bitwise_or:8 tvm.topi.broadcast.bitwise_xor:8
#: tvm.topi.broadcast.divide:8 tvm.topi.broadcast.equal:8
#: tvm.topi.broadcast.floor_divide:8 tvm.topi.broadcast.floor_mod:8
#: tvm.topi.broadcast.greater:8 tvm.topi.broadcast.greater_equal:8
#: tvm.topi.broadcast.left_shift:8 tvm.topi.broadcast.less:8
#: tvm.topi.broadcast.less_equal:8 tvm.topi.broadcast.logical_and:8
#: tvm.topi.broadcast.logical_or:8 tvm.topi.broadcast.logical_xor:8
#: tvm.topi.broadcast.maximum:8 tvm.topi.broadcast.minimum:8
#: tvm.topi.broadcast.mod:8 tvm.topi.broadcast.multiply:8
#: tvm.topi.broadcast.not_equal:8 tvm.topi.broadcast.power:8
#: tvm.topi.broadcast.right_shift:8 tvm.topi.broadcast.subtract:8
msgid "The right operand"
msgstr ""

#: of tvm.topi.broadcast.add:13 tvm.topi.broadcast.bitwise_and:13
#: tvm.topi.broadcast.bitwise_not:10 tvm.topi.broadcast.bitwise_or:13
#: tvm.topi.broadcast.bitwise_xor:13 tvm.topi.broadcast.divide:13
#: tvm.topi.broadcast.equal:13 tvm.topi.broadcast.floor_divide:13
#: tvm.topi.broadcast.floor_mod:13 tvm.topi.broadcast.greater:13
#: tvm.topi.broadcast.greater_equal:13 tvm.topi.broadcast.left_shift:13
#: tvm.topi.broadcast.less:13 tvm.topi.broadcast.less_equal:13
#: tvm.topi.broadcast.logical_and:13 tvm.topi.broadcast.logical_not:10
#: tvm.topi.broadcast.logical_or:13 tvm.topi.broadcast.logical_xor:13
#: tvm.topi.broadcast.maximum:13 tvm.topi.broadcast.minimum:13
#: tvm.topi.broadcast.mod:13 tvm.topi.broadcast.multiply:13
#: tvm.topi.broadcast.not_equal:13 tvm.topi.broadcast.power:13
#: tvm.topi.broadcast.right_shift:13 tvm.topi.broadcast.subtract:13
#: tvm.topi.reduction.collapse_sum:25 tvm.topi.transform.one_hot:28
#: tvm.topi.transform.reverse_sequence:23 tvm.topi.transform.trilu:22
msgid "ret"
msgstr ""

#: of tvm.topi.broadcast.add:13 tvm.topi.broadcast.bitwise_and:13
#: tvm.topi.broadcast.bitwise_or:13 tvm.topi.broadcast.bitwise_xor:13
#: tvm.topi.broadcast.divide:13 tvm.topi.broadcast.equal:13
#: tvm.topi.broadcast.floor_divide:13 tvm.topi.broadcast.floor_mod:13
#: tvm.topi.broadcast.greater:13 tvm.topi.broadcast.greater_equal:13
#: tvm.topi.broadcast.left_shift:13 tvm.topi.broadcast.less:13
#: tvm.topi.broadcast.less_equal:13 tvm.topi.broadcast.logical_and:13
#: tvm.topi.broadcast.logical_or:13 tvm.topi.broadcast.logical_xor:13
#: tvm.topi.broadcast.maximum:13 tvm.topi.broadcast.minimum:13
#: tvm.topi.broadcast.mod:13 tvm.topi.broadcast.multiply:13
#: tvm.topi.broadcast.not_equal:13 tvm.topi.broadcast.power:13
#: tvm.topi.broadcast.right_shift:13 tvm.topi.broadcast.subtract:13
msgid "Returns Expr if both operands are Expr. Otherwise returns Tensor."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:20 tvm.topi.broadcast.broadcast_to:9
#: tvm.topi.image.grid_sample.affine_grid:10
#: tvm.topi.image.grid_sample.grid_sample:34
#: tvm.topi.image.resize.crop_and_resize:8 tvm.topi.image.resize.resize1d:8
#: tvm.topi.image.resize.resize2d:8 tvm.topi.image.resize.resize3d:8
#: tvm.topi.nn.batch_norm.batch_norm:10
#: tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:7
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:6
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:6
#: tvm.topi.nn.bitserial_dense.bitserial_dense:5
#: tvm.topi.nn.bnn.binarize_pack:6 tvm.topi.nn.bnn.binary_dense:6
#: tvm.topi.nn.conv1d.conv1d:7 tvm.topi.nn.conv1d.group_conv1d_ncw:6
#: tvm.topi.nn.conv1d.group_conv1d_nwc:6
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:6
#: tvm.topi.nn.conv2d.conv2d_NCHWc:6 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:6
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:6
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:6
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:6
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:6
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:6
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:8
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:8
#: tvm.topi.nn.dense.dense:8 tvm.topi.nn.dense.dense_pack:6
#: tvm.topi.nn.depth_to_space.depth_to_space:6 tvm.topi.nn.dilate.dilate:6
#: tvm.topi.nn.fifo_buffer.fifo_buffer:19 tvm.topi.nn.flatten.flatten:6
#: tvm.topi.nn.group_norm.group_norm:8
#: tvm.topi.nn.instance_norm.instance_norm:6
#: tvm.topi.nn.layer_norm.layer_norm:8 tvm.topi.nn.local_response_norm.lrn:11
#: tvm.topi.nn.pad.mirror_pad:6 tvm.topi.nn.pad.pad:6
#: tvm.topi.nn.pooling.adaptive_pool:14 tvm.topi.nn.pooling.global_pool:12
#: tvm.topi.nn.pooling.pool1d:12 tvm.topi.nn.pooling.pool2d:12
#: tvm.topi.nn.pooling.pool3d:12 tvm.topi.nn.pooling.pool_grad:15
#: tvm.topi.nn.rms_norm.rms_norm:6 tvm.topi.nn.softmax.fast_softmax:7
#: tvm.topi.nn.softmax.log_softmax:6 tvm.topi.nn.softmax.softmax:6
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:7
#: tvm.topi.nn.space_to_depth.space_to_depth:6
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:7 tvm.topi.reduction.all:6
#: tvm.topi.reduction.any:6 tvm.topi.reduction.argmax:6
#: tvm.topi.reduction.argmin:6 tvm.topi.reduction.collapse_sum:18
#: tvm.topi.reduction.max:6 tvm.topi.reduction.min:6 tvm.topi.reduction.prod:6
#: tvm.topi.reduction.sum:6 tvm.topi.scan.cumprod:6 tvm.topi.scan.cumsum:6
#: tvm.topi.scan.scanop:11 tvm.topi.scatter.scatter_nd:22
#: tvm.topi.scatter_elements.scatter_elements:18 tvm.topi.sort.argsort:8
#: tvm.topi.sort.sort:7 tvm.topi.sort.topk:6 tvm.topi.sparse.dense.dense:7
#: tvm.topi.transform.adv_index:6 tvm.topi.transform.gather:17
#: tvm.topi.transform.invert_permutation:6 tvm.topi.transform.matrix_set_diag:6
#: tvm.topi.transform.sequence_mask:17 tvm.topi.transform.sliding_window:6
#: tvm.topi.unique.unique:7
msgid "data"
msgstr ""

#: of tvm.topi.transform.adv_index:6
msgid "Input data."
msgstr ""

#: of tvm.topi.scatter.scatter_nd:25
#: tvm.topi.scatter_elements.scatter_elements:21
#: tvm.topi.searchsorted.searchsorted:28 tvm.topi.transform.adv_index:9
#: tvm.topi.transform.gather:23 tvm.topi.transform.gather_nd:9
#: tvm.topi.transform.one_hot:8 tvm.topi.transform.take:9
#: tvm.topi.transform.unravel_index:9 tvm.topi.unique.unique:25
msgid "indices"
msgstr ""

#: of tvm.topi.transform.adv_index:-1
msgid "A list of tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.adv_index:9
msgid "Tensor index."
msgstr ""

#: of tvm.topi.math.erf_legalize:14
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:14
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:14
#: tvm.topi.nn.conv2d.conv2d_legalize:14
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:14
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:14
#: tvm.topi.nn.dense.dense_legalize:14 tvm.topi.nn.dense.matmul_legalize:14
#: tvm.topi.nn.fifo_buffer.fifo_buffer:28 tvm.topi.nn.group_norm.group_norm:30
#: tvm.topi.nn.instance_norm.instance_norm:23
#: tvm.topi.nn.layer_norm.layer_norm:24 tvm.topi.nn.lstm.lstm:35
#: tvm.topi.nn.rms_norm.rms_norm:19 tvm.topi.scan.cumprod:25
#: tvm.topi.scan.cumsum:25 tvm.topi.scan.scanop:40
#: tvm.topi.transform.adv_index:13 tvm.topi.transform.arange:20
#: tvm.topi.transform.invert_permutation:11
#: tvm.topi.transform.matrix_set_diag:29 tvm.topi.transform.meshgrid:13
#: tvm.topi.transform.ndarray_size:13 tvm.topi.transform.shape:13
#: tvm.topi.transform.sliding_window:24 tvm.topi.transform.sparse_to_dense:23
#: tvm.topi.transform.take_legalize:13 tvm.topi.transform.unravel_index:16
#: tvm.topi.transform.where:16
msgid "result"
msgstr ""

#: of tvm.topi.transform.adv_index:14 tvm.topi.transform.invert_permutation:11
msgid "Output tensor"
msgstr ""

#: of tvm.topi.reduction.all:6 tvm.topi.reduction.any:6
msgid "The input tvm boolean tensor"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:25 tvm.topi.nn.bnn.binarize_pack:10
#: tvm.topi.nn.elemwise.prelu:16 tvm.topi.nn.fifo_buffer.fifo_buffer:24
#: tvm.topi.nn.instance_norm.instance_norm:16
#: tvm.topi.nn.layer_norm.layer_norm:17 tvm.topi.nn.local_response_norm.lrn:18
#: tvm.topi.nn.rms_norm.rms_norm:12 tvm.topi.nn.softmax.fast_softmax:10
#: tvm.topi.nn.softmax.softmax:9 tvm.topi.reduction.all:11
#: tvm.topi.reduction.any:11 tvm.topi.reduction.argmax:11
#: tvm.topi.reduction.argmin:11 tvm.topi.reduction.max:11
#: tvm.topi.reduction.min:11 tvm.topi.reduction.prod:11
#: tvm.topi.reduction.sum:11 tvm.topi.scan.cumprod:10 tvm.topi.scan.cumsum:10
#: tvm.topi.scan.scanop:24 tvm.topi.scatter_elements.scatter_elements:27
#: tvm.topi.sort.argsort:15 tvm.topi.sort.sort:11 tvm.topi.sort.topk:12
#: tvm.topi.transform.concatenate:9 tvm.topi.transform.flip:9
#: tvm.topi.transform.one_hot:20 tvm.topi.transform.sequence_mask:26
#: tvm.topi.transform.sliding_window:12 tvm.topi.transform.squeeze:9
#: tvm.topi.transform.stack:10 tvm.topi.transform.take:13
msgid "axis"
msgstr ""

#: of tvm.topi.reduction.all:-1 tvm.topi.reduction.any:-1
#: tvm.topi.reduction.argmax:-1 tvm.topi.reduction.argmin:-1
#: tvm.topi.reduction.max:-1 tvm.topi.reduction.min:-1
#: tvm.topi.reduction.prod:-1 tvm.topi.reduction.sum:-1
msgid "None or int or tuple of int"
msgstr ""

#: of tvm.topi.reduction.all:9
msgid ""
"Axis or axes along which a logical AND is performed. The default, "
"axis=None, will perform logical AND over all elements of the input array."
" If axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.topi.reduction.all:16 tvm.topi.reduction.any:16
#: tvm.topi.reduction.argmax:16 tvm.topi.reduction.argmin:16
#: tvm.topi.reduction.max:16 tvm.topi.reduction.min:16
#: tvm.topi.reduction.prod:16 tvm.topi.reduction.sum:16
msgid "keepdims"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:-1 tvm.topi.nn.pooling.pool1d:-1
#: tvm.topi.nn.pooling.pool2d:-1 tvm.topi.nn.pooling.pool3d:-1
#: tvm.topi.nn.pooling.pool_grad:-1 tvm.topi.reduction.all:-1
#: tvm.topi.reduction.any:-1 tvm.topi.reduction.argmax:-1
#: tvm.topi.reduction.argmin:-1 tvm.topi.reduction.max:-1
#: tvm.topi.reduction.min:-1 tvm.topi.reduction.prod:-1
#: tvm.topi.reduction.sum:-1 tvm.topi.signal.dft:-1 tvm.topi.signal.stft:-1
#: tvm.topi.unique.unique:-1 tvm.topi.utils.equal_const_int:-1
msgid "bool"
msgstr ""

#: of tvm.topi.reduction.all:14 tvm.topi.reduction.any:14
#: tvm.topi.reduction.argmax:14 tvm.topi.reduction.argmin:14
#: tvm.topi.reduction.max:14 tvm.topi.reduction.min:14
#: tvm.topi.reduction.prod:14 tvm.topi.reduction.sum:14
msgid ""
"If this is set to True, the axes which are reduced are left in the result"
" as dimensions with size one. With this option, the result will broadcast"
" correctly against the input array."
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:16 tvm.topi.reduction.all:20
#: tvm.topi.reduction.any:20 tvm.topi.reduction.argmax:24
#: tvm.topi.reduction.argmin:24 tvm.topi.reduction.max:20
#: tvm.topi.reduction.min:20 tvm.topi.reduction.prod:20
#: tvm.topi.reduction.sum:20 tvm.topi.scatter.scatter_nd:37
#: tvm.topi.scatter_elements.scatter_elements:41
#: tvm.topi.transform.concatenate:13
#: tvm.topi.transform.dynamic_strided_slice:24
#: tvm.topi.transform.expand_dims:13 tvm.topi.transform.expand_like:33
#: tvm.topi.transform.flip:13 tvm.topi.transform.gather:27
#: tvm.topi.transform.gather_nd:13 tvm.topi.transform.repeat:16
#: tvm.topi.transform.reshape:12 tvm.topi.transform.stack:14
#: tvm.topi.transform.strided_set:24 tvm.topi.transform.strided_slice:32
#: tvm.topi.transform.take:26 tvm.topi.transform.tile:13
#: tvm.topi.transform.transpose:13
msgid "ret : tvm.te.Tensor"
msgstr ""

#: of tvm.topi.reduction.any:9
msgid ""
"Axis or axes along which a logical OR is performed. The default, "
"axis=None, will perform logical OR over all elements of the input array. "
"If axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.topi.transform.arange:7
msgid "start"
msgstr ""

#: of tvm.topi.transform.arange:-1
msgid "tvm.Expr, optional"
msgstr ""

#: of tvm.topi.transform.arange:6
msgid ""
"Start of interval. The interval includes this value. The default start "
"value is 0."
msgstr ""

#: of tvm.topi.transform.arange:10
msgid "stop"
msgstr ""

#: of tvm.topi.transform.arange:-1 tvm.topi.utils.equal_const_int:-1
msgid "tvm.Expr"
msgstr ""

#: of tvm.topi.transform.arange:10
msgid "Stop of interval. The interval does not include this value."
msgstr ""

#: of tvm.topi.transform.arange:13
msgid "step"
msgstr ""

#: of tvm.topi.transform.arange:13
msgid "Spacing between values. The default step size is 1."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:-1 tvm.topi.nn.bnn.binarize_pack:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:-1
#: tvm.topi.nn.dilate.dilate:-1 tvm.topi.nn.lstm.lstm:-1
#: tvm.topi.nn.pad.mirror_pad:-1 tvm.topi.nn.pad.pad:-1
#: tvm.topi.transform.arange:-1 tvm.topi.transform.ndarray_size:-1
#: tvm.topi.transform.shape:-1 tvm.topi.transform.strided_slice:-1
#: tvm.topi.transform.take:-1
msgid "str, optional"
msgstr ""

#: of tvm.topi.transform.arange:16 tvm.topi.transform.ndarray_size:9
#: tvm.topi.transform.shape:9
msgid "The target data type."
msgstr ""

#: of tvm.topi.transform.arange:21 tvm.topi.transform.ndarray_size:14
#: tvm.topi.transform.shape:14 tvm.topi.transform.sliding_window:25
msgid "The resulting tensor."
msgstr ""

#: of tvm.topi.reduction.argmax:6 tvm.topi.reduction.argmin:6
#: tvm.topi.reduction.max:6 tvm.topi.reduction.min:6 tvm.topi.reduction.prod:6
#: tvm.topi.reduction.sum:6
msgid "The input tvm tensor"
msgstr ""

#: of tvm.topi.reduction.argmax:9
msgid ""
"Axis or axes along which a argmax operation is performed. The default, "
"axis=None, will find the indices of the maximum element of the elements "
"of the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.topi.reduction.argmax:20 tvm.topi.reduction.argmin:20
msgid "select_last_index: bool"
msgstr ""

#: of tvm.topi.reduction.argmax:19
msgid ""
"Whether to select the last index if the maximum element appears multiple "
"times, else select the first index."
msgstr ""

#: of tvm.topi.reduction.argmin:9
msgid ""
"Axis or axes along which a argmin operation is performed. The default, "
"axis=None, will find the indices of minimum element all of the elements "
"of the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.topi.reduction.argmin:19
msgid ""
"Whether to select the last index if the minimum element appears multiple "
"times, else select the first index."
msgstr ""

#: of tvm.topi.reduction.collapse_sum:18 tvm.topi.sort.argsort:8
#: tvm.topi.sort.sort:7 tvm.topi.sort.topk:6
msgid "The input tensor."
msgstr ""

#: of tvm.topi.sort.argsort:11
msgid "valid_count"
msgstr ""

#: of tvm.topi.sort.argsort:-1 tvm.topi.sparse.csrmm.csrmm:-1
#: tvm.topi.sparse.csrmv.csrmv:-1
msgid "tvm.te.Tensor, optional"
msgstr ""

#: of tvm.topi.sort.argsort:11
msgid "1-D tensor for valid number of boxes."
msgstr ""

#: of tvm.topi.scan.cumprod:-1 tvm.topi.scan.cumsum:-1 tvm.topi.scan.scanop:-1
#: tvm.topi.sort.argsort:-1 tvm.topi.sort.sort:-1 tvm.topi.sort.topk:-1
#: tvm.topi.transform.concatenate:-1 tvm.topi.transform.flip:-1
#: tvm.topi.transform.reverse_sequence:-1 tvm.topi.transform.sequence_mask:-1
#: tvm.topi.transform.stack:-1 tvm.topi.transform.take:-1
msgid "int, optional"
msgstr ""

#: of tvm.topi.sort.argsort:14 tvm.topi.sort.sort:10
msgid ""
"Axis along which to sort the input tensor. By default the flattened array"
" is used."
msgstr ""

#: of tvm.topi.sort.argsort:18 tvm.topi.sort.sort:14 tvm.topi.sort.topk:21
msgid "is_ascend"
msgstr ""

#: of tvm.topi.sort.argsort:-1 tvm.topi.sort.sort:-1 tvm.topi.sort.topk:-1
msgid "boolean, optional"
msgstr ""

#: of tvm.topi.sort.argsort:18 tvm.topi.sort.sort:14 tvm.topi.sort.topk:21
msgid "Whether to sort in ascending or descending order."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:-1
#: tvm.topi.image.resize.resize1d:-1 tvm.topi.image.resize.resize2d:-1
#: tvm.topi.image.resize.resize3d:-1 tvm.topi.nn.upsampling.upsampling:-1
#: tvm.topi.nn.upsampling.upsampling3d:-1 tvm.topi.scan.cumprod:-1
#: tvm.topi.scan.cumsum:-1 tvm.topi.scan.scanop:-1
#: tvm.topi.searchsorted.searchsorted:-1 tvm.topi.sort.argsort:-1
#: tvm.topi.sort.sort:-1 tvm.topi.sort.topk:-1
#: tvm.topi.transform.matrix_set_diag:-1
msgid "string, optional"
msgstr ""

#: of tvm.topi.sort.argsort:21 tvm.topi.sort.sort:17
msgid "DType of the output indices."
msgstr ""

#: of tvm.topi.argwhere.argwhere:10 tvm.topi.argwhere.hybrid_argwhere_1d:10
#: tvm.topi.argwhere.hybrid_argwhere_2d:10
#: tvm.topi.argwhere.hybrid_argwhere_3d:10
#: tvm.topi.argwhere.hybrid_argwhere_4d:10
#: tvm.topi.argwhere.hybrid_argwhere_5d:10 tvm.topi.einsum.einsum:18
#: tvm.topi.sort.argsort:26 tvm.topi.sort.sort:21 tvm.topi.sort.topk:28
#: tvm.topi.utils.simplify:10
msgid "out"
msgstr ""

#: of tvm.topi.sort.argsort:26 tvm.topi.sort.sort:22
msgid "Sorted index tensor."
msgstr ""

#: of tvm.te.operation.extern:51 tvm.tir.buffer.decl_buffer:62
#: tvm.topi.sort.argsort:29
msgid "Example"
msgstr ""

#: of tvm.topi.argwhere.argwhere:6 tvm.topi.argwhere.hybrid_argwhere_1d:6
#: tvm.topi.argwhere.hybrid_argwhere_2d:6
#: tvm.topi.argwhere.hybrid_argwhere_3d:6
#: tvm.topi.argwhere.hybrid_argwhere_4d:6
#: tvm.topi.argwhere.hybrid_argwhere_5d:6 tvm.topi.transform.where:6
msgid "condition"
msgstr ""

#: of tvm.topi.argwhere.argwhere:6
msgid "Tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.argwhere:11 tvm.topi.argwhere.hybrid_argwhere_1d:11
#: tvm.topi.argwhere.hybrid_argwhere_2d:11
#: tvm.topi.argwhere.hybrid_argwhere_3d:11
#: tvm.topi.argwhere.hybrid_argwhere_4d:11
#: tvm.topi.argwhere.hybrid_argwhere_5d:11
msgid "Indices of non-zero elements."
msgstr ""

#: of tvm.topi.searchsorted.binary_search:3
msgid ""
"`sorted_sequence` is a N-D Buffer whose innermost dimension we want to "
"search for `value`, and `search_range` is the size of the innermost "
"dimension. `sequence_offset` is a 1-D linearlized offset specifying which"
" of innermost sequences to search."
msgstr ""

#: of tvm.topi.searchsorted.binary_search:7
msgid ""
"So the search for `value` is performed over "
"`sorted_sequence[sequence_offset:(sequence_offset + search_range)]`. Note"
" that we index N-D Buffer by 1-D linearlized indices."
msgstr ""

#: of tvm.topi.broadcast.bitwise_not:5 tvm.topi.broadcast.logical_not:5
msgid "data : tvm.te.Tensor or Expr"
msgstr ""

#: of tvm.topi.broadcast.bitwise_not:10 tvm.topi.broadcast.logical_not:10
msgid "Returns Expr if the operand are Expr. Otherwise returns Tensor."
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:3
msgid ""
"We follows the numpy broadcasting rule. See also "
"https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:9 tvm.topi.nn.fifo_buffer.fifo_buffer:20
msgid "The input data"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:11 tvm.topi.broadcast.broadcast_to:12
#: tvm.topi.reduction.collapse_sum:21 tvm.topi.tensor.full:5
#: tvm.topi.transform.unravel_index:12
msgid "shape"
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:-1
msgid "list or tuple"
msgstr ""

#: of tvm.topi.broadcast.broadcast_to:12
msgid "The target shape to be broadcasted."
msgstr ""

#: of tvm.topi.math.cast:9 tvm.topi.math.reinterpret:9
msgid "Data type."
msgstr ""

#: of tvm.topi.math.cast:12
msgid "The location of the cast in the source."
msgstr ""

#: of tvm.topi.math.ceil_log2:1
msgid ""
"Compute integer ceil log2 with a special code path for vulkan SPIR-V does"
" not support log2 on fp64. Instead, we compute integer ceil_log2 via clz "
"intrinsic when the target is vulkan."
msgstr ""

#: of tvm.topi.math.clip:1
msgid ""
"Clip (limit) the values in an array. Given an interval, values outside "
"the interval are clipped to the interval edges."
msgstr ""

#: of tvm.topi.math.clip:8
msgid "a_min"
msgstr ""

#: of tvm.topi.math.clip:-1
msgid "tvm.tir.PrimExpr"
msgstr ""

#: of tvm.topi.math.clip:9
msgid "Minimum value."
msgstr ""

#: of tvm.topi.math.clip:11
msgid "a_max"
msgstr ""

#: of tvm.topi.math.clip:11
msgid "Maximum value."
msgstr ""

#: of tvm.topi.reduction.collapse_sum:3
msgid ""
"collapse_sum is intended as the backward operator of topi broadcast "
"operators in the automatic differentiation process."
msgstr ""

#: of tvm.topi.reduction.collapse_sum:6
msgid ""
"We expect that data is the result of broadcasting some tensor of "
"target_shape in some broadcast operation. Thus target_shape and "
"data.shape must follow broadcast rules."
msgstr ""

#: of tvm.topi.reduction.collapse_sum:9
msgid ""
"During computation, the axes of data.shape and target_shape are checked "
"from right to left. For every axis, if it either: - exist in data but not"
" in target_shape, or - is larger than 1 in data and equals to 1 in "
"target_shape, data will be summed over this axis."
msgstr ""

#: of tvm.topi.reduction.collapse_sum:-1
msgid "Tuple[int]"
msgstr ""

#: of tvm.topi.reduction.collapse_sum:21
msgid "The shape to collapse to."
msgstr ""

#: of tvm.topi.reduction.collapse_sum:26
msgid "The result tensor after summation."
msgstr ""

#: of tvm.topi.einsum.einsum:14 tvm.topi.transform.concatenate:6
#: tvm.topi.transform.meshgrid:6
msgid "a_tuple"
msgstr ""

#: of tvm.topi.einsum.einsum:-1 tvm.topi.transform.concatenate:-1
#: tvm.topi.transform.meshgrid:-1
msgid "tuple of tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.concatenate:6
msgid "The arrays to concatenate"
msgstr ""

#: of tvm.topi.transform.concatenate:9
msgid "The axis along which the arrays will be joined. Default is 0."
msgstr ""

#: of tvm.topi.utils.const_vector:5
msgid "vector: numpy.ndarray"
msgstr ""

#: of tvm.topi.utils.const_vector:6
msgid "Const input array"
msgstr ""

#: of tvm.te.operation.extern:26 tvm.topi.utils.const_vector:8
msgid "name: str, optional"
msgstr ""

#: of tvm.topi.utils.const_vector:8
msgid "The name of output op"
msgstr ""

#: of tvm.topi.utils.const_vector:12
msgid "tensor: Tensor"
msgstr ""

#: of tvm.topi.utils.const_vector:13
msgid "The created tensor"
msgstr ""

#: of tvm.topi.scan.cumprod:1
msgid ""
"Numpy style cumprod op. Return the cumulative product of the elements "
"along a given axis."
msgstr ""

#: of tvm.topi.scan.cumprod:6 tvm.topi.scan.cumsum:6 tvm.topi.scan.scanop:11
#: tvm.topi.transform.gather:17 tvm.topi.transform.sliding_window:6
msgid "The input data to the operator."
msgstr ""

#: of tvm.topi.scan.cumprod:9
msgid ""
"Axis along which the cumulative product is computed. The default (None) "
"is to compute the cumproduct over the flattened array."
msgstr ""

#: of tvm.topi.scan.cumprod:13
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are multiplied. If dtype is not specified, it defaults to the dtype of "
"data."
msgstr ""

#: of tvm.topi.scan.cumprod:20 tvm.topi.scan.cumsum:20 tvm.topi.scan.scanop:35
msgid "exclusive"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:-1 tvm.topi.nn.sparse.sparse_dense:-1
#: tvm.topi.scan.cumprod:-1 tvm.topi.scan.cumsum:-1 tvm.topi.scan.scanop:-1
#: tvm.topi.searchsorted.searchsorted:-1
msgid "bool, optional"
msgstr ""

#: of tvm.topi.scan.cumprod:17
msgid ""
"If True, will return exclusive product in which the first element is not "
"included. In other terms, if True, the j-th output element would be the "
"product of the first (j-1) elements. Otherwise, it would be the product "
"of the first j elements."
msgstr ""

#: of tvm.topi.scan.cumprod:25 tvm.topi.scan.cumsum:25 tvm.topi.scan.scanop:40
msgid ""
"The result has the same size as data, and the same shape as data if axis "
"is not None. If axis is None, the result is a 1-d array."
msgstr ""

#: of tvm.topi.scan.cumsum:1
msgid ""
"Numpy style cumsum op. Return the cumulative sum of the elements along a "
"given axis."
msgstr ""

#: of tvm.topi.scan.cumsum:9
msgid ""
"Axis along which the cumulative sum is computed. The default (None) is to"
" compute the cumsum over the flattened array."
msgstr ""

#: of tvm.topi.scan.cumsum:13
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are summed. If dtype is not specified, it defaults to the dtype of data."
msgstr ""

#: of tvm.topi.scan.cumsum:17
msgid ""
"If True, will return exclusive sum in which the first element is not "
"included. In other terms, if True, the j-th output element would be the "
"sum of the first (j-1) elements. Otherwise, it would be the sum of the "
"first j elements."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:3
msgid ""
"Normally buffer is created automatically during lower and build. This is "
"only needed if user want to specify their own buffer layout."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:6
msgid "See the note below for detailed discussion on usage of buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:-1 tvm.topi.utils.get_const_tuple:-1
msgid "tuple of Expr"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:11
msgid "The shape of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:14
msgid "The data type of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:17 tvm.topi.nn.bnn.binarize_pack:13
#: tvm.topi.nn.dilate.dilate:15 tvm.topi.nn.pad.mirror_pad:18
#: tvm.topi.nn.pad.pad:18
msgid "name"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:17
msgid "The name of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:-1
msgid "Var, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:20
msgid "The data pointer in the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:23
msgid "strides: array of Expr"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:23
msgid "The stride of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:27
msgid "elem_offset: Expr, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:26
msgid ""
"The beginning offset of the array to data. In terms of number of elements"
" of dtype."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:31
msgid "scope: str, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:30
msgid ""
"The storage scope of the buffer, if not global. If scope equals empty "
"string, it means it is global memory."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:35
msgid "data_alignment: int, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:34
msgid ""
"The alignment of data pointer in bytes. If -1 is passed, the alignment "
"will be set to TVM's internal default."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:41
msgid "offset_factor: int, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:38
msgid ""
"The factor of elem_offset field, when set, elem_offset is required to be "
"multiple of offset_factor. If 0 is pssed, the alignment will be set to 1."
" if non-zero is passed, we will created a Var for elem_offset if "
"elem_offset is not None."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:46
msgid "buffer_type: str, optional, {\"\", \"auto_broadcast\"}"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:44
msgid ""
"auto_broadcast buffer allows one to implement broadcast computation "
"without considering whether dimension size equals to one. TVM maps "
"buffer[i][j][k] -> buffer[i][0][k] if dimension j's shape equals 1."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:51
msgid "axis_separators"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:-1 tvm.topi.transform.strided_slice:-1
msgid "list of int, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:49
msgid ""
"If passed, a list of separators between groups of axes, each of which is "
"flattened to an output axis.  For flat memory spaces, should either be "
"None, or an empty list."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:54
msgid "span: Optional[Span]"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:54
msgid "The location of the decl_buffer creation in the source."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:59 tvm.topi.nn.fifo_buffer.fifo_buffer:21
msgid "buffer"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:-1
msgid "tvm.tir.Buffer"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:59
msgid "The created buffer"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:63
msgid ""
"Here's an example of how broadcast buffer can be used to define a "
"symbolic broadcast operation,"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:85 tvm.tir.op.div:19
#: tvm.topi.nn.conv2d.conv2d_alter_layout:15
#: tvm.topi.nn.conv3d.conv3d_alter_layout:15
#: tvm.topi.nn.dense.dense_alter_layout:15 tvm.topi.nn.qnn.add_alter_layout:19
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:15
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:15
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:18
msgid "Note"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:86
msgid ""
"Buffer data structure reflects the DLTensor structure in dlpack. While "
"DLTensor data structure is very general, it is usually helpful to create "
"function that only handles specific case of data structure and make "
"compiled function benefit from it."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:91
msgid ""
"If user pass strides and elem_offset is passed as None when constructing "
"the function, then the function will be specialized for the DLTensor that"
" is compact and aligned. If user pass a fully generic symbolic array to "
"the strides, then the resulting function becomes fully generic."
msgstr ""

#: of tvm.topi.signal.dft:1
msgid ""
"Computes the discrete Fourier transform of input (calculation along the "
"last axis). This gives frequency components of the signal as they change "
"over time."
msgstr ""

#: of tvm.topi.signal.dft:7
msgid "re_data"
msgstr ""

#: of tvm.topi.signal.dft:-1 tvm.topi.signal.stft:-1
#: tvm.topi.sparse_reshape.sparse_reshape:-1
#: tvm.topi.transform.matrix_set_diag:-1 tvm.topi.transform.one_hot:-1
#: tvm.topi.transform.sliding_window:-1 tvm.topi.transform.trilu:-1
#: tvm.topi.transform.unravel_index:-1
msgid "relay.Expr"
msgstr ""

#: of tvm.topi.signal.dft:7
msgid "N-D tensor, real part of the input signal."
msgstr ""

#: of tvm.topi.signal.dft:11
msgid "im_data"
msgstr ""

#: of tvm.topi.signal.dft:10
msgid ""
"N-D tensor, imaginary part of the input signal. If the signal is real, "
"then the values of this tensor are zeros."
msgstr ""

#: of tvm.topi.signal.dft:14
msgid "inverse"
msgstr ""

#: of tvm.topi.signal.dft:14
msgid "Whether to perform the inverse discrete fourier transform."
msgstr ""

#: of tvm.topi.signal.dft:18
msgid "re_output"
msgstr ""

#: of tvm.topi.signal.dft:19
msgid "The Fourier Transform of the input (Real part)."
msgstr ""

#: of tvm.topi.signal.dft:20
msgid "im_output"
msgstr ""

#: of tvm.topi.signal.dft:21
msgid "The Fourier Transform of the input (Imaginary part)."
msgstr ""

#: of tvm.tir.op.div:6 tvm.tir.op.floordiv:6 tvm.tir.op.floormod:6
#: tvm.topi.sparse.csrmm.csrmm:7 tvm.topi.sparse.csrmv.csrmv:7
#: tvm.topi.transform.dynamic_strided_slice:6 tvm.topi.transform.expand_dims:6
#: tvm.topi.transform.expand_like:24 tvm.topi.transform.flip:6
#: tvm.topi.transform.gather_nd:6 tvm.topi.transform.repeat:6
#: tvm.topi.transform.reshape:5 tvm.topi.transform.reverse_sequence:7
#: tvm.topi.transform.stack:6 tvm.topi.transform.strided_set:6
#: tvm.topi.transform.strided_slice:6 tvm.topi.transform.take:6
#: tvm.topi.transform.tile:6 tvm.topi.transform.transpose:6
msgid "a"
msgstr ""

#: of tvm.tir.op.div:6
msgid "The left hand operand, known to be non-negative."
msgstr ""

#: of tvm.tir.op.div:9 tvm.tir.op.floordiv:9 tvm.tir.op.floormod:9
#: tvm.topi.sparse.csrmm.csrmm:10 tvm.topi.utils.make_idx:10
#: tvm.topi.utils.within_index:6
msgid "b"
msgstr ""

#: of tvm.tir.op.div:9
msgid "The right hand operand, known to be non-negative."
msgstr ""

#: of tvm.tir.op.div:12 tvm.tir.op.floordiv:12 tvm.tir.op.floormod:12
msgid "The location of this operator in the source."
msgstr ""

#: of tvm.tir.op.div:16 tvm.tir.op.floordiv:16 tvm.tir.op.floormod:16
msgid "res"
msgstr ""

#: of tvm.tir.op.div:17 tvm.tir.op.floordiv:17 tvm.tir.op.floormod:17
msgid "The result expression."
msgstr ""

#: of tvm.tir.op.div:20
msgid "When operands are integers, returns truncdiv(a, b, span)."
msgstr ""

#: of tvm.topi.transform.dynamic_strided_slice:6
#: tvm.topi.transform.strided_set:6 tvm.topi.transform.strided_slice:6
msgid "The tensor to be sliced."
msgstr ""

#: of tvm.topi.transform.dynamic_strided_slice:9
#: tvm.topi.transform.strided_slice:9
msgid "begin"
msgstr ""

#: of tvm.topi.transform.dynamic_strided_slice:9
#: tvm.topi.transform.strided_set:12 tvm.topi.transform.strided_slice:9
msgid "The indices to begin with in the slicing."
msgstr ""

#: of tvm.topi.transform.dynamic_strided_slice:12
#: tvm.topi.transform.strided_slice:12
msgid "end"
msgstr ""

#: of tvm.topi.transform.dynamic_strided_slice:12
#: tvm.topi.transform.strided_set:15 tvm.topi.transform.strided_slice:12
msgid "Indices indicating end of the slice."
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:14 tvm.topi.nn.conv1d.group_conv1d_ncw:12
#: tvm.topi.nn.conv1d.group_conv1d_nwc:12 tvm.topi.nn.conv2d.conv2d:12
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:10
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:10
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:10
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:10
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:12
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:12
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:18
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:18
#: tvm.topi.nn.dilate.dilate:9 tvm.topi.transform.dynamic_strided_slice:17
#: tvm.topi.transform.sliding_window:20 tvm.topi.transform.strided_slice:17
msgid "strides"
msgstr ""

#: of tvm.topi.transform.dynamic_strided_slice:15
#: tvm.topi.transform.strided_set:18 tvm.topi.transform.strided_slice:15
msgid ""
"Specifies the stride values, it can be negative in that case, the input "
"tensor will be reversed in that particular axis."
msgstr ""

#: of tvm.topi.transform.dynamic_strided_slice:20
msgid "output_shape: list of PrimExpr"
msgstr ""

#: of tvm.topi.transform.dynamic_strided_slice:20
msgid "Specifies the output shape"
msgstr ""

#: of tvm.topi.einsum.einsum:9
msgid "subscripts"
msgstr ""

#: of tvm.topi.einsum.einsum:-1 tvm.topi.nn.depth_to_space.depth_to_space:-1
#: tvm.topi.nn.loss.nll_loss:-1 tvm.topi.nn.space_to_depth.space_to_depth:-1
#: tvm.topi.scatter.scatter_nd:-1
msgid "string"
msgstr ""

#: of tvm.topi.einsum.einsum:6
msgid ""
"Specifies the subscripts for summation as comma separated list of "
"subscript labels. An implicit (classical Einstein summation) calculation "
"is performed unless the explicit indicator -> is included as well as "
"subscript labels of the precise output form."
msgstr ""

#: of tvm.topi.einsum.einsum:12
msgid ""
"These are the Tensors for the operation. The only difference of einsum "
"between in tvm and numpy is it needs an extra brackets for the tensors. "
"For example, topi.einsum(\"ij, jk -> ik\", (A, B))."
msgstr ""

#: of tvm.topi.einsum.einsum:19
msgid "The calculation based on the Einstein summation convention."
msgstr ""

#: of tvm.topi.tensor.elemwise_sum:6
msgid "xs"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:-1 tvm.topi.tensor.elemwise_sum:-1
msgid "list of tvm.te.Tensor"
msgstr ""

#: of tvm.topi.tensor.elemwise_sum:6
msgid "Input arguments."
msgstr ""

#: of tvm.topi.math.erf_legalize:5
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:5
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:5
#: tvm.topi.nn.conv2d.conv2d_alter_layout:5
#: tvm.topi.nn.conv2d.conv2d_legalize:5
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:5
#: tvm.topi.nn.conv3d.conv3d_alter_layout:5
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:5
#: tvm.topi.nn.dense.dense_alter_layout:5 tvm.topi.nn.dense.dense_legalize:5
#: tvm.topi.nn.dense.matmul_legalize:5 tvm.topi.nn.qnn.add_alter_layout:9
#: tvm.topi.nn.qnn.bias_add_legalize:8
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:5
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:6
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:5
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:8
#: tvm.topi.transform.take_legalize:5
msgid "attrs"
msgstr ""

#: of tvm.topi.math.erf_legalize:-1
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:-1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:-1
#: tvm.topi.nn.conv2d.conv2d_alter_layout:-1
#: tvm.topi.nn.conv2d.conv2d_legalize:-1
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:-1
#: tvm.topi.nn.conv3d.conv3d_alter_layout:-1
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:-1
#: tvm.topi.nn.dense.dense_alter_layout:-1 tvm.topi.nn.dense.dense_legalize:-1
#: tvm.topi.nn.dense.matmul_legalize:-1 tvm.topi.nn.qnn.add_alter_layout:-1
#: tvm.topi.nn.qnn.bias_add_legalize:-1
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:-1
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:-1
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:-1
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:-1
#: tvm.topi.transform.take_legalize:-1
msgid "tvm.ir.Attrs"
msgstr ""

#: of tvm.topi.math.erf_legalize:6
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:6
#: tvm.topi.nn.conv2d.conv2d_alter_layout:6
#: tvm.topi.nn.conv2d.conv2d_legalize:6
#: tvm.topi.nn.conv3d.conv3d_alter_layout:6
#: tvm.topi.nn.dense.dense_alter_layout:6 tvm.topi.nn.qnn.add_alter_layout:10
#: tvm.topi.nn.qnn.bias_add_legalize:9
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:6
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:6
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:9
msgid "Attributes of current convolution"
msgstr ""

#: of tvm.topi.math.erf_legalize:7
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:7
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:7
#: tvm.topi.nn.conv2d.conv2d_alter_layout:7
#: tvm.topi.nn.conv2d.conv2d_legalize:7
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:7
#: tvm.topi.nn.conv3d.conv3d_alter_layout:7
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:7
#: tvm.topi.nn.dense.dense_alter_layout:7 tvm.topi.nn.dense.dense_legalize:7
#: tvm.topi.nn.dense.matmul_legalize:7 tvm.topi.nn.qnn.add_alter_layout:11
#: tvm.topi.nn.qnn.bias_add_legalize:10
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:7
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:8
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:7
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:10
#: tvm.topi.nn.upsampling.upsampling:9 tvm.topi.nn.upsampling.upsampling3d:9
#: tvm.topi.transform.take_legalize:7
msgid "inputs"
msgstr ""

#: of tvm.topi.math.erf_legalize:-1
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:-1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:-1
#: tvm.topi.nn.conv2d.conv2d_legalize:-1
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:-1
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:-1
#: tvm.topi.nn.dense.dense_legalize:-1 tvm.topi.nn.dense.matmul_legalize:-1
#: tvm.topi.transform.take_legalize:-1
msgid "list of tvm.relay.Expr"
msgstr ""

#: of tvm.topi.math.erf_legalize:8
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:8
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:8
#: tvm.topi.nn.conv2d.conv2d_legalize:8
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:8
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:8
#: tvm.topi.nn.dense.dense_legalize:8 tvm.topi.nn.dense.matmul_legalize:8
#: tvm.topi.transform.take_legalize:8
msgid "The args of the Relay expr to be legalized"
msgstr ""

#: of tvm.topi.math.erf_legalize:10
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:10
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:10
#: tvm.topi.nn.conv2d.conv2d_legalize:10
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:10
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:10
#: tvm.topi.nn.dense.dense_legalize:10 tvm.topi.nn.dense.matmul_legalize:10
#: tvm.topi.transform.take_legalize:9
msgid "types"
msgstr ""

#: of tvm.topi.math.erf_legalize:-1
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:-1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:-1
#: tvm.topi.nn.conv2d.conv2d_legalize:-1
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:-1
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:-1
#: tvm.topi.nn.dense.dense_legalize:-1 tvm.topi.nn.dense.matmul_legalize:-1
#: tvm.topi.transform.take_legalize:-1
msgid "list of types"
msgstr ""

#: of tvm.topi.math.erf_legalize:10
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:10
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:10
#: tvm.topi.nn.conv2d.conv2d_legalize:10
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:10
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:10
#: tvm.topi.nn.dense.dense_legalize:10 tvm.topi.nn.dense.matmul_legalize:10
#: tvm.topi.transform.take_legalize:10
msgid "List of input and output types"
msgstr ""

#: of tvm.topi.math.erf_legalize:-1
#: tvm.topi.nn.batch_matmul.batch_matmul_legalize:-1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:-1
#: tvm.topi.nn.conv2d.conv2d_alter_layout:-1
#: tvm.topi.nn.conv2d.conv2d_legalize:-1
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:-1
#: tvm.topi.nn.conv3d.conv3d_alter_layout:-1
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:-1
#: tvm.topi.nn.dense.dense_alter_layout:-1 tvm.topi.nn.dense.dense_legalize:-1
#: tvm.topi.nn.dense.matmul_legalize:-1 tvm.topi.nn.qnn.add_alter_layout:-1
#: tvm.topi.nn.qnn.bias_add_legalize:-1
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:-1
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:-1
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:-1
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:-1
#: tvm.topi.transform.take_legalize:-1
msgid "tvm.relay.Expr"
msgstr ""

#: of tvm.topi.math.erf_legalize:15
msgid "The legalized expr."
msgstr ""

#: of tvm.topi.transform.expand_dims:6 tvm.topi.transform.expand_like:25
#: tvm.topi.transform.flip:6 tvm.topi.transform.transpose:6
msgid "The tensor to be expanded."
msgstr ""

#: of tvm.topi.transform.expand_dims:9
msgid "num_newaxis: int, optional"
msgstr ""

#: of tvm.topi.transform.expand_dims:9
msgid "Number of newaxis to be inserted on axis"
msgstr ""

#: of tvm.topi.transform.expand_like:1
msgid ""
"Expand an input array with the shape of second array. This operation can "
"always be composed of unsqueezing and expanding dims on those unsqueezed "
"axes."
msgstr ""

#: of tvm.topi.signal.stft:24 tvm.topi.sparse_reshape.sparse_reshape:21
#: tvm.topi.transform.expand_like:6 tvm.topi.transform.invert_permutation:14
#: tvm.topi.transform.matrix_set_diag:32 tvm.topi.transform.one_hot:31
#: tvm.topi.transform.trilu:25 tvm.topi.unique.unique:39
msgid "Examples"
msgstr ""

#: of tvm.topi.transform.expand_like:26
msgid "shape_like"
msgstr ""

#: of tvm.topi.transform.expand_like:27
msgid "The tensor to with target shape."
msgstr ""

#: of tvm.topi.transform.expand_like:29
msgid "axis: list of int"
msgstr ""

#: of tvm.topi.transform.expand_like:29
msgid "axis to be expanded on"
msgstr ""

#: of tvm.te.operation.extern:6
msgid "shape: tuple or list of tuples."
msgstr ""

#: of tvm.te.operation.extern:6
msgid "The shape of the outputs."
msgstr ""

#: of tvm.te.operation.extern:9
msgid "inputs: list of Tensor"
msgstr ""

#: of tvm.te.operation.extern:9
msgid "The inputs"
msgstr ""

#: of tvm.te.operation.extern:23
msgid "fcompute: lambda function of inputs, outputs-> stmt"
msgstr ""

#: of tvm.te.operation.extern:12
msgid ""
"Specifies the IR statement to do the computation. See the following note "
"for function signature of fcompute"
msgstr ""

#: of tvm.te.operation.extern:16
msgid "**Parameters**"
msgstr ""

#: of tvm.te.operation.extern:18
msgid "**ins** (list of :any:`tvm.tir.Buffer`) - Placeholder for each inputs"
msgstr ""

#: of tvm.te.operation.extern:19
msgid "**outs** (list of :any:`tvm.tir.Buffer`) - Placeholder for each outputs"
msgstr ""

#: of tvm.te.operation.extern:21
msgid "**Returns**"
msgstr ""

#: of tvm.te.operation.extern:23
msgid ""
"**stmt** (:any:`tvm.tir.Stmt`) - The statement that carries out array "
"computation."
msgstr ""

#: of tvm.te.operation.extern:26
msgid "The name hint of the tensor"
msgstr ""

#: of tvm.te.operation.extern:30
msgid "dtype: str or list of str, optional"
msgstr ""

#: of tvm.te.operation.extern:29
msgid "The data types of outputs, by default dtype will be same as inputs."
msgstr ""

#: of tvm.te.operation.extern:33
msgid "in_buffers: tvm.tir.Buffer or list of tvm.tir.Buffer, optional"
msgstr ""

#: of tvm.te.operation.extern:33
msgid "Input buffers."
msgstr ""

#: of tvm.te.operation.extern:37
msgid "out_buffers: tvm.tir.Buffer or list of tvm.tir.Buffer, optional"
msgstr ""

#: of tvm.te.operation.extern:36
msgid "Output buffers."
msgstr ""

#: of tvm.te.operation.extern:40
msgid "tag: str, optional"
msgstr ""

#: of tvm.te.operation.extern:40
msgid "Additonal tag information about the compute."
msgstr ""

#: of tvm.te.operation.extern:43
msgid "attrs: dict, optional"
msgstr ""

#: of tvm.te.operation.extern:43
msgid "The additional auxiliary attributes about the compute."
msgstr ""

#: of tvm.te.operation.extern:48
msgid "tensor: Tensor or list of Tensors"
msgstr ""

#: of tvm.te.operation.extern:48
msgid "The created tensor or tuple of tensors contains multiple outputs."
msgstr ""

#: of tvm.te.operation.extern:52
msgid ""
"In the code below, C is generated by calling external PackedFunc "
"`tvm.contrib.cblas.matmul`"
msgstr ""

#: of tvm.topi.math.fixed_point_multiply:9
msgid "multiplier"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nhwc:-1
#: tvm.topi.math.fixed_point_multiply:-1
#: tvm.topi.math.fixed_point_multiply_per_axis:-1
#: tvm.topi.nn.bitserial_util.bitpack:-1 tvm.topi.nn.conv1d.group_conv1d_ncw:-1
#: tvm.topi.nn.conv1d.group_conv1d_nwc:-1 tvm.topi.nn.conv2d.conv:-1
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:-1
#: tvm.topi.nn.conv2d.group_conv2d_nchw:-1
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:-1
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:-1
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:-1
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:-1
#: tvm.topi.nn.depth_to_space.depth_to_space:-1 tvm.topi.nn.elemwise.prelu:-1
#: tvm.topi.nn.fifo_buffer.fifo_buffer:-1 tvm.topi.nn.group_norm.group_norm:-1
#: tvm.topi.nn.local_response_norm.lrn:-1 tvm.topi.nn.loss.nll_loss:-1
#: tvm.topi.nn.softmax.fast_softmax:-1 tvm.topi.nn.softmax.softmax:-1
#: tvm.topi.nn.space_to_depth.space_to_depth:-1
#: tvm.topi.nn.utils.get_pad_tuple:-1 tvm.topi.nn.utils.get_pad_tuple1d:-1
#: tvm.topi.nn.utils.get_pad_tuple3d:-1
#: tvm.topi.nn.utils.get_pad_tuple_generic:-1 tvm.topi.signal.stft:-1
#: tvm.topi.transform.one_hot:-1 tvm.topi.transform.sliding_window:-1
#: tvm.topi.transform.take:-1 tvm.topi.utils.get_const_int:-1
msgid "int"
msgstr ""

#: of tvm.topi.math.fixed_point_multiply:10
#: tvm.topi.math.fixed_point_multiply_per_axis:9
msgid ""
"Multiplier of a fixed floating point number described as "
"multiplier*2^(-shift)."
msgstr ""

#: of tvm.topi.math.fixed_point_multiply:12
msgid "shift"
msgstr ""

#: of tvm.topi.math.fixed_point_multiply:12
msgid "Shift of a fixed floating point number described as multiplier*2^(-shift)."
msgstr ""

#: of tvm.topi.math.fixed_point_multiply_per_axis:10
msgid "lshift"
msgstr ""

#: of tvm.topi.math.fixed_point_multiply_per_axis:11
msgid ""
"Left shifts of a fixed floating point number described as "
"multiplier*2^(-shift)."
msgstr ""

#: of tvm.topi.math.fixed_point_multiply_per_axis:12
msgid "rshift"
msgstr ""

#: of tvm.topi.math.fixed_point_multiply_per_axis:13
msgid ""
"Right shifts of a fixed floating point number described as "
"multiplier*2^(-shift)."
msgstr ""

#: of tvm.topi.math.fixed_point_multiply_per_axis:14
msgid "is_lshift_required"
msgstr ""

#: of tvm.topi.math.fixed_point_multiply_per_axis:15
msgid "Whether we need to do left shift or not."
msgstr ""

#: of tvm.topi.math.fixed_point_multiply_per_axis:17
msgid "is_rshift_required"
msgstr ""

#: of tvm.topi.math.fixed_point_multiply_per_axis:17
msgid "Whether we need to do right shift or not."
msgstr ""

#: of tvm.topi.math.fixed_point_multiply_per_axis:21 tvm.topi.utils.make_idx:19
msgid "z"
msgstr ""

#: of tvm.topi.transform.flip:9
msgid "The axis along which the tensors will be reveresed."
msgstr ""

#: of tvm.tir.op.floordiv:6 tvm.tir.op.floormod:6
msgid "The left hand operand"
msgstr ""

#: of tvm.tir.op.floordiv:9 tvm.tir.op.floormod:9
msgid "The right hand operand"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:-1
#: tvm.topi.tensor.full:-1
msgid "tuple"
msgstr ""

#: of tvm.topi.tensor.full:6
msgid "Input tensor shape."
msgstr ""

#: of tvm.topi.tensor.full:8
msgid "Data type"
msgstr ""

#: of tvm.topi.tensor.full:10 tvm.topi.tensor.full_like:9
msgid "fill_value"
msgstr ""

#: of tvm.topi.nn.elemwise.leaky_relu:-1 tvm.topi.nn.group_norm.group_norm:-1
#: tvm.topi.nn.instance_norm.instance_norm:-1
#: tvm.topi.nn.layer_norm.layer_norm:-1 tvm.topi.nn.local_response_norm.lrn:-1
#: tvm.topi.nn.rms_norm.rms_norm:-1 tvm.topi.nn.upsampling.upsampling:-1
#: tvm.topi.nn.upsampling.upsampling3d:-1 tvm.topi.tensor.full:-1
#: tvm.topi.tensor.full_like:-1
msgid "float"
msgstr ""

#: of tvm.topi.tensor.full:10 tvm.topi.tensor.full_like:9
msgid "Value to be filled"
msgstr ""

#: of tvm.topi.tensor.full_like:2
msgid "then fill tensor with fill_value."
msgstr ""

#: of tvm.topi.transform.gather:3
msgid "E.g. for a 3D tensor, output is computed as:"
msgstr ""

#: of tvm.topi.transform.gather:11
msgid ""
"``indices`` must have same shape as ``data``, except at dimension "
"``axis`` which must just be not null. Output will have same shape as "
"``indices``."
msgstr ""

#: of tvm.topi.transform.gather:20
msgid "axis: int"
msgstr ""

#: of tvm.topi.transform.gather:20
msgid "The axis along which to index."
msgstr ""

#: of tvm.topi.scatter.scatter_nd:25
#: tvm.topi.scatter_elements.scatter_elements:21 tvm.topi.transform.gather:23
#: tvm.topi.transform.gather_nd:9 tvm.topi.transform.take:9
msgid "The indices of the values to extract."
msgstr ""

#: of tvm.topi.scatter.scatter_nd:22
#: tvm.topi.scatter_elements.scatter_elements:18 tvm.topi.transform.gather_nd:6
#: tvm.topi.transform.layout_transform:6 tvm.topi.transform.take:6
msgid "The source array."
msgstr ""

#: of tvm.topi.utils.get_const_tuple:6
msgid "in_tuple"
msgstr ""

#: of tvm.topi.utils.get_const_tuple:6 tvm.topi.utils.simplify:6
msgid "The input."
msgstr ""

#: of tvm.topi.utils.get_const_tuple:10
msgid "out_tuple"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:-1 tvm.topi.nn.utils.get_pad_tuple:-1
#: tvm.topi.nn.utils.get_pad_tuple1d:-1 tvm.topi.nn.utils.get_pad_tuple3d:-1
#: tvm.topi.nn.utils.get_pad_tuple_generic:-1 tvm.topi.utils.get_const_tuple:-1
msgid "tuple of int"
msgstr ""

#: of tvm.topi.utils.get_const_int:11 tvm.topi.utils.get_const_tuple:11
msgid "The output."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_1d:6
msgid "1-D tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_2d:6
msgid "2-D tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_3d:6
msgid "3-D tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_4d:6
msgid "4-D tensor with boolean values."
msgstr ""

#: of tvm.topi.argwhere.hybrid_argwhere_5d:6
msgid "5-D tensor with boolean values."
msgstr ""

#: of tvm.topi.transform.invert_permutation:6
msgid "Input data"
msgstr ""

#: of tvm.topi.transform.layout_transform:6 tvm.topi.transform.ndarray_size:6
#: tvm.topi.transform.shape:6
msgid "array"
msgstr ""

#: of tvm.topi.transform.layout_transform:9
msgid "src_layout"
msgstr ""

#: of tvm.topi.transform.layout_transform:9
msgid "the source layout."
msgstr ""

#: of tvm.topi.transform.layout_transform:12
msgid "dst_layout"
msgstr ""

#: of tvm.topi.transform.layout_transform:12
msgid "the destination layout."
msgstr ""

#: of tvm.topi.transform.layout_transform:14
msgid "schedule_rule"
msgstr ""

#: of tvm.topi.transform.layout_transform:15
msgid "the schedule rule to apply if any"
msgstr ""

#: of tvm.topi.utils.make_idx:4
msgid ""
"The returned value is only meaningful if within_index() returns True for "
"the same set of parameters."
msgstr ""

#: of tvm.topi.utils.make_idx:-1 tvm.topi.utils.within_index:-1
msgid "Expr"
msgstr ""

#: of tvm.topi.utils.make_idx:10 tvm.topi.utils.within_index:6
msgid "beginning of the index"
msgstr ""

#: of tvm.topi.utils.make_idx:13 tvm.topi.utils.within_index:9
msgid "e"
msgstr ""

#: of tvm.topi.utils.make_idx:13 tvm.topi.utils.within_index:9
msgid "end of the index"
msgstr ""

#: of tvm.topi.utils.make_idx:16 tvm.topi.utils.within_index:12
msgid "s"
msgstr ""

#: of tvm.topi.utils.make_idx:16 tvm.topi.utils.within_index:12
msgid "strides of index"
msgstr ""

#: of tvm.topi.utils.make_idx:19
msgid "size of the indexed dimension"
msgstr ""

#: of tvm.topi.utils.make_idx:22 tvm.topi.utils.within_index:15
msgid "i"
msgstr ""

#: of tvm.topi.utils.make_idx:22 tvm.topi.utils.within_index:15
msgid "array position"
msgstr ""

#: of tvm.topi.utils.make_idx:26
msgid "position: Expr"
msgstr ""

#: of tvm.topi.utils.make_idx:27
msgid "int expression that corresponds to an array position in the selection."
msgstr ""

#: of tvm.topi.transform.matmul:7
msgid ""
"a : The matrix A b : The matrix B trans_a : Is A's layout transposed? "
"trans_b : Is B's layout transposed?"
msgstr ""

#: of tvm.topi.transform.matmul:14
msgid "A Tensor whose op member is the matmul operation"
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:6
msgid "Input Tensor."
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:9
msgid "diagonal"
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:9
msgid "Values to be filled in the diagonal."
msgstr ""

#: of tvm.topi.sort.topk:9 tvm.topi.transform.matrix_set_diag:16
msgid "k"
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:-1
msgid "int or tuple of int, optional"
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:12
msgid ""
"Diagonal Offset(s). The diagonal or range of diagonals to set. (0 by "
"default) Positive value means superdiagonal, 0 refers to the main "
"diagonal, and negative value means subdiagonals. k can be a single "
"integer (for a single diagonal) or a pair of integers specifying the low "
"and high ends of a matrix band. k[0] must not be larger than k[1]."
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:24
msgid "align"
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:19
msgid ""
"Some diagonals are shorter than max_diag_len and need to be padded. align"
" is a string specifying how superdiagonals and subdiagonals should be "
"aligned, respectively. There are four possible alignments: \"RIGHT_LEFT\""
" (default), \"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". "
"\"RIGHT_LEFT\" aligns superdiagonals to the right (left-pads the row) and"
" subdiagonals to the left (right-pads the row). It is the packing format "
"LAPACK uses. cuSPARSE uses \"LEFT_RIGHT\", which is the opposite "
"alignment."
msgstr ""

#: of tvm.topi.transform.matrix_set_diag:29
msgid "New tensor with given diagonal values."
msgstr ""

#: of tvm.topi.reduction.max:9
msgid ""
"Axis or axes along which the max operation is performed. The default, "
"axis=None, will find the max element from all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.topi.transform.meshgrid:6
msgid "The coordinate vectors or scalars."
msgstr ""

#: of tvm.topi.transform.meshgrid:9
msgid "indexing"
msgstr ""

#: of tvm.topi.transform.meshgrid:9
msgid "Indexing mode, either \"ij\" or \"xy\"."
msgstr ""

#: of tvm.topi.transform.meshgrid:14
msgid "The resulting grids for each axis."
msgstr ""

#: of tvm.topi.reduction.min:9
msgid ""
"Axis or axes along which a minimum operation is performed. The default, "
"axis=None, will find the minimum element from all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.topi.transform.ndarray_size:6 tvm.topi.transform.shape:6
msgid "The source tensor."
msgstr ""

#: of tvm.topi.transform.one_hot:1
msgid ""
"Returns a one-hot tensor where the locations repsented by indices take "
"value on_value, other locations take value off_value. Final dimension is "
"<indices outer dimensions> x depth x <indices inner dimensions>."
msgstr ""

#: of tvm.topi.transform.one_hot:8
msgid "Locations to set to on_value."
msgstr ""

#: of tvm.topi.transform.one_hot:11
msgid "on_value"
msgstr ""

#: of tvm.topi.transform.one_hot:11
msgid "Value to fill at indices."
msgstr ""

#: of tvm.topi.transform.one_hot:14
msgid "off_value"
msgstr ""

#: of tvm.topi.transform.one_hot:14
msgid "Value to fill at all other positions besides indices."
msgstr ""

#: of tvm.topi.transform.one_hot:17
msgid "depth"
msgstr ""

#: of tvm.topi.transform.one_hot:17
msgid "Depth of the one-hot dimension."
msgstr ""

#: of tvm.topi.transform.one_hot:20
msgid "Axis to fill."
msgstr ""

#: of tvm.topi.transform.one_hot:-1
msgid "relay.DataType"
msgstr ""

#: of tvm.topi.transform.one_hot:23
msgid "Data type of the output tensor."
msgstr ""

#: of tvm.topi.transform.one_hot:28
msgid "The one-hot tensor."
msgstr ""

#: of tvm.topi.reduction.prod:9
msgid ""
"Axis or axes along which a prod operation is performed. The default, "
"axis=None, will get the prod element over all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.topi.transform.repeat:6
msgid "The tensor to be repeated."
msgstr ""

#: of tvm.topi.transform.repeat:9
msgid "repeats: int, required"
msgstr ""

#: of tvm.topi.transform.repeat:9
msgid "Number of repetitions for each element"
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:24
#: tvm.topi.nn.qnn.simulated_quantize:24 tvm.topi.transform.repeat:12
msgid "axis: int, optional"
msgstr ""

#: of tvm.topi.transform.repeat:12
msgid "The axis along which to repeat values"
msgstr ""

#: of tvm.topi.transform.reshape:6
msgid "The tensor to be reshaped"
msgstr ""

#: of tvm.topi.transform.reshape:8
msgid "newshape"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:-1
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:-1
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:-1
#: tvm.topi.transform.reshape:-1
msgid "tuple of ints"
msgstr ""

#: of tvm.topi.transform.reshape:8
msgid "The new shape"
msgstr ""

#: of tvm.topi.transform.reverse_sequence:1
msgid ""
"Reverse the tensor for variable length slices. Input is first sliced "
"along batch axis and then elements are reversed along seq axis."
msgstr ""

#: of tvm.topi.transform.reverse_sequence:7
msgid "The tensor to be reversed."
msgstr ""

#: of tvm.topi.transform.reverse_sequence:13
msgid "seq_lengths"
msgstr ""

#: of tvm.topi.transform.reverse_sequence:10
msgid ""
"A 1D Tensor with length a.dims[batch_axis] Must be one of the following "
"types: int32, int64 if seq_lengths[i] > a.dims[seq_axis], it is rounded "
"to a.dims[seq_axis] if seq_lengths[i] < 1, it is rounded to 1"
msgstr ""

#: of tvm.topi.transform.reverse_sequence:16
msgid "seq_axis"
msgstr ""

#: of tvm.topi.transform.reverse_sequence:16
msgid "The axis along which the elements will be reversed. Default is 1."
msgstr ""

#: of tvm.topi.transform.reverse_sequence:19
msgid "batch_axis"
msgstr ""

#: of tvm.topi.transform.reverse_sequence:19
msgid "The axis along which the tensor will be sliced. Default is 0."
msgstr ""

#: of tvm.topi.transform.reverse_sequence:24
msgid "The computed result of same shape and type as of input."
msgstr ""

#: of tvm.topi.scan.scanop:3
msgid "See cumprod and cumsum for an example of use."
msgstr ""

#: of tvm.topi.scan.scanop:5
msgid ""
"E.g. if * is your binary operator and the input tensor is [1, 2, 3, 4] "
"the output may be [1, 1 * 2, 1 * 2 * 3, 1 * 2 * 3 * 4]"
msgstr ""

#: of tvm.topi.scan.scanop:15
msgid "binop: Callable (tvm.Expr, tvm.Expr) -> tvm.Expr"
msgstr ""

#: of tvm.topi.scan.scanop:14
msgid ""
"A binary operator which should be associative and commutative. E.g. if * "
"is your operator then a * (b * c) = (a * b) * c and a * b = b * a"
msgstr ""

#: of tvm.topi.scan.scanop:20
msgid "identity_value: tvm.Expr"
msgstr ""

#: of tvm.topi.scan.scanop:18
msgid ""
"A value for the binary operation which provides the identity property. "
"E.g. if * is your operator and i is the identity_value then a * i = a for"
" all a in the domain of your operation."
msgstr ""

#: of tvm.topi.scan.scanop:23
msgid ""
"Axis along which the operation is computed. The default (None) is to "
"compute the cumulative operation over the flattened array."
msgstr ""

#: of tvm.topi.scan.scanop:27
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are computed. If dtype is not specified, it defaults to the dtype of "
"data."
msgstr ""

#: of tvm.topi.scan.scanop:31
msgid ""
"If True will return exclusive cumulative operation in which the first "
"element is not included. In other terms, if True, the j-th output element"
" would be the cumulative operation of the first (j-1) elements. "
"Otherwise, it would be the cumulative operation of the first j elements. "
"The cumulative operation of zero elements is assumed to be the "
"identity_value."
msgstr ""

#: of tvm.topi.scatter_elements.scatter_elements:3
msgid ""
"Data, indices, updates and output have the same shape. Indices can not "
"have duplicates (if idx1 != idx2, then indices[idx1] != indices[idx2]) if"
" reduction == \"update\"."
msgstr ""

#: of tvm.topi.scatter_elements.scatter_elements:12
msgid ""
"where the update function f is determined by the reduction. Five types of"
" the function are supported: \"update\", \"add\", \"mul\", \"min\" and "
"\"max\" (see below)"
msgstr ""

#: of tvm.topi.scatter.scatter_nd:28
#: tvm.topi.scatter_elements.scatter_elements:24
msgid "updates"
msgstr ""

#: of tvm.topi.scatter.scatter_nd:28
#: tvm.topi.scatter_elements.scatter_elements:24
msgid "The updates to apply at the Indices"
msgstr ""

#: of tvm.topi.scatter_elements.scatter_elements:-1
msgid "optional, int"
msgstr ""

#: of tvm.topi.scatter_elements.scatter_elements:27
msgid "The axis to scatter on. It is zero by default."
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:26
#: tvm.topi.scatter_elements.scatter_elements:37
msgid "reduction"
msgstr ""

#: of tvm.topi.scatter_elements.scatter_elements:-1
msgid "optional, string"
msgstr ""

#: of tvm.topi.scatter_elements.scatter_elements:30
msgid ""
"The update mode for the algorithm, either \"update\", \"add\", \"mul\", "
"\"min\" or \"max\" If update, the update values will replace the input "
"data If add, the update values will be added to the input data If mul, "
"the input data will be multiplied on the update values If mean, the input"
" data will be mean between the update values and the input data If min, "
"there is choice of minimal between the update values and the input data "
"If max, there is choice of maximal between the update values and the "
"input data It is \"update\" by default"
msgstr ""

#: of tvm.topi.scatter.scatter_nd:3
msgid ""
"Given updates with shape (Y_0, ..., Y_{K-1}, X_M, ..., X_{N-1}), indices "
"with shape (M, Y_0, ..., Y_{K-1}), and output copied from data with shape"
" (X_0, X_1, ..., X_{N-1}), scatter_nd computes"
msgstr ""

#: of tvm.topi.scatter.scatter_nd:17
msgid "where the update function f is determinted by the mode."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:17
#: tvm.topi.scatter.scatter_nd:33 tvm.topi.transform.take:22
msgid "mode"
msgstr ""

#: of tvm.topi.scatter.scatter_nd:31
msgid ""
"The update mode for the algorithm, either \"update\" or \"add\" If "
"update, the update values will replace the input data If add, the update "
"values will be added to the input data"
msgstr ""

#: of tvm.topi.searchsorted.searchsorted:2
msgid ""
"If `sorted_sequence` is N-dimensional, the innermost dimension of "
"`values` are searched in the corresponding dimension of "
"`sorted_sequence`."
msgstr ""

#: of tvm.topi.searchsorted.searchsorted:9
msgid "sorted_sequence"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:-1 tvm.topi.searchsorted.searchsorted:-1
msgid "te.Tensor"
msgstr ""

#: of tvm.topi.searchsorted.searchsorted:8
msgid ""
"N-D or 1-D Tensor, containing monotonically increasing sequence on the "
"innermost dimension."
msgstr ""

#: of tvm.topi.searchsorted.searchsorted:14
msgid "values"
msgstr ""

#: of tvm.topi.searchsorted.searchsorted:12
msgid ""
"N-D Tensor containing the search values. When `sorted_sequence` is 1-D, "
"the shape of `values` can be arbitrary. Otherwise, ranks of "
"`sorted_sequence` and `values` must be the same, and outer N-1 axes must "
"have the same size."
msgstr ""

#: of tvm.topi.searchsorted.searchsorted:20
msgid "right"
msgstr ""

#: of tvm.topi.searchsorted.searchsorted:17
msgid ""
"Controls which index is returned if a value lands exactly on one of "
"sorted values. If False, the index of the first suitable location found "
"is given. If true, return the last such index. If there is no suitable "
"index, return either 0 or N (where N is the size of the innermost "
"dimension)."
msgstr ""

#: of tvm.topi.searchsorted.searchsorted:23
msgid "The data type of the output indices."
msgstr ""

#: of tvm.topi.searchsorted.searchsorted:28
msgid ""
"Tensor with same shape as values, representing the indices of elements of"
" `values` if they are inserted in `sorted_sequence`."
msgstr ""

#: of tvm.topi.transform.sequence_mask:3
msgid ""
"This function takes an n-dimensional input array of the form [MAX_LENGTH,"
" batch_size, ...] or [batch_size, MAX_LENGTH, ...] and returns an array "
"of the same shape."
msgstr ""

#: of tvm.topi.transform.sequence_mask:6
msgid ""
"`axis` means the axis of the length dimension and can only be 0 or 1. If "
"`axis` is 0, the data must have shape [MAX_LENGTH, batch_size, ...]. "
"Otherwise (axis=1), the data must have shape [batch_size, MAX_LENGTH, "
"...]."
msgstr ""

#: of tvm.topi.transform.sequence_mask:10
msgid ""
"`valid_length` gives the length of each sequence. `valid_length` should "
"be a 1D int array with positive ints and has dimension [batch_size,]."
msgstr ""

#: of tvm.topi.transform.sequence_mask:16 tvm.topi.transform.sequence_mask:31
msgid ""
"N-D with shape [MAX_LENGTH, batch_size, ...] or [batch_size, MAX_LENGTH, "
"...] depending on the value of `axis`."
msgstr ""

#: of tvm.topi.transform.sequence_mask:20
msgid "valid_length"
msgstr ""

#: of tvm.topi.transform.sequence_mask:20
msgid "1-D with shape [batch_size,]"
msgstr ""

#: of tvm.topi.transform.sequence_mask:23
msgid "mask_value"
msgstr ""

#: of tvm.topi.nn.pad.pad:-1 tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:-1
#: tvm.topi.transform.sequence_mask:-1
msgid "float, optional"
msgstr ""

#: of tvm.topi.transform.sequence_mask:23
msgid "The masking value, default 0"
msgstr ""

#: of tvm.topi.transform.sequence_mask:26
msgid "axis of the length dimension, must be 0 or 1, default 0"
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:36
#: tvm.topi.image.resize.resize1d:59 tvm.topi.image.resize.resize2d:53
#: tvm.topi.image.resize.resize3d:54 tvm.topi.nn.batch_matmul.batch_matmul:35
#: tvm.topi.nn.batch_norm.batch_norm:48
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:34
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:34
#: tvm.topi.nn.bitserial_dense.bitserial_dense:12
#: tvm.topi.nn.bnn.binarize_pack:17 tvm.topi.nn.bnn.binary_dense:13
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:26
#: tvm.topi.nn.conv2d.conv2d:31 tvm.topi.nn.conv2d.conv2d_NCHWc:35
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:38
#: tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:14
#: tvm.topi.nn.conv2d.conv2d_hwcn:24 tvm.topi.nn.conv2d.conv2d_nhwc:33
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:27
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:25
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:27
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:25
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:12
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:12
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:12
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:34
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:34
#: tvm.topi.nn.dense.dense:27 tvm.topi.nn.dense.dense_pack:19
#: tvm.topi.nn.dense.matmul:31 tvm.topi.nn.depth_to_space.depth_to_space:21
#: tvm.topi.nn.flatten.flatten:10 tvm.topi.nn.local_response_norm.lrn:31
#: tvm.topi.nn.loss.nll_loss:34 tvm.topi.nn.pooling.adaptive_pool:33
#: tvm.topi.nn.pooling.global_pool:29 tvm.topi.nn.pooling.pool1d:46
#: tvm.topi.nn.pooling.pool2d:46 tvm.topi.nn.pooling.pool3d:46
#: tvm.topi.nn.pooling.pool_grad:46 tvm.topi.nn.softmax.fast_softmax:14
#: tvm.topi.nn.softmax.log_softmax:10 tvm.topi.nn.softmax.softmax:13
#: tvm.topi.nn.space_to_depth.space_to_depth:16
#: tvm.topi.nn.sparse.sparse_add:19 tvm.topi.nn.sparse.sparse_conv2d:28
#: tvm.topi.nn.sparse.sparse_dense:29 tvm.topi.nn.sparse.sparse_dense_sp_lhs:23
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:23
#: tvm.topi.nn.upsampling.upsampling:30 tvm.topi.nn.upsampling.upsampling3d:39
#: tvm.topi.signal.stft:21 tvm.topi.sparse.csrmm.csrmm:17
#: tvm.topi.sparse.csrmv.csrmv:17 tvm.topi.sparse.dense.dense:17
#: tvm.topi.transform.sequence_mask:31
msgid "output"
msgstr ""

#: of tvm.topi.transform.sliding_window:9
msgid ""
"What axis the window begins sliding over. Window will be slid over this "
"axis and all following axes. The axis value determines the window shape "
"(and thus, the number of strides): window shape and strides must both be "
"of length `data.ndim-axis`."
msgstr ""

#: of tvm.topi.transform.sliding_window:16
msgid "window_shape"
msgstr ""

#: of tvm.topi.transform.sliding_window:-1
msgid "List[int]"
msgstr ""

#: of tvm.topi.transform.sliding_window:15
msgid ""
"The window shape to form over the input. Window shape must be of length "
"`data.ndim-axis`."
msgstr ""

#: of tvm.topi.transform.sliding_window:19
msgid ""
"How to stride the window along each dimension. Strides must be of length "
"`data.ndim-axis`."
msgstr ""

#: of tvm.topi.nn.sparse.sparse_add:12 tvm.topi.nn.sparse.sparse_conv2d:17
#: tvm.topi.nn.sparse.sparse_dense:18 tvm.topi.nn.sparse.sparse_transpose:11
#: tvm.topi.sparse_reshape.sparse_reshape:7
#: tvm.topi.transform.sparse_to_dense:9
msgid "sparse_indices"
msgstr ""

#: of tvm.topi.sparse_reshape.sparse_reshape:6
msgid ""
"A 2-D tensor[N, n_dim] of integers containing location of sparse values, "
"where N is the number of sparse values and n_dim is the number of "
"dimensions of the dense_shape"
msgstr ""

#: of tvm.topi.sparse_reshape.sparse_reshape:10
msgid "prev_shape"
msgstr ""

#: of tvm.topi.sparse_reshape.sparse_reshape:10
msgid "A 1-D tensor containing the previous shape of the dense tensor"
msgstr ""

#: of tvm.topi.sparse_reshape.sparse_reshape:13
msgid "new_shape"
msgstr ""

#: of tvm.topi.sparse_reshape.sparse_reshape:13
msgid "A 1-D tensor containing the new shape of the dense tensor"
msgstr ""

#: of tvm.topi.sparse_reshape.sparse_reshape:18
msgid "result: relay.Expr"
msgstr ""

#: of tvm.topi.sparse_reshape.sparse_reshape:18
msgid "Output tensor."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:3
msgid ""
"Example:: -   sparse_to_dense([[0, 0], [1, 1]], [2, 2], [3, 3], 0) = [[3,"
" 0], [0, 3]]"
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:9
msgid ""
"A 0-D, 1-D, or 2-D tensor of integers containing location of sparse "
"values."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:12
msgid "output_shape"
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:-1
msgid "A list of integers"
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:12
msgid "Shape of the dense output tensor."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:15
msgid "sparse_values"
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:15
msgid "A 0-D or 1-D tensor containing the sparse values for the sparse indices."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:19
msgid "default_value"
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:18
msgid ""
"A 0-D tensor containing the default value for the remaining locations. "
"Defaults to 0."
msgstr ""

#: of tvm.topi.transform.sparse_to_dense:24
msgid "Dense tensor of shape output_shape. Has the same type as sparse_values."
msgstr ""

#: of tvm.topi.transform.split:5
msgid "ary : tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.split:7
msgid "indices_or_sections : int or 1-D array"
msgstr ""

#: of tvm.topi.transform.split:9
msgid "axis : int"
msgstr ""

#: of tvm.topi.transform.split:13
msgid "ret : tuple of tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.squeeze:5
msgid "a : tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.squeeze:-1
msgid "None or int or tuple of ints, optional"
msgstr ""

#: of tvm.topi.transform.squeeze:8
msgid ""
"Selects a subset of the single-dimensional entries in the shape. If an "
"axis is selected with shape entry greater than one, an error is raised."
msgstr ""

#: of tvm.topi.transform.squeeze:13
msgid "squeezed : tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.stack:6
msgid "The tensor to be stacked."
msgstr ""

#: of tvm.topi.transform.stack:9
msgid "The axis in the result array along which the input arrays are stacked."
msgstr ""

#: of tvm.topi.signal.stft:1
msgid ""
"The STFT computes the Fourier transform of short overlapping windows of "
"the input. This gives frequency components of the signal as they change "
"over time. Parameters ---------- data : relay.Expr"
msgstr ""

#: of tvm.topi.signal.stft:6
msgid "Either a 1-D tensor or a 2-D batch tensor."
msgstr ""

#: of tvm.topi.signal.stft:7
msgid "n_fft"
msgstr ""

#: of tvm.topi.signal.stft:8
msgid "The size of Fourier transform"
msgstr ""

#: of tvm.topi.signal.stft:9
msgid "hop_length"
msgstr ""

#: of tvm.topi.signal.stft:10
msgid "The distance between neighboring sliding window frames"
msgstr ""

#: of tvm.topi.signal.stft:11
msgid "win_length"
msgstr ""

#: of tvm.topi.signal.stft:12
msgid "The size of window frame and STFT filter"
msgstr ""

#: of tvm.topi.signal.stft:13
msgid "window"
msgstr ""

#: of tvm.topi.signal.stft:14
msgid "A 1-D tensor window frame"
msgstr ""

#: of tvm.topi.signal.stft:15
msgid "normalized"
msgstr ""

#: of tvm.topi.signal.stft:16
msgid "Whether to return the normalized STFT results"
msgstr ""

#: of tvm.topi.signal.stft:17
msgid "onesided"
msgstr ""

#: of tvm.topi.signal.stft:18
msgid "Whether to return onesided result or fill with conjugate symmetry"
msgstr ""

#: of tvm.topi.signal.stft:22
msgid "Tensor containing the STFT result"
msgstr ""

#: of tvm.topi.transform.strided_set:9
msgid "v"
msgstr ""

#: of tvm.topi.transform.strided_set:9
msgid "The values to set"
msgstr ""

#: of tvm.topi.transform.strided_set:12
msgid "begin: tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.strided_set:15
msgid "end: tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.strided_set:20
msgid "strides: tvm.te.Tensor, optional"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:-1
#: tvm.topi.nn.instance_norm.instance_norm:-1
#: tvm.topi.nn.layer_norm.layer_norm:-1 tvm.topi.nn.rms_norm.rms_norm:-1
#: tvm.topi.transform.strided_slice:-1
msgid "list of int"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:23 tvm.topi.transform.strided_slice:21
msgid "axes"
msgstr ""

#: of tvm.topi.transform.strided_slice:20
msgid ""
"Axes along which slicing is applied. When it is specified, begin, end "
"strides, and axes need to a list of integers of the same length."
msgstr ""

#: of tvm.topi.transform.strided_slice:28
msgid "slice_mode"
msgstr ""

#: of tvm.topi.transform.strided_slice:24
msgid ""
"The slice mode [end, size]. end - The ending indices for the slice "
"[default]. size - The input strides will be ignored, input end in this "
"mode indicates the sizeof a slice starting at the location specified by "
"begin. If end[i] is -1, all remaining elements in that dimension are "
"included in the slice."
msgstr ""

#: of tvm.topi.reduction.sum:9
msgid ""
"Axis or axes along which a sum is performed. The default, axis=None, will"
" sum all of the elements of the input array. If axis is negative it "
"counts from the last to the first axis."
msgstr ""

#: of tvm.topi.transform.take:12
msgid ""
"The axis over which to select values. By default, the flattened input "
"array is used."
msgstr ""

#: of tvm.topi.transform.take:16
msgid "batch_dims"
msgstr ""

#: of tvm.topi.transform.take:16
msgid "The number of batch dimensions. By default is 0."
msgstr ""

#: of tvm.topi.transform.take:19
msgid ""
"Specifies how out-of-bound indices will behave. clip - clip to the range "
"(default) wrap - wrap around the indices fast - no clip or wrap around "
"(user must make sure indices are in-bound)"
msgstr ""

#: of tvm.topi.transform.take_legalize:6
msgid "Attributes of current op"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul_legalize:15
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:15
#: tvm.topi.nn.conv2d.conv2d_legalize:15
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:15
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:15
#: tvm.topi.nn.dense.dense_legalize:15 tvm.topi.nn.dense.matmul_legalize:15
#: tvm.topi.transform.take_legalize:14
msgid "The legalized expr"
msgstr ""

#: of tvm.topi.transform.tensordot:5
msgid ""
"a : The tensor A b : The tensor B axes : The number of dimensions to "
"reduce over"
msgstr ""

#: of tvm.topi.transform.tensordot:11
msgid "A Tensor computing the result"
msgstr ""

#: of tvm.topi.transform.tile:6
msgid "The tensor to be tiled."
msgstr ""

#: of tvm.topi.transform.tile:9
msgid "reps: tuple of ints, required"
msgstr ""

#: of tvm.topi.transform.tile:9
msgid "The number of times for repeating the tensor"
msgstr ""

#: of tvm.topi.sort.topk:-1
msgid "int or tvm.te.Tensor, optional"
msgstr ""

#: of tvm.topi.sort.topk:9
msgid "Number of top elements to select. Return all elements if k < 1."
msgstr ""

#: of tvm.topi.sort.topk:12
msgid "Axis long which to sort the input tensor."
msgstr ""

#: of tvm.topi.sort.topk:18
msgid "ret_type: str, optional"
msgstr ""

#: of tvm.topi.sort.topk:15
msgid ""
"The return type [both, values, indices]. \"both\": return both top k data"
" and indices. \"values\": return top k data only. \"indices\": return top"
" k indices only."
msgstr ""

#: of tvm.topi.sort.topk:24
msgid "The data type of the indices output."
msgstr ""

#: of tvm.topi.sort.topk:-1
msgid "tvm.te.Tensor or List[tvm.te.Tensor]"
msgstr ""

#: of tvm.topi.sort.topk:29
msgid "The computed result."
msgstr ""

#: of tvm.topi.transform.transpose:9
msgid "axes: tuple of ints, optional"
msgstr ""

#: of tvm.topi.transform.transpose:9
msgid "By default, reverse the dimensions."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:9
#: tvm.topi.nn.qnn.simulated_quantize:9 tvm.topi.transform.trilu:8
msgid "data: tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.trilu:7
msgid ""
"The tensor that trilu will be applied to. Must be either a 2D matrix or a"
" tensor of batches of 2D matrices."
msgstr ""

#: of tvm.topi.transform.trilu:12
msgid "k: tvm.te.Tensor"
msgstr ""

#: of tvm.topi.transform.trilu:11
msgid ""
"The number of diagonals above or below the main diagonal to exclude or "
"include."
msgstr ""

#: of tvm.topi.transform.trilu:17
msgid "upper: bool"
msgstr ""

#: of tvm.topi.transform.trilu:15
msgid ""
"If True, only upper triangular values of input are kept, if False, the "
"lower triangular values are kept."
msgstr ""

#: of tvm.topi.transform.trilu:22
msgid "The new tensor with appropriate diagonals set to zero."
msgstr ""

#: of tvm.topi.unique.unique:1
msgid ""
"Find the unique elements of a 1-D tensor. Please note `output` and "
"`counts` are all padded to have the same length of `data` and element "
"with index >= num_unique[0] has undefined value."
msgstr ""

#: of tvm.topi.unique.unique:7
msgid "A 1-D tensor of integers."
msgstr ""

#: of tvm.topi.unique.unique:10
msgid "sorted"
msgstr ""

#: of tvm.topi.unique.unique:10
msgid ""
"Whether to sort the unique elements in ascending order before returning "
"as output."
msgstr ""

#: of tvm.topi.unique.unique:13
msgid "return_counts"
msgstr ""

#: of tvm.topi.unique.unique:13
msgid "Whether to return the count of each unique element."
msgstr ""

#: of tvm.topi.unique.unique:20
msgid "unique"
msgstr ""

#: of tvm.topi.unique.unique:18
msgid ""
"A 1-D tensor containing the unique elements of the input data tensor. The"
" same size as the input data. If there are less unique elements than "
"input data, the end of the tensor is padded with zeros."
msgstr ""

#: of tvm.topi.unique.unique:23
msgid ""
"A 1-D tensor. The same size as output. For each entry in output, it "
"contains the index of its first occurence in the input data. The end of "
"the tensor is padded with the length of the input data."
msgstr ""

#: of tvm.topi.unique.unique:30
msgid "inverse_indices"
msgstr ""

#: of tvm.topi.unique.unique:28
msgid ""
"A 1-D tensor. For each entry in data, it contains the index of that data "
"element in the unique array. (Note that inverse_indices is very similar "
"to indices if output is not sorted.)"
msgstr ""

#: of tvm.topi.unique.unique:33
msgid "num_unique"
msgstr ""

#: of tvm.topi.unique.unique:33
msgid ""
"A 1-D tensor with size=1 containing the number of unique elements in the "
"input data tensor."
msgstr ""

#: of tvm.topi.unique.unique:36
msgid "counts (optional)"
msgstr ""

#: of tvm.topi.unique.unique:36
msgid "A 1-D tensor containing the count of each unique element in the output."
msgstr ""

#: of tvm.topi.transform.unravel_index:3
msgid "Example:: -   unravel_index([22, 41, 37], [7, 6]) = [[3, 6, 6], [4, 5, 1]]"
msgstr ""

#: of tvm.topi.transform.unravel_index:9
msgid "An integer array containing indices."
msgstr ""

#: of tvm.topi.transform.unravel_index:12
msgid "The shape of the array."
msgstr ""

#: of tvm.topi.transform.unravel_index:17
msgid "The tuple of coordinate arrays."
msgstr ""

#: of tvm.topi.transform.where:6
msgid "The condition array."
msgstr ""

#: of tvm.topi.transform.where:9
msgid "First array to be selected."
msgstr ""

#: of tvm.topi.transform.where:12
msgid "Second array to be selected."
msgstr ""

#: of tvm.topi.transform.where:17
msgid "A Tensor selected from x or y depending on condition."
msgstr ""

#: of tvm.topi.utils.within_index:20
msgid "selected: Expr"
msgstr ""

#: of tvm.topi.utils.within_index:20
msgid ""
"bool expression that is True is the array position would be selected by "
"the index and False otherwise"
msgstr ""

#: ../../notebook/docs/reference/api/python/topi.rst:27
msgid "tvm.topi.nn"
msgstr ""

#: of tvm.topi.nn:1
msgid "Neural network operators"
msgstr ""

#: of tvm.topi.nn:1:<autosummary>:1
msgid ""
":py:obj:`Workload <tvm.topi.nn.Workload>`\\ \\(in\\_dtype\\, "
"out\\_dtype\\, height\\, width\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_pool <tvm.topi.nn.adaptive_pool>`\\ \\(data\\, "
"output\\_size\\, pool\\_type\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.adaptive_pool:9 tvm.topi.nn.pooling.pool2d:7
msgid "Perform pooling on height and width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_pool3d <tvm.topi.nn.adaptive_pool3d>`\\ \\(data\\, "
"output\\_size\\, pool\\_type\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Perform pooling on three dimensional data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`add_alter_layout <tvm.topi.nn.add_alter_layout>`\\ "
"\\(\\_attrs\\, \\_inputs\\, \\_tinfos\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.qnn.add_alter_layout:1
msgid "Change add layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`batch_matmul <tvm.topi.nn.batch_matmul>`\\ \\(tensor\\_a\\, "
"tensor\\_b\\[\\, oshape\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Compute batch matrix multiplication of `tensor_a` and `tensor_b`."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`batch_matmul_legalize <tvm.topi.nn.batch_matmul_legalize>`\\ "
"\\(attrs\\, inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes batch_matmul op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`batch_norm <tvm.topi.nn.batch_norm>`\\ \\(data\\, gamma\\, "
"beta\\, moving\\_mean\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Batch normalization layer (Ioffe and Szegedy, 2014)."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`batch_to_space_nd <tvm.topi.nn.batch_to_space_nd>`\\ \\(data\\, "
"block\\_shape\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Perform space to batch transformation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`bias_add_legalize <tvm.topi.nn.bias_add_legalize>`\\ "
"\\(\\_attrs\\, \\_inputs\\, \\_tinfos\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.qnn.bias_add_legalize:1
msgid "Legalize bias_add layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`binarize_pack <tvm.topi.nn.binarize_pack>`\\ \\(data\\[\\, "
"axis\\, name\\]\\)"
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Binarization and bit-packing along a certain axis."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`binary_dense <tvm.topi.nn.binary_dense>`\\ \\(data\\, weight\\)"
msgstr ""

#: of tvm.topi.nn.bnn.binary_dense:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Binary matrix multiplication using xor and bit-count."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`bitpack <tvm.topi.nn.bitpack>`\\ \\(data\\, bits\\, "
"pack\\_axis\\, bit\\_axis\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.bitserial_util.bitpack:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Packs data into format necessary for bitserial computation"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`bitserial_conv2d_legalize "
"<tvm.topi.nn.bitserial_conv2d_legalize>`\\ \\(attrs\\, inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes Bitserial Conv2D op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`bitserial_conv2d_nchw <tvm.topi.nn.bitserial_conv2d_nchw>`\\ "
"\\(data\\, kernel\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Bitserial Conv2D operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`bitserial_conv2d_nhwc <tvm.topi.nn.bitserial_conv2d_nhwc>`\\ "
"\\(data\\, kernel\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`bitserial_dense <tvm.topi.nn.bitserial_dense>`\\ \\(data\\, "
"weight\\, data\\_bits\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.bitserial_dense.bitserial_dense:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "The default implementation of bitserial dense in topi."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`concatenate <tvm.topi.nn.concatenate>`\\ \\(a\\_tuple\\[\\, "
"axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv <tvm.topi.nn.conv>`\\ \\(inp\\, filt\\, stride\\, "
"padding\\, dilation\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Convolution operator in NCHW or NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv1d <tvm.topi.nn.conv1d>`\\ \\(data\\, kernel\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "1D convolution forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv1d_ncw <tvm.topi.nn.conv1d_ncw>`\\ \\(data\\, kernel\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "1D convolution in NCW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv1d_nwc <tvm.topi.nn.conv1d_nwc>`\\ \\(data\\, kernel\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "1D convolution in NWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv1d_transpose_ncw <tvm.topi.nn.conv1d_transpose_ncw>`\\ "
"\\(data\\, kernel\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Transposed 1D convolution ncw forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d <tvm.topi.nn.conv2d>`\\ \\(input\\, filter\\, strides\\, "
"padding\\, dilation\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_NCHWc <tvm.topi.nn.conv2d_NCHWc>`\\ \\(data\\, kernel\\, "
"stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:1 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D operator for nChw[x]c layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_NCHWc_int8 <tvm.topi.nn.conv2d_NCHWc_int8>`\\ \\(data\\, "
"kernel\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_alter_layout <tvm.topi.nn.conv2d_alter_layout>`\\ "
"\\(attrs\\, inputs\\, tinfos\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Change Conv2D layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_gemm_weight_transform "
"<tvm.topi.nn.conv2d_gemm_weight_transform>`\\ \\(kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:1
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:1
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Weight transformation for winograd"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_hwcn <tvm.topi.nn.conv2d_hwcn>`\\ \\(Input\\, Filter\\, "
"stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_hwcn:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Convolution operator in HWCN layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_infer_layout <tvm.topi.nn.conv2d_infer_layout>`\\ "
"\\(workload\\, cfg\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:1
msgid "Infer input/output shapes and layouts from a workload and cfg."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_legalize <tvm.topi.nn.conv2d_legalize>`\\ \\(attrs\\, "
"inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes Conv2D op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_nchw <tvm.topi.nn.conv2d_nchw>`\\ \\(Input\\, Filter\\, "
"stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Convolution operator in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_nhwc <tvm.topi.nn.conv2d_nhwc>`\\ \\(Input\\, Filter\\, "
"stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_nhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Convolution operator in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_transpose_legalize "
"<tvm.topi.nn.conv2d_transpose_legalize>`\\ \\(attrs\\, inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes Transposed 2D convolution op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_transpose_nchw <tvm.topi.nn.conv2d_transpose_nchw>`\\ "
"\\(Input\\, Filter\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Transposed 2D convolution nchw forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_transpose_nchw_preprocess "
"<tvm.topi.nn.conv2d_transpose_nchw_preprocess>`\\ \\(data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw_preprocess:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Preprocess data and kernel to make the compute pattern of "
"conv2d_transpose the same as conv2d"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_winograd_nchw <tvm.topi.nn.conv2d_winograd_nchw>`\\ "
"\\(data\\, weight\\, strides\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D Winograd in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_winograd_nchw_without_weight_transform "
"<tvm.topi.nn.conv2d_winograd_nchw_without_weight_transform>`\\ \\(...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D Winograd without layout transform in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_winograd_nhwc <tvm.topi.nn.conv2d_winograd_nhwc>`\\ "
"\\(data\\, weight\\, strides\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D Winograd in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_winograd_nhwc_without_weight_transform "
"<tvm.topi.nn.conv2d_winograd_nhwc_without_weight_transform>`\\ \\(...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv2D Winograd without layout transform in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_winograd_nnpack_weight_transform "
"<tvm.topi.nn.conv2d_winograd_nnpack_weight_transform>`\\ \\(...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_winograd_weight_transform "
"<tvm.topi.nn.conv2d_winograd_weight_transform>`\\ \\(kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv3d_alter_layout <tvm.topi.nn.conv3d_alter_layout>`\\ "
"\\(attrs\\, inputs\\, tinfos\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_alter_layout:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Change Conv3D layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv3d_ncdhw <tvm.topi.nn.conv3d_ncdhw>`\\ \\(Input\\, Filter\\,"
" stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Conv3D operator in NCDHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv3d_ndhwc <tvm.topi.nn.conv3d_ndhwc>`\\ \\(Input\\, Filter\\,"
" stride\\, padding\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Convolution operator in NDHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv3d_transpose_legalize "
"<tvm.topi.nn.conv3d_transpose_legalize>`\\ \\(attrs\\, inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes Transposed 3D convolution op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv3d_transpose_ncdhw <tvm.topi.nn.conv3d_transpose_ncdhw>`\\ "
"\\(Input\\, Filter\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Transposed 3D convolution ncdhw forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv3d_transpose_ncdhw_preprocess "
"<tvm.topi.nn.conv3d_transpose_ncdhw_preprocess>`\\ \\(data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw_preprocess:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Preprocess data and kernel to make the compute pattern of "
"conv3d_transpose the same as conv3d"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`conv3d_winograd_weight_transform "
"<tvm.topi.nn.conv3d_winograd_weight_transform>`\\ \\(kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Weight transformation for 3D winograd"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`correlation_nchw <tvm.topi.nn.correlation_nchw>`\\ \\(data1\\, "
"data2\\, kernel\\_size\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Correlation operator in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`declaration_conv2d_transpose_impl "
"<tvm.topi.nn.declaration_conv2d_transpose_impl>`\\ \\(data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.declaration_conv2d_transpose_impl:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Implementation of conv2d transpose"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`declaration_conv3d_transpose_impl "
"<tvm.topi.nn.declaration_conv3d_transpose_impl>`\\ \\(data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.declaration_conv3d_transpose_impl:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Implementation of conv3d transpose"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`deformable_conv2d_nchw <tvm.topi.nn.deformable_conv2d_nchw>`\\ "
"\\(data\\, offset\\, kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Deformable conv2D operator in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`deformable_conv2d_nhwc <tvm.topi.nn.deformable_conv2d_nhwc>`\\ "
"\\(data\\, offset\\, kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Deformable conv2D operator in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`dense <tvm.topi.nn.dense>`\\ \\(data\\, weight\\[\\, bias\\, "
"out\\_dtype\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "The default implementation of dense in topi."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`dense_alter_layout <tvm.topi.nn.dense_alter_layout>`\\ "
"\\(attrs\\, inputs\\, tinfos\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.dense.dense_alter_layout:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Change dense layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`dense_legalize <tvm.topi.nn.dense_legalize>`\\ \\(attrs\\, "
"inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.dense.dense_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes dense op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`dense_pack <tvm.topi.nn.dense_pack>`\\ \\(data\\, weight\\[\\, "
"bias\\, out\\_dtype\\]\\)"
msgstr ""

#: of tvm.topi.nn.dense.dense_pack:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "The default implementation of dense_pack in topi."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`depth_to_space <tvm.topi.nn.depth_to_space>`\\ \\(data\\, "
"block\\_size\\[\\, layout\\, mode\\]\\)"
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Perform depth to space transformation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`depthwise_conv2d_NCHWc <tvm.topi.nn.depthwise_conv2d_NCHWc>`\\ "
"\\(Input\\, Filter\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:1
msgid "Depthwise convolution NCHW[x]c forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`depthwise_conv2d_backward_input_nhwc "
"<tvm.topi.nn.depthwise_conv2d_backward_input_nhwc>`\\ \\(Filter\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:1
msgid "Depthwise convolution nhwc backward wrt input operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`depthwise_conv2d_backward_weight_nhwc "
"<tvm.topi.nn.depthwise_conv2d_backward_weight_nhwc>`\\ \\(Input\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:1
msgid "Depthwise convolution nhwc backward wrt weight operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`depthwise_conv2d_infer_layout "
"<tvm.topi.nn.depthwise_conv2d_infer_layout>`\\ \\(workload\\, cfg\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`depthwise_conv2d_nchw <tvm.topi.nn.depthwise_conv2d_nchw>`\\ "
"\\(Input\\, Filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:1
msgid "Depthwise convolution nchw forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`depthwise_conv2d_nhwc <tvm.topi.nn.depthwise_conv2d_nhwc>`\\ "
"\\(Input\\, Filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:1
msgid "Depthwise convolution nhwc forward operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`dilate <tvm.topi.nn.dilate>`\\ \\(data\\, strides\\[\\, "
"dilation\\_value\\, name\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.dilate.dilate:1
msgid "Dilate data with given dilation value (0 by default)."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`equal_const_int <tvm.topi.nn.equal_const_int>`\\ \\(expr\\, "
"value\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.utils.equal_const_int:1
msgid "Returns if expr equals value."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`fast_softmax <tvm.topi.nn.fast_softmax>`\\ \\(x\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.softmax.softmax:1
msgid "Perform softmax activation on the data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`fifo_buffer <tvm.topi.nn.fifo_buffer>`\\ \\(data\\, buffer\\, "
"axis\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.fifo_buffer.fifo_buffer:1
msgid "FIFO buffer to enable computation reuse in CNNs with sliding indow input"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`flatten <tvm.topi.nn.flatten>`\\ \\(data\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.flatten.flatten:1
msgid ""
"Flattens the input array into a 2-D array by collapsing the higher "
"dimensions."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`get_const_int <tvm.topi.nn.get_const_int>`\\ \\(expr\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.utils.get_const_int:1
msgid "Verifies expr is integer and get the constant value."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`get_const_tuple <tvm.topi.nn.get_const_tuple>`\\ \\(in\\_tuple\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`get_pad_tuple <tvm.topi.nn.get_pad_tuple>`\\ \\(padding\\, "
"kernel\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.utils.get_pad_tuple:1 tvm.topi.nn.utils.get_pad_tuple1d:1
#: tvm.topi.nn.utils.get_pad_tuple3d:1
#: tvm.topi.nn.utils.get_pad_tuple_generic:1
msgid "Common code to get the pad option"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`get_pad_tuple1d <tvm.topi.nn.get_pad_tuple1d>`\\ \\(padding\\, "
"kernel\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`get_pad_tuple3d <tvm.topi.nn.get_pad_tuple3d>`\\ \\(padding\\, "
"kernel\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`get_pad_tuple_generic <tvm.topi.nn.get_pad_tuple_generic>`\\ "
"\\(padding\\, kernel\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`global_pool <tvm.topi.nn.global_pool>`\\ \\(data\\, "
"pool\\_type\\[\\, layout\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.global_pool:7
msgid "Perform global pooling on height and width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`group_conv1d_ncw <tvm.topi.nn.group_conv1d_ncw>`\\ \\(data\\, "
"kernel\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_ncw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "1D convolution forward operator for NCW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`group_conv1d_nwc <tvm.topi.nn.group_conv1d_nwc>`\\ \\(data\\, "
"kernel\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_nwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "1D convolution forward operator for NWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`group_conv2d_nchw <tvm.topi.nn.group_conv2d_nchw>`\\ \\(Input\\,"
" Filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nchw:1
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Group convolution operator in NCHW layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`group_conv2d_nhwc <tvm.topi.nn.group_conv2d_nhwc>`\\ \\(Input\\,"
" Filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nhwc:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Group convolution operator in NHWC layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`group_conv2d_transpose_nchw "
"<tvm.topi.nn.group_conv2d_transpose_nchw>`\\ \\(data\\, kernel\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`group_norm <tvm.topi.nn.group_norm>`\\ \\(data\\, gamma\\, "
"beta\\, num\\_groups\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Group normalization operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`instance_norm <tvm.topi.nn.instance_norm>`\\ \\(data\\, gamma\\,"
" beta\\, axis\\[\\, epsilon\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.instance_norm.instance_norm:1
msgid "Instance normalization operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`layer_norm <tvm.topi.nn.layer_norm>`\\ \\(data\\, gamma\\, "
"beta\\, axis\\[\\, epsilon\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Layer normalization operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`layout_transform <tvm.topi.nn.layout_transform>`\\ \\(tensor\\, "
"current\\_layout\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.layout_transform:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Transform a tensor with the current layout to the desired layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`leaky_relu <tvm.topi.nn.leaky_relu>`\\ \\(x\\, alpha\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.elemwise.leaky_relu:1
msgid "Take leaky relu of input x."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`log_softmax <tvm.topi.nn.log_softmax>`\\ \\(x\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.softmax.log_softmax:1
msgid "Perform log softmax activation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`lrn <tvm.topi.nn.lrn>`\\ \\(data\\, size\\[\\, axis\\, alpha\\, "
"beta\\, bias\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.local_response_norm.lrn:1
msgid ""
"Perform the across channels local response normalisation on the input "
"data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`lstm <tvm.topi.nn.lstm>`\\ \\(Xs\\, Wi\\, Wh\\[\\, Bi\\, Bh\\, "
"h\\_init\\, c\\_init\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.lstm.lstm:1
msgid "General LSTM implemented using TE scan."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`matmul <tvm.topi.nn.matmul>`\\ \\(tensor\\_a\\, tensor\\_b\\[\\,"
" bias\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.dense.matmul:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "The default implementation of matmul in topi."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`matmul_legalize <tvm.topi.nn.matmul_legalize>`\\ \\(attrs\\, "
"inputs\\, types\\)"
msgstr ""

#: of tvm.topi.nn.dense.matmul_legalize:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Legalizes matmul op."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`mirror_pad <tvm.topi.nn.mirror_pad>`\\ \\(data\\, "
"pad\\_before\\[\\, pad\\_after\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pad.mirror_pad:1
msgid "Pad Input with mirroring either symmetric or reflected."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`namedtuple <tvm.topi.nn.namedtuple>`\\ \\(typename\\, "
"field\\_names\\, \\*\\[\\, ...\\]\\)"
msgstr ""

#: collections.namedtuple:1 of
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Returns a new subclass of tuple with named fields."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`nll_loss <tvm.topi.nn.nll_loss>`\\ \\(predictions\\, targets\\, "
"weights\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.loss.nll_loss:1
msgid "Negative log likelihood loss on the input data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`pad <tvm.topi.nn.pad>`\\ \\(data\\, pad\\_before\\[\\, "
"pad\\_after\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pad.pad:1
msgid "Pad Input with zeros."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`pool1d <tvm.topi.nn.pool1d>`\\ \\(data\\, kernel\\, stride\\, "
"dilation\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.pool1d:7
msgid "Perform pooling on width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`pool2d <tvm.topi.nn.pool2d>`\\ \\(data\\, kernel\\, stride\\, "
"dilation\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`pool3d <tvm.topi.nn.pool3d>`\\ \\(data\\, kernel\\, stride\\, "
"dilation\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.pool3d:7
msgid "Perform pooling on depth, height and width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`pool_grad <tvm.topi.nn.pool_grad>`\\ \\(grads\\, data\\, "
"kernel\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.pooling.pool_grad:7
msgid "Gradient of pooling on height and width dimension of data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`prelu <tvm.topi.nn.prelu>`\\ \\(x\\, slope\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "PReLU."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`qnn_conv2d_alter_layout <tvm.topi.nn.qnn_conv2d_alter_layout>`\\"
" \\(\\_attrs\\, \\_inputs\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:1
msgid "Change qnn.conv2d layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`qnn_dense_alter_layout <tvm.topi.nn.qnn_dense_alter_layout>`\\ "
"\\(\\_attrs\\, \\_inputs\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Change qnn.dense layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`qnn_requantize_alter_layout "
"<tvm.topi.nn.qnn_requantize_alter_layout>`\\ \\(\\_attrs\\, \\_inputs\\, "
"...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:1
msgid "Change requantize layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`reduce <tvm.topi.nn.reduce>`\\ \\(function\\, iterable\\[\\, "
"initial\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Apply a function of two arguments cumulatively to the items of a sequence"
" or iterable, from left to right, so as to reduce the iterable to a "
"single value."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`relu <tvm.topi.nn.relu>`\\ \\(x\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.elemwise.relu:1
msgid "Take relu of input x."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`rms_norm <tvm.topi.nn.rms_norm>`\\ \\(data\\, weight\\, "
"axis\\[\\, epsilon\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Root mean square normalization operator."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`scale_shift_nchw <tvm.topi.nn.scale_shift_nchw>`\\ \\(Input\\, "
"Scale\\, Shift\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.mapping.scale_shift_nchw:1
#: tvm.topi.nn.mapping.scale_shift_nchwc:1
#: tvm.topi.nn.mapping.scale_shift_nhwc:1
msgid "Batch normalization operator in inference."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`scale_shift_nchwc <tvm.topi.nn.scale_shift_nchwc>`\\ \\(Input\\,"
" Scale\\, Shift\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`scale_shift_nhwc <tvm.topi.nn.scale_shift_nhwc>`\\ \\(Input\\, "
"Scale\\, Shift\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`simplify <tvm.topi.nn.simplify>`\\ \\(expr\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.utils.simplify:1
msgid "Simplify the expression if it is Expr, directly return if it is int."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`simulated_dequantize <tvm.topi.nn.simulated_dequantize>`\\ "
"\\(data\\, in\\_dtype\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Simulated QNN dequantize operator that mimics QNN outputs without "
"changing datatype."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`simulated_quantize <tvm.topi.nn.simulated_quantize>`\\ "
"\\(data\\, out\\_dtype\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Simulated QNN quantize operator that mimics QNN outputs without changing "
"datatype."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ":py:obj:`softmax <tvm.topi.nn.softmax>`\\ \\(x\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`softmax_common <tvm.topi.nn.softmax_common>`\\ \\(x\\, axis\\, "
"use\\_fast\\_exp\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.softmax.softmax_common:1
msgid "The common part of softmax and fast_softmax"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`space_to_batch_nd <tvm.topi.nn.space_to_batch_nd>`\\ \\(data\\, "
"block\\_shape\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:1
msgid "Perform batch to space transformation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`space_to_depth <tvm.topi.nn.space_to_depth>`\\ \\(data\\, "
"block\\_size\\[\\, layout\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.space_to_depth.space_to_depth:1
msgid "Perform space to depth transformation on the data"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`sparse_add <tvm.topi.nn.sparse_add>`\\ \\(dense\\_data\\, "
"sparse\\_data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_add:1
msgid "Computes sparse-dense addition"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`sparse_conv2d <tvm.topi.nn.sparse_conv2d>`\\ \\(dense\\_data\\, "
"sparse\\_data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_conv2d:1
msgid ""
"Computes sparse-conv2d(1*1) of ``data`` and ``(weight_data, "
"weight_indices, weight_indptr)``"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`sparse_dense <tvm.topi.nn.sparse_dense>`\\ \\(dense\\_data\\, "
"sparse\\_data\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_dense:1
msgid ""
"Computes sparse-dense matrix multiplication of `data` and `(weight_data, "
"weight_indices, weight_indptr).T`, if sparse_lhs=False or Computes "
"sparse-dense matrix multiplication of `(data_data, data_indices, "
"data_indptr)` and `weight.T`, if sparse_lhs=True"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`sparse_dense_alter_layout "
"<tvm.topi.nn.sparse_dense_alter_layout>`\\ \\(\\_attrs\\, \\_inputs\\, "
"...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:1
msgid "Change Sparse Dense layout."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`sparse_dense_sp_lhs <tvm.topi.nn.sparse_dense_sp_lhs>`\\ "
"\\(data\\_data\\, data\\_indices\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_dense_sp_lhs:1
msgid ""
"Computes sparse-dense matrix multiplication of `(data_data, data_indices,"
" data_indptr)` and `weight.T`"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`sparse_dense_sp_rhs <tvm.topi.nn.sparse_dense_sp_rhs>`\\ "
"\\(data\\, weight\\_data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:1
msgid ""
"Computes sparse-dense matrix multiplication of `data` and `(weight_data, "
"weight_indices, weight_indptr).T`"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`sparse_transpose <tvm.topi.nn.sparse_transpose>`\\ "
"\\(sparse\\_data\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
"Transpose a square sparse matrix, `A` is an n-by-n sparse matrix in the "
"CSR format."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`strided_slice <tvm.topi.nn.strided_slice>`\\ \\(a\\, begin\\, "
"end\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`try_get_conv2d_sparse_input "
"<tvm.topi.nn.try_get_conv2d_sparse_input>`\\ \\(args\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.sparse.try_get_conv2d_sparse_input:1
#: tvm.topi.nn.sparse.try_get_sparse_input:1
msgid "Analyze the input data from the given args."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`try_get_sparse_input <tvm.topi.nn.try_get_sparse_input>`\\ "
"\\(args\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`unpack_NCHWc_to_nchw <tvm.topi.nn.unpack_NCHWc_to_nchw>`\\ "
"\\(packed\\_out\\, out\\_dtype\\)"
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:1
#: tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid "Unpack conv2d_NCHWc output from layout NCHWc to NCHW"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`upsampling <tvm.topi.nn.upsampling>`\\ \\(data\\, scale\\_h\\, "
"scale\\_w\\[\\, layout\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.upsampling.upsampling:2 tvm.topi.nn.upsampling.upsampling3d:2
msgid "Perform upsampling on the data."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`upsampling3d <tvm.topi.nn.upsampling3d>`\\ \\(data\\, "
"scale\\_d\\, scale\\_h\\, scale\\_w\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
msgid ""
":py:obj:`winograd_transform_matrices "
"<tvm.topi.nn.winograd_transform_matrices>`\\ \\(tile\\_size\\, ...\\)"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1:<autosummary>:1
#: tvm.topi.nn.winograd_util.winograd_transform_matrices:1
msgid ""
"Compute the A, B, and G transform matrices for `tile_size` as a "
"`tvm.Expr`."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.Workload:1
msgid "**Attributes:**"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`dilation_h <tvm.topi.nn.Workload.dilation_h>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1
#: tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid "Alias for field number 12"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`dilation_w <tvm.topi.nn.Workload.dilation_w>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.dilation_w:1
msgid "Alias for field number 13"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`height <tvm.topi.nn.Workload.height>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.height:1
msgid "Alias for field number 2"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`in_dtype <tvm.topi.nn.Workload.in_dtype>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.in_dtype:1
msgid "Alias for field number 0"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`in_filter <tvm.topi.nn.Workload.in_filter>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.in_filter:1
msgid "Alias for field number 4"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`kernel_h <tvm.topi.nn.Workload.kernel_h>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.kernel_h:1
msgid "Alias for field number 6"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`kernel_w <tvm.topi.nn.Workload.kernel_w>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.kernel_w:1
msgid "Alias for field number 7"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`out_dtype <tvm.topi.nn.Workload.out_dtype>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.out_dtype:1
msgid "Alias for field number 1"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`out_filter <tvm.topi.nn.Workload.out_filter>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.out_filter:1
msgid "Alias for field number 5"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`padb <tvm.topi.nn.Workload.padb>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.padb:1
msgid "Alias for field number 10"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`padl <tvm.topi.nn.Workload.padl>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.padl:1
msgid "Alias for field number 9"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`padr <tvm.topi.nn.Workload.padr>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.padr:1
msgid "Alias for field number 11"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`padt <tvm.topi.nn.Workload.padt>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.padt:1
msgid "Alias for field number 8"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`stride_h <tvm.topi.nn.Workload.stride_h>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.stride_h:1
msgid "Alias for field number 14"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`stride_w <tvm.topi.nn.Workload.stride_w>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.stride_w:1
msgid "Alias for field number 15"
msgstr ""

#: of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
msgid ":py:obj:`width <tvm.topi.nn.Workload.width>`\\"
msgstr ""

#: ../../docstring of tvm.topi.nn.Workload.dilation_h:1:<autosummary>:1
#: tvm.topi.nn.Workload.width:1
msgid "Alias for field number 3"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:2
msgid ""
"The pooling kernel and stride sizes are automatically chosen for desired "
"output sizes. It decides the height and width dimension according to the "
"layout string, in which 'W' and 'H' means width and height respectively. "
"Width and height dimension cannot be split. For example, NCHW, NCHW16c, "
"etc. are valid for pool, while NCHW16w, NCHW16h are not. See parameter "
"`layout` for more information of the layout string convention."
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:14 tvm.topi.nn.pooling.global_pool:12
#: tvm.topi.nn.pooling.pool1d:12 tvm.topi.nn.pooling.pool2d:12
#: tvm.topi.nn.pooling.pool3d:12 tvm.topi.nn.pooling.pool_grad:12
#: tvm.topi.nn.pooling.pool_grad:15
msgid "n-D with shape of layout"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:17
msgid "output_size"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:17
msgid "output height and width."
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:20 tvm.topi.nn.pooling.global_pool:15
#: tvm.topi.nn.pooling.pool1d:27 tvm.topi.nn.pooling.pool2d:27
#: tvm.topi.nn.pooling.pool3d:27 tvm.topi.nn.pooling.pool_grad:27
msgid "pool_type"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:20 tvm.topi.nn.pooling.global_pool:15
#: tvm.topi.nn.pooling.pool1d:27 tvm.topi.nn.pooling.pool2d:27
#: tvm.topi.nn.pooling.pool3d:27 tvm.topi.nn.pooling.pool_grad:27
msgid "Pool type, 'max' or 'avg'"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:29 tvm.topi.nn.pooling.pool1d:39
#: tvm.topi.nn.pooling.pool2d:39 tvm.topi.nn.pooling.pool3d:39
#: tvm.topi.nn.pooling.pool_grad:39
msgid "layout: string"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:23 tvm.topi.nn.pooling.global_pool:18
#: tvm.topi.nn.pooling.pool2d:33 tvm.topi.nn.pooling.pool_grad:33
msgid ""
"Layout of the input data. The layout is supposed to be composed of upper "
"cases, lower cases and numbers, where upper case indicates a dimension "
"and the corresponding lower case with factor size indicates the split "
"dimension. For example, NCHW16c can describe a 5-D tensor of [batch_size,"
" channel, height, width, channel_block], in which channel_block=16 is a "
"split of dimension channel."
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool:34 tvm.topi.nn.pooling.pool1d:47
#: tvm.topi.nn.pooling.pool2d:47 tvm.topi.nn.pooling.pool3d:47
#: tvm.topi.nn.pooling.pool_grad:47
msgid "n-D in the same layout"
msgstr ""

#: of tvm.topi.nn.pooling.adaptive_pool3d:1
msgid ""
"Perform pooling on three dimensional data. See the two dimensional "
"version above for details."
msgstr ""

#: of tvm.topi.nn.qnn.add_alter_layout:3
msgid ""
"Add is not a QNN-specific function, but this generic exists so that bias "
"add operations can be fused with input zero point add optimizations, "
"which only happens if the previous operator is quantized."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:8
#: tvm.topi.nn.conv3d.conv3d_alter_layout:8
#: tvm.topi.nn.dense.dense_alter_layout:8 tvm.topi.nn.qnn.add_alter_layout:12
#: tvm.topi.nn.qnn.bias_add_legalize:11
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:8
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:9
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:8
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:11
msgid "Grouped input symbols"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:9
#: tvm.topi.nn.conv3d.conv3d_alter_layout:9
#: tvm.topi.nn.dense.dense_alter_layout:9 tvm.topi.nn.qnn.add_alter_layout:13
#: tvm.topi.nn.qnn.bias_add_legalize:12
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:9
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:10
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:9
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:12
msgid "tinfos"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:-1
#: tvm.topi.nn.conv3d.conv3d_alter_layout:-1
#: tvm.topi.nn.dense.dense_alter_layout:-1 tvm.topi.nn.qnn.add_alter_layout:-1
#: tvm.topi.nn.qnn.bias_add_legalize:-1
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:-1
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:-1
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:-1
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:-1
msgid "list"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:10
#: tvm.topi.nn.conv3d.conv3d_alter_layout:10
#: tvm.topi.nn.dense.dense_alter_layout:10 tvm.topi.nn.qnn.add_alter_layout:14
#: tvm.topi.nn.qnn.bias_add_legalize:13
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:10
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:11
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:10
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:13
msgid "Input shape and dtype"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:12
#: tvm.topi.nn.conv3d.conv3d_alter_layout:12
#: tvm.topi.nn.dense.dense_alter_layout:12 tvm.topi.nn.qnn.add_alter_layout:16
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:12
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:12
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:12
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:15
msgid "out_type: type"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:12
#: tvm.topi.nn.conv3d.conv3d_alter_layout:12
#: tvm.topi.nn.dense.dense_alter_layout:12 tvm.topi.nn.qnn.add_alter_layout:16
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:12
#: tvm.topi.nn.qnn.qnn_dense_alter_layout:13
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:12
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:15
msgid "The output type"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_alter_layout:16
#: tvm.topi.nn.conv3d.conv3d_alter_layout:16
#: tvm.topi.nn.dense.dense_alter_layout:16 tvm.topi.nn.qnn.add_alter_layout:20
#: tvm.topi.nn.qnn.qnn_conv2d_alter_layout:16
#: tvm.topi.nn.qnn.qnn_requantize_alter_layout:16
#: tvm.topi.nn.sparse.sparse_dense_alter_layout:19
msgid ""
"Unlike other TOPI functions, this function operates on both graph level "
"and operator level."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:3
msgid ""
"Both `tensor_a` and `tensor_b` can be transposed. For legacy reason, we "
"use NT format (transpose_a=False, transpose_b=True) by default."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:9 tvm.topi.nn.dense.matmul:6
msgid "tensor_a"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:9
msgid "3-D with shape [batch, M, K] or [batch, K, M]."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:12 tvm.topi.nn.dense.matmul:9
msgid "tensor_b"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:12
msgid "3-D with shape [batch, K, N] or [batch, N, K]."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:16
msgid "oshape"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:-1
msgid "List[Optional]"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:15
msgid ""
"Explicit intended output shape of the computation. Can be useful in cases"
" with dynamic input shapes."
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:21
#: tvm.topi.image.dilation2d.dilation2d_nhwc:21
#: tvm.topi.image.resize.crop_and_resize:31
#: tvm.topi.nn.batch_matmul.batch_matmul:19 tvm.topi.nn.conv1d.conv1d:29
#: tvm.topi.nn.conv1d.group_conv1d_ncw:24
#: tvm.topi.nn.conv1d.group_conv1d_nwc:24
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:18
#: tvm.topi.nn.conv2d.conv:42 tvm.topi.nn.conv2d.conv2d_NCHWc:31
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:31
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:16
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:16
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:16
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:16
#: tvm.topi.nn.conv2d.group_conv2d_nchw:26
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:26
#: tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:9
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:18
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:20
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:29
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:18
#: tvm.topi.nn.dense.dense:17 tvm.topi.nn.dense.dense_pack:15
#: tvm.topi.nn.dense.matmul:15
msgid "out_dtype"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:-1
#: tvm.topi.image.dilation2d.dilation2d_nhwc:-1
#: tvm.topi.nn.batch_matmul.batch_matmul:-1 tvm.topi.nn.conv2d.conv2d:-1
#: tvm.topi.nn.dense.dense:-1 tvm.topi.nn.dense.dense_pack:-1
#: tvm.topi.nn.dense.matmul:-1
msgid "Optional[str]"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:19
msgid "Specifies the output data type for mixed precision batch matmul."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:22 tvm.topi.nn.dense.matmul:18
msgid "transpose_a"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:-1 tvm.topi.nn.dense.matmul:-1
msgid "Optional[bool] = False"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:22
msgid "Whether the first tensor is in transposed format."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:25 tvm.topi.nn.dense.matmul:21
msgid "transpose_b"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:-1
msgid "Optional[bool] = True"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:25
msgid "Whether the second tensor is in transposed format."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:28 tvm.topi.nn.dense.matmul:24
msgid "auto_scheduler_rewritten_layout: Optional[str] = \"\""
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:28
#: tvm.topi.nn.conv2d.conv2d_nhwc:26 tvm.topi.nn.conv2d.conv2d_winograd_nchw:21
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:19
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:21
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:19
#: tvm.topi.nn.conv3d.conv3d_ndhwc:27 tvm.topi.nn.dense.dense:20
#: tvm.topi.nn.dense.matmul:24
msgid "The layout after auto-scheduler's layout rewrite pass."
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:31
#: tvm.topi.nn.conv2d.conv2d_nhwc:29 tvm.topi.nn.conv2d.conv2d_winograd_nchw:23
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:21
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:23
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:21
#: tvm.topi.nn.dense.dense:23 tvm.topi.nn.dense.matmul:27
msgid "meta_schedule_original_shape: Optional[List[PrimExpr]] = None"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:31
msgid "The original shape of the tensor"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul:36
msgid "3-D with shape [batch, M, N]"
msgstr ""

#: of tvm.topi.nn.batch_matmul.batch_matmul_legalize:6
msgid "Attributes of current batch_matmul"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:3
msgid ""
"Normalizes the input at each batch, i.e. applies a transformation that "
"maintains the mean activation close to 0 and the activation standard "
"deviation close to 1."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:10
msgid "Input to be batch-normalized."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:13
msgid "gamma"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:13
msgid "Scale factor to be applied to the normalized tensor."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:16
#: tvm.topi.nn.local_response_norm.lrn:27
msgid "beta"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:16
msgid "Offset to be applied to the normalized tensor."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:19 tvm.topi.nn.batch_norm.batch_norm:51
msgid "moving_mean"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:19 tvm.topi.nn.batch_norm.batch_norm:51
msgid "Running mean of input."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:22 tvm.topi.nn.batch_norm.batch_norm:53
msgid "moving_var"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:22 tvm.topi.nn.batch_norm.batch_norm:54
msgid "Running variance of input."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:-1
msgid "int, optional, default=1"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:25
msgid "Specify along which shape axis the normalization should occur."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:28 tvm.topi.nn.group_norm.group_norm:26
#: tvm.topi.nn.instance_norm.instance_norm:19
#: tvm.topi.nn.layer_norm.layer_norm:20 tvm.topi.nn.rms_norm.rms_norm:15
msgid "epsilon"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:-1
msgid "float, optional, default=1e-5"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:28
msgid "Small float added to variance to avoid dividing by zero."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:32
msgid "center"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:-1
msgid "bool, optional, default=True"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:31
msgid ""
"If True, add offset of beta to normalized tensor, If False, beta is "
"ignored."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:36
msgid "scale"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:-1
msgid "bool, optional, defualt=True"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:35
msgid "If True, scale normalized tensor by gamma. If False, gamma is ignored."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:40
msgid "training"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:-1
msgid "bool, optional, defualt=False"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:39
msgid ""
"Indicating whether it is in training mode. If True, update moving_mean "
"and moving_var."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:43
msgid "momentum"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:-1
msgid "float, optional, default=0.1"
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:43
msgid "The value used for the moving_mean and moving_var update."
msgstr ""

#: of tvm.topi.nn.batch_norm.batch_norm:48
msgid "Normalized data with same shape as input"
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:6
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:6
msgid ""
"N-D Tensor with shape [batch, spatial_shape, remaining_shapes], where "
"spatial_shape has M dimensions."
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:11
#: tvm.topi.nn.depth_to_space.depth_to_space:9
#: tvm.topi.nn.space_to_depth.space_to_depth:9
msgid "block_size"
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:-1
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:-1
msgid "list of ints"
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:10
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:10
msgid ""
"list of size [M] where M is number of spatial dims, specifies block size "
"for each spatial dimension."
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:15
msgid "crop_begin_list"
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:14
msgid ""
"list of shape [M] where M is number of spatial dims, specifies begin crop"
" size for each spatial dimension."
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:19
msgid "crop_end_list"
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:18
msgid ""
"list of shape [M] where M is number of spatial dims, specifies end crop "
"size for each spatial dimension."
msgstr ""

#: of tvm.topi.nn.batch_to_space_nd.batch_to_space_nd:23
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:26
msgid "output : tvm.te.Tensor"
msgstr ""

#: of tvm.topi.nn.qnn.bias_add_legalize:3
msgid ""
"Bias add is not a QNN-specific function, but this generic exists so that "
"empty channels can be excised from quantized conv2d operators and folded "
"into bias adds."
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:6 tvm.topi.nn.pad.mirror_pad:6
#: tvm.topi.nn.pad.pad:6
msgid "n-D input, can be any layout."
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:-1
msgid "None or int"
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:9
msgid ""
"The axis along which to do binarization and bit-packing, default is the "
"last axis."
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:13
msgid "The name prefix operators generate."
msgstr ""

#: of tvm.topi.nn.bnn.binarize_pack:18
msgid "n-D, the same layout as input, dtype is uint32."
msgstr ""

#: of tvm.topi.nn.bnn.binary_dense:6
msgid "2-D with shape [batch, in_dim], dtype is uint32."
msgstr ""

#: of tvm.topi.nn.bitserial_dense.bitserial_dense:8
#: tvm.topi.nn.bnn.binary_dense:9 tvm.topi.nn.conv2d.conv2d_winograd_nchw:8
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:8
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:8
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:8
#: tvm.topi.nn.dense.dense:11 tvm.topi.nn.dense.dense_pack:9
#: tvm.topi.sparse.dense.dense:10
msgid "weight"
msgstr ""

#: of tvm.topi.nn.bnn.binary_dense:9
msgid "2-D with shape [out_dim, in_dim], dtype is uint32."
msgstr ""

#: of tvm.topi.nn.bnn.binary_dense:14
msgid "2-D with shape [batch, out_dim], dtype is float32."
msgstr ""

#: of tvm.topi.nn.bitserial_util.bitpack:5
msgid "pack_axis"
msgstr ""

#: of tvm.topi.nn.bitserial_util.bitpack:6
msgid "index of the axis to pack in data"
msgstr ""

#: of tvm.topi.nn.bitserial_util.bitpack:7
msgid "bit_axis"
msgstr ""

#: of tvm.topi.nn.bitserial_util.bitpack:8
msgid "index of axis to place bit axis in resulting packed data"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:6
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:6
#: tvm.topi.nn.conv2d.conv2d_nchw:6 tvm.topi.nn.conv2d.conv2d_winograd_nchw:7
#: tvm.topi.nn.conv2d.group_conv2d_nchw:6
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:6
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:6
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:8
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:6
msgid "4-D with shape [batch, in_channel, in_height, in_width]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:9
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:9
#: tvm.topi.nn.conv1d.conv1d:11 tvm.topi.nn.conv1d.group_conv1d_ncw:9
#: tvm.topi.nn.conv1d.group_conv1d_nwc:9
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:9
#: tvm.topi.nn.conv2d.conv2d_NCHWc:11 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:11
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:9
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:15
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:15
#: tvm.topi.nn.pooling.pool1d:15 tvm.topi.nn.pooling.pool2d:15
#: tvm.topi.nn.pooling.pool3d:15 tvm.topi.nn.pooling.pool_grad:18
#: tvm.topi.nn.utils.get_pad_tuple:9 tvm.topi.nn.utils.get_pad_tuple1d:9
#: tvm.topi.nn.utils.get_pad_tuple3d:9
#: tvm.topi.nn.utils.get_pad_tuple_generic:9
msgid "kernel"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:9
#: tvm.topi.nn.conv2d.conv2d_nchw:9
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:15
msgid "4-D with shape [num_filter, in_channel, filter_height, filter_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:12
#: tvm.topi.image.dilation2d.dilation2d_nhwc:12
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:12
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:12
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:12
#: tvm.topi.nn.conv2d.conv:16 tvm.topi.nn.conv2d.conv2d_NCHWc:14
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:14 tvm.topi.nn.conv2d.conv2d_hwcn:12
#: tvm.topi.nn.conv2d.conv2d_nchw:12 tvm.topi.nn.conv2d.conv2d_nhwc:12
#: tvm.topi.nn.conv2d.group_conv2d_nchw:12
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:12
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:12
#: tvm.topi.nn.conv3d.conv3d_ncdhw:12 tvm.topi.nn.conv3d.conv3d_ndhwc:12
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:14
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:12
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:12
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:12
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:12
#: tvm.topi.nn.pooling.pool1d:18 tvm.topi.nn.pooling.pool2d:18
#: tvm.topi.nn.pooling.pool3d:18 tvm.topi.nn.pooling.pool_grad:21
msgid "stride"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:-1
#: tvm.topi.image.dilation2d.dilation2d_nhwc:-1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:-1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:-1
#: tvm.topi.nn.conv2d.conv:-1 tvm.topi.nn.conv2d.conv2d:-1
#: tvm.topi.nn.conv2d.conv2d_NCHWc:-1 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:-1
#: tvm.topi.nn.conv2d.conv2d_hwcn:-1 tvm.topi.nn.conv2d.conv2d_nchw:-1
#: tvm.topi.nn.conv2d.conv2d_nhwc:-1 tvm.topi.nn.conv2d.conv2d_winograd_nchw:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:-1
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:-1
#: tvm.topi.nn.conv2d.group_conv2d_nchw:-1
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:-1
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:-1
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:-1
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:-1
msgid "int or a list/tuple of two ints"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:12
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:12
#: tvm.topi.nn.conv2d.conv2d:12 tvm.topi.nn.conv2d.conv2d_NCHWc:14
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:14
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:11
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:11
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:11
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:11
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:18
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:18
msgid "stride size, or [stride_height, stride_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:15
#: tvm.topi.image.dilation2d.dilation2d_nhwc:15
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:15
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:15
#: tvm.topi.nn.conv1d.conv1d:17 tvm.topi.nn.conv1d.group_conv1d_ncw:16
#: tvm.topi.nn.conv1d.group_conv1d_nwc:16
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:15
#: tvm.topi.nn.conv2d.conv:22 tvm.topi.nn.conv2d.conv2d:17
#: tvm.topi.nn.conv2d.conv2d_NCHWc:19 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:19
#: tvm.topi.nn.conv2d.conv2d_hwcn:17 tvm.topi.nn.conv2d.conv2d_nchw:17
#: tvm.topi.nn.conv2d.conv2d_nhwc:17 tvm.topi.nn.conv2d.conv2d_winograd_nchw:12
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:12
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:12
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:12
#: tvm.topi.nn.conv2d.group_conv2d_nchw:17
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:17
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:15
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:17
#: tvm.topi.nn.conv3d.conv3d_ncdhw:15 tvm.topi.nn.conv3d.conv3d_ndhwc:15
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:15
#: tvm.topi.nn.correlation.correlation_nchw:26
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:21
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:21
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:17
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:15
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:15
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:15
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:15
#: tvm.topi.nn.pooling.pool1d:24 tvm.topi.nn.pooling.pool2d:24
#: tvm.topi.nn.pooling.pool3d:24 tvm.topi.nn.pooling.pool_grad:24
#: tvm.topi.nn.utils.get_pad_tuple:6 tvm.topi.nn.utils.get_pad_tuple1d:6
#: tvm.topi.nn.utils.get_pad_tuple3d:6
#: tvm.topi.nn.utils.get_pad_tuple_generic:6
msgid "padding"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:-1
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:-1
msgid "int or a list/tuple of two or four ints"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:15
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:15
msgid ""
"padding size, [pad_height, pad_width], [pad_top, pad_left, pad_down, "
"pad_right]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:18
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:18
msgid "activation_bits: int"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:18
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:18
msgid "number of bits used for activations/input elements"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:21
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:21
msgid "weight_bits: int"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:21
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:21
msgid "number of bits used for weight elements"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:24
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:24
msgid "out_dtype: str"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:24
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:24
msgid "return type of convolution"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:27
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:27
msgid "pack_dtype: str"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:27
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:27
msgid "bit packing type"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:30
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:30
msgid "unipolar: bool"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:30
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:30
msgid ""
"if binarization style is in unipolar 1/0 format, instead of bipolar -1/+1"
" format"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nchw:35
#: tvm.topi.nn.conv2d.conv2d:32 tvm.topi.nn.conv2d.conv2d_nchw:25
#: tvm.topi.nn.conv2d.group_conv2d_nchw:31
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:26
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:34
#: tvm.topi.nn.correlation.correlation_nchw:34
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:35
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:26
msgid "4-D with shape [batch, out_channel, out_height, out_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nhwc:6
#: tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:6
#: tvm.topi.nn.conv2d.conv2d_nhwc:6
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:7
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:7
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:7
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:8
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:20
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:6
msgid "4-D with shape [batch, in_height, in_width, in_channel]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:9
#: tvm.topi.nn.conv2d.conv2d_hwcn:9 tvm.topi.nn.conv2d.conv2d_nhwc:9
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:9
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:9
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:9
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:9
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:15
msgid "4-D with shape [filter_height, filter_width, in_channel, num_filter]"
msgstr ""

#: of tvm.topi.nn.bitserial_conv2d.bitserial_conv2d_nhwc:35
#: tvm.topi.nn.conv2d.conv2d_nhwc:34 tvm.topi.nn.conv2d.conv2d_winograd_nchw:28
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:26
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:28
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:26
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:31
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:35
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:26
msgid "4-D with shape [batch, out_height, out_width, out_channel]"
msgstr ""

#: of tvm.topi.nn.bitserial_dense.bitserial_dense:6 tvm.topi.nn.dense.dense:8
#: tvm.topi.nn.dense.dense_pack:6 tvm.topi.nn.dense.matmul:6
#: tvm.topi.sparse.dense.dense:7
msgid "2-D with shape [batch, in_dim]"
msgstr ""

#: of tvm.topi.nn.bitserial_dense.bitserial_dense:8
msgid ""
"2-D with shape [out_dim, in_dim] or 3-D with shape [out_dim, weight_bits,"
" in_dim]"
msgstr ""

#: of tvm.topi.nn.bitserial_dense.bitserial_dense:13 tvm.topi.nn.dense.dense:28
#: tvm.topi.nn.dense.dense_pack:20 tvm.topi.nn.dense.matmul:32
#: tvm.topi.sparse.dense.dense:18
msgid "2-D with shape [batch, out_dim]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:3
msgid "Supports 1D, 2D, 3D, ... and grouping."
msgstr ""

#: of tvm.topi.nn.conv2d.conv:8
msgid "inp"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:8
msgid ""
"N-D with shape [batch, in_channel, in_height, in_width, ...] in "
"`data_layout`"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:12
msgid "filt"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:11
msgid ""
"N-D with shape [num_filter, in_channel // groups, filter_height, "
"filter_width, ...] in `kernel_layout`"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:-1
msgid "int or a list/tuple of dim ints"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:15
msgid ""
"(where dim=2 for NCHW, dim=1 for NCH, etc.) Stride size, or "
"[stride_height, stride_width, ...]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:-1
msgid "int or a list/tuple of dim or 2*dim ints"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:19
msgid ""
"(where dim=2 for NCHW, dim=1 for NCH, etc.) padding size, or [pad_height,"
" pad_width, ...] for dim ints, or [pad_top, pad_left, pad_bottom, "
"pad_right] for 2*dim ints"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:20 tvm.topi.nn.conv1d.group_conv1d_ncw:19
#: tvm.topi.nn.conv1d.group_conv1d_nwc:19 tvm.topi.nn.conv2d.conv:25
#: tvm.topi.nn.conv2d.group_conv2d_nchw:20
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:20
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:24
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:24
msgid "dilation"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:18
#: tvm.topi.image.dilation2d.dilation2d_nhwc:18 tvm.topi.nn.conv2d.conv:25
#: tvm.topi.nn.conv2d.conv2d:20 tvm.topi.nn.conv2d.conv2d_NCHWc:22
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:22 tvm.topi.nn.conv2d.conv2d_hwcn:20
#: tvm.topi.nn.conv2d.conv2d_nchw:20 tvm.topi.nn.conv2d.conv2d_nhwc:20
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:15
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:15
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:15
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:15
#: tvm.topi.nn.conv2d.group_conv2d_nchw:20
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:20
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:24
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:24
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:20
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:18
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:18
msgid "dilation size, or [dilation_height, dilation_width]"
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_ncw:22
#: tvm.topi.nn.conv1d.group_conv1d_nwc:22 tvm.topi.nn.conv2d.conv:28
#: tvm.topi.nn.conv2d.group_conv2d_nchw:23
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:23
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:26
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:30
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:30
msgid "groups"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:28 tvm.topi.nn.conv2d.group_conv2d_nchw:23
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:23
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:26
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:30
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:30
msgid "number of groups"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:23 tvm.topi.nn.conv2d.conv:32
#: tvm.topi.nn.conv2d.conv2d:23
msgid "data_layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:31
msgid ""
"Layout of the input. N indicates batch dimension, C indicates channels, "
"any other character indicates HW (or H or HWD for 1D and 3D)."
msgstr ""

#: of tvm.topi.nn.conv2d.conv:38
msgid "kernel_layout: Optional[str]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:35
msgid ""
"Layout of the filter. I indicates input channels, O indicates output "
"channels, any other character indicates HW dimension of the filter (or H "
"or HWD for 1D and 3D). If kernel_layout is empty, use data_layout to "
"infer the default kernel_layout. Default kernel_layout is OIHW for NCHW "
"data layout, HWIO for NHWC data layout."
msgstr ""

#: of tvm.topi.nn.conv2d.conv:41
msgid ""
"Elements are converted to this type before elementwise multiplication and"
" summation."
msgstr ""

#: of tvm.topi.nn.conv2d.conv:45
msgid "auto_scheduler_rewritten_layout: str"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:45
msgid "Layout from autoscheduler's layout rewritting."
msgstr ""

#: of tvm.topi.nn.conv2d.conv:48
msgid "meta_schedule_original_shape"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:-1
msgid "Optional[List[PrimExpr]]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:48 tvm.topi.nn.conv2d.conv2d_nhwc:29
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:23
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:21
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:23
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:21
#: tvm.topi.nn.conv3d.conv3d_ndhwc:30 tvm.topi.nn.dense.dense:23
#: tvm.topi.nn.dense.matmul:27
msgid "The original shape of the input tensor."
msgstr ""

#: of tvm.topi.nn.conv2d.conv:53
msgid "auto_scheduler_should_rewrite_layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:51
msgid ""
"Should auto scheduler be allowed to rewrite the layout of the filter "
"tensor. Defaults to false. This can cause errors if used with grouped "
"convs."
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:25
#: tvm.topi.image.dilation2d.dilation2d_nhwc:25
#: tvm.topi.image.grid_sample.affine_grid:17
#: tvm.topi.image.grid_sample.grid_sample:60 tvm.topi.nn.conv2d.conv:57
#: tvm.topi.nn.conv2d.conv2d_infer_layout:13 tvm.topi.nn.conv2d.conv2d_nchw:24
#: tvm.topi.nn.conv2d.group_conv2d_nchw:30
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:30
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:25
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:33
#: tvm.topi.nn.conv3d.conv3d_ncdhw:25 tvm.topi.nn.conv3d.conv3d_ndhwc:34
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:25
#: tvm.topi.nn.correlation.correlation_nchw:33
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:33
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:19
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:19
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:13
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:25
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:25
#: tvm.topi.nn.dilate.dilate:19 tvm.topi.nn.mapping.scale_shift_nchw:16
#: tvm.topi.nn.mapping.scale_shift_nchwc:16
#: tvm.topi.nn.mapping.scale_shift_nhwc:16 tvm.topi.nn.pad.mirror_pad:22
#: tvm.topi.nn.pad.pad:22
msgid "Output"
msgstr ""

#: of tvm.topi.nn.conv2d.conv:58
msgid ""
"N-D with shape [batch, out_channel, out_height, out_width, ...] in "
"`data_layout`"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:6
msgid ""
"3-D input shape [batch, in_channel, in_width] for data_layout == 'NCW' "
"and [batch, in_width, in_channel] for data_layout == 'NWC'"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:10
msgid ""
"3-D kernel with shape [num_filter, in_channel, filter_size] for "
"kernel_layout == 'OIW' and [filter_size, in_channel, num_filter] for "
"kernel_layout == 'WIO'"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:-1 tvm.topi.nn.conv1d.group_conv1d_ncw:-1
#: tvm.topi.nn.conv1d.group_conv1d_nwc:-1
msgid "int or tuple"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:14 tvm.topi.nn.conv1d.group_conv1d_ncw:12
#: tvm.topi.nn.conv1d.group_conv1d_nwc:12
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:12
msgid "The spatial stride along width"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:-1 tvm.topi.nn.conv1d.conv1d:-1
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:-1
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:-1
#: tvm.topi.nn.conv3d.conv3d_ncdhw:-1 tvm.topi.nn.conv3d.conv3d_ndhwc:-1
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:-1
#: tvm.topi.nn.utils.get_pad_tuple:-1 tvm.topi.nn.utils.get_pad_tuple1d:-1
#: tvm.topi.nn.utils.get_pad_tuple3d:-1
#: tvm.topi.nn.utils.get_pad_tuple_generic:-1
msgid "int or str"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:17
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:15
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:15
#: tvm.topi.nn.conv3d.conv3d_ncdhw:15 tvm.topi.nn.conv3d.conv3d_ndhwc:15
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:15
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:17
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:15
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:15
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:15
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:15
#: tvm.topi.nn.utils.get_pad_tuple:6 tvm.topi.nn.utils.get_pad_tuple1d:6
#: tvm.topi.nn.utils.get_pad_tuple3d:6
#: tvm.topi.nn.utils.get_pad_tuple_generic:6
msgid "Padding size, or ['VALID', 'SAME']"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:20 tvm.topi.nn.conv1d.group_conv1d_ncw:19
#: tvm.topi.nn.conv1d.group_conv1d_nwc:19
msgid "Dilation rate if convolution should be dilated."
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:23
msgid "How input data is laid out, must be one of ['NCW', 'NWC']"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:27
msgid "kernel_layout: Optiona[str]"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:26
msgid ""
"The layout of the kernel. If unspecified, use default layout. \"OIW\" if "
"data_layout == \"NCW\", \"WIO\" if data_layout == \"NWC\"."
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d:30 tvm.topi.nn.conv1d.group_conv1d_ncw:25
#: tvm.topi.nn.conv1d.group_conv1d_nwc:25
msgid "The output data type. If None then output is same type as input."
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d_ncw:1
msgid ""
"1D convolution in NCW layout. See :py:func:`conv` for details on "
"parameters"
msgstr ""

#: of tvm.topi.nn.conv1d.conv1d_nwc:1
msgid ""
"1D convolution in NWC layout. See :py:func:`conv` for details on "
"parameters"
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_ncw:6
#: tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:6
msgid "3-D with shape [batch, in_channel, in_width]"
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:9
msgid "3-D with shape [in_channel, num_filter, filter_width]"
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:-1
msgid "ints"
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:18
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:18
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:20
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:18
msgid "The output data type. This is used for mixed precision."
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:22
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:21
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:23
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:21
msgid "output_padding"
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:21
msgid ""
"Used to recover the actual output shape in case there are more than one "
"possible shape.  Must be smaller than stride."
msgstr ""

#: of tvm.topi.nn.conv1d_transpose.conv1d_transpose_ncw:27
msgid "3-D with shape [batch, out_channel, out_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:6
#: tvm.topi.image.dilation2d.dilation2d_nhwc:6 tvm.topi.nn.conv2d.conv2d:6
msgid "input"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:6
msgid "4-D with shape [batch, in_channel, in_height, in_width] in data_layout"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:9
#: tvm.topi.image.dilation2d.dilation2d_nhwc:9 tvm.topi.nn.conv2d.conv2d:9
msgid "filter"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:9
msgid ""
"4-D with shape [num_filter, in_channel, filter_height, filter_width] in "
"kernel_layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:-1 tvm.topi.nn.conv2d.conv2d_NCHWc:-1
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:-1 tvm.topi.nn.conv2d.conv2d_hwcn:-1
#: tvm.topi.nn.conv2d.conv2d_nchw:-1 tvm.topi.nn.conv2d.conv2d_nhwc:-1
#: tvm.topi.nn.conv2d.group_conv2d_nchw:-1
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:-1
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:-1
#: tvm.topi.nn.correlation.correlation_nchw:-1
msgid "int or a list/tuple of 2 or 4 ints"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:15 tvm.topi.nn.conv2d.conv2d_NCHWc:17
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:17 tvm.topi.nn.conv2d.conv2d_hwcn:15
#: tvm.topi.nn.conv2d.conv2d_nchw:15 tvm.topi.nn.conv2d.conv2d_nhwc:15
#: tvm.topi.nn.conv2d.group_conv2d_nchw:15
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:15
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:15
msgid ""
"padding size, or [pad_height, pad_width] for 2 ints, or [pad_top, "
"pad_left, pad_bottom, pad_right] for 4 ints"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:20 tvm.topi.nn.conv2d.conv2d_NCHWc:22
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:22 tvm.topi.nn.conv2d.conv2d_hwcn:20
#: tvm.topi.nn.conv2d.conv2d_nchw:20 tvm.topi.nn.conv2d.conv2d_nhwc:20
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:14
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:14
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:14
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:14
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:20
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:18
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:18
msgid "dilation: int or a list/tuple of two ints"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:23 tvm.topi.nn.sparse.sparse_conv2d:23
msgid "layout of data"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:27
msgid "kernel_layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d:26
msgid ""
"layout of kernel. If unspecified, use default layout inferred from "
"data_layout. \"OIHW\" if data_layout == \"NCHW\", \"HWIO\" if data_layout"
" == \"NHWC\"."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:6 tvm.topi.nn.conv2d.conv2d_NCHWc_int8:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:6
msgid ""
"5-D with shape [batch, in_channel_chunk, in_height, in_width, "
"in_channel_block]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:9
msgid ""
"6-D with shape [num_filter_chunk, in_channel_chunk, filter_height, "
"filter_width, in_channel_block, num_filter_block]"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:45
#: tvm.topi.image.resize.crop_and_resize:22 tvm.topi.nn.conv2d.conv2d_NCHWc:25
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:25
#: tvm.topi.nn.depth_to_space.depth_to_space:12
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:23
#: tvm.topi.nn.pooling.global_pool:24
#: tvm.topi.nn.space_to_depth.space_to_depth:12
#: tvm.topi.nn.sparse.sparse_conv2d:23 tvm.topi.nn.upsampling.upsampling:18
#: tvm.topi.nn.upsampling.upsampling3d:21
msgid "layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:25
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:25
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:23
msgid "Input data layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:28
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:28
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:26
msgid "out_layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:28
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:28
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:26
msgid "Output data layout"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:31
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:31
msgid "output data type"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc:36
#: tvm.topi.nn.conv2d.conv2d_NCHWc_int8:39
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:34
msgid ""
"5-D with shape [batch, out_channel_chunk, out_height, out_width, "
"out_channel_block]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc_int8:9
msgid ""
"7-D with shape [num_filter_chunk, in_channel_chunk, filter_height, "
"filter_width, in_channel_block/4, num_filter_block, 4]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc_int8:34
msgid "n_elems"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_NCHWc_int8:34
msgid "numer of int8 elements accumulated"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:5
#: tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:5
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:5
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:5
msgid "kernel: Tensor"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:6
msgid "The raw kernel tensor with layout \"NHWC\"."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:7
msgid "tile_rows: int"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:8
msgid "Tile rows of the weight transformation for ConvGemm."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:10
msgid "tile_cols: int"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:10
msgid "Tile columns of the weight transformation for ConvGemm."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_gemm_weight_transform:15
msgid "2-D with shape [CI*KH*KW,CO]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_hwcn:6 tvm.topi.nn.conv2d.conv2d_nchw:6
#: tvm.topi.nn.conv2d.conv2d_nhwc:6 tvm.topi.nn.conv2d.group_conv2d_nchw:6
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:6
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:6
#: tvm.topi.nn.conv3d.conv3d_ncdhw:6 tvm.topi.nn.conv3d.conv3d_ndhwc:6
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:6
#: tvm.topi.nn.mapping.scale_shift_nchw:6
#: tvm.topi.nn.mapping.scale_shift_nchwc:6
#: tvm.topi.nn.mapping.scale_shift_nhwc:6
msgid "Input"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_hwcn:6
msgid "4-D with shape [in_height, in_width, in_channel, batch]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_hwcn:9 tvm.topi.nn.conv2d.conv2d_nchw:9
#: tvm.topi.nn.conv2d.conv2d_nhwc:9 tvm.topi.nn.conv2d.group_conv2d_nchw:9
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:9
#: tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:9
#: tvm.topi.nn.conv3d.conv3d_ncdhw:9 tvm.topi.nn.conv3d.conv3d_ndhwc:9
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:11
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:9
msgid "Filter"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:12
#: tvm.topi.image.dilation2d.dilation2d_nhwc:12
#: tvm.topi.nn.conv2d.conv2d_hwcn:12 tvm.topi.nn.conv2d.conv2d_nchw:12
#: tvm.topi.nn.conv2d.conv2d_nhwc:12 tvm.topi.nn.conv2d.group_conv2d_nchw:12
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:12
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:12
msgid "Stride size, or [stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_hwcn:25
msgid "4-D with shape [out_height, out_width, out_channel, batch]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:6
msgid "workload"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:6
msgid "conv2d workload"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:9
msgid "cfg"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:9
msgid "tvm.autotvm config"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:-1
msgid "[tuple of tuple and str, tuple of tuple and str]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_infer_layout:14
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_infer_layout:14
msgid "Input shapes and layouts, and output shapes and layouts"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_nhwc:23 tvm.topi.nn.conv3d.conv3d_ndhwc:24
msgid "out_dtype: str = \"float32\","
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_nhwc:23 tvm.topi.nn.conv3d.conv3d_ndhwc:24
msgid "The type of output tensor"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_nhwc:26
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:20
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:18
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:20
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:18
#: tvm.topi.nn.conv3d.conv3d_ndhwc:27 tvm.topi.nn.dense.dense:20
msgid "auto_scheduler_rewritten_layout: str = \"\""
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_legalize:6
msgid "Attributes of current Transposed 2D convolution"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:9
msgid "4-D with shape [in_channel, num_filter, filter_height, filter_width]"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:-1
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:-1
msgid "tuple of two ints"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:12
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:14
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:12
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:12
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:12
msgid "The spatial stride along height and width"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.conv2d_transpose_nchw:21
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:23
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:21
msgid "Used to get the right output shape for gradients"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nchw:1
msgid ""
"Conv2D Winograd in NCHW layout. This is a clean version to be used by the"
" auto-scheduler for both CPU and GPU."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nchw:13
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:13
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:13
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:13
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:21
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:21
msgid "padding size, or [pad_height, pad_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:21
#: tvm.topi.image.dilation2d.dilation2d_nhwc:21
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw:17
#: tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:17
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:17
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:17
msgid "Specifies the output data type."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nchw:18
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:18
msgid "pre_computed: bool"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nchw:19
#: tvm.topi.nn.conv2d.conv2d_winograd_nhwc:19
msgid "Whether the kernel is precomputed"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nchw_without_weight_transform:1
msgid ""
"Conv2D Winograd without layout transform in NCHW layout. This is a clean "
"version to be used by meta-schedule for both CPU and GPU."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nhwc:1
msgid ""
"Conv2D Winograd in NHWC layout. This is a clean version to be used by the"
" auto-scheduler for both CPU and GPU."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nhwc_without_weight_transform:1
msgid ""
"Conv2D Winograd without layout transform in NHWC layout. This is a clean "
"version to be used by the auto-scheduler for both CPU and GPU."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:6
msgid ""
"The raw kernel tensor with layout \"NCHW\". Only 3x3 kernel is supported "
"for now."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:8
msgid "convolution_algorithm: int"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:8
msgid "The convolution algorithm for Winograd NNPACK."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_nnpack_weight_transform:13
#: tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:13
msgid "4-D with shape [alpha, alpha, CO, CI]"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:6
msgid "The raw kernel tensor with layout \"NCHW\"."
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:8
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:8
msgid "tile_size: int"
msgstr ""

#: of tvm.topi.nn.conv2d.conv2d_winograd_weight_transform:8
#: tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:8
msgid ""
"Tile size of winograd transform. e.g. 2 for F(2x2, 3x3) and 4 for F(4x4, "
"3x3)"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:6
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:6
msgid "5-D with shape [batch, in_channel, in_depth, in_height, in_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:9
msgid ""
"5-D with shape [num_filter, in_channel, filter_depth, filter_height, "
"filter_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:-1 tvm.topi.nn.conv3d.conv3d_ndhwc:-1
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:-1
msgid "int or a list/tuple of three ints"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:12
msgid "Stride size, or [strid_depth, stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:18 tvm.topi.nn.conv3d.conv3d_ndhwc:18
msgid "dilation: int or a list/tuple of three ints"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:18 tvm.topi.nn.conv3d.conv3d_ndhwc:18
msgid "dilation size, or [dilation_depth, dilation_height, dilation_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:21 tvm.topi.nn.conv3d.conv3d_ndhwc:21
msgid "groups: int"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:21 tvm.topi.nn.conv3d.conv3d_ndhwc:21
msgid "Number of groups."
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ncdhw:26
#: tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:26
msgid "5-D with shape [batch, out_channel, out_depth, out_height, out_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:6
msgid "5-D with shape [batch, in_depth, in_height, in_width, in_channel]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:9
msgid ""
"5-D with shape [filter_depth, filter_height, filter_width, in_channel, "
"num_filter]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:12
msgid "Stride size, or [stride_depth, stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:30
msgid "meta_schedule_origin_shape: Optional[List[PrimExpr]] = None"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_ndhwc:35
msgid "5-D with shape [batch, out_depth, out_height, out_width, out_channel]"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_legalize:6
msgid "Attributes of current Transposed 3D convolution"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:9
msgid ""
"5-D with shape [in_channel, num_filter, filter_depth, filter_height, "
"filter_width]"
msgstr ""

#: of tvm.topi.nn.conv3d_transpose.conv3d_transpose_ncdhw:12
msgid "The spatial stride along depth,height and width"
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:6
msgid "The raw kernel tensor with layout \"NCDHW\"."
msgstr ""

#: of tvm.topi.nn.conv3d.conv3d_winograd_weight_transform:13
msgid "5-D with shape [alpha, alpha, alpha, CO, CI]"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:6
msgid "data1"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:6
#: tvm.topi.nn.correlation.correlation_nchw:9
#: tvm.topi.nn.local_response_norm.lrn:11
msgid "4-D with shape [batch, channel, height, width]"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:9
msgid "data2"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:12
msgid "kernel_size: int"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:12
msgid "Kernel size for correlation, must be an odd number"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:15
msgid "max_displacement: int"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:15
msgid "Max displacement of Correlation"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:18
msgid "stride1: int"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:18
msgid "Stride for data1"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:21
msgid "stride2: int"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:21
msgid "Stride for data2 within the neightborhood centered around data1"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:24
msgid ""
"Padding size, or [pad_height, pad_width] for 2 ints, or [pad_top, "
"pad_left, pad_bottom, pad_right] for 4 ints"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:29
msgid "is_multiply: bool"
msgstr ""

#: of tvm.topi.nn.correlation.correlation_nchw:29
msgid "operation type is either multiplication or substraction"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:3
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:3
msgid ""
"The deformable convolution operation is described in "
"https://arxiv.org/abs/1703.06211"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:12
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:12
msgid "offset"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:11
msgid ""
"4-D with shape [batch, deformable_groups * filter_height * filter_width *"
" 2, out_height, out_width]."
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:27
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:27
msgid "deformable_groups"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nchw:27
#: tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:27
msgid "number of deformable groups"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:12
msgid "4-D with shape [batch, out_height, out_width,"
msgstr ""

#: of tvm.topi.nn.deformable_conv2d.deformable_conv2d_nhwc:12
msgid "deformable_groups * filter_height * filter_width * 2]."
msgstr ""

#: of tvm.topi.nn.dense.dense:1
msgid ""
"The default implementation of dense in topi. This is an alias of "
"matmul_nt operator for data tensor in non-transposed format and weight "
"tensor in transposed format."
msgstr ""

#: of tvm.topi.nn.dense.dense:11 tvm.topi.nn.dense.dense_pack:9
#: tvm.topi.nn.dense.matmul:9 tvm.topi.sparse.dense.dense:10
msgid "2-D with shape [out_dim, in_dim]"
msgstr ""

#: of tvm.topi.nn.dense.dense:14 tvm.topi.nn.dense.dense_pack:12
#: tvm.topi.nn.dense.matmul:12 tvm.topi.nn.local_response_norm.lrn:21
#: tvm.topi.sparse.dense.dense:13
msgid "bias"
msgstr ""

#: of tvm.topi.nn.dense.dense:-1 tvm.topi.nn.dense.dense_pack:-1
#: tvm.topi.nn.dense.matmul:-1
msgid "Optional[tvm.te.Tensor]"
msgstr ""

#: of tvm.topi.nn.dense.dense:14 tvm.topi.nn.dense.dense_pack:12
#: tvm.topi.nn.dense.matmul:12 tvm.topi.sparse.dense.dense:13
msgid "1-D with shape [out_dim]"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nchw:26
#: tvm.topi.nn.conv2d.group_conv2d_nhwc:26
#: tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:29
#: tvm.topi.nn.dense.dense:17 tvm.topi.nn.dense.dense_pack:15
#: tvm.topi.nn.dense.matmul:15
msgid "The output type. This is used for mixed precision."
msgstr ""

#: of tvm.topi.nn.dense.dense_legalize:6
msgid "Attributes of current dense"
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:6
#: tvm.topi.nn.space_to_depth.space_to_depth:6
msgid "4-D tensor in either NCHW or NHWC layout."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:9
msgid "Size of blocks to compose from channel dimension."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:12
#: tvm.topi.nn.space_to_depth.space_to_depth:12
msgid "Either NCHW or NHWC, indicating data layout."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:15
msgid ""
"Either DCR or CDR, indicates how channels should be accessed. In DCR, "
"channels are interwoven in the Tensorflow style while in CDR channels are"
" accessed sequentially as in Pytorch."
msgstr ""

#: of tvm.topi.nn.depth_to_space.depth_to_space:22
msgid "Output of shape [N, C / block_size**2, H * block_size, W * block_size]"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:9
msgid ""
"6-D with shape [out_channel_chunk, 1, filter_height, filter_width, 1, "
"out_channel_block] In NCHWc depthwise convolution, we group kernel's "
"in_channel and channel_multiplier together then do the tiling."
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:29
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:21
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:21
msgid "out_dtype: str, optional"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_NCHWc:29
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:21
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:21
msgid "Output data type"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:6
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:20
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nhwc:9
msgid ""
"4-D with shape [filter_height, filter_width, in_channel, "
"channel_multiplier]"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_input_nhwc:9
#: tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_backward_weight_nhwc:9
msgid "Out_grad"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:9
msgid ""
"4-D with shape [in_channel, channel_multiplier, filter_height, "
"filter_width]"
msgstr ""

#: of tvm.topi.nn.depthwise_conv2d.depthwise_conv2d_nchw:12
msgid "The spatial stride, or (stride_height, stride_width)."
msgstr ""

#: of tvm.topi.nn.dilate.dilate:6
msgid "n-D, can be any layout."
msgstr ""

#: of tvm.topi.nn.dilate.dilate:-1 tvm.topi.nn.pad.mirror_pad:-1
#: tvm.topi.nn.pad.pad:-1
msgid "list / tuple of n ints"
msgstr ""

#: of tvm.topi.nn.dilate.dilate:9
msgid "Dilation stride on each dimension, 1 means no dilation."
msgstr ""

#: of tvm.topi.nn.dilate.dilate:12
msgid "dilation_value"
msgstr ""

#: of tvm.topi.nn.dilate.dilate:-1
msgid "int/float, optional"
msgstr ""

#: of tvm.topi.nn.dilate.dilate:12
msgid "Value used to dilate the input."
msgstr ""

#: of tvm.topi.nn.dilate.dilate:15 tvm.topi.nn.pad.mirror_pad:18
#: tvm.topi.nn.pad.pad:18
msgid "The name prefix operators generated"
msgstr ""

#: of tvm.topi.nn.dilate.dilate:20
msgid "n-D, the same layout as data."
msgstr ""

#: of tvm.topi.utils.equal_const_int:6 tvm.topi.utils.get_const_int:6
#: tvm.topi.utils.simplify:6
msgid "expr"
msgstr ""

#: of tvm.topi.utils.equal_const_int:6 tvm.topi.utils.get_const_int:6
msgid "The input expression."
msgstr ""

#: of tvm.topi.utils.equal_const_int:10
msgid "equal"
msgstr ""

#: of tvm.topi.utils.equal_const_int:11
msgid "Whether they equals."
msgstr ""

#: of tvm.topi.nn.softmax.fast_softmax:1
msgid ""
"Perform softmax activation on the data. Use approximation to compute "
"exponent for faster speed."
msgstr ""

#: of tvm.topi.nn.softmax.fast_softmax:7 tvm.topi.nn.softmax.softmax:6
msgid "can be any dimension"
msgstr ""

#: of tvm.topi.nn.softmax.fast_softmax:10 tvm.topi.nn.softmax.softmax:9
msgid "channel axis"
msgstr ""

#: of tvm.topi.nn.softmax.fast_softmax:15 tvm.topi.nn.softmax.softmax:14
msgid "output shape is the same as input"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:3
msgid "Compute equivalent of"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:12
msgid "Useful for"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:14
msgid ""
"Encoding explicit re-use of computation in convolution ops operated on a "
"sliding window input"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:15
msgid ""
"Implementing a FIFO queue to cache intermediate results, e.g. as in Fast "
"WaveNet."
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:22
msgid "Previous value of the FIFO buffer"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:24
msgid "Specify which axis should be used for buffering"
msgstr ""

#: of tvm.topi.nn.fifo_buffer.fifo_buffer:29
msgid "Updated value for the buffer"
msgstr ""

#: of tvm.topi.nn.flatten.flatten:6
msgid "Input array."
msgstr ""

#: of tvm.topi.nn.flatten.flatten:11
msgid "2-D array with collapsed higher dimensions."
msgstr ""

#: of tvm.topi.utils.get_const_int:-1
msgid "tvm.Expr or int"
msgstr ""

#: of tvm.topi.utils.get_const_int:10
msgid "out_value"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:9 tvm.topi.nn.utils.get_pad_tuple1d:9
#: tvm.topi.nn.utils.get_pad_tuple3d:9
#: tvm.topi.nn.utils.get_pad_tuple_generic:9
msgid "Conv kernel size"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:14 tvm.topi.nn.utils.get_pad_tuple3d:17
#: tvm.topi.nn.utils.get_pad_tuple_generic:14
msgid "pad_top"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:14 tvm.topi.nn.utils.get_pad_tuple3d:17
#: tvm.topi.nn.utils.get_pad_tuple_generic:14
msgid "Padding size on top"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:17 tvm.topi.nn.utils.get_pad_tuple1d:14
#: tvm.topi.nn.utils.get_pad_tuple3d:20
#: tvm.topi.nn.utils.get_pad_tuple_generic:20
msgid "pad_left"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:17 tvm.topi.nn.utils.get_pad_tuple1d:14
#: tvm.topi.nn.utils.get_pad_tuple3d:20
#: tvm.topi.nn.utils.get_pad_tuple_generic:20
msgid "Padding size on left"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:20 tvm.topi.nn.utils.get_pad_tuple3d:26
#: tvm.topi.nn.utils.get_pad_tuple_generic:17
msgid "pad_down"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:20 tvm.topi.nn.utils.get_pad_tuple3d:26
#: tvm.topi.nn.utils.get_pad_tuple_generic:17
msgid "Padding size on down."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:22 tvm.topi.nn.utils.get_pad_tuple1d:16
#: tvm.topi.nn.utils.get_pad_tuple3d:28
#: tvm.topi.nn.utils.get_pad_tuple_generic:22
msgid "pad_right"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple:23 tvm.topi.nn.utils.get_pad_tuple1d:17
#: tvm.topi.nn.utils.get_pad_tuple3d:29
#: tvm.topi.nn.utils.get_pad_tuple_generic:23
msgid "Padding size on right."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple3d:14
msgid "pad_front"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple3d:14
msgid "Padding size on front."
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple3d:23
msgid "pad_back"
msgstr ""

#: of tvm.topi.nn.utils.get_pad_tuple3d:23
msgid "Padding size on back."
msgstr ""

#: of tvm.topi.nn.pooling.global_pool:2 tvm.topi.nn.pooling.pool2d:2
#: tvm.topi.nn.pooling.pool_grad:2
msgid ""
"It decides the height and width dimension according to the layout string,"
" in which 'W' and 'H' means width and height respectively. Width and "
"height dimension cannot be split. For example, NCHW, NCHW16c, etc. are "
"valid for pool, while NCHW16w, NCHW16h are not. See parameter `layout` "
"for more information of the layout string convention."
msgstr ""

#: of tvm.topi.nn.pooling.global_pool:29
msgid ""
"n-D in same layout with height and width dimension size of 1. e.g., for "
"NCHW, the output shape will be [batch, channel, 1, 1]"
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_ncw:9
msgid "3-D with shape [num_filter, in_channel, filter_size]"
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_ncw:-1
#: tvm.topi.nn.conv1d.group_conv1d_nwc:-1
msgid "int, tuple, or str"
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_ncw:15
#: tvm.topi.nn.conv1d.group_conv1d_nwc:15
msgid ""
"Padding size can be an integer for equal padding, a tuple of (left, "
"right) or a string in ['VALID', 'SAME']."
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_ncw:22
#: tvm.topi.nn.conv1d.group_conv1d_nwc:22
msgid "Number of groups"
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_nwc:6
msgid "3-D with shape [batch, in_width, in_channel]"
msgstr ""

#: of tvm.topi.nn.conv1d.group_conv1d_nwc:9
msgid "3-D with shape [filter_size, in_channel, num_filter]"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nchw:9
msgid ""
"4-D with shape [num_filter, in_channel // groups, filter_height, "
"filter_width]"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nhwc:6
msgid "4-D with shape [batch, in_height, in_width, in_channel, ...]"
msgstr ""

#: of tvm.topi.nn.conv2d.group_conv2d_nhwc:9
msgid ""
"4-D with shape [filter_height, filter_width, in_channel // groups, "
"num_filter]"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.group_conv2d_transpose_nchw:9
msgid ""
"4-D with shape [in_channel, out_channel // groups, filter_height, "
"filter_width]"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:1
msgid ""
"Group normalization operator. It accepts fp16 and fp32 as input data "
"type. It will cast the input to fp32 to perform the computation. The "
"output will have the same data type as input."
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:8 tvm.topi.nn.group_norm.group_norm:31
#: tvm.topi.nn.instance_norm.instance_norm:6
#: tvm.topi.nn.instance_norm.instance_norm:24
#: tvm.topi.nn.layer_norm.layer_norm:8 tvm.topi.nn.layer_norm.layer_norm:25
#: tvm.topi.nn.rms_norm.rms_norm:6 tvm.topi.nn.rms_norm.rms_norm:20
msgid "N-D with shape (d_0, d_1, ..., d_{N-1})"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:11
#: tvm.topi.nn.instance_norm.instance_norm:9
#: tvm.topi.nn.layer_norm.layer_norm:11
msgid "gamma: tvm.te.Tensor"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:11
msgid "1-D with shape (r_0) where r_0 == d_{channel_axis}"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:14
#: tvm.topi.nn.instance_norm.instance_norm:12
#: tvm.topi.nn.layer_norm.layer_norm:14
msgid "beta: tvm.te.Tensor"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:14
msgid "Optional, 1-D with shape (r_0) where r_0 == d_{channel_axis}"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:17
msgid "num_groups"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:17
msgid "The number of groups"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:20
msgid "channel_axis"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:20
msgid "The channel axis"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:23
msgid "Axis over the normalization applied, excluding the channel axis"
msgstr ""

#: of tvm.topi.nn.group_norm.group_norm:26
#: tvm.topi.nn.instance_norm.instance_norm:19
#: tvm.topi.nn.layer_norm.layer_norm:20 tvm.topi.nn.rms_norm.rms_norm:15
msgid "The epsilon value to avoid division by zero."
msgstr ""

#: of tvm.topi.nn.instance_norm.instance_norm:9
#: tvm.topi.nn.layer_norm.layer_norm:11 tvm.topi.nn.rms_norm.rms_norm:9
msgid ""
"K-D with shape (r_0, r_1, ..., r_{K-1}) where K == len(axis) and "
"d_{axis_k} == r_k"
msgstr ""

#: of tvm.topi.nn.instance_norm.instance_norm:12
#: tvm.topi.nn.layer_norm.layer_norm:14
msgid ""
"Optional, K-D with shape (r_0, r_1, ..., r_{K-1}) where K == len(axis) "
"and d_{axis_k} == r_k"
msgstr ""

#: of tvm.topi.nn.instance_norm.instance_norm:15
msgid ""
"Axis over the normalization applied (the axis along which the mean and "
"variance are computed)"
msgstr ""

#: of tvm.topi.nn.layer_norm.layer_norm:1
msgid ""
"Layer normalization operator. It accepts fp16 and fp32 as input data "
"type. It will cast the input to fp32 to perform the computation. The "
"output will have the same data type as input."
msgstr ""

#: of tvm.topi.nn.layer_norm.layer_norm:17 tvm.topi.nn.rms_norm.rms_norm:12
msgid "Axis over the normalization applied"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.layout_transform:3
msgid ""
"E.g. layout_transform(t, \"NCHW\", \"CNHW\") --> relay.transpose(t, [1, "
"0, 2, 3])"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.layout_transform:8
msgid "tensor: relay.Expr"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.layout_transform:8
msgid "The Tensor to transpose"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.layout_transform:11
msgid "current_layout: str"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.layout_transform:11
msgid "The current layout e.g. NCHW or OIHW"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.layout_transform:14
msgid "desired_layout: str"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.layout_transform:14
msgid "The desired layout, must be compatible with current_layout"
msgstr ""

#: of tvm.topi.nn.conv2d_transpose.layout_transform:18
msgid "The layout_transformed tensor."
msgstr ""

#: of tvm.topi.nn.elemwise.leaky_relu:9 tvm.topi.nn.local_response_norm.lrn:24
msgid "alpha"
msgstr ""

#: of tvm.topi.nn.elemwise.leaky_relu:9
msgid "The slope for the small gradient when x < 0"
msgstr ""

#: of tvm.topi.nn.softmax.log_softmax:6
msgid "N-D input data"
msgstr ""

#: of tvm.topi.nn.softmax.log_softmax:11
msgid "N-D output with same shape"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:4
msgid ""
"sum_sqr_up^i{x, y} = (bias+((alpha/size)*"
"                                 {sum_{j=max(0, "
"i-size/2)}^{min(N-1,i+size/2)}                                      "
"(data^j{x,y})^2}))^beta output^i{x, y} = data^i{x, y}/sum_sqr_up^i{x, y} "
"N is the number for input channels"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:14
msgid "size"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:14
msgid "normalisation window size"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:17
msgid "input data layout channel axis default value is 1 for NCHW format"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:21
msgid "offset to avoid dividing by 0"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:24
msgid "to be divided"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:27
msgid "exponent"
msgstr ""

#: of tvm.topi.nn.local_response_norm.lrn:32
msgid "4-D output with same shape"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:5
msgid "Xs"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:6
msgid "Input sequence with shape `(seq_len, batch_size, in_dim)`"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:8
msgid "Wi"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:8
msgid ""
"Input weight matrix with shape `(4 * hidden_dim, in_dim)`. The weights "
"are packed according to `weight_layout`."
msgstr ""

#: of tvm.topi.nn.lstm.lstm:10
msgid "Wh"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:11
msgid ""
"Hidden weight matrix with shape `(4 * hidden_dim, hidden_dim or "
"proj_dim)`. Packed as `Wh`."
msgstr ""

#: of tvm.topi.nn.lstm.lstm:12
msgid "Bi"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:-1
msgid "te.Tensor, optional"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:13
msgid ""
"Input bias with shape `(4 * hidden_dim,)`, by default None. Packed as "
"`Wh`."
msgstr ""

#: of tvm.topi.nn.lstm.lstm:14
msgid "Bh"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:15
msgid "Hidden bias with shape as `Bi`, by default None. Packed as `Wh`."
msgstr ""

#: of tvm.topi.nn.lstm.lstm:16
msgid "h_init"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:17
msgid ""
"Initial hidden state with shape `(batch_size, hidden_dim or proj_dim)`, "
"zero if None"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:18
msgid "c_init"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:19
msgid "Initial cell state with same shape as `h_init`, zero if None"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:20
msgid "proj"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:21
msgid "Projection matrix with shape `(proj_dim, hidden_dim)`, by default None"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:22
msgid "p_i, p_f, p_o"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:23
msgid ""
"Peephole LSTM matrices with shape `(batch_size, hidden_dim)`, by default "
"None"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:24
msgid "f_act, g_act, h_act"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:-1
msgid "F, optional"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:25
msgid "Gate activation functions"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:26
msgid "reverse"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:27
msgid "Whether to process `Xs` in reverse, by default False"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:30
msgid "weight_layout"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:29
msgid ""
"The packed weight layout for gates, by default \"IFGO\". Note: I = input,"
" F = forget, G = cell, O = output."
msgstr ""

#: of tvm.topi.nn.lstm.lstm:-1
msgid "te.Tensor, te.Tensor"
msgstr ""

#: of tvm.topi.nn.lstm.lstm:35
msgid ""
"Tuple of hidden states (with shape `(seq_len, batch_size, hidden_dim or "
"proj_dim)`), and cell states (with shape `(seq_len, batch_size, "
"hidden_dim)`)."
msgstr ""

#: of tvm.topi.nn.dense.matmul:18
msgid "Whether the tensor_a is in transposed format."
msgstr ""

#: of tvm.topi.nn.dense.matmul:21
msgid "Whether the tensor_b is in transposed format."
msgstr ""

#: of tvm.topi.nn.dense.matmul_legalize:6
msgid "Attributes of current matmul"
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:9 tvm.topi.nn.pad.pad:9
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:15
msgid "pad_before"
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:9 tvm.topi.nn.pad.pad:9
msgid "Pad width on each dimension to pad the before the axis begin."
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:12 tvm.topi.nn.pad.pad:12
#: tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:19
msgid "pad_after"
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:-1 tvm.topi.nn.pad.pad:-1
msgid "list / tuple of n ints, optional"
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:12 tvm.topi.nn.pad.pad:12
msgid "Pad width each dimension to pad the after the axis end."
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:15
msgid "mode: str, optional"
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:15
msgid "Type of mirror padding to apply. Must be SYMMETRIC or REFLECT"
msgstr ""

#: of tvm.topi.nn.pad.mirror_pad:23 tvm.topi.nn.pad.pad:23
msgid "n-D, the same layout as Input."
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:6
msgid "output{n, i_1, i_2, ..., i_k} = -p * w"
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:6
msgid "where t = target{n, i_1, i_2, ..., i_k}"
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:5
msgid ""
"p = predictions{n, t, i_1, i_2, i_k} w = weights{n, i_1, i_2, ..., i_k} "
"if t != ignore_index else 0"
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:8
msgid "result = reduction(output)"
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:14
msgid "predictions"
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:13
msgid ""
"(k+2)-D with shape (N, C, d_1, d_2, ..., d_k), where C is the number of "
"target classes"
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:18
msgid "targets"
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:17
msgid "(k+1)-D with shape (N, d_1, d_2, ..., d_k) The target value of the input."
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:22
msgid "weights"
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:21
msgid "1-D with shape (C,) The weight of each target value."
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:25
msgid ""
"The reduction method to apply to output. Can be \"mean\", \"sum\" or "
"\"none\"."
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:29
msgid "ignore_index"
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:29
msgid "The target value to ignore."
msgstr ""

#: of tvm.topi.nn.loss.nll_loss:34
msgid ""
"a scalar if the reduction type is \"mean\" or \"sum\", otherwise the same"
" shape as `target`."
msgstr ""

#: of tvm.topi.nn.pad.pad:15 tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:22
msgid "pad_value"
msgstr ""

#: of tvm.topi.nn.pad.pad:15
msgid "The value to be padded."
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:2
msgid ""
"Width axis is determined according to the layout string. in which 'w' "
"means width. Width dimension cannot be split. For example, NCW, NCW16c, "
"etc. are valid for pool, while NCW16w is not. See parameter `layout` for "
"more information of the layout string convention."
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:-1
msgid "list/tuple of one int or int"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:15
msgid "Kernel size, [kernel_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:18
msgid "Stride size, [stride_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:21 tvm.topi.nn.pooling.pool2d:21
#: tvm.topi.nn.pooling.pool3d:21
msgid "dilation: list/tuple of two ints"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:21 tvm.topi.nn.pooling.pool2d:21
#: tvm.topi.nn.pooling.pool3d:21
msgid "Dilation size, [dilation_height, dilation_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:-1 tvm.topi.nn.pooling.pool2d:-1
#: tvm.topi.nn.pooling.pool_grad:-1
msgid "list/tuple of two ints"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:24
msgid "Pad size, [pad_left, pad_right]"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:30 tvm.topi.nn.pooling.pool2d:30
#: tvm.topi.nn.pooling.pool3d:30 tvm.topi.nn.pooling.pool_grad:30
msgid "ceil_mode"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:30 tvm.topi.nn.pooling.pool2d:30
#: tvm.topi.nn.pooling.pool3d:30 tvm.topi.nn.pooling.pool_grad:30
msgid "Whether to use ceil when calculating output size."
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:33
msgid ""
"Layout of the input data. The layout is supposed to be composed of upper "
"cases, lower cases and numbers, where upper case indicates a dimension "
"and the corresponding lower case with factor size indicates the split "
"dimension. For example, NCW16c can describe a 4-D tensor of [batch_size, "
"channel, width, channel_block], in which channel_block=16 is a split of "
"dimension channel."
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:42 tvm.topi.nn.pooling.pool2d:42
#: tvm.topi.nn.pooling.pool3d:42 tvm.topi.nn.pooling.pool_grad:42
msgid "count_include_pad: bool"
msgstr ""

#: of tvm.topi.nn.pooling.pool1d:42 tvm.topi.nn.pooling.pool2d:42
#: tvm.topi.nn.pooling.pool3d:42 tvm.topi.nn.pooling.pool_grad:42
msgid "Whether include padding in the calculation when pool_type is 'avg'"
msgstr ""

#: of tvm.topi.nn.pooling.pool2d:15 tvm.topi.nn.pooling.pool_grad:18
msgid "Kernel size, [kernel_height, kernel_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool2d:18 tvm.topi.nn.pooling.pool_grad:21
msgid "Stride size, [stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool2d:-1 tvm.topi.nn.pooling.pool_grad:-1
msgid "list/tuple of four ints"
msgstr ""

#: of tvm.topi.nn.pooling.pool2d:24 tvm.topi.nn.pooling.pool_grad:24
msgid "Pad size, [pad_top, pad_left, pad_bottom, pad_right]]"
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:2
msgid ""
"It decides the depth, height and width dimension according to the layout "
"string, in which 'D', 'W' and 'H' means depth, width and height "
"respectively. Depth, width and height dimension cannot be split. For "
"example, NCDHW, NCDHW16c, etc. are valid for pool, while NCDHW16d, "
"NCDHW16w, NCDHW16h are not. See parameter `layout` for more information "
"of the layout string convention."
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:-1
msgid "list/tuple of three ints"
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:15
msgid "Kernel size, [kernel_depth, kernel_height, kernel_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:18
msgid "Stride size, [stride_depth, stride_height, stride_width]"
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:-1
msgid "list/tuple of six ints"
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:24
msgid "Pad size, [pad_front, pad_top, pad_left, pad_back, pad_bottom, pad_right]"
msgstr ""

#: of tvm.topi.nn.pooling.pool3d:33
msgid ""
"Layout of the input data. The layout is supposed to be composed of upper "
"cases, lower cases and numbers, where upper case indicates a dimension "
"and the corresponding lower case with factor size indicates the split "
"dimension. For example, NCDHW16c can describe a 6-D tensor of "
"[batch_size, channel, depth, height, width, channel_block], in which "
"channel_block=16 is a split of dimension channel."
msgstr ""

#: of tvm.topi.nn.pooling.pool_grad:12
msgid "grads"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:1
msgid ""
"PReLU. It accepts two arguments: an input ``x`` and a weight array ``W`` "
"and computes the output as :math:`PReLU(x) y = x > 0 ? x : W * x`, where "
":math:`*` is an elementwise multiplication for each sample in the batch."
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:13
msgid "slope"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:13
msgid "Channelised slope tensor for prelu"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:16
msgid "The axis where the channel data needs to be applied"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:24
msgid "Links"
msgstr ""

#: of tvm.topi.nn.elemwise.prelu:25
msgid "[http://arxiv.org/pdf/1502.01852v1.pdf]"
msgstr ""

#: of tvm.topi.nn.qnn.qnn_dense_alter_layout:1
msgid "Change qnn.dense layout. Not to change by default"
msgstr ""

#: of tvm.topi.nn.qnn.qnn_dense_alter_layout:7
msgid "Attributes of current dense op"
msgstr ""

#: _functools.reduce:1 of
msgid ""
"Apply a function of two arguments cumulatively to the items of a sequence"
" or iterable, from left to right, so as to reduce the iterable to a "
"single value.  For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) "
"calculates ((((1+2)+3)+4)+5).  If initial is present, it is placed before"
" the items of the iterable in the calculation, and serves as a default "
"when the iterable is empty."
msgstr ""

#: of tvm.topi.nn.rms_norm.rms_norm:1
msgid ""
"Root mean square normalization operator. The output will have the same "
"data type as input."
msgstr ""

#: of tvm.topi.nn.rms_norm.rms_norm:9
msgid "weight: tvm.te.Tensor"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:6
msgid "4-D input tensor, NCHW layout [batch, channel, height, width]"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:9
#: tvm.topi.nn.mapping.scale_shift_nchwc:9
#: tvm.topi.nn.mapping.scale_shift_nhwc:9
msgid "Scale"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:9
#: tvm.topi.nn.mapping.scale_shift_nhwc:9
msgid "Scale tensor, 1-D of size channel number"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:12
#: tvm.topi.nn.mapping.scale_shift_nchwc:12
#: tvm.topi.nn.mapping.scale_shift_nhwc:12
msgid "Shift"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:12
#: tvm.topi.nn.mapping.scale_shift_nhwc:12
msgid "Shift tensor, 1-D of size channel number"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchw:17
msgid "Output tensor, layout is NCHW"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchwc:6
msgid ""
"5-D input tensor, NCHWc layout [batch, channel_chunk, height, width, "
"channel_block]"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchwc:9
msgid "Scale tensor, 2-D of size [channel_chunk, channel_block]"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchwc:12
msgid "Shift tensor, 2-D of size [channel_chunk, channel_block]"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nchwc:17
#: tvm.topi.nn.mapping.scale_shift_nhwc:17
msgid "Output tensor, layout is NHWC"
msgstr ""

#: of tvm.topi.nn.mapping.scale_shift_nhwc:6
msgid "4-D input tensor, NHWC layout [batch, height, width, channel]"
msgstr ""

#: of tvm.topi.utils.simplify:-1
msgid "Expr or int"
msgstr ""

#: of tvm.topi.utils.simplify:11
msgid "The simplified output"
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:1
msgid ""
"Simulated QNN dequantize operator that mimics QNN outputs without "
"changing datatype. The benefit of this operator over true QNN dequantize "
"is that this operator allows dynamic datatype selection and can operate "
"on both per-channel and scalar scales and zero points while QNN "
"dequantize requires both of these to be fixed at compile time."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:9
#: tvm.topi.nn.qnn.simulated_quantize:9
msgid "An N-D input tensor to the operator."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:14
msgid "in_dtype: tvm.te.Tensor"
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:12
msgid ""
"A scalar variable that indicates which datatype to simulate "
"dequantization with. Use SQNN_DTYPE_TO_CODE to convert a dtype string "
"into the corresponding variable value."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:18
msgid "input_scale: tvm.te.Tensor, optional"
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:17
msgid ""
"A scalar tensor representing the scale to use when dequantizing from "
"integer datatypes. When it contains more than a single value, N must "
"match the number of channels in data."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:22
msgid "input_zero_point: tvm.te.Tensor, optional"
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:21
msgid ""
"A 1-D tensor representing the zero point to use when dequantizing from "
"integer datatypes. When it contains more than a single value, N must "
"match the number of channels in data."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_dequantize:25
#: tvm.topi.nn.qnn.simulated_quantize:25
msgid ""
"The channel axis for quantization. Default value is -1 which corresponds "
"to the last axis."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_quantize:1
msgid ""
"Simulated QNN quantize operator that mimics QNN outputs without changing "
"datatype. The benefit of this operator over true QNN quantize is that "
"this operator allows dynamic datatype selection and can operate on both "
"per-channel and scalar scales and zero points while QNN quantize requires"
" both of these to be fixed at compile time."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_quantize:14
msgid "out_dtype: tvm.te.Tensor"
msgstr ""

#: of tvm.topi.nn.qnn.simulated_quantize:12
msgid ""
"A scalar variable that indicates which datatype to simulate quantization "
"with. Use SQNN_DTYPE_TO_CODE to convert a dtype string into the "
"corresponding variable value."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_quantize:18
msgid "output_scale: tvm.te.Tensor, optional"
msgstr ""

#: of tvm.topi.nn.qnn.simulated_quantize:17
msgid ""
"A scalar tensor representing the scale to use when quantizing to integer "
"datatypes. When it contains more than a single value, N must match the "
"number of channels in data."
msgstr ""

#: of tvm.topi.nn.qnn.simulated_quantize:22
msgid "output_zero_point: tvm.te.Tensor, optional"
msgstr ""

#: of tvm.topi.nn.qnn.simulated_quantize:21
msgid ""
"A 1-D tensor representing the zero point to use when quantizing to "
"integer datatypes. When it contains more than a single value, N must "
"match the number of channels in data."
msgstr ""

#: of tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:11
msgid "block_shape"
msgstr ""

#: of tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:14
msgid ""
"list of shape [M] where M is number of spatial dims, specifies zero-"
"padding size before each spatial dimension."
msgstr ""

#: of tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:18
msgid ""
"list of shape [M] where M is number of spatial dims, specifies zero-"
"padding size after each spatial dimension."
msgstr ""

#: of tvm.topi.nn.space_to_batch_nd.space_to_batch_nd:22
msgid "The value used for padding."
msgstr ""

#: of tvm.topi.nn.space_to_depth.space_to_depth:9
msgid "Size of blocks to decompose into channel dimension."
msgstr ""

#: of tvm.topi.nn.space_to_depth.space_to_depth:17
msgid "Output of shape [N, C * block_size**2, H / block_size, W / block_size]"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_add:6 tvm.topi.nn.sparse.sparse_conv2d:9
#: tvm.topi.nn.sparse.sparse_dense:10
msgid "dense_data"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_add:6 tvm.topi.nn.sparse.sparse_add:20
#: tvm.topi.nn.sparse.sparse_dense:30 tvm.topi.nn.sparse.sparse_dense_sp_lhs:24
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:24
msgid "2-D with shape [M, N]"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_add:9 tvm.topi.nn.sparse.sparse_conv2d:14
#: tvm.topi.nn.sparse.sparse_dense:14 tvm.topi.nn.sparse.sparse_transpose:8
msgid "sparse_data"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_add:9 tvm.topi.nn.sparse.sparse_add:12
msgid "1-D with shape [nnz] (CSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_add:15 tvm.topi.nn.sparse.sparse_conv2d:20
#: tvm.topi.nn.sparse.sparse_dense:22 tvm.topi.nn.sparse.sparse_transpose:14
msgid "sparse_indptr"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_add:15
msgid "1-D with shape [M + 1] (CSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_conv2d:7
msgid "4-D with shape ``[M, H, W, K]`` (layout=NHWC)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_conv2d:9
msgid "4-D with shape ``[M, K, H, W]`` (layout=NCHW)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_conv2d:12
msgid "2-D with shape ``[num_blocks, bs_r]`` (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_conv2d:14
msgid "3-D with shape ``[num_blocks, bs_r, bs_c]`` (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_conv2d:17
msgid "1-D with shape ``[num_blocks]`` (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_conv2d:20
msgid "1-D with shape ``[(N + 1) // bs_r]`` (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_conv2d:28
msgid ""
"4-D with shape [M, H, W, N] (layout=NHWC) 4-D with shape [M, N, H ,W] "
"(layout=NCHW)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:10
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:7
msgid "2-D with shape [M, K]"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:13
#: tvm.topi.nn.sparse.sparse_dense_sp_lhs:7
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:10
msgid ""
"1-D with shape [nnz] (CSR) or 3-D with shape [num_blocks, bs_r, bs_c] "
"(BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:17
#: tvm.topi.nn.sparse.sparse_dense_sp_lhs:11
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:14
msgid "1-D with shape [nnz] (CSR) or 1-D with shape [num_blocks] (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:21
#: tvm.topi.nn.sparse.sparse_dense_sp_rhs:18
msgid "1-D with shape [N + 1] (CSR) or 1-D with shape [(N + 1) // bs_r] (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:25
msgid "sparse_lhs"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense:25
msgid "Indicates whether lhs or rhs matrix is sparse. Default value is False."
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_alter_layout:3
msgid ""
"This is used for modifying the inputs weights so they are more amenable "
"for the target."
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_sp_lhs:8
msgid "data_data:"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_sp_lhs:12
msgid "data_indices:"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_sp_lhs:16
msgid "data_indptr:"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_sp_lhs:15
msgid "1-D with shape [M + 1] (CSR) or 1-D with shape [(M + 1) // bs_r] (BSR)"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_sp_lhs:19
msgid "weight:"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_sp_lhs:19
msgid "2-D with shape [N, K]"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_sp_rhs:11
msgid "weight_data"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_sp_rhs:15
msgid "weight_indices"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_dense_sp_rhs:19
msgid "weight_indptr"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:1
msgid ""
"Transpose a square sparse matrix, `A` is an n-by-n sparse matrix in the "
"CSR format. ** Currently only support Square Matrices **"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:8
#: tvm.topi.nn.sparse.sparse_transpose:19
msgid "1-D with shape [nonzeros]"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:11
#: tvm.topi.nn.sparse.sparse_transpose:22
msgid "1-D with shape [nonzeros], dtype of 'int32'"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:14
#: tvm.topi.nn.sparse.sparse_transpose:25
msgid "1-D with shape [n+1], dtype of 'int32'"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:19
msgid "out_data"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:22
msgid "out_indices"
msgstr ""

#: of tvm.topi.nn.sparse.sparse_transpose:24
msgid "out_indptr"
msgstr ""

#: of tvm.topi.nn.sparse.try_get_conv2d_sparse_input:6
#: tvm.topi.nn.sparse.try_get_sparse_input:6
msgid "args"
msgstr ""

#: of tvm.topi.nn.sparse.try_get_conv2d_sparse_input:-1
#: tvm.topi.nn.sparse.try_get_sparse_input:-1
msgid "List[Tensor]"
msgstr ""

#: of tvm.topi.nn.sparse.try_get_conv2d_sparse_input:6
#: tvm.topi.nn.sparse.try_get_sparse_input:6
msgid "Input/output Tensor of a TVM subgraph."
msgstr ""

#: of tvm.topi.nn.sparse.try_get_conv2d_sparse_input:11
#: tvm.topi.nn.sparse.try_get_sparse_input:11
msgid "Dict[Tensor, str] :"
msgstr ""

#: of tvm.topi.nn.sparse.try_get_conv2d_sparse_input:11
#: tvm.topi.nn.sparse.try_get_sparse_input:11
msgid "Map from the input Tensor to its buffer name."
msgstr ""

#: of tvm.topi.nn.sparse.try_get_conv2d_sparse_input:14
#: tvm.topi.nn.sparse.try_get_sparse_input:14
msgid "Notes"
msgstr ""

#: of tvm.topi.nn.sparse.try_get_conv2d_sparse_input:15
#: tvm.topi.nn.sparse.try_get_sparse_input:15
msgid ""
"The buffer name is specially designed, and these buffer should be "
"provided in `SearchTask(..., task_inputs={...})`."
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:6
msgid "packed_out"
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:6
msgid "The output tensor of conv2d_NCHWc."
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:9
msgid "The output dtype."
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:13
msgid "unpacked_out"
msgstr ""

#: of tvm.topi.nn.conv2d.unpack_NCHWc_to_nchw:14
msgid "The unpacked output tensor in NCHW layout."
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:2 tvm.topi.nn.upsampling.upsampling3d:2
msgid "Nearest neighbor and bilinear upsampling are supported."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:6 tvm.topi.image.resize.resize2d:6
#: tvm.topi.nn.upsampling.upsampling:7
msgid ""
"inputs is a 4-D tensor with shape [batch, channel, in_height, in_width] "
"or  [batch, in_height, in_width, channel]"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:12
#: tvm.topi.nn.upsampling.upsampling3d:15
msgid "scale_h"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:12
#: tvm.topi.nn.upsampling.upsampling3d:15
msgid "Scaling factor for height"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:15
#: tvm.topi.nn.upsampling.upsampling3d:18
msgid "scale_w"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:15
#: tvm.topi.nn.upsampling.upsampling3d:18
msgid "Scaling factor for width"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:18
msgid "either \"NCHW\" or \"NHWC\""
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:42
#: tvm.topi.image.resize.crop_and_resize:25
#: tvm.topi.nn.upsampling.upsampling:21 tvm.topi.nn.upsampling.upsampling3d:24
msgid "method"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:-1
msgid "{\"bilinear\", \"nearest_neighbor\", \"bicubic\"}"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:21
#: tvm.topi.nn.upsampling.upsampling3d:24
msgid "Method to be used for upsampling."
msgstr ""

#: of tvm.topi.image.resize.resize1d:53 tvm.topi.image.resize.resize2d:47
#: tvm.topi.image.resize.resize3d:47 tvm.topi.nn.upsampling.upsampling:25
#: tvm.topi.nn.upsampling.upsampling3d:34
msgid "output_shape: tvm.tir.container.Array, optional"
msgstr ""

#: of tvm.topi.image.resize.resize1d:52 tvm.topi.image.resize.resize2d:46
#: tvm.topi.image.resize.resize3d:46 tvm.topi.nn.upsampling.upsampling:24
#: tvm.topi.nn.upsampling.upsampling3d:33
msgid ""
"Shape to return. If left None will be inferred (If shape is determined "
"dynamically, pass out_dtype.shape as output_shape)"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling:30
msgid ""
"4-D with shape [batch, channel, in_height*scale_h, in_width*scale_w] or "
"[batch, in_height*scale, in_width*scale, channel]"
msgstr ""

#: of tvm.topi.image.resize.resize3d:6 tvm.topi.nn.upsampling.upsampling3d:7
msgid ""
"inputs is a 5-D tensor with shape [batch, channel, in_depth, in_height, "
"in_width] or  [batch, in_depth, in_height, in_width, channel]"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling3d:12
msgid "scale_d"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling3d:12
msgid "Scaling factor for depth"
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling3d:21
msgid "either \"NCDHW\" or \"NDHWC\""
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling3d:-1
msgid "{\"trilinear\", \"nearest_neighbor\"}"
msgstr ""

#: of tvm.topi.image.resize.resize1d:25 tvm.topi.nn.upsampling.upsampling3d:30
msgid "coordinate_transformation_mode: string, optional"
msgstr ""

#: of tvm.topi.image.resize.resize1d:22 tvm.topi.nn.upsampling.upsampling3d:27
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. Refer to the ONNX Resize operator "
"specification for details. Available options are \"half_pixel\", "
"\"align_corners\" and \"asymmetric\"."
msgstr ""

#: of tvm.topi.nn.upsampling.upsampling3d:39
msgid ""
"5-D with shape [batch, channel, in_depth*scale, in_height*scale, "
"in_width*scale] or [batch, in_depth*scale, in_height*scale, "
"in_width*scale, channel]"
msgstr ""

#: ../../notebook/docs/reference/api/python/topi.rst:35
msgid "tvm.topi.image"
msgstr ""

#: of tvm.topi.image:1
msgid "IMAGE network operators"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`affine_grid <tvm.topi.image.affine_grid>`\\ \\(data\\, "
"target\\_shape\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1
#: tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid "affine_grid operator that generates 2D sampling grid."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`can_convert_multiply_to_intdiv "
"<tvm.topi.image.can_convert_multiply_to_intdiv>`\\ \\(origin\\_size\\, "
"...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.can_convert_multiply_to_intdiv:1
msgid "Check whether can convert multiplication to division"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`crop_and_resize <tvm.topi.image.crop_and_resize>`\\ \\(data\\, "
"boxes\\, box\\_indices\\, ...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.crop_and_resize:1
msgid "Perform crop and resize operation on the data."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`dilation2d_nchw <tvm.topi.image.dilation2d_nchw>`\\ \\(input\\, "
"filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:1
#: tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid "Morphological dilation operator in NCHW layout."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`dilation2d_nhwc <tvm.topi.image.dilation2d_nhwc>`\\ \\(input\\, "
"filter\\, stride\\, ...\\)"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nhwc:1
#: tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid "Morphological 2d dilation NHWC layout."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`get_1d_indices <tvm.topi.image.get_1d_indices>`\\ "
"\\(indices\\[\\, layout\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_1d_indices:1
msgid "Get 1d indices"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`get_1d_pixel <tvm.topi.image.get_1d_pixel>`\\ \\(data\\, "
"layout\\, image\\_width\\, n\\, ...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_1d_pixel:1
msgid "Get 1d pixel"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`get_2d_indices <tvm.topi.image.get_2d_indices>`\\ "
"\\(indices\\[\\, layout\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_2d_indices:1
msgid "Get 2d indices"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`get_2d_pixel <tvm.topi.image.get_2d_pixel>`\\ \\(data\\, "
"layout\\, image\\_height\\, ...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_2d_pixel:1
msgid "Get 2d pixel"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`get_3d_indices <tvm.topi.image.get_3d_indices>`\\ "
"\\(indices\\[\\, layout\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_3d_indices:1
msgid "Get 3d indices"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`get_3d_pixel <tvm.topi.image.get_3d_pixel>`\\ \\(data\\, "
"layout\\, image\\_depth\\, ...\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_3d_pixel:1
msgid "Get 3d pixel"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`get_closest_index <tvm.topi.image.get_closest_index>`\\ "
"\\(in\\_x\\, rounding\\_method\\, boxes\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_closest_index:1
msgid "get the closest index to a value based on a certain rounding method"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`get_inx <tvm.topi.image.get_inx>`\\ \\(x\\, image\\_width\\, "
"target\\_width\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.get_inx:1
msgid "Infer input x from output x with various coordinate transformation methods"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`get_pad_tuple <tvm.topi.image.get_pad_tuple>`\\ \\(padding\\, "
"kernel\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`grid_sample <tvm.topi.image.grid_sample>`\\ \\(data\\, "
"grid\\[\\, method\\, layout\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.grid_sample.grid_sample:1
msgid "Applies grid sampling to input feature map."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`nchw_pack_layout <tvm.topi.image.nchw_pack_layout>`\\ "
"\\(layout\\_info\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.utils.nchw_pack_layout:1
msgid "Check whether the layout type is NCHWinic"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`nchw_xc_layout <tvm.topi.image.nchw_xc_layout>`\\ "
"\\(layout\\_info\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.utils.nchw_xc_layout:1
msgid "Check whether the layout type is NCHWxc"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`pad <tvm.topi.image.pad>`\\ \\(data\\, pad\\_before\\[\\, "
"pad\\_after\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`resize1d <tvm.topi.image.resize1d>`\\ \\(data\\, roi\\, "
"size\\[\\, layout\\, method\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
#: tvm.topi.image.resize.resize1d:1 tvm.topi.image.resize.resize2d:1
#: tvm.topi.image.resize.resize3d:1
msgid "Perform resize operation on the data."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`resize2d <tvm.topi.image.resize2d>`\\ \\(data\\, roi\\, "
"size\\[\\, layout\\, method\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ""
":py:obj:`resize3d <tvm.topi.image.resize3d>`\\ \\(data\\, roi\\, "
"size\\[\\, layout\\, method\\, ...\\]\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:1:<autosummary>:1
msgid ":py:obj:`simplify <tvm.topi.image.simplify>`\\ \\(expr\\)"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:3
msgid ""
"This operation is described in https://arxiv.org/pdf/1506.02025.pdf. It "
"generates a uniform sampling grid within the target shape and normalizes "
"it to [-1, 1]. The provided affine transformation is then applied on the "
"sampling grid."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:-1
#: tvm.topi.image.grid_sample.grid_sample:-1
msgid "tvm.Tensor"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:10
msgid "3-D with shape [batch, 2, 3]. The affine matrix."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:13
msgid "target_shape: list/tuple of two int"
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:13
msgid "Specifies the output shape (H, W)."
msgstr ""

#: of tvm.topi.image.grid_sample.affine_grid:18
msgid "4-D with shape [batch, 2, target_height, target_width]"
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:12
msgid "boxes"
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:11
msgid ""
"A 2-D tensor of shape [num_boxes, 4]. Each row of the tensor specifies "
"the coordinates of a box."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:16
msgid "box_indices"
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:15
msgid ""
"A 1-D tensor of shape [num_boxes], box_indices[i] specifies the data that"
" the i-th box refers to."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:19
msgid "crop_size"
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:-1
msgid "Tuple"
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:19
msgid "The target size of each box."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:22
msgid "\"NCHW\", \"NHWC\""
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:-1
msgid "{\"bilinear\", \"nearest_neighbor\"}"
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:25
msgid "Method to be used for resizing."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:28
#: tvm.topi.image.resize.resize1d:46 tvm.topi.image.resize.resize2d:40
#: tvm.topi.image.resize.resize3d:40
msgid "extrapolation_value: float, optional"
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:28
#: tvm.topi.image.resize.resize1d:46 tvm.topi.image.resize.resize2d:40
#: tvm.topi.image.resize.resize3d:40
msgid "Value used for extrapolation, when applicable."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:31
#: tvm.topi.image.resize.resize1d:49 tvm.topi.image.resize.resize2d:43
#: tvm.topi.image.resize.resize3d:43
msgid "Type to return. If left None will be same as input type."
msgstr ""

#: of tvm.topi.image.resize.crop_and_resize:36
msgid ""
"4-D with shape [num_boxes, channel, crop_height, crop_width] or "
"[num_boxes, crop_height, crop_width, channel]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:9
msgid "3-D with shape [ in_channel, filter_height, filter_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:15
#: tvm.topi.image.dilation2d.dilation2d_nhwc:15
msgid "Padding size"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:18
#: tvm.topi.image.dilation2d.dilation2d_nhwc:18
msgid "dilations: int or a list/tuple of two ints"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nchw:26
msgid "4-D with shape [batch, in_channel, out_height, out_width]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nhwc:9
msgid "3-D with shape [filter_height, filter_width, in_channel]"
msgstr ""

#: of tvm.topi.image.dilation2d.dilation2d_nhwc:26
msgid "4-D with shape [batch, out_height, out_width, in_channel]"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:3
msgid ""
"Given :math:`data` and :math:`grid`, then for 4-D the output is computed "
"by"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:5
msgid ""
"x_{src} = grid[batch, 0, y_{dst}, x_{dst}] \\\n"
"y_{src} = grid[batch, 1, y_{dst}, x_{dst}] \\\n"
"output[batch, channel, y_{dst}, x_{dst}] = G(data[batch, channel, "
"y_{src}, x_{src}])"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:11
msgid ""
":math:`x_{dst}`, :math:`y_{dst}` enumerate all spatial locations in "
":math:`output`, and :math:`G()` denotes the interpolation function."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:14
msgid ""
"The out-boundary points will be padded with zeros if padding_mode is "
"\"zeros\", or border pixel value if padding_mode is \"border\", or inner "
"pixel value if padding_mode is \"reflection\"."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:18
msgid ""
"The left-top corner (-1, -1) and right-bottom corner (1, 1) in grid will "
"be map to (0, 0) and (h - 1, w - 1) of data if align_corners is \"True\","
" or (-0.5, -0.5) and (h - 0.5, w - 0.5) of data if align_corners is "
"\"False\"."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:22
msgid ""
"The shape of the output will be 4-D (data.shape[0], data.shape[1], "
"grid.shape[2], grid.shape[3]), or 5-D (data.shape[0], data.shape[1], "
"grid.shape[2], grid.shape[3], grid.shape[4])."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:26
msgid "The operator assumes that :math:`grid` has been normalized to [-1, 1]."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:28
msgid ""
"grid_sample often cooperates with affine_grid which generates sampling "
"grids for grid_sample."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:33
msgid ""
"4-D with shape [batch, in_channel, in_height, in_width], or 5-D with "
"shape [batch, in_channel, in_depth, in_height, in_width]"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:38
msgid "grid"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:37
msgid ""
"4-D with shape [batch, 2, out_height, out_width], or 5-D with shape "
"[batch, 3, out_depth, out_height, out_width]"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:41
msgid ""
"The interpolation method, 4-D \"nearest\", \"bilinear\", \"bicubic\" and "
"5-D \"nearest\", \"bilinear\"(\"trilinear\") are supported."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:45
msgid "The layout of input data and the output."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:48
msgid "padding_mode"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:48
msgid ""
"The padding mode for outside grid values, \"zeros\", \"border\", "
"\"reflection\" are supported."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:55
msgid "align_corners: bool"
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:51
msgid ""
"Geometrically, we consider the pixels of the input as squares rather than"
" points. If set to \"True\", the extrema (\"-1\" and \"1\") are "
"considered as referring to the center points of the input corner pixels. "
"If set to \"False\", they are instead considered as referring to the "
"corner points of the input corner pixels, making the sampling more "
"resolution agnostic."
msgstr ""

#: of tvm.topi.image.grid_sample.grid_sample:60
msgid ""
"4-D with shape [batch, in_channel, out_height, out_width], or 5-D with "
"shape [batch, in_channel, out_depth, out_height, out_width]"
msgstr ""

#: of tvm.topi.image.resize.resize1d:6
msgid ""
"inputs is a 3-D tensor with shape [batch, channel in_width] or  [batch "
"in_width, channel]"
msgstr ""

#: of tvm.topi.image.resize.resize1d:13 tvm.topi.image.resize.resize2d:13
#: tvm.topi.image.resize.resize3d:13
msgid "roi: Tuple of Float or Expr"
msgstr ""

#: of tvm.topi.image.resize.resize1d:11
msgid ""
"The region of interest for cropping the input image. Expected to be of "
"size 2, and format [start_w, end_w]. Only used if "
"coordinate_transformation_mode is tf_crop_and_resize."
msgstr ""

#: of tvm.topi.image.resize.resize1d:16 tvm.topi.image.resize.resize2d:16
#: tvm.topi.image.resize.resize3d:16
msgid "size: Tuple"
msgstr ""

#: of tvm.topi.image.resize.resize1d:16 tvm.topi.image.resize.resize2d:16
#: tvm.topi.image.resize.resize3d:16
msgid "Output resolution scale to"
msgstr ""

#: of tvm.topi.image.resize.resize1d:19 tvm.topi.image.resize.resize2d:19
#: tvm.topi.image.resize.resize3d:19
msgid "layout: string, optional"
msgstr ""

#: of tvm.topi.image.resize.resize1d:19
msgid "\"NCW\", \"NWC\", or \"NCWc\"."
msgstr ""

#: of tvm.topi.image.resize.resize1d:28 tvm.topi.image.resize.resize2d:22
#: tvm.topi.image.resize.resize3d:22
msgid "method: string, optional"
msgstr ""

#: of tvm.topi.image.resize.resize1d:28 tvm.topi.image.resize.resize2d:22
#: tvm.topi.image.resize.resize3d:22
msgid "method of interpolation (\"nearest\", \"linear\", \"bicubic\")"
msgstr ""

#: of tvm.topi.image.resize.resize1d:34 tvm.topi.image.resize.resize2d:28
#: tvm.topi.image.resize.resize3d:28
msgid "coordinate_transformation_mode"
msgstr ""

#: of tvm.topi.image.resize.resize1d:31 tvm.topi.image.resize.resize2d:25
#: tvm.topi.image.resize.resize3d:25
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. [half_pixel, align_corners, "
"asymmetric, pytorch_half_pixel, tf_half_pixel_for_nn, and "
"tf_crop_and_resize]."
msgstr ""

#: of tvm.topi.image.resize.resize1d:37 tvm.topi.image.resize.resize2d:31
#: tvm.topi.image.resize.resize3d:31
msgid "rounding_method:"
msgstr ""

#: of tvm.topi.image.resize.resize1d:37 tvm.topi.image.resize.resize2d:31
#: tvm.topi.image.resize.resize3d:31
msgid "Method for rounding coordinate locations"
msgstr ""

#: of tvm.topi.image.resize.resize1d:40 tvm.topi.image.resize.resize2d:34
#: tvm.topi.image.resize.resize3d:34
msgid "bicubic_alpha: float, optional"
msgstr ""

#: of tvm.topi.image.resize.resize1d:40 tvm.topi.image.resize.resize2d:34
#: tvm.topi.image.resize.resize3d:34
msgid "Bicubic spline coefficient"
msgstr ""

#: of tvm.topi.image.resize.resize1d:43 tvm.topi.image.resize.resize2d:37
#: tvm.topi.image.resize.resize3d:37
msgid "bicubic_exclude: bool, optional:"
msgstr ""

#: of tvm.topi.image.resize.resize1d:43 tvm.topi.image.resize.resize2d:37
#: tvm.topi.image.resize.resize3d:37
msgid "Exclude values outside the image fdor bicubic interpolation"
msgstr ""

#: of tvm.topi.image.resize.resize1d:49 tvm.topi.image.resize.resize2d:43
#: tvm.topi.image.resize.resize3d:43
msgid "out_dtype: string, optional"
msgstr ""

#: of tvm.topi.image.resize.resize1d:58
msgid ""
"4-D with shape [batch, chananel, in_width*scale] or [batch, "
"in_width*scale, channel] or 5-D with shape [batch, channel-major, "
"in_width*scale, channel-minor]"
msgstr ""

#: of tvm.topi.image.resize.resize2d:11
msgid ""
"The region of interest for cropping the input image. Expected to be of "
"size 4, and format [start_h, start_w, end_h, end_w]. Only used if "
"coordinate_transformation_mode is tf_crop_and_resize."
msgstr ""

#: of tvm.topi.image.resize.resize2d:19
msgid "\"NCHW\", \"NHWC\", or \"NCHWc\"."
msgstr ""

#: of tvm.topi.image.resize.resize2d:52
msgid ""
"4-D with shape [batch, channel, in_height*scale, in_width*scale] or "
"[batch, in_height*scale, in_width*scale, channel] or 5-D with shape "
"[batch, channel-major, in_height*scale, in_width*scale, channel-minor]"
msgstr ""

#: of tvm.topi.image.resize.resize3d:11
msgid ""
"The region of interest for cropping the input image. Expected to be of "
"size 6, and format [start_d, start_h, start_w, end_d, end_h, end_w]. Only"
" used if coordinate_transformation_mode is tf_crop_and_resize."
msgstr ""

#: of tvm.topi.image.resize.resize3d:19
msgid "\"NCDHW\", \"NDHWC\", or \"NCDHWc\"."
msgstr ""

#: of tvm.topi.image.resize.resize3d:52
msgid ""
"4-D with shape [batch, channel, in_depth*scale, in_height*scale, "
"in_width*scale] or [batch, in_depth*scale, in_height*scale, "
"in_width*scale, channel] or 5-D with shape [batch, channel-major, "
"in_depth*scale, in_height*scale, in_width*scale, channel-minor]"
msgstr ""

#: ../../notebook/docs/reference/api/python/topi.rst:43
msgid "tvm.topi.sparse"
msgstr ""

#: of tvm.topi.sparse:1
msgid "Sparse operators"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:1:<autosummary>:1
msgid ":py:obj:`csrmm <tvm.topi.sparse.csrmm>`\\ \\(a\\, b\\[\\, c\\]\\)"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:1
#: tvm.topi.sparse.csrmm.csrmm:1:<autosummary>:1
msgid ""
"The `csrmm` routine performs a matrix-matrix operation defined as "
":math:`C := A*B + C`, where `B` and `C` are dense matrices, `A` is an "
"m-by-k sparse matrix in the CSR format."
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:1:<autosummary>:1
msgid ":py:obj:`csrmv <tvm.topi.sparse.csrmv>`\\ \\(a\\, x\\[\\, y\\]\\)"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:1:<autosummary>:1
#: tvm.topi.sparse.csrmv.csrmv:1
msgid ""
"The `csrmv` routine performs a matrix-vector operation defined as "
":math:`y := A*x + y`, where `x` and `y` are vectors, `A` is an m-by-k "
"sparse matrix in the CSR format."
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:1:<autosummary>:1
msgid ""
":py:obj:`dense <tvm.topi.sparse.dense>`\\ \\(data\\, weight\\[\\, "
"bias\\]\\)"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:1:<autosummary>:1
msgid "Applies a linear transformation: :math:`Y = XW^T + b`."
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:-1 tvm.topi.sparse.csrmv.csrmv:-1
msgid "tvm.contrib.sparse.CSRNDArray"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:7 tvm.topi.sparse.csrmv.csrmv:7
msgid "2-D sparse matrix with shape [m, k]"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:10
msgid "2-D dense matrix with shape [k, n]"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:13
msgid "c"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:13
msgid "1-D dense vector with shape [n]"
msgstr ""

#: of tvm.topi.sparse.csrmm.csrmm:18
msgid "2-D with shape [m, n]"
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:10
msgid "2-D dense matrix with shape [k, 1]"
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:13
msgid "1-D dense vector with shape [1]"
msgstr ""

#: of tvm.topi.sparse.csrmv.csrmv:18
msgid "2-D dense matrix with shape [m, 1]"
msgstr ""

#: of tvm.topi.sparse.dense.dense:1
msgid ""
"Applies a linear transformation: :math:`Y = XW^T + b`. Either data or "
"weight should be tvm.contrib.sparse.CSRNDArray."
msgstr ""

#: of tvm.topi.sparse.dense.dense:-1
msgid "tvm.contrib.sparse.CSRNDArray or te.tensor.Tensor"
msgstr ""

#: of tvm.topi.sparse.dense.dense:-1
msgid "te.tensor.Tensor or tvm.contrib.sparse.CSRNDArray"
msgstr ""

#: of tvm.topi.sparse.dense.dense:-1
msgid "te.tensor.Tensor, optional"
msgstr ""

#~ msgid ""
#~ ":py:obj:`sliding_window <tvm.topi.tvm.topi.sliding_window>`\\"
#~ " \\(data\\, axis\\, window\\_shape\\, strides\\)"
#~ msgstr ""

#~ msgid "Slide a window over the data tensor."
#~ msgstr ""

#~ msgid ""
#~ "What axis the window begins sliding "
#~ "over. Window will be slid over "
#~ "this axis and all following axes. "
#~ "The axis value determines the window "
#~ "shape (and thus, the number of "
#~ "strides): window shape and strides must"
#~ " both be of length `data.ndim-axis`."
#~ msgstr ""

#~ msgid ""
#~ "The window shape to form over the"
#~ " input. Window shape must be of "
#~ "length `data.ndim-axis`."
#~ msgstr ""

#~ msgid ""
#~ "How to stride the window along "
#~ "each dimension. Strides must be of "
#~ "length `data.ndim-axis`."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`grid_sample "
#~ "<tvm.topi.image.tvm.topi.image.grid_sample>`\\ \\(data\\, "
#~ "grid\\[\\, method\\, layout\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":math:`x_{dst}`, :math:`y_{dst}` enumerate all "
#~ "spatial locations in :math:`output`, and "
#~ ":math:`G()` denotes the interpolation method."
#~ " The out-boundary points will be "
#~ "padded with zeros. The shape of "
#~ "the output will be (data.shape[0], "
#~ "data.shape[1], grid.shape[2], grid.shape[3])."
#~ msgstr ""

#~ msgid "**Output** -- 4-D with shape [batch, 2, out_height, out_width]"
#~ msgstr ""

#~ msgid "TVM Operator Inventory."
#~ msgstr ""

#~ msgid ""
#~ "TOPI is the operator collection library"
#~ " for TVM, to provide sugars for "
#~ "constructing compute declaration as well "
#~ "as optimized schedules."
#~ msgstr ""

#~ msgid ""
#~ "Some of the schedule function may "
#~ "have been specially optimized for a "
#~ "specific workload."
#~ msgstr ""

#~ msgid "**Classes:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AssertStmt <tvm.topi.tvm.topi.AssertStmt>`\\ "
#~ "\\(condition\\, message\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "AssertStmt node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Cast <tvm.topi.tvm.topi.Cast>`\\ \\(dtype\\, "
#~ "value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Cast expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Evaluate <tvm.topi.tvm.topi.Evaluate>`\\ "
#~ "\\(value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Evaluate node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`StringImm <tvm.topi.tvm.topi.StringImm>`\\ "
#~ "\\(value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "String constant."
#~ msgstr ""

#~ msgid "**Exceptions:**"
#~ msgstr ""

#~ msgid ":py:obj:`InvalidShapeError <tvm.topi.tvm.topi.InvalidShapeError>`\\"
#~ msgstr ""

#~ msgid "Invalid shape for a topi function."
#~ msgstr ""

#~ msgid "**Functions:**"
#~ msgstr ""

#~ msgid ":py:obj:`abs <tvm.topi.tvm.topi.abs>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take absolute value of the input of x, element-wise."
#~ msgstr ""

#~ msgid ":py:obj:`acos <tvm.topi.tvm.topi.acos>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take arc cos of input x."
#~ msgstr ""

#~ msgid ":py:obj:`acosh <tvm.topi.tvm.topi.acosh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take arc cosh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`add <tvm.topi.tvm.topi.add>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Addition with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adv_index <tvm.topi.tvm.topi.adv_index>`\\ "
#~ "\\(data\\, indices\\)"
#~ msgstr ""

#~ msgid "Numpy style indexing with tensors."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`all <tvm.topi.tvm.topi.all>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\]\\)"
#~ msgstr ""

#~ msgid "Logical AND of array elements over a given axis or a list of axes"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`any <tvm.topi.tvm.topi.any>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\]\\)"
#~ msgstr ""

#~ msgid "Logical OR of array elements over a given axis or a list of axes"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`arange <tvm.topi.tvm.topi.arange>`\\ "
#~ "\\(start\\[\\, stop\\, step\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Creates a tensor with evenly spaced values within a given interval."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`argmax <tvm.topi.tvm.topi.argmax>`\\ "
#~ "\\(data\\[\\, axis\\, keepdims\\, "
#~ "select\\_last\\_index\\]\\)"
#~ msgstr ""

#~ msgid "Returns the indices of the maximum values along an axis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`argmin <tvm.topi.tvm.topi.argmin>`\\ "
#~ "\\(data\\[\\, axis\\, keepdims\\, "
#~ "select\\_last\\_index\\]\\)"
#~ msgstr ""

#~ msgid "Returns the indices of the minimum values along an axis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`argsort <tvm.topi.tvm.topi.argsort>`\\ "
#~ "\\(data\\[\\, valid\\_count\\, axis\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Performs sorting along the given axis"
#~ " and returns an array of indices "
#~ "having the same shape as an input"
#~ " array that index data in sorted "
#~ "order."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`argwhere <tvm.topi.tvm.topi.argwhere>`\\ "
#~ "\\(output\\_shape\\, condition\\)"
#~ msgstr ""

#~ msgid "Find the indices of elements of a tensor that are non-zero."
#~ msgstr ""

#~ msgid ":py:obj:`asin <tvm.topi.tvm.topi.asin>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take arc sin of input x."
#~ msgstr ""

#~ msgid ":py:obj:`asinh <tvm.topi.tvm.topi.asinh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take arc sinh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`atan <tvm.topi.tvm.topi.atan>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take atan of input x."
#~ msgstr ""

#~ msgid ":py:obj:`atanh <tvm.topi.tvm.topi.atanh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take atanh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`binary_search <tvm.topi.tvm.topi.binary_search>`\\ "
#~ "\\(ib\\, sequence\\_offset\\, ...\\)"
#~ msgstr ""

#~ msgid "Common IR generator for binary search used by CPU and GPU backends."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitwise_and <tvm.topi.tvm.topi.bitwise_and>`\\ "
#~ "\\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute element-wise bitwise and of data."
#~ msgstr ""

#~ msgid ":py:obj:`bitwise_not <tvm.topi.tvm.topi.bitwise_not>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise bitwise not of data."
#~ msgstr ""

#~ msgid ":py:obj:`bitwise_or <tvm.topi.tvm.topi.bitwise_or>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute element-wise bitwise or of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitwise_xor <tvm.topi.tvm.topi.bitwise_xor>`\\ "
#~ "\\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute element-wise bitwise xor of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`broadcast_to <tvm.topi.tvm.topi.broadcast_to>`\\ "
#~ "\\(data\\, shape\\)"
#~ msgstr ""

#~ msgid "Broadcast the src to the target shape"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cast <tvm.topi.tvm.topi.cast>`\\ \\(x\\, "
#~ "dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Cast input to specified data type."
#~ msgstr ""

#~ msgid ":py:obj:`ceil <tvm.topi.tvm.topi.ceil>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take ceil of input x."
#~ msgstr ""

#~ msgid ":py:obj:`ceil_log2 <tvm.topi.tvm.topi.ceil_log2>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid ""
#~ "Compute integer ceil log2 with a "
#~ "special code path for vulkan SPIR-V "
#~ "does not support log2 on fp64."
#~ msgstr ""

#~ msgid ":py:obj:`clip <tvm.topi.tvm.topi.clip>`\\ \\(x\\, a\\_min\\, a\\_max\\)"
#~ msgstr ""

#~ msgid "Clip (limit) the values in an array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`concatenate <tvm.topi.tvm.topi.concatenate>`\\ "
#~ "\\(a\\_tuple\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Join a sequence of arrays along an existing axis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`const_vector <tvm.topi.tvm.topi.const_vector>`\\ "
#~ "\\(vector\\[\\, name\\]\\)"
#~ msgstr ""

#~ msgid "convert a const numpy 1-dimensional vector to tvm tensor"
#~ msgstr ""

#~ msgid ":py:obj:`cos <tvm.topi.tvm.topi.cos>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take cos of input x."
#~ msgstr ""

#~ msgid ":py:obj:`cosh <tvm.topi.tvm.topi.cosh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take cosh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cumprod <tvm.topi.tvm.topi.cumprod>`\\ "
#~ "\\(data\\[\\, axis\\, dtype\\, exclusive\\]\\)"
#~ msgstr ""

#~ msgid "Numpy style cumprod op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cumsum <tvm.topi.tvm.topi.cumsum>`\\ "
#~ "\\(data\\[\\, axis\\, dtype\\, exclusive\\]\\)"
#~ msgstr ""

#~ msgid "Numpy style cumsum op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`decl_buffer <tvm.topi.tvm.topi.decl_buffer>`\\ "
#~ "\\(shape\\[\\, dtype\\, name\\, data\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Declare a new symbolic buffer."
#~ msgstr ""

#~ msgid ":py:obj:`div <tvm.topi.tvm.topi.div>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute a / b as in C/C++ semantics."
#~ msgstr ""

#~ msgid ":py:obj:`divide <tvm.topi.tvm.topi.divide>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Division with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`einsum <tvm.topi.tvm.topi.einsum>`\\ "
#~ "\\(subscripts\\, \\*operand\\)"
#~ msgstr ""

#~ msgid "Evaluates the Einstein summation convention on the operands."
#~ msgstr ""

#~ msgid ":py:obj:`elemwise_sum <tvm.topi.tvm.topi.elemwise_sum>`\\ \\(xs\\)"
#~ msgstr ""

#~ msgid "Perform element-wise sum on inputs"
#~ msgstr ""

#~ msgid ":py:obj:`equal <tvm.topi.tvm.topi.equal>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute (lhs==rhs) with auto-broadcasting"
#~ msgstr ""

#~ msgid ":py:obj:`erf <tvm.topi.tvm.topi.erf>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take gauss error function of input x."
#~ msgstr ""

#~ msgid ":py:obj:`exp <tvm.topi.tvm.topi.exp>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take exponential of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`expand_dims <tvm.topi.tvm.topi.expand_dims>`\\ "
#~ "\\(a\\, axis\\[\\, num\\_newaxis\\]\\)"
#~ msgstr ""

#~ msgid "Expand the shape of an array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`expand_like <tvm.topi.tvm.topi.expand_like>`\\ "
#~ "\\(a\\, shape\\_like\\, axis\\)"
#~ msgstr ""

#~ msgid "Expand an input array with the shape of second array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`extern <tvm.topi.tvm.topi.extern>`\\ \\(shape\\,"
#~ " inputs\\, fcompute\\[\\, name\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Compute several tensors via an extern function."
#~ msgstr ""

#~ msgid ":py:obj:`fast_erf <tvm.topi.tvm.topi.fast_erf>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take gauss error function of input x using fast_erf implementation."
#~ msgstr ""

#~ msgid ":py:obj:`fast_exp <tvm.topi.tvm.topi.fast_exp>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take exponential of input x using fast_exp implementation"
#~ msgstr ""

#~ msgid ":py:obj:`fast_tanh <tvm.topi.tvm.topi.fast_tanh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take hyperbolic tangent of input x using fast_tanh implementation"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`fixed_point_multiply "
#~ "<tvm.topi.tvm.topi.fixed_point_multiply>`\\ \\(x\\, "
#~ "multiplier\\, shift\\)"
#~ msgstr ""

#~ msgid ""
#~ "Fixed point multiplication between data "
#~ "and a fixed point constant expressed "
#~ "as multiplier * 2^(-shift), where "
#~ "multiplier is a Q-number with 31 "
#~ "fractional bits"
#~ msgstr ""

#~ msgid ":py:obj:`flip <tvm.topi.tvm.topi.flip>`\\ \\(a\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Flip/reverse elements of an array in a particular axis."
#~ msgstr ""

#~ msgid ":py:obj:`floor <tvm.topi.tvm.topi.floor>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take floor of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`floor_divide <tvm.topi.tvm.topi.floor_divide>`\\ "
#~ "\\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Floor division with auto-broadcasting"
#~ msgstr ""

#~ msgid ":py:obj:`floor_mod <tvm.topi.tvm.topi.floor_mod>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Floor modulus with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`floordiv <tvm.topi.tvm.topi.floordiv>`\\ \\(a\\,"
#~ " b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the floordiv of two expressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`floormod <tvm.topi.tvm.topi.floormod>`\\ \\(a\\,"
#~ " b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the floormod of two expressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`full <tvm.topi.tvm.topi.full>`\\ \\(shape\\, "
#~ "dtype\\, fill\\_value\\)"
#~ msgstr ""

#~ msgid "Fill tensor with fill_value"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`full_like <tvm.topi.tvm.topi.full_like>`\\ "
#~ "\\(x\\, fill\\_value\\)"
#~ msgstr ""

#~ msgid "Construct a tensor with same shape as input tensor,"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`gather <tvm.topi.tvm.topi.gather>`\\ \\(data\\,"
#~ " axis\\, indices\\)"
#~ msgstr ""

#~ msgid "Gather values along given axis from given indices."
#~ msgstr ""

#~ msgid ":py:obj:`gather_nd <tvm.topi.tvm.topi.gather_nd>`\\ \\(a\\, indices\\)"
#~ msgstr ""

#~ msgid "Gather elements from a n-dimension array.."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_const_tuple <tvm.topi.tvm.topi.get_const_tuple>`\\"
#~ " \\(in\\_tuple\\)"
#~ msgstr ""

#~ msgid "Verifies input tuple is IntImm or Var, returns tuple of int or Var."
#~ msgstr ""

#~ msgid ":py:obj:`greater <tvm.topi.tvm.topi.greater>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute (lhs>rhs) with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`greater_equal <tvm.topi.tvm.topi.greater_equal>`\\ "
#~ "\\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute (lhs>=rhs) with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`hybrid_argwhere_1d "
#~ "<tvm.topi.tvm.topi.hybrid_argwhere_1d>`\\ \\(output\\_shape\\,"
#~ " condition\\)"
#~ msgstr ""

#~ msgid "Find the indices of elements of a 1-D tensor that are non-zero."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`hybrid_argwhere_2d "
#~ "<tvm.topi.tvm.topi.hybrid_argwhere_2d>`\\ \\(output\\_shape\\,"
#~ " condition\\)"
#~ msgstr ""

#~ msgid "Find the indices of elements of a 2-D tensor that are non-zero."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`hybrid_argwhere_3d "
#~ "<tvm.topi.tvm.topi.hybrid_argwhere_3d>`\\ \\(output\\_shape\\,"
#~ " condition\\)"
#~ msgstr ""

#~ msgid "Find the indices of elements of a 3-D tensor that are non-zero."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`hybrid_argwhere_4d "
#~ "<tvm.topi.tvm.topi.hybrid_argwhere_4d>`\\ \\(output\\_shape\\,"
#~ " condition\\)"
#~ msgstr ""

#~ msgid "Find the indices of elements of a 4-D tensor that are non-zero."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`hybrid_argwhere_5d "
#~ "<tvm.topi.tvm.topi.hybrid_argwhere_5d>`\\ \\(output\\_shape\\,"
#~ " condition\\)"
#~ msgstr ""

#~ msgid "Find the indices of elements of a 5-D tensor that are non-zero."
#~ msgstr ""

#~ msgid ":py:obj:`identity <tvm.topi.tvm.topi.identity>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take identity of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`invert_permutation "
#~ "<tvm.topi.tvm.topi.invert_permutation>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Computes the inverse permutation of data."
#~ msgstr ""

#~ msgid ":py:obj:`isfinite <tvm.topi.tvm.topi.isfinite>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Check if value of x is finite, element-wise."
#~ msgstr ""

#~ msgid ":py:obj:`isinf <tvm.topi.tvm.topi.isinf>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Check if value of x is infinite, element-wise."
#~ msgstr ""

#~ msgid ":py:obj:`isnan <tvm.topi.tvm.topi.isnan>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Check if value of x is NaN, element-wise."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`layout_transform "
#~ "<tvm.topi.tvm.topi.layout_transform>`\\ \\(array\\, "
#~ "src\\_layout\\, dst\\_layout\\)"
#~ msgstr ""

#~ msgid "Transform the layout according to src_layout and dst_layout"
#~ msgstr ""

#~ msgid ":py:obj:`left_shift <tvm.topi.tvm.topi.left_shift>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Left shift with auto-broadcasting"
#~ msgstr ""

#~ msgid ":py:obj:`less <tvm.topi.tvm.topi.less>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute (lhs<rhs) with auto-broadcasting"
#~ msgstr ""

#~ msgid ":py:obj:`less_equal <tvm.topi.tvm.topi.less_equal>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute (lhs<=rhs) with auto-broadcasting"
#~ msgstr ""

#~ msgid ":py:obj:`log <tvm.topi.tvm.topi.log>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take logarithm of input x."
#~ msgstr ""

#~ msgid ":py:obj:`log10 <tvm.topi.tvm.topi.log10>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take logarithm to the base 10 of input x."
#~ msgstr ""

#~ msgid ":py:obj:`log2 <tvm.topi.tvm.topi.log2>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take logarithm to the base 2 of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`logical_and <tvm.topi.tvm.topi.logical_and>`\\ "
#~ "\\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute element-wise logical and of data."
#~ msgstr ""

#~ msgid ":py:obj:`logical_not <tvm.topi.tvm.topi.logical_not>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise logical not of data."
#~ msgstr ""

#~ msgid ":py:obj:`logical_or <tvm.topi.tvm.topi.logical_or>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute element-wise logical or of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`logical_xor <tvm.topi.tvm.topi.logical_xor>`\\ "
#~ "\\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute element-wise logical xor of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`make_idx <tvm.topi.tvm.topi.make_idx>`\\ \\(b\\,"
#~ " e\\, s\\, z\\, i\\)"
#~ msgstr ""

#~ msgid ""
#~ "Return the array position in the "
#~ "selection that corresponds to an array"
#~ " position in the full array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`matmul <tvm.topi.tvm.topi.matmul>`\\ \\(a\\, "
#~ "b\\[\\, transp\\_a\\, transp\\_b\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Creates an operation that calculates a"
#~ " matrix multiplication (row-major "
#~ "notation): A(i, k) * B(k, j) if"
#~ " trans_a == trans_b, the usual "
#~ "transposed combinations, otherwise"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`matrix_set_diag <tvm.topi.tvm.topi.matrix_set_diag>`\\"
#~ " \\(data\\, diagonal\\[\\, k\\, align\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns a tensor with the diagonals "
#~ "of input tensor replaced with the "
#~ "provided diagonal values."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max <tvm.topi.tvm.topi.max>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\]\\)"
#~ msgstr ""

#~ msgid "Maximum of array elements over a given axis or a list of axes"
#~ msgstr ""

#~ msgid ":py:obj:`maximum <tvm.topi.tvm.topi.maximum>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Take element-wise maximum of two tensors with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`meshgrid <tvm.topi.tvm.topi.meshgrid>`\\ "
#~ "\\(a\\_tuple\\, indexing\\)"
#~ msgstr ""

#~ msgid "Create coordinate matrices from coordinate vectors."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`min <tvm.topi.tvm.topi.min>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\]\\)"
#~ msgstr ""

#~ msgid "Minimum of array elements over a given axis or a list of axes"
#~ msgstr ""

#~ msgid ":py:obj:`minimum <tvm.topi.tvm.topi.minimum>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid ":py:obj:`mod <tvm.topi.tvm.topi.mod>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Modulus with auto-broadcasting"
#~ msgstr ""

#~ msgid ":py:obj:`multiply <tvm.topi.tvm.topi.multiply>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Multiplication with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ndarray_size <tvm.topi.tvm.topi.ndarray_size>`\\ "
#~ "\\(array\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Get the number of elements of input array"
#~ msgstr ""

#~ msgid ":py:obj:`negative <tvm.topi.tvm.topi.negative>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take negation of input x."
#~ msgstr ""

#~ msgid ":py:obj:`not_equal <tvm.topi.tvm.topi.not_equal>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Compute (lhs!=rhs) with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`one_hot <tvm.topi.tvm.topi.one_hot>`\\ "
#~ "\\(indices\\, on\\_value\\, off\\_value\\, depth\\,"
#~ " ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns a one-hot tensor where the"
#~ " locations repsented by indices take "
#~ "value on_value, other locations take "
#~ "value off_value."
#~ msgstr ""

#~ msgid ":py:obj:`power <tvm.topi.tvm.topi.power>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Power with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`prod <tvm.topi.tvm.topi.prod>`\\ \\(data\\[\\,"
#~ " axis\\, keepdims\\]\\)"
#~ msgstr ""

#~ msgid "Product of array elements over a given axis or a list of axes"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reinterpret <tvm.topi.tvm.topi.reinterpret>`\\ "
#~ "\\(x\\, dtype\\)"
#~ msgstr ""

#~ msgid "Reinterpret input to specified data type."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`repeat <tvm.topi.tvm.topi.repeat>`\\ \\(a\\, "
#~ "repeats\\, axis\\)"
#~ msgstr ""

#~ msgid "Repeats elements of an array."
#~ msgstr ""

#~ msgid ":py:obj:`reshape <tvm.topi.tvm.topi.reshape>`\\ \\(a\\, newshape\\)"
#~ msgstr ""

#~ msgid "Reshape the array"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reverse_sequence "
#~ "<tvm.topi.tvm.topi.reverse_sequence>`\\ \\(a\\, "
#~ "seq\\_lengths\\[\\, seq\\_axis\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Reverse the tensor for variable length slices."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`right_shift <tvm.topi.tvm.topi.right_shift>`\\ "
#~ "\\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Right shift with auto-broadcasting"
#~ msgstr ""

#~ msgid ":py:obj:`round <tvm.topi.tvm.topi.round>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Round elements of x to nearest integer."
#~ msgstr ""

#~ msgid ":py:obj:`rsqrt <tvm.topi.tvm.topi.rsqrt>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take inverse square root of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scanop <tvm.topi.tvm.topi.scanop>`\\ \\(data\\,"
#~ " binop\\, identity\\_value\\, op\\_name\\)"
#~ msgstr ""

#~ msgid ""
#~ "Cumulative binary operator (scan) with "
#~ "similar axis behavior as np.cumsum and"
#~ " np.cumprod."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scatter <tvm.topi.tvm.topi.scatter>`\\ \\(data\\,"
#~ " indices\\, updates\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Update data at positions defined by indices with values in updates"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scatter_add <tvm.topi.tvm.topi.scatter_add>`\\ "
#~ "\\(data\\, indices\\, updates\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Update data by adding values in updates at positions defined by indices"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scatter_nd <tvm.topi.tvm.topi.scatter_nd>`\\ "
#~ "\\(data\\, indices\\, updates\\, mode\\)"
#~ msgstr ""

#~ msgid "Scatter elements from a n-dimension array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`searchsorted <tvm.topi.tvm.topi.searchsorted>`\\ "
#~ "\\(sorted\\_sequence\\, values\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Find indices where elements should be inserted to maintain order."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sequence_mask <tvm.topi.tvm.topi.sequence_mask>`\\ "
#~ "\\(data\\, valid\\_length\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Sets all elements outside the expected"
#~ " length of the sequence to a "
#~ "constant value."
#~ msgstr ""

#~ msgid ":py:obj:`shape <tvm.topi.tvm.topi.shape>`\\ \\(array\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Get the shape of input array"
#~ msgstr ""

#~ msgid ":py:obj:`sigmoid <tvm.topi.tvm.topi.sigmoid>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take sigmoid tanh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`sign <tvm.topi.tvm.topi.sign>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Returns -1, 0, 1 based on sign of x."
#~ msgstr ""

#~ msgid ":py:obj:`sin <tvm.topi.tvm.topi.sin>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take sin of input x."
#~ msgstr ""

#~ msgid ":py:obj:`sinh <tvm.topi.tvm.topi.sinh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take sinh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sort <tvm.topi.tvm.topi.sort>`\\ \\(data\\[\\,"
#~ " axis\\, is\\_ascend\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Performs sorting along the given axis"
#~ " and returns an array in sorted "
#~ "order."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_reshape <tvm.topi.tvm.topi.sparse_reshape>`\\"
#~ " \\(sparse\\_indices\\, prev\\_shape\\, ...\\)"
#~ msgstr ""

#~ msgid "Reshape a Sparse Tensor"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_to_dense <tvm.topi.tvm.topi.sparse_to_dense>`\\"
#~ " \\(sparse\\_indices\\, ...\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Converts a sparse representation into a dense tensor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`split <tvm.topi.tvm.topi.split>`\\ \\(ary\\, "
#~ "indices\\_or\\_sections\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Split an array into multiple sub-arrays."
#~ msgstr ""

#~ msgid ":py:obj:`sqrt <tvm.topi.tvm.topi.sqrt>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take square root of input x."
#~ msgstr ""

#~ msgid ":py:obj:`squeeze <tvm.topi.tvm.topi.squeeze>`\\ \\(a\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Remove single-dimensional entries from the shape of an array."
#~ msgstr ""

#~ msgid ":py:obj:`stack <tvm.topi.tvm.topi.stack>`\\ \\(a\\, axis\\)"
#~ msgstr ""

#~ msgid "Repeats the whole array multiple times."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`strided_set <tvm.topi.tvm.topi.strided_set>`\\ "
#~ "\\(a\\, v\\, begin\\, end\\[\\, strides\\]\\)"
#~ msgstr ""

#~ msgid "Set slice of an array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`strided_slice <tvm.topi.tvm.topi.strided_slice>`\\ "
#~ "\\(a\\, begin\\, end\\[\\, strides\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Slice of an array."
#~ msgstr ""

#~ msgid ":py:obj:`subtract <tvm.topi.tvm.topi.subtract>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Subtraction with auto-broadcasting"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sum <tvm.topi.tvm.topi.sum>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\]\\)"
#~ msgstr ""

#~ msgid "Sum of array elements over a given axis or a list of axes"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`take <tvm.topi.tvm.topi.take>`\\ \\(a\\, "
#~ "indices\\[\\, axis\\, batch\\_dims\\, mode\\]\\)"
#~ msgstr ""

#~ msgid "Take elements from an array along an axis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`take_legalize <tvm.topi.tvm.topi.take_legalize>`\\ "
#~ "\\(attrs\\, inputs\\, types\\)"
#~ msgstr ""

#~ msgid "Legalizes dyn.topk op."
#~ msgstr ""

#~ msgid ":py:obj:`tan <tvm.topi.tvm.topi.tan>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take tan of input x."
#~ msgstr ""

#~ msgid ":py:obj:`tanh <tvm.topi.tvm.topi.tanh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take hyperbolic tanh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tensordot <tvm.topi.tvm.topi.tensordot>`\\ "
#~ "\\(a\\, b\\, axes\\)"
#~ msgstr ""

#~ msgid "A generalization of matrix multiplication to tensor."
#~ msgstr ""

#~ msgid ":py:obj:`tile <tvm.topi.tvm.topi.tile>`\\ \\(a\\, reps\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`topk <tvm.topi.tvm.topi.topk>`\\ \\(data\\[\\,"
#~ " k\\, axis\\, ret\\_type\\, is\\_ascend\\, "
#~ "dtype\\]\\)"
#~ msgstr ""

#~ msgid "Get the top k elements in an input tensor along the given axis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`transpose <tvm.topi.tvm.topi.transpose>`\\ "
#~ "\\(a\\[\\, axes\\]\\)"
#~ msgstr ""

#~ msgid "Permute the dimensions of an array."
#~ msgstr ""

#~ msgid ":py:obj:`trunc <tvm.topi.tvm.topi.trunc>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take truncated value of the input of x, element-wise."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`unique <tvm.topi.tvm.topi.unique>`\\ "
#~ "\\(data\\[\\, is\\_sorted\\, return\\_counts\\]\\)"
#~ msgstr ""

#~ msgid "Find the unique elements of a 1-D tensor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`unravel_index <tvm.topi.tvm.topi.unravel_index>`\\ "
#~ "\\(indices\\, shape\\)"
#~ msgstr ""

#~ msgid ""
#~ "Convert a flat index or array of"
#~ " flat indices into a tuple of "
#~ "coordinate arrays."
#~ msgstr ""

#~ msgid ":py:obj:`where <tvm.topi.tvm.topi.where>`\\ \\(condition\\, x\\, y\\)"
#~ msgstr ""

#~ msgid "Get the elements, either from x or y, depending on the condition."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`within_index <tvm.topi.tvm.topi.within_index>`\\ "
#~ "\\(b\\, e\\, s\\, i\\)"
#~ msgstr ""

#~ msgid "Return a boolean value that indicates if i is within the given index."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "The assert condition."
#~ msgstr ""

#~ msgid "The error message."
#~ msgstr ""

#~ msgid "The body statement."
#~ msgstr ""

#~ msgid "The location of this itervar in the source code."
#~ msgstr ""

#~ msgid "The data type"
#~ msgstr ""

#~ msgid "The value of the function."
#~ msgstr ""

#~ msgid "The expression to be evalued."
#~ msgstr ""

#~ msgid ""
#~ "Invalid shape for a topi function. "
#~ "i.e. call winograd template for non-"
#~ "3x3 kernel)"
#~ msgstr ""

#~ msgid "Input argument."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "**y** -- The result."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "The left operand"
#~ msgstr ""

#~ msgid "The right operand"
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- Returns Expr if both "
#~ "operands are Expr. Otherwise returns "
#~ "Tensor."
#~ msgstr ""

#~ msgid "Input data."
#~ msgstr ""

#~ msgid "Tensor index."
#~ msgstr ""

#~ msgid "**result** -- Output tensor"
#~ msgstr ""

#~ msgid "The input tvm boolean tensor"
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a logical"
#~ " AND is performed. The default, "
#~ "axis=None, will perform logical AND over"
#~ " all elements of the input array. "
#~ "If axis is negative it counts from"
#~ " the last to the first axis."
#~ msgstr ""

#~ msgid ""
#~ "If this is set to True, the "
#~ "axes which are reduced are left in"
#~ " the result as dimensions with size"
#~ " one. With this option, the result"
#~ " will broadcast correctly against the "
#~ "input array."
#~ msgstr ""

#~ msgid "**ret**"
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a logical"
#~ " OR is performed. The default, "
#~ "axis=None, will perform logical OR over"
#~ " all elements of the input array. "
#~ "If axis is negative it counts from"
#~ " the last to the first axis."
#~ msgstr ""

#~ msgid ""
#~ "Start of interval. The interval includes"
#~ " this value. The default start value"
#~ " is 0."
#~ msgstr ""

#~ msgid "Stop of interval. The interval does not include this value."
#~ msgstr ""

#~ msgid "Spacing between values. The default step size is 1."
#~ msgstr ""

#~ msgid "The target data type."
#~ msgstr ""

#~ msgid "**result** -- The resulting tensor."
#~ msgstr ""

#~ msgid "The input tvm tensor"
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a argmax"
#~ " operation is performed. The default, "
#~ "axis=None, will find the indices of "
#~ "the maximum element of the elements "
#~ "of the input array. If axis is "
#~ "negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid ""
#~ "Whether to select the last index "
#~ "if the maximum element appears multiple"
#~ " times, else select the first index."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a argmin"
#~ " operation is performed. The default, "
#~ "axis=None, will find the indices of "
#~ "minimum element all of the elements "
#~ "of the input array. If axis is "
#~ "negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid ""
#~ "Whether to select the last index "
#~ "if the minimum element appears multiple"
#~ " times, else select the first index."
#~ msgstr ""

#~ msgid "The input tensor."
#~ msgstr ""

#~ msgid "1-D tensor for valid number of boxes."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which to sort the input"
#~ " tensor. By default the flattened "
#~ "array is used."
#~ msgstr ""

#~ msgid "Whether to sort in ascending or descending order."
#~ msgstr ""

#~ msgid "DType of the output indices."
#~ msgstr ""

#~ msgid "**out** -- Sorted index tensor."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "Tensor with boolean values."
#~ msgstr ""

#~ msgid "**out** -- Indices of non-zero elements."
#~ msgstr ""

#~ msgid ""
#~ "`sorted_sequence` is a N-D Buffer whose"
#~ " innermost dimension we want to "
#~ "search for `value`, and `search_range` "
#~ "is the size of the innermost "
#~ "dimension. `sequence_offset` is a 1-D "
#~ "linearlized offset specifying which of "
#~ "innermost sequences to search."
#~ msgstr ""

#~ msgid ""
#~ "So the search for `value` is "
#~ "performed over "
#~ "`sorted_sequence[sequence_offset:(sequence_offset + "
#~ "search_range)]`. Note that we index N-D"
#~ " Buffer by 1-D linearlized indices."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- Returns Expr if the "
#~ "operand are Expr. Otherwise returns "
#~ "Tensor."
#~ msgstr ""

#~ msgid ""
#~ "We follows the numpy broadcasting rule."
#~ " See also "
#~ "https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"
#~ msgstr ""

#~ msgid "The input data"
#~ msgstr ""

#~ msgid "The target shape to be broadcasted."
#~ msgstr ""

#~ msgid "Data type."
#~ msgstr ""

#~ msgid "The location of the cast in the source."
#~ msgstr ""

#~ msgid ""
#~ "Compute integer ceil log2 with a "
#~ "special code path for vulkan SPIR-V "
#~ "does not support log2 on fp64. "
#~ "Instead, we compute integer ceil_log2 "
#~ "via clz intrinsic when the target "
#~ "is vulkan."
#~ msgstr ""

#~ msgid ""
#~ "Clip (limit) the values in an "
#~ "array. Given an interval, values outside"
#~ " the interval are clipped to the "
#~ "interval edges."
#~ msgstr ""

#~ msgid "Minimum value."
#~ msgstr ""

#~ msgid "Maximum value."
#~ msgstr ""

#~ msgid "The arrays to concatenate"
#~ msgstr ""

#~ msgid "The axis along which the arrays will be joined. Default is 0."
#~ msgstr ""

#~ msgid "Const input array"
#~ msgstr ""

#~ msgid "The name of output op"
#~ msgstr ""

#~ msgid "**tensor** -- The created tensor"
#~ msgstr ""

#~ msgid ""
#~ "Numpy style cumprod op. Return the "
#~ "cumulative product of the elements along"
#~ " a given axis."
#~ msgstr ""

#~ msgid "The input data to the operator."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which the cumulative product"
#~ " is computed. The default (None) is"
#~ " to compute the cumproduct over the"
#~ " flattened array."
#~ msgstr ""

#~ msgid ""
#~ "Type of the returned array and of"
#~ " the accumulator in which the "
#~ "elements are multiplied. If dtype is "
#~ "not specified, it defaults to the "
#~ "dtype of data."
#~ msgstr ""

#~ msgid ""
#~ "If True, will return exclusive product"
#~ " in which the first element is "
#~ "not included. In other terms, if "
#~ "True, the j-th output element would "
#~ "be the product of the first (j-1)"
#~ " elements. Otherwise, it would be the"
#~ " product of the first j elements."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The result has the "
#~ "same size as data, and the same"
#~ " shape as data if axis is not"
#~ " None. If axis is None, the "
#~ "result is a 1-d array."
#~ msgstr ""

#~ msgid ""
#~ "Numpy style cumsum op. Return the "
#~ "cumulative sum of the elements along "
#~ "a given axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which the cumulative sum "
#~ "is computed. The default (None) is "
#~ "to compute the cumsum over the "
#~ "flattened array."
#~ msgstr ""

#~ msgid ""
#~ "Type of the returned array and of"
#~ " the accumulator in which the "
#~ "elements are summed. If dtype is "
#~ "not specified, it defaults to the "
#~ "dtype of data."
#~ msgstr ""

#~ msgid ""
#~ "If True, will return exclusive sum "
#~ "in which the first element is not"
#~ " included. In other terms, if True,"
#~ " the j-th output element would be "
#~ "the sum of the first (j-1) "
#~ "elements. Otherwise, it would be the "
#~ "sum of the first j elements."
#~ msgstr ""

#~ msgid ""
#~ "Normally buffer is created automatically "
#~ "during lower and build. This is "
#~ "only needed if user want to "
#~ "specify their own buffer layout."
#~ msgstr ""

#~ msgid "See the note below for detailed discussion on usage of buffer."
#~ msgstr ""

#~ msgid "The shape of the buffer."
#~ msgstr ""

#~ msgid "The data type of the buffer."
#~ msgstr ""

#~ msgid "The name of the buffer."
#~ msgstr ""

#~ msgid "The data pointer in the buffer."
#~ msgstr ""

#~ msgid "The stride of the buffer."
#~ msgstr ""

#~ msgid ""
#~ "The beginning offset of the array "
#~ "to data. In terms of number of "
#~ "elements of dtype."
#~ msgstr ""

#~ msgid ""
#~ "The storage scope of the buffer, "
#~ "if not global. If scope equals "
#~ "empty string, it means it is "
#~ "global memory."
#~ msgstr ""

#~ msgid ""
#~ "The alignment of data pointer in "
#~ "bytes. If -1 is passed, the "
#~ "alignment will be set to TVM's "
#~ "internal default."
#~ msgstr ""

#~ msgid ""
#~ "The factor of elem_offset field, when"
#~ " set, elem_offset is required to be"
#~ " multiple of offset_factor. If 0 is"
#~ " pssed, the alignment will be set "
#~ "to 1. if non-zero is passed, "
#~ "we will created a Var for "
#~ "elem_offset if elem_offset is not None."
#~ msgstr ""

#~ msgid ""
#~ "auto_broadcast buffer allows one to "
#~ "implement broadcast computation without "
#~ "considering whether dimension size equals "
#~ "to one. TVM maps buffer[i][j][k] -> "
#~ "buffer[i][0][k] if dimension j's shape "
#~ "equals 1."
#~ msgstr ""

#~ msgid "The location of the decl_buffer creation in the source."
#~ msgstr ""

#~ msgid "**buffer** -- The created buffer"
#~ msgstr ""

#~ msgid ""
#~ "Here's an example of how broadcast "
#~ "buffer can be used to define a "
#~ "symbolic broadcast operation,"
#~ msgstr ""

#~ msgid ""
#~ "Buffer data structure reflects the "
#~ "DLTensor structure in dlpack. While "
#~ "DLTensor data structure is very general,"
#~ " it is usually helpful to create "
#~ "function that only handles specific case"
#~ " of data structure and make compiled"
#~ " function benefit from it."
#~ msgstr ""

#~ msgid ""
#~ "If user pass strides and elem_offset "
#~ "is passed as None when constructing "
#~ "the function, then the function will "
#~ "be specialized for the DLTensor that "
#~ "is compact and aligned. If user "
#~ "pass a fully generic symbolic array "
#~ "to the strides, then the resulting "
#~ "function becomes fully generic."
#~ msgstr ""

#~ msgid "The left hand operand, known to be non-negative."
#~ msgstr ""

#~ msgid "The right hand operand, known to be non-negative."
#~ msgstr ""

#~ msgid "The location of this operator in the source."
#~ msgstr ""

#~ msgid "**res** -- The result expression."
#~ msgstr ""

#~ msgid "When operands are integers, returns truncdiv(a, b, span)."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the subscripts for summation "
#~ "as comma separated list of subscript "
#~ "labels. An implicit (classical Einstein "
#~ "summation) calculation is performed unless "
#~ "the explicit indicator -> is included"
#~ " as well as subscript labels of "
#~ "the precise output form."
#~ msgstr ""

#~ msgid ""
#~ "These are the Tensors for the "
#~ "operation. The only difference of einsum"
#~ " between in tvm and numpy is it"
#~ " needs an extra brackets for the "
#~ "tensors. For example, topi.einsum(\"ij, jk "
#~ "-> ik\", (A, B))."
#~ msgstr ""

#~ msgid "**out** -- The calculation based on the Einstein summation convention."
#~ msgstr ""

#~ msgid "Input arguments."
#~ msgstr ""

#~ msgid "The tensor to be expanded."
#~ msgstr ""

#~ msgid "Number of newaxis to be inserted on axis"
#~ msgstr ""

#~ msgid ""
#~ "Expand an input array with the "
#~ "shape of second array. This operation"
#~ " can always be composed of "
#~ "unsqueezing and expanding dims on those"
#~ " unsqueezed axes."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "The tensor to with target shape."
#~ msgstr ""

#~ msgid "axis to be expanded on"
#~ msgstr ""

#~ msgid "The shape of the outputs."
#~ msgstr ""

#~ msgid "The inputs"
#~ msgstr ""

#~ msgid ""
#~ "Specifies the IR statement to do "
#~ "the computation. See the following note"
#~ " for function signature of fcompute  "
#~ ".. note::      **Parameters**       - **ins**"
#~ " (list of :any:`tvm.tir.Buffer`) - "
#~ "Placeholder for each inputs      - "
#~ "**outs** (list of :any:`tvm.tir.Buffer`) - "
#~ "Placeholder for each outputs       **Returns**"
#~ "       - **stmt** (:any:`tvm.tir.Stmt`) - "
#~ "The statement that carries out array "
#~ "computation."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the IR statement to do "
#~ "the computation. See the following note"
#~ " for function signature of fcompute"
#~ msgstr ""

#~ msgid "**Parameters**"
#~ msgstr ""

#~ msgid "**ins** (list of :any:`tvm.tir.Buffer`) - Placeholder for each inputs"
#~ msgstr ""

#~ msgid "**outs** (list of :any:`tvm.tir.Buffer`) - Placeholder for each outputs"
#~ msgstr ""

#~ msgid "**Returns**"
#~ msgstr ""

#~ msgid ""
#~ "**stmt** (:any:`tvm.tir.Stmt`) - The statement"
#~ " that carries out array computation."
#~ msgstr ""

#~ msgid "The name hint of the tensor"
#~ msgstr ""

#~ msgid "The data types of outputs, by default dtype will be same as inputs."
#~ msgstr ""

#~ msgid "Input buffers."
#~ msgstr ""

#~ msgid "Output buffers."
#~ msgstr ""

#~ msgid "tag: str, optional"
#~ msgstr ""

#~ msgid "Additonal tag information about the compute."
#~ msgstr ""

#~ msgid "attrs: dict, optional"
#~ msgstr ""

#~ msgid "The additional auxiliary attributes about the compute."
#~ msgstr ""

#~ msgid ""
#~ "**tensor** -- The created tensor or "
#~ "tuple of tensors it it contains "
#~ "multiple outputs."
#~ msgstr ""

#~ msgid ""
#~ "In the code below, C is generated"
#~ " by calling external PackedFunc "
#~ "`tvm.contrib.cblas.matmul`"
#~ msgstr ""

#~ msgid ""
#~ "Multiplier of a fixed floating point "
#~ "number described as multiplier*2^(-shift)."
#~ msgstr ""

#~ msgid ""
#~ "Shift of a fixed floating point "
#~ "number described as multiplier*2^(-shift)."
#~ msgstr ""

#~ msgid "The axis along which the tensors will be reveresed."
#~ msgstr ""

#~ msgid "The left hand operand"
#~ msgstr ""

#~ msgid "The right hand operand"
#~ msgstr ""

#~ msgid "Input tensor shape."
#~ msgstr ""

#~ msgid "Data type"
#~ msgstr ""

#~ msgid "Value to be filled"
#~ msgstr ""

#~ msgid "then fill tensor with fill_value."
#~ msgstr ""

#~ msgid "E.g. for a 3D tensor, output is computed as:"
#~ msgstr ""

#~ msgid ""
#~ "``indices`` must have same shape as "
#~ "``data``, except at dimension ``axis`` "
#~ "which must just be not null. "
#~ "Output will have same shape as "
#~ "``indices``."
#~ msgstr ""

#~ msgid "The axis along which to index."
#~ msgstr ""

#~ msgid "The indices of the values to extract."
#~ msgstr ""

#~ msgid "The source array."
#~ msgstr ""

#~ msgid "The input."
#~ msgstr ""

#~ msgid "**out_tuple** -- The output."
#~ msgstr ""

#~ msgid "1-D tensor with boolean values."
#~ msgstr ""

#~ msgid "2-D tensor with boolean values."
#~ msgstr ""

#~ msgid "3-D tensor with boolean values."
#~ msgstr ""

#~ msgid "4-D tensor with boolean values."
#~ msgstr ""

#~ msgid "5-D tensor with boolean values."
#~ msgstr ""

#~ msgid "Input data"
#~ msgstr ""

#~ msgid "the source layout."
#~ msgstr ""

#~ msgid "the destination layout."
#~ msgstr ""

#~ msgid ""
#~ "The returned value is only meaningful"
#~ " if within_index() returns True for "
#~ "the same set of parameters."
#~ msgstr ""

#~ msgid "beginning of the index"
#~ msgstr ""

#~ msgid "end of the index"
#~ msgstr ""

#~ msgid "strides of index"
#~ msgstr ""

#~ msgid "size of the indexed dimension"
#~ msgstr ""

#~ msgid "array position"
#~ msgstr ""

#~ msgid ""
#~ "**position** -- int expression that "
#~ "corresponds to an array position in "
#~ "the selection."
#~ msgstr ""

#~ msgid "Input Tensor."
#~ msgstr ""

#~ msgid "Values to be filled in the diagonal."
#~ msgstr ""

#~ msgid ""
#~ "Diagonal Offset(s). The diagonal or "
#~ "range of diagonals to set. (0 by"
#~ " default) Positive value means "
#~ "superdiagonal, 0 refers to the main "
#~ "diagonal, and negative value means "
#~ "subdiagonals. k can be a single "
#~ "integer (for a single diagonal) or "
#~ "a pair of integers specifying the "
#~ "low and high ends of a matrix "
#~ "band. k[0] must not be larger than"
#~ " k[1]."
#~ msgstr ""

#~ msgid ""
#~ "Some diagonals are shorter than "
#~ "max_diag_len and need to be padded. "
#~ "align is a string specifying how "
#~ "superdiagonals and subdiagonals should be "
#~ "aligned, respectively. There are four "
#~ "possible alignments: \"RIGHT_LEFT\" (default), "
#~ "\"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\"."
#~ " \"RIGHT_LEFT\" aligns superdiagonals to "
#~ "the right (left-pads the row) and"
#~ " subdiagonals to the left (right-pads"
#~ " the row). It is the packing "
#~ "format LAPACK uses. cuSPARSE uses "
#~ "\"LEFT_RIGHT\", which is the opposite "
#~ "alignment."
#~ msgstr ""

#~ msgid "**result** -- New tensor with given diagonal values."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which the max"
#~ " operation is performed. The default, "
#~ "axis=None, will find the max element "
#~ "from all of the elements of the"
#~ " input array. If axis is negative "
#~ "it counts from the last to the "
#~ "first axis."
#~ msgstr ""

#~ msgid "The coordinate vectors or scalars."
#~ msgstr ""

#~ msgid "Indexing mode, either \"ij\" or \"xy\"."
#~ msgstr ""

#~ msgid "**result** -- The resulting grids for each axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a minimum"
#~ " operation is performed. The default, "
#~ "axis=None, will find the minimum element"
#~ " from all of the elements of "
#~ "the input array. If axis is "
#~ "negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid "The source tensor."
#~ msgstr ""

#~ msgid ""
#~ "Returns a one-hot tensor where the"
#~ " locations repsented by indices take "
#~ "value on_value, other locations take "
#~ "value off_value. Final dimension is "
#~ "<indices outer dimensions> x depth x "
#~ "<indices inner dimensions>."
#~ msgstr ""

#~ msgid "Locations to set to on_value."
#~ msgstr ""

#~ msgid "Value to fill at indices."
#~ msgstr ""

#~ msgid "Value to fill at all other positions besides indices."
#~ msgstr ""

#~ msgid "Depth of the one-hot dimension."
#~ msgstr ""

#~ msgid "Axis to fill."
#~ msgstr ""

#~ msgid "Data type of the output tensor."
#~ msgstr ""

#~ msgid "**ret** -- The one-hot tensor."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a prod"
#~ " operation is performed. The default, "
#~ "axis=None, will get the prod element "
#~ "over all of the elements of the"
#~ " input array. If axis is negative "
#~ "it counts from the last to the "
#~ "first axis."
#~ msgstr ""

#~ msgid "The tensor to be repeated."
#~ msgstr ""

#~ msgid "Number of repetitions for each element"
#~ msgstr ""

#~ msgid "The axis along which to repeat values"
#~ msgstr ""

#~ msgid "The tensor to be reshaped"
#~ msgstr ""

#~ msgid "The new shape"
#~ msgstr ""

#~ msgid ""
#~ "Reverse the tensor for variable length"
#~ " slices. Input is first sliced along"
#~ " batch axis and then elements are "
#~ "reversed along seq axis."
#~ msgstr ""

#~ msgid "The tensor to be reversed."
#~ msgstr ""

#~ msgid ""
#~ "A 1D Tensor with length "
#~ "a.dims[batch_axis] Must be one of the"
#~ " following types: int32, int64 if "
#~ "seq_lengths[i] > a.dims[seq_axis], it is "
#~ "rounded to a.dims[seq_axis] if seq_lengths[i]"
#~ " < 1, it is rounded to 1"
#~ msgstr ""

#~ msgid "The axis along which the elements will be reversed. Default is 1."
#~ msgstr ""

#~ msgid "The axis along which the tensor will be sliced. Default is 0."
#~ msgstr ""

#~ msgid "**ret** -- The computed result of same shape and type as of input."
#~ msgstr ""

#~ msgid "See cumprod and cumsum for an example of use."
#~ msgstr ""

#~ msgid ""
#~ "E.g. if * is your binary operator"
#~ " and the input tensor is [1, 2,"
#~ " 3, 4] the output may be [1,"
#~ " 1 * 2, 1 * 2 * 3, "
#~ "1 * 2 * 3 * 4]"
#~ msgstr ""

#~ msgid ""
#~ "A binary operator which should be "
#~ "associative and commutative. E.g. if *"
#~ " is your operator then a * (b"
#~ " * c) = (a * b) * c "
#~ "and a * b = b * a"
#~ msgstr ""

#~ msgid ""
#~ "A value for the binary operation "
#~ "which provides the identity property. "
#~ "E.g. if * is your operator and "
#~ "i is the identity_value then a *"
#~ " i = a for all a in the"
#~ " domain of your operation."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which the operation is "
#~ "computed. The default (None) is to "
#~ "compute the cumulative operation over "
#~ "the flattened array."
#~ msgstr ""

#~ msgid ""
#~ "Type of the returned array and of"
#~ " the accumulator in which the "
#~ "elements are computed. If dtype is "
#~ "not specified, it defaults to the "
#~ "dtype of data."
#~ msgstr ""

#~ msgid ""
#~ "If True will return exclusive cumulative"
#~ " operation in which the first element"
#~ " is not included. In other terms, "
#~ "if True, the j-th output element "
#~ "would be the cumulative operation of "
#~ "the first (j-1) elements. Otherwise, it"
#~ " would be the cumulative operation of"
#~ " the first j elements. The cumulative"
#~ " operation of zero elements is "
#~ "assumed to be the identity_value."
#~ msgstr ""

#~ msgid "The index locations to update."
#~ msgstr ""

#~ msgid "The values to update."
#~ msgstr ""

#~ msgid "The axis to scatter on"
#~ msgstr ""

#~ msgid "**ret** -- The computed result."
#~ msgstr ""

#~ msgid "The axis to scatter_add on"
#~ msgstr ""

#~ msgid ""
#~ "Given updates with shape (Y_0, ..., "
#~ "Y_{K-1}, X_M, ..., X_{N-1}), indices "
#~ "with shape (M, Y_0, ..., Y_{K-1}), "
#~ "and output copied from data with "
#~ "shape (X_0, X_1, ..., X_{N-1}), "
#~ "scatter_nd computes"
#~ msgstr ""

#~ msgid "where the update function f is determinted by the mode."
#~ msgstr ""

#~ msgid "The updates to apply at the Indices"
#~ msgstr ""

#~ msgid ""
#~ "The update mode for the algorithm, "
#~ "either \"update\" or \"add\" If update,"
#~ " the update values will replace the"
#~ " input data If add, the update "
#~ "values will be added to the input"
#~ " data"
#~ msgstr ""

#~ msgid ""
#~ "If `sorted_sequence` is N-dimensional, the "
#~ "innermost dimension of `values` are "
#~ "searched in the corresponding dimension "
#~ "of `sorted_sequence`."
#~ msgstr ""

#~ msgid ""
#~ "N-D or 1-D Tensor, containing "
#~ "monotonically increasing sequence on the "
#~ "innermost dimension."
#~ msgstr ""

#~ msgid ""
#~ "N-D Tensor containing the search values."
#~ " When `sorted_sequence` is 1-D, the "
#~ "shape of `values` can be arbitrary. "
#~ "Otherwise, ranks of `sorted_sequence` and "
#~ "`values` must be the same, and "
#~ "outer N-1 axes must have the same"
#~ " size."
#~ msgstr ""

#~ msgid ""
#~ "Controls which index is returned if "
#~ "a value lands exactly on one of"
#~ " sorted values. If False, the index"
#~ " of the first suitable location found"
#~ " is given. If true, return the "
#~ "last such index. If there is no"
#~ " suitable index, return either 0 or"
#~ " N (where N is the size of "
#~ "the innermost dimension)."
#~ msgstr ""

#~ msgid "The data type of the output indices."
#~ msgstr ""

#~ msgid ""
#~ "**indices** -- Tensor with same shape"
#~ " as values, representing the indices "
#~ "of elements of `values` if they "
#~ "are inserted in `sorted_sequence`."
#~ msgstr ""

#~ msgid ""
#~ "This function takes an n-dimensional "
#~ "input array of the form [MAX_LENGTH, "
#~ "batch_size, ...] or [batch_size, MAX_LENGTH,"
#~ " ...] and returns an array of "
#~ "the same shape."
#~ msgstr ""

#~ msgid ""
#~ "`axis` means the axis of the "
#~ "length dimension and can only be 0"
#~ " or 1. If `axis` is 0, the "
#~ "data must have shape [MAX_LENGTH, "
#~ "batch_size, ...]. Otherwise (axis=1), the "
#~ "data must have shape [batch_size, "
#~ "MAX_LENGTH, ...]."
#~ msgstr ""

#~ msgid ""
#~ "`valid_length` gives the length of each"
#~ " sequence. `valid_length` should be a "
#~ "1D int array with positive ints "
#~ "and has dimension [batch_size,]."
#~ msgstr ""

#~ msgid ""
#~ "N-D with shape [MAX_LENGTH, batch_size, "
#~ "...] or [batch_size, MAX_LENGTH, ...] "
#~ "depending on the value of `axis`."
#~ msgstr ""

#~ msgid "1-D with shape [batch_size,]"
#~ msgstr ""

#~ msgid "The masking value, default 0"
#~ msgstr ""

#~ msgid "axis of the length dimension, must be 0 or 1, default 0"
#~ msgstr ""

#~ msgid ""
#~ "**output** -- N-D with shape "
#~ "[MAX_LENGTH, batch_size, ...] or [batch_size,"
#~ " MAX_LENGTH, ...] depending on the "
#~ "value of `axis`."
#~ msgstr ""

#~ msgid ""
#~ "A 2-D tensor[N, n_dim] of integers "
#~ "containing location of sparse values, "
#~ "where N is the number of sparse"
#~ " values and n_dim is the number "
#~ "of dimensions of the dense_shape"
#~ msgstr ""

#~ msgid "A 1-D tensor containing the previous shape of the dense tensor"
#~ msgstr ""

#~ msgid "A 1-D tensor containing the new shape of the dense tensor"
#~ msgstr ""

#~ msgid "**result** -- Output tensor."
#~ msgstr ""

#~ msgid ""
#~ "Example:: -   sparse_to_dense([[0, 0], [1, "
#~ "1]], [2, 2], [3, 3], 0) = "
#~ "[[3, 0], [0, 3]]"
#~ msgstr ""

#~ msgid ""
#~ "A 0-D, 1-D, or 2-D tensor of "
#~ "integers containing location of sparse "
#~ "values."
#~ msgstr ""

#~ msgid "Shape of the dense output tensor."
#~ msgstr ""

#~ msgid ""
#~ "A 0-D or 1-D tensor containing the"
#~ " sparse values for the sparse "
#~ "indices."
#~ msgstr ""

#~ msgid ""
#~ "A 0-D tensor containing the default "
#~ "value for the remaining locations. "
#~ "Defaults to 0."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- Dense tensor of shape "
#~ "output_shape. Has the same type as "
#~ "sparse_values."
#~ msgstr ""

#~ msgid ""
#~ "Selects a subset of the single-"
#~ "dimensional entries in the shape. If "
#~ "an axis is selected with shape "
#~ "entry greater than one, an error "
#~ "is raised."
#~ msgstr ""

#~ msgid "**squeezed**"
#~ msgstr ""

#~ msgid "The tensor to be stacked."
#~ msgstr ""

#~ msgid "The axis in the result array along which the input arrays are stacked."
#~ msgstr ""

#~ msgid "The tensor to be sliced."
#~ msgstr ""

#~ msgid "The values to set"
#~ msgstr ""

#~ msgid "The indices to begin with in the slicing."
#~ msgstr ""

#~ msgid "Indicies indicating end of the slice."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the stride values, it can "
#~ "be negative in that case, the "
#~ "input tensor will be reversed in "
#~ "that particular axis."
#~ msgstr ""

#~ msgid ""
#~ "Axes along which slicing is applied. "
#~ "When it is specified, begin, end "
#~ "strides, and axes need to a list"
#~ " of integers of the same length."
#~ msgstr ""

#~ msgid ""
#~ "The slice mode [end, size]. end -"
#~ " The ending indices for the slice "
#~ "[default]. size - The input strides "
#~ "will be ignored, input end in this"
#~ " mode indicates the sizeof a slice"
#~ " starting at the location specified "
#~ "by begin. If end[i] is -1, all "
#~ "remaining elements in that dimension are"
#~ " included in the slice."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a sum "
#~ "is performed. The default, axis=None, "
#~ "will sum all of the elements of"
#~ " the input array. If axis is "
#~ "negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid ""
#~ "The axis over which to select "
#~ "values. By default, the flattened input"
#~ " array is used."
#~ msgstr ""

#~ msgid "The number of batch dimensions. By default is 0."
#~ msgstr ""

#~ msgid ""
#~ "Specifies how out-of-bound indices "
#~ "will behave. clip - clip to the"
#~ " range (default) wrap - wrap around"
#~ " the indices fast - no clip or"
#~ " wrap around (user must make sure "
#~ "indices are in-bound)"
#~ msgstr ""

#~ msgid "Attributes of current op"
#~ msgstr ""

#~ msgid "The args of the Relay expr to be legalized"
#~ msgstr ""

#~ msgid "List of input and output types"
#~ msgstr ""

#~ msgid "**result** -- The legalized expr"
#~ msgstr ""

#~ msgid "The tensor to be tiled."
#~ msgstr ""

#~ msgid "The number of times for repeating the tensor"
#~ msgstr ""

#~ msgid "Number of top elements to select. Return all elements if k < 1."
#~ msgstr ""

#~ msgid "Axis long which to sort the input tensor."
#~ msgstr ""

#~ msgid ""
#~ "The return type [both, values, indices]."
#~ " \"both\": return both top k data "
#~ "and indices. \"values\": return top k"
#~ " data only. \"indices\": return top k"
#~ " indices only."
#~ msgstr ""

#~ msgid "The data type of the indices output."
#~ msgstr ""

#~ msgid "**out** -- The computed result."
#~ msgstr ""

#~ msgid "By default, reverse the dimensions."
#~ msgstr ""

#~ msgid ""
#~ "Find the unique elements of a 1-D"
#~ " tensor. Please note `output` and "
#~ "`counts` are all padded to have "
#~ "the same length of `data` and "
#~ "element with index >= num_unique[0] has"
#~ " undefined value."
#~ msgstr ""

#~ msgid "A 1-D tensor of integers."
#~ msgstr ""

#~ msgid ""
#~ "Whether to sort the unique elements "
#~ "in ascending order before returning as"
#~ " output."
#~ msgstr ""

#~ msgid "Whether to return the count of each unique element."
#~ msgstr ""

#~ msgid ""
#~ "* **unique** (*tvm.te.Tensor*) -- A 1-D"
#~ " tensor containing the unique elements "
#~ "of the input data tensor. The same"
#~ " size as   the input data. If "
#~ "there are less unique elements than "
#~ "input data, the end of the tensor"
#~ "   is padded with zeros. * "
#~ "**indices** (*tvm.te.Tensor*) -- A 1-D "
#~ "tensor. The same size as output. "
#~ "For each entry in output, it "
#~ "contains   the index of its first "
#~ "occurence in the input data. The "
#~ "end of the tensor is padded   with"
#~ " the length of the input data. "
#~ "* **inverse_indices** (*tvm.te.Tensor*) -- A"
#~ " 1-D tensor. For each entry in "
#~ "data, it contains the index of "
#~ "that data element in   the unique "
#~ "array. (Note that inverse_indices is "
#~ "very similar to indices if output "
#~ "is not   sorted.) * **num_unique** "
#~ "(*tvm.te.Tensor*) -- A 1-D tensor with"
#~ " size=1 containing the number of "
#~ "unique elements in the input data "
#~ "tensor. * **counts (optional)** "
#~ "(*tvm.te.Tensor*) -- A 1-D tensor "
#~ "containing the count of each unique "
#~ "element in the output."
#~ msgstr ""

#~ msgid ""
#~ "**unique** (*tvm.te.Tensor*) -- A 1-D "
#~ "tensor containing the unique elements of"
#~ " the input data tensor. The same "
#~ "size as the input data. If there"
#~ " are less unique elements than input"
#~ " data, the end of the tensor is"
#~ " padded with zeros."
#~ msgstr ""

#~ msgid ""
#~ "**indices** (*tvm.te.Tensor*) -- A 1-D "
#~ "tensor. The same size as output. "
#~ "For each entry in output, it "
#~ "contains the index of its first "
#~ "occurence in the input data. The "
#~ "end of the tensor is padded with"
#~ " the length of the input data."
#~ msgstr ""

#~ msgid ""
#~ "**inverse_indices** (*tvm.te.Tensor*) -- A 1-D"
#~ " tensor. For each entry in data, "
#~ "it contains the index of that data"
#~ " element in the unique array. (Note"
#~ " that inverse_indices is very similar "
#~ "to indices if output is not "
#~ "sorted.)"
#~ msgstr ""

#~ msgid ""
#~ "**num_unique** (*tvm.te.Tensor*) -- A 1-D "
#~ "tensor with size=1 containing the number"
#~ " of unique elements in the input "
#~ "data tensor."
#~ msgstr ""

#~ msgid ""
#~ "**counts (optional)** (*tvm.te.Tensor*) -- A"
#~ " 1-D tensor containing the count of"
#~ " each unique element in the output."
#~ msgstr ""

#~ msgid ""
#~ "Example:: -   unravel_index([22, 41, 37], "
#~ "[7, 6]) = [[3, 6, 6], [4, 5,"
#~ " 1]]"
#~ msgstr ""

#~ msgid "An integer array containing indices."
#~ msgstr ""

#~ msgid "The shape of the array."
#~ msgstr ""

#~ msgid "**result** -- The tuple of coordinate arrays."
#~ msgstr ""

#~ msgid "The condition array."
#~ msgstr ""

#~ msgid "First array to be selected."
#~ msgstr ""

#~ msgid "Second array to be selected."
#~ msgstr ""

#~ msgid "**result** -- A Tensor selected from x or y depending on condition."
#~ msgstr ""

#~ msgid ""
#~ "**selected** -- bool expression that is"
#~ " True is the array position would "
#~ "be selected by the index and False"
#~ " otherwise"
#~ msgstr ""

#~ msgid "Neural network operators"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Workload <tvm.topi.nn.tvm.topi.nn.Workload>`\\ "
#~ "\\(in\\_dtype\\, out\\_dtype\\, height\\, width\\,"
#~ " ...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_pool "
#~ "<tvm.topi.nn.tvm.topi.nn.adaptive_pool>`\\ \\(data\\, "
#~ "output\\_size\\, pool\\_type\\)"
#~ msgstr ""

#~ msgid "Perform pooling on height and width dimension of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_pool3d "
#~ "<tvm.topi.nn.tvm.topi.nn.adaptive_pool3d>`\\ \\(data\\, "
#~ "output\\_size\\, pool\\_type\\)"
#~ msgstr ""

#~ msgid "Perform pooling on three dimensional data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_matmul <tvm.topi.nn.tvm.topi.nn.batch_matmul>`\\"
#~ " \\(tensor\\_a\\, tensor\\_b\\[\\, oshape\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Compute batch matrix multiplication of `tensor_a` and `tensor_b`."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_matmul_legalize "
#~ "<tvm.topi.nn.tvm.topi.nn.batch_matmul_legalize>`\\ \\(attrs\\,"
#~ " inputs\\, types\\)"
#~ msgstr ""

#~ msgid "Legalizes batch_matmul op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_norm <tvm.topi.nn.tvm.topi.nn.batch_norm>`\\ "
#~ "\\(data\\, gamma\\, beta\\, moving\\_mean\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Batch normalization layer (Ioffe and Szegedy, 2014)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_to_space_nd "
#~ "<tvm.topi.nn.tvm.topi.nn.batch_to_space_nd>`\\ \\(data\\, "
#~ "block\\_shape\\, ...\\)"
#~ msgstr ""

#~ msgid "Perform space to batch transformation on the data"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`binarize_pack "
#~ "<tvm.topi.nn.tvm.topi.nn.binarize_pack>`\\ \\(data\\[\\, "
#~ "axis\\, name\\]\\)"
#~ msgstr ""

#~ msgid "Binarization and bit-packing along a certain axis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`binary_dense <tvm.topi.nn.tvm.topi.nn.binary_dense>`\\"
#~ " \\(data\\, weight\\)"
#~ msgstr ""

#~ msgid "Binary matrix multiplication using xor and bit-count."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitpack <tvm.topi.nn.tvm.topi.nn.bitpack>`\\ "
#~ "\\(data\\, bits\\, pack\\_axis\\, bit\\_axis\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Packs data into format necessary for bitserial computation"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitserial_conv2d_legalize "
#~ "<tvm.topi.nn.tvm.topi.nn.bitserial_conv2d_legalize>`\\ "
#~ "\\(attrs\\, inputs\\, types\\)"
#~ msgstr ""

#~ msgid "Legalizes Bitserial Conv2D op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitserial_conv2d_nchw "
#~ "<tvm.topi.nn.tvm.topi.nn.bitserial_conv2d_nchw>`\\ \\(data\\,"
#~ " kernel\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid "Bitserial Conv2D operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitserial_conv2d_nhwc "
#~ "<tvm.topi.nn.tvm.topi.nn.bitserial_conv2d_nhwc>`\\ \\(data\\,"
#~ " kernel\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitserial_dense "
#~ "<tvm.topi.nn.tvm.topi.nn.bitserial_dense>`\\ \\(data\\, "
#~ "weight\\, data\\_bits\\, ...\\)"
#~ msgstr ""

#~ msgid "The default implementation of bitserial dense in topi."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`concatenate <tvm.topi.nn.tvm.topi.nn.concatenate>`\\"
#~ " \\(a\\_tuple\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv <tvm.topi.nn.tvm.topi.nn.conv>`\\ \\(inp\\,"
#~ " filt\\, stride\\, padding\\, dilation\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Convolution operator in NCHW or NHWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv1d <tvm.topi.nn.tvm.topi.nn.conv1d>`\\ "
#~ "\\(data\\, kernel\\[\\, strides\\, padding\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "1D convolution forward operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv1d_ncw <tvm.topi.nn.tvm.topi.nn.conv1d_ncw>`\\ "
#~ "\\(data\\, kernel\\[\\, strides\\, padding\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "1D convolution in NCW layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv1d_nwc <tvm.topi.nn.tvm.topi.nn.conv1d_nwc>`\\ "
#~ "\\(data\\, kernel\\[\\, strides\\, padding\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "1D convolution in NWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv1d_transpose_ncw "
#~ "<tvm.topi.nn.tvm.topi.nn.conv1d_transpose_ncw>`\\ \\(data\\, "
#~ "kernel\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid "Transposed 1D convolution ncw forward operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d <tvm.topi.nn.tvm.topi.nn.conv2d>`\\ "
#~ "\\(input\\, filter\\, strides\\, padding\\, "
#~ "dilation\\)"
#~ msgstr ""

#~ msgid "Conv2D operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_NCHWc <tvm.topi.nn.tvm.topi.nn.conv2d_NCHWc>`\\"
#~ " \\(data\\, kernel\\, stride\\, padding\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Conv2D operator for nChw[x]c layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_NCHWc_int8 "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_NCHWc_int8>`\\ \\(data\\, "
#~ "kernel\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_alter_layout "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_alter_layout>`\\ \\(attrs\\, "
#~ "inputs\\, tinfos\\, ...\\)"
#~ msgstr ""

#~ msgid "Change Conv2D layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_gemm_weight_transform "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_gemm_weight_transform>`\\ "
#~ "\\(kernel\\, ...\\)"
#~ msgstr ""

#~ msgid "Weight transformation for winograd"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_hwcn <tvm.topi.nn.tvm.topi.nn.conv2d_hwcn>`\\"
#~ " \\(Input\\, Filter\\, stride\\, padding\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Convolution operator in HWCN layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_infer_layout "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_infer_layout>`\\ \\(workload\\,"
#~ " cfg\\)"
#~ msgstr ""

#~ msgid "Infer input/output shapes and layouts from a workload and cfg."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_legalize "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_legalize>`\\ \\(attrs\\, "
#~ "inputs\\, types\\)"
#~ msgstr ""

#~ msgid "Legalizes Conv2D op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_nchw <tvm.topi.nn.tvm.topi.nn.conv2d_nchw>`\\"
#~ " \\(Input\\, Filter\\, stride\\, padding\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Convolution operator in NCHW layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_nhwc <tvm.topi.nn.tvm.topi.nn.conv2d_nhwc>`\\"
#~ " \\(Input\\, Filter\\, stride\\, padding\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Convolution operator in NHWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_transpose_legalize "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_transpose_legalize>`\\ "
#~ "\\(attrs\\, inputs\\, types\\)"
#~ msgstr ""

#~ msgid "Legalizes Transposed 2D convolution op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_transpose_nchw "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_transpose_nchw>`\\ \\(Input\\,"
#~ " Filter\\, ...\\)"
#~ msgstr ""

#~ msgid "Transposed 2D convolution nchw forward operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_transpose_nchw_preprocess "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_transpose_nchw_preprocess>`\\ "
#~ "\\(data\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Preprocess data and kernel to make "
#~ "the compute pattern of conv2d_transpose "
#~ "the same as conv2d"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_winograd_nhwc "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_winograd_nhwc>`\\ \\(data\\, "
#~ "weight\\, strides\\, ...\\)"
#~ msgstr ""

#~ msgid "Conv2D Winograd in NHWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_winograd_nhwc_without_weight_transform "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_winograd_nhwc_without_weight_transform>`\\"
#~ " \\(...\\)"
#~ msgstr ""

#~ msgid "Conv2D Winograd without layout transform in NHWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_winograd_nnpack_weight_transform "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_winograd_nnpack_weight_transform>`\\"
#~ " \\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_winograd_weight_transform "
#~ "<tvm.topi.nn.tvm.topi.nn.conv2d_winograd_weight_transform>`\\ "
#~ "\\(kernel\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d_alter_layout "
#~ "<tvm.topi.nn.tvm.topi.nn.conv3d_alter_layout>`\\ \\(attrs\\, "
#~ "inputs\\, tinfos\\, ...\\)"
#~ msgstr ""

#~ msgid "Change Conv3D layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d_ncdhw <tvm.topi.nn.tvm.topi.nn.conv3d_ncdhw>`\\"
#~ " \\(Input\\, Filter\\, stride\\, padding\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Conv3D operator in NCDHW layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d_ndhwc <tvm.topi.nn.tvm.topi.nn.conv3d_ndhwc>`\\"
#~ " \\(Input\\, Filter\\, stride\\, padding\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Convolution operator in NDHWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d_transpose_legalize "
#~ "<tvm.topi.nn.tvm.topi.nn.conv3d_transpose_legalize>`\\ "
#~ "\\(attrs\\, inputs\\, types\\)"
#~ msgstr ""

#~ msgid "Legalizes Transposed 3D convolution op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d_transpose_ncdhw "
#~ "<tvm.topi.nn.tvm.topi.nn.conv3d_transpose_ncdhw>`\\ \\(Input\\,"
#~ " Filter\\, ...\\)"
#~ msgstr ""

#~ msgid "Transposed 3D convolution ncdhw forward operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d_transpose_ncdhw_preprocess "
#~ "<tvm.topi.nn.tvm.topi.nn.conv3d_transpose_ncdhw_preprocess>`\\ "
#~ "\\(data\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Preprocess data and kernel to make "
#~ "the compute pattern of conv3d_transpose "
#~ "the same as conv3d"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d_winograd_weight_transform "
#~ "<tvm.topi.nn.tvm.topi.nn.conv3d_winograd_weight_transform>`\\ "
#~ "\\(kernel\\, ...\\)"
#~ msgstr ""

#~ msgid "Weight transformation for 3D winograd"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`correlation_nchw "
#~ "<tvm.topi.nn.tvm.topi.nn.correlation_nchw>`\\ \\(data1\\, "
#~ "data2\\, kernel\\_size\\, ...\\)"
#~ msgstr ""

#~ msgid "Correlation operator in NCHW layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`declaration_conv2d_transpose_impl "
#~ "<tvm.topi.nn.tvm.topi.nn.declaration_conv2d_transpose_impl>`\\ "
#~ "\\(data\\, ...\\)"
#~ msgstr ""

#~ msgid "Implementation of conv2d transpose"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`declaration_conv3d_transpose_impl "
#~ "<tvm.topi.nn.tvm.topi.nn.declaration_conv3d_transpose_impl>`\\ "
#~ "\\(data\\, ...\\)"
#~ msgstr ""

#~ msgid "Implementation of conv3d transpose"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`deformable_conv2d_nchw "
#~ "<tvm.topi.nn.tvm.topi.nn.deformable_conv2d_nchw>`\\ \\(data\\,"
#~ " offset\\, kernel\\, ...\\)"
#~ msgstr ""

#~ msgid "Deformable conv2D operator in NCHW layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`deformable_conv2d_nhwc "
#~ "<tvm.topi.nn.tvm.topi.nn.deformable_conv2d_nhwc>`\\ \\(data\\,"
#~ " offset\\, kernel\\, ...\\)"
#~ msgstr ""

#~ msgid "Deformable conv2D operator in NHWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dense <tvm.topi.nn.tvm.topi.nn.dense>`\\ "
#~ "\\(data\\, weight\\[\\, bias\\, out\\_dtype\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "The default implementation of dense in topi."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dense_alter_layout "
#~ "<tvm.topi.nn.tvm.topi.nn.dense_alter_layout>`\\ \\(attrs\\, "
#~ "inputs\\, tinfos\\, ...\\)"
#~ msgstr ""

#~ msgid "Change dense layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dense_legalize "
#~ "<tvm.topi.nn.tvm.topi.nn.dense_legalize>`\\ \\(attrs\\, "
#~ "inputs\\, types\\)"
#~ msgstr ""

#~ msgid "Legalizes dense op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dense_pack <tvm.topi.nn.tvm.topi.nn.dense_pack>`\\ "
#~ "\\(data\\, weight\\[\\, bias\\, out\\_dtype\\]\\)"
#~ msgstr ""

#~ msgid "The default implementation of dense_pack in topi."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`depth_to_space "
#~ "<tvm.topi.nn.tvm.topi.nn.depth_to_space>`\\ \\(data\\, "
#~ "block\\_size\\[\\, layout\\, mode\\]\\)"
#~ msgstr ""

#~ msgid "Perform depth to space transformation on the data"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`depthwise_conv2d_NCHWc "
#~ "<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_NCHWc>`\\ \\(Input\\,"
#~ " Filter\\, ...\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Depthwise convolution NCHW[x]c forward operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`depthwise_conv2d_backward_input_nhwc "
#~ "<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_backward_input_nhwc>`\\ "
#~ "\\(Filter\\, ...\\)"
#~ msgstr ""

#~ msgid "Depthwise convolution nhwc backward wrt input operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`depthwise_conv2d_backward_weight_nhwc "
#~ "<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_backward_weight_nhwc>`\\ "
#~ "\\(Input\\, ...\\)"
#~ msgstr ""

#~ msgid "Depthwise convolution nhwc backward wrt weight operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`depthwise_conv2d_infer_layout "
#~ "<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_infer_layout>`\\ "
#~ "\\(workload\\, cfg\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`depthwise_conv2d_nchw "
#~ "<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_nchw>`\\ \\(Input\\,"
#~ " Filter\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid "Depthwise convolution nchw forward operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`depthwise_conv2d_nhwc "
#~ "<tvm.topi.nn.tvm.topi.nn.depthwise_conv2d_nhwc>`\\ \\(Input\\,"
#~ " Filter\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid "Depthwise convolution nhwc forward operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dilate <tvm.topi.nn.tvm.topi.nn.dilate>`\\ "
#~ "\\(data\\, strides\\[\\, dilation\\_value\\, "
#~ "name\\]\\)"
#~ msgstr ""

#~ msgid "Dilate data with given dilation value (0 by default)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`equal_const_int "
#~ "<tvm.topi.nn.tvm.topi.nn.equal_const_int>`\\ \\(expr\\, "
#~ "value\\)"
#~ msgstr ""

#~ msgid "Returns if expr equals value."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`fast_softmax <tvm.topi.nn.tvm.topi.nn.fast_softmax>`\\"
#~ " \\(x\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Perform softmax activation on the data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`fifo_buffer <tvm.topi.nn.tvm.topi.nn.fifo_buffer>`\\"
#~ " \\(data\\, buffer\\, axis\\)"
#~ msgstr ""

#~ msgid ""
#~ "FIFO buffer to enable computation reuse"
#~ " in CNNs with sliding indow input"
#~ msgstr ""

#~ msgid ":py:obj:`flatten <tvm.topi.nn.tvm.topi.nn.flatten>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid ""
#~ "Flattens the input array into a "
#~ "2-D array by collapsing the higher "
#~ "dimensions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_const_int "
#~ "<tvm.topi.nn.tvm.topi.nn.get_const_int>`\\ \\(expr\\)"
#~ msgstr ""

#~ msgid "Verifies expr is integer and get the constant value."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_const_tuple "
#~ "<tvm.topi.nn.tvm.topi.nn.get_const_tuple>`\\ \\(in\\_tuple\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple "
#~ "<tvm.topi.nn.tvm.topi.nn.get_pad_tuple>`\\ \\(padding\\, "
#~ "kernel\\)"
#~ msgstr ""

#~ msgid "Common code to get the pad option"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple1d "
#~ "<tvm.topi.nn.tvm.topi.nn.get_pad_tuple1d>`\\ \\(padding\\, "
#~ "kernel\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple3d "
#~ "<tvm.topi.nn.tvm.topi.nn.get_pad_tuple3d>`\\ \\(padding\\, "
#~ "kernel\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple_generic "
#~ "<tvm.topi.nn.tvm.topi.nn.get_pad_tuple_generic>`\\ \\(padding\\,"
#~ " kernel\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_pool <tvm.topi.nn.tvm.topi.nn.global_pool>`\\"
#~ " \\(data\\, pool\\_type\\[\\, layout\\]\\)"
#~ msgstr ""

#~ msgid "Perform global pooling on height and width dimension of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`group_conv1d_ncw "
#~ "<tvm.topi.nn.tvm.topi.nn.group_conv1d_ncw>`\\ \\(data\\, "
#~ "kernel\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "1D convolution forward operator for NCW layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`group_conv1d_nwc "
#~ "<tvm.topi.nn.tvm.topi.nn.group_conv1d_nwc>`\\ \\(data\\, "
#~ "kernel\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "1D convolution forward operator for NWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`group_conv2d_nchw "
#~ "<tvm.topi.nn.tvm.topi.nn.group_conv2d_nchw>`\\ \\(Input\\, "
#~ "Filter\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid "Group convolution operator in NCHW layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`group_conv2d_nhwc "
#~ "<tvm.topi.nn.tvm.topi.nn.group_conv2d_nhwc>`\\ \\(Input\\, "
#~ "Filter\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid "Group convolution operator in NHWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`group_conv2d_transpose_nchw "
#~ "<tvm.topi.nn.tvm.topi.nn.group_conv2d_transpose_nchw>`\\ "
#~ "\\(data\\, kernel\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`layout_transform "
#~ "<tvm.topi.nn.tvm.topi.nn.layout_transform>`\\ \\(tensor\\, "
#~ "current\\_layout\\, ...\\)"
#~ msgstr ""

#~ msgid "Transform a tensor with the current layout to the desired layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`leaky_relu <tvm.topi.nn.tvm.topi.nn.leaky_relu>`\\ "
#~ "\\(x\\, alpha\\)"
#~ msgstr ""

#~ msgid "Take leaky relu of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`log_softmax <tvm.topi.nn.tvm.topi.nn.log_softmax>`\\"
#~ " \\(x\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Perform log softmax activation on the data"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`lrn <tvm.topi.nn.tvm.topi.nn.lrn>`\\ \\(data\\,"
#~ " size\\[\\, axis\\, alpha\\, beta\\, "
#~ "bias\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Perform the across channels local "
#~ "response normalisation on the input "
#~ "data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`matmul <tvm.topi.nn.tvm.topi.nn.matmul>`\\ "
#~ "\\(tensor\\_a\\, tensor\\_b\\[\\, bias\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "The default implementation of matmul in topi."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`matmul_legalize "
#~ "<tvm.topi.nn.tvm.topi.nn.matmul_legalize>`\\ \\(attrs\\, "
#~ "inputs\\, types\\)"
#~ msgstr ""

#~ msgid "Legalizes matmul op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`mirror_pad <tvm.topi.nn.tvm.topi.nn.mirror_pad>`\\ "
#~ "\\(data\\, pad\\_before\\[\\, pad\\_after\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Pad Input with mirroring either symmetric or reflected."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`namedtuple <tvm.topi.nn.tvm.topi.nn.namedtuple>`\\ "
#~ "\\(typename\\, field\\_names\\, \\*\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Returns a new subclass of tuple with named fields."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`nll_loss <tvm.topi.nn.tvm.topi.nn.nll_loss>`\\ "
#~ "\\(predictions\\, targets\\, weights\\, ...\\)"
#~ msgstr ""

#~ msgid "Negative log likelihood loss on the input data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pad <tvm.topi.nn.tvm.topi.nn.pad>`\\ \\(data\\,"
#~ " pad\\_before\\[\\, pad\\_after\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Pad Input with zeros."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pool1d <tvm.topi.nn.tvm.topi.nn.pool1d>`\\ "
#~ "\\(data\\, kernel\\, stride\\, dilation\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Perform pooling on width dimension of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pool2d <tvm.topi.nn.tvm.topi.nn.pool2d>`\\ "
#~ "\\(data\\, kernel\\, stride\\, dilation\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pool3d <tvm.topi.nn.tvm.topi.nn.pool3d>`\\ "
#~ "\\(data\\, kernel\\, stride\\, dilation\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Perform pooling on depth, height and width dimension of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pool_grad <tvm.topi.nn.tvm.topi.nn.pool_grad>`\\ "
#~ "\\(grads\\, data\\, kernel\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid "Gradient of pooling on height and width dimension of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`prelu <tvm.topi.nn.tvm.topi.nn.prelu>`\\ \\(x\\,"
#~ " slope\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "PReLU."
#~ msgstr ""

#~ msgid ":py:obj:`relu <tvm.topi.nn.tvm.topi.nn.relu>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take relu of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scale_shift_nchw "
#~ "<tvm.topi.nn.tvm.topi.nn.scale_shift_nchw>`\\ \\(Input\\, "
#~ "Scale\\, Shift\\)"
#~ msgstr ""

#~ msgid "Batch normalization operator in inference."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scale_shift_nchwc "
#~ "<tvm.topi.nn.tvm.topi.nn.scale_shift_nchwc>`\\ \\(Input\\, "
#~ "Scale\\, Shift\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scale_shift_nhwc "
#~ "<tvm.topi.nn.tvm.topi.nn.scale_shift_nhwc>`\\ \\(Input\\, "
#~ "Scale\\, Shift\\)"
#~ msgstr ""

#~ msgid ":py:obj:`simplify <tvm.topi.nn.tvm.topi.nn.simplify>`\\ \\(expr\\)"
#~ msgstr ""

#~ msgid "Simplify the expression if it is Expr, directly return if it is int."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`simulated_dequantize "
#~ "<tvm.topi.nn.tvm.topi.nn.simulated_dequantize>`\\ \\(data\\, "
#~ "in\\_dtype\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Simulated QNN dequantize operator that "
#~ "mimics QNN outputs without changing "
#~ "datatype."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`simulated_quantize "
#~ "<tvm.topi.nn.tvm.topi.nn.simulated_quantize>`\\ \\(data\\, "
#~ "out\\_dtype\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Simulated QNN quantize operator that "
#~ "mimics QNN outputs without changing "
#~ "datatype."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`softmax <tvm.topi.nn.tvm.topi.nn.softmax>`\\ "
#~ "\\(x\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`softmax_common "
#~ "<tvm.topi.nn.tvm.topi.nn.softmax_common>`\\ \\(x\\, "
#~ "axis\\, use\\_fast\\_exp\\)"
#~ msgstr ""

#~ msgid "The common part of softmax and fast_softmax"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`space_to_batch_nd "
#~ "<tvm.topi.nn.tvm.topi.nn.space_to_batch_nd>`\\ \\(data\\, "
#~ "block\\_shape\\, ...\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Perform batch to space transformation on the data"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`space_to_depth "
#~ "<tvm.topi.nn.tvm.topi.nn.space_to_depth>`\\ \\(data\\, "
#~ "block\\_size\\[\\, layout\\]\\)"
#~ msgstr ""

#~ msgid "Perform space to depth transformation on the data"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_add <tvm.topi.nn.tvm.topi.nn.sparse_add>`\\ "
#~ "\\(dense\\_data\\, sparse\\_data\\, ...\\)"
#~ msgstr ""

#~ msgid "Computes sparse-dense addition"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_conv2d "
#~ "<tvm.topi.nn.tvm.topi.nn.sparse_conv2d>`\\ \\(dense\\_data\\,"
#~ " sparse\\_data\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Computes sparse-conv2d(1*1) of ``data`` "
#~ "and ``(weight_data, weight_indices, weight_indptr)``"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_dense <tvm.topi.nn.tvm.topi.nn.sparse_dense>`\\"
#~ " \\(dense\\_data\\, sparse\\_data\\, ...\\[\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Computes sparse-dense matrix multiplication"
#~ " of `data` and `(weight_data, "
#~ "weight_indices, weight_indptr).T`, if "
#~ "sparse_lhs=False or Computes sparse-dense "
#~ "matrix multiplication of `(data_data, "
#~ "data_indices, data_indptr)` and `weight.T`, if"
#~ " sparse_lhs=True"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_dense_alter_layout "
#~ "<tvm.topi.nn.tvm.topi.nn.sparse_dense_alter_layout>`\\ "
#~ "\\(\\_attrs\\, \\_inputs\\, ...\\)"
#~ msgstr ""

#~ msgid "Change Sparse Dense layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_dense_sp_lhs "
#~ "<tvm.topi.nn.tvm.topi.nn.sparse_dense_sp_lhs>`\\ "
#~ "\\(data\\_data\\, data\\_indices\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Computes sparse-dense matrix multiplication"
#~ " of `(data_data, data_indices, data_indptr)` "
#~ "and `weight.T`"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_dense_sp_rhs "
#~ "<tvm.topi.nn.tvm.topi.nn.sparse_dense_sp_rhs>`\\ \\(data\\, "
#~ "weight\\_data\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Computes sparse-dense matrix multiplication"
#~ " of `data` and `(weight_data, "
#~ "weight_indices, weight_indptr).T`"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_transpose "
#~ "<tvm.topi.nn.tvm.topi.nn.sparse_transpose>`\\ "
#~ "\\(sparse\\_data\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Transpose a square sparse matrix, `A`"
#~ " is an n-by-n sparse matrix in "
#~ "the CSR format."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`strided_slice "
#~ "<tvm.topi.nn.tvm.topi.nn.strided_slice>`\\ \\(a\\, "
#~ "begin\\, end\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`try_get_conv2d_sparse_input "
#~ "<tvm.topi.nn.tvm.topi.nn.try_get_conv2d_sparse_input>`\\ "
#~ "\\(args\\)"
#~ msgstr ""

#~ msgid "Analyze the input data from the given args."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`try_get_sparse_input "
#~ "<tvm.topi.nn.tvm.topi.nn.try_get_sparse_input>`\\ \\(args\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`unpack_NCHWc_to_nchw "
#~ "<tvm.topi.nn.tvm.topi.nn.unpack_NCHWc_to_nchw>`\\ "
#~ "\\(packed\\_out\\, out\\_dtype\\)"
#~ msgstr ""

#~ msgid "Unpack conv2d_NCHWc output from layout NCHWc to NCHW"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`upsampling <tvm.topi.nn.tvm.topi.nn.upsampling>`\\ "
#~ "\\(data\\, scale\\_h\\, scale\\_w\\[\\, layout\\,"
#~ " ...\\]\\)"
#~ msgstr ""

#~ msgid "Perform upsampling on the data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`upsampling3d <tvm.topi.nn.tvm.topi.nn.upsampling3d>`\\"
#~ " \\(data\\, scale\\_d\\, scale\\_h\\, "
#~ "scale\\_w\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`winograd_transform_matrices "
#~ "<tvm.topi.nn.tvm.topi.nn.winograd_transform_matrices>`\\ "
#~ "\\(tile\\_size\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Compute the A, B, and G transform"
#~ " matrices for `tile_size` as a "
#~ "`tvm.Expr`."
#~ msgstr ""

#~ msgid "**Attributes:**"
#~ msgstr ""

#~ msgid ":py:obj:`dilation_h <tvm.topi.nn.tvm.topi.nn.Workload.dilation_h>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 12"
#~ msgstr ""

#~ msgid ":py:obj:`dilation_w <tvm.topi.nn.tvm.topi.nn.Workload.dilation_w>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 13"
#~ msgstr ""

#~ msgid ":py:obj:`height <tvm.topi.nn.tvm.topi.nn.Workload.height>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 2"
#~ msgstr ""

#~ msgid ":py:obj:`in_dtype <tvm.topi.nn.tvm.topi.nn.Workload.in_dtype>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 0"
#~ msgstr ""

#~ msgid ":py:obj:`in_filter <tvm.topi.nn.tvm.topi.nn.Workload.in_filter>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 4"
#~ msgstr ""

#~ msgid ":py:obj:`kernel_h <tvm.topi.nn.tvm.topi.nn.Workload.kernel_h>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 6"
#~ msgstr ""

#~ msgid ":py:obj:`kernel_w <tvm.topi.nn.tvm.topi.nn.Workload.kernel_w>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 7"
#~ msgstr ""

#~ msgid ":py:obj:`out_dtype <tvm.topi.nn.tvm.topi.nn.Workload.out_dtype>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 1"
#~ msgstr ""

#~ msgid ":py:obj:`out_filter <tvm.topi.nn.tvm.topi.nn.Workload.out_filter>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 5"
#~ msgstr ""

#~ msgid ":py:obj:`padb <tvm.topi.nn.tvm.topi.nn.Workload.padb>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 10"
#~ msgstr ""

#~ msgid ":py:obj:`padl <tvm.topi.nn.tvm.topi.nn.Workload.padl>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 9"
#~ msgstr ""

#~ msgid ":py:obj:`padr <tvm.topi.nn.tvm.topi.nn.Workload.padr>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 11"
#~ msgstr ""

#~ msgid ":py:obj:`padt <tvm.topi.nn.tvm.topi.nn.Workload.padt>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 8"
#~ msgstr ""

#~ msgid ":py:obj:`stride_h <tvm.topi.nn.tvm.topi.nn.Workload.stride_h>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 14"
#~ msgstr ""

#~ msgid ":py:obj:`stride_w <tvm.topi.nn.tvm.topi.nn.Workload.stride_w>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 15"
#~ msgstr ""

#~ msgid ":py:obj:`width <tvm.topi.nn.tvm.topi.nn.Workload.width>`\\"
#~ msgstr ""

#~ msgid "Alias for field number 3"
#~ msgstr ""

#~ msgid ""
#~ "The pooling kernel and stride sizes "
#~ "are automatically chosen for desired "
#~ "output sizes. It decides the height "
#~ "and width dimension according to the "
#~ "layout string, in which 'W' and "
#~ "'H' means width and height respectively."
#~ " Width and height dimension cannot be"
#~ " split. For example, NCHW, NCHW16c, "
#~ "etc. are valid for pool, while "
#~ "NCHW16w, NCHW16h are not. See parameter"
#~ " `layout` for more information of the"
#~ " layout string convention."
#~ msgstr ""

#~ msgid "n-D with shape of layout"
#~ msgstr ""

#~ msgid "output height and width."
#~ msgstr ""

#~ msgid "Pool type, 'max' or 'avg'"
#~ msgstr ""

#~ msgid ""
#~ "Layout of the input data. The "
#~ "layout is supposed to be composed "
#~ "of upper cases, lower cases and "
#~ "numbers, where upper case indicates a"
#~ " dimension and the corresponding lower "
#~ "case with factor size indicates the "
#~ "split dimension. For example, NCHW16c "
#~ "can describe a 5-D tensor of "
#~ "[batch_size, channel, height, width, "
#~ "channel_block], in which channel_block=16 is"
#~ " a split of dimension channel."
#~ msgstr ""

#~ msgid "**output** -- n-D in the same layout"
#~ msgstr ""

#~ msgid ""
#~ "Perform pooling on three dimensional "
#~ "data. See the two dimensional version"
#~ " above for details."
#~ msgstr ""

#~ msgid ""
#~ "Both `tensor_a` and `tensor_b` can be"
#~ " transposed. For legacy reason, we "
#~ "use NT format (transpose_a=False, "
#~ "transpose_b=True) by default."
#~ msgstr ""

#~ msgid "3-D with shape [batch, M, K] or [batch, K, M]."
#~ msgstr ""

#~ msgid "3-D with shape [batch, K, N] or [batch, N, K]."
#~ msgstr ""

#~ msgid ""
#~ "Explicit intended output shape of the"
#~ " computation. Can be useful in cases"
#~ " with dynamic input shapes."
#~ msgstr ""

#~ msgid "Specifies the output data type for mixed precision batch matmul."
#~ msgstr ""

#~ msgid "Whether the first tensor is in transposed format."
#~ msgstr ""

#~ msgid "Whether the second tensor is in transposed format."
#~ msgstr ""

#~ msgid "The layout after auto-scheduler's layout rewrite pass."
#~ msgstr ""

#~ msgid "**output** -- 3-D with shape [batch, M, N]"
#~ msgstr ""

#~ msgid "Attributes of current batch_matmul"
#~ msgstr ""

#~ msgid ""
#~ "Normalizes the input at each batch, "
#~ "i.e. applies a transformation that "
#~ "maintains the mean activation close to"
#~ " 0 and the activation standard "
#~ "deviation close to 1."
#~ msgstr ""

#~ msgid "Input to be batch-normalized."
#~ msgstr ""

#~ msgid "Scale factor to be applied to the normalized tensor."
#~ msgstr ""

#~ msgid "Offset to be applied to the normalized tensor."
#~ msgstr ""

#~ msgid "Running mean of input."
#~ msgstr ""

#~ msgid "Running variance of input."
#~ msgstr ""

#~ msgid "Specify along which shape axis the normalization should occur."
#~ msgstr ""

#~ msgid "Small float added to variance to avoid dividing by zero."
#~ msgstr ""

#~ msgid ""
#~ "If True, add offset of beta to "
#~ "normalized tensor, If False, beta is "
#~ "ignored."
#~ msgstr ""

#~ msgid "If True, scale normalized tensor by gamma. If False, gamma is ignored."
#~ msgstr ""

#~ msgid ""
#~ "* **output** (*list of tvm.te.Tensor*) "
#~ "-- Normalized data with same shape "
#~ "as input * **moving_mean** (*tvm.te.Tensor*)"
#~ " -- Running mean of input. * "
#~ "**moving_var** (*tvm.te.Tensor*) -- Running "
#~ "variance of input."
#~ msgstr ""

#~ msgid ""
#~ "**output** (*list of tvm.te.Tensor*) -- "
#~ "Normalized data with same shape as "
#~ "input"
#~ msgstr ""

#~ msgid "**moving_mean** (*tvm.te.Tensor*) -- Running mean of input."
#~ msgstr ""

#~ msgid "**moving_var** (*tvm.te.Tensor*) -- Running variance of input."
#~ msgstr ""

#~ msgid ""
#~ "N-D Tensor with shape [batch, "
#~ "spatial_shape, remaining_shapes], where "
#~ "spatial_shape has M dimensions."
#~ msgstr ""

#~ msgid ""
#~ "list of size [M] where M is "
#~ "number of spatial dims, specifies block"
#~ " size for each spatial dimension."
#~ msgstr ""

#~ msgid ""
#~ "list of shape [M] where M is "
#~ "number of spatial dims, specifies begin"
#~ " crop size for each spatial "
#~ "dimension."
#~ msgstr ""

#~ msgid ""
#~ "list of shape [M] where M is "
#~ "number of spatial dims, specifies end"
#~ " crop size for each spatial "
#~ "dimension."
#~ msgstr ""

#~ msgid "**output**"
#~ msgstr ""

#~ msgid "n-D input, can be any layout."
#~ msgstr ""

#~ msgid ""
#~ "The axis along which to do "
#~ "binarization and bit-packing, default is"
#~ " the last axis."
#~ msgstr ""

#~ msgid "The name prefix operators generate."
#~ msgstr ""

#~ msgid "**output** -- n-D, the same layout as input, dtype is uint32."
#~ msgstr ""

#~ msgid "2-D with shape [batch, in_dim], dtype is uint32."
#~ msgstr ""

#~ msgid "2-D with shape [out_dim, in_dim], dtype is uint32."
#~ msgstr ""

#~ msgid "**output** -- 2-D with shape [batch, out_dim], dtype is float32."
#~ msgstr ""

#~ msgid "index of the axis to pack in data"
#~ msgstr ""

#~ msgid "index of axis to place bit axis in resulting packed data"
#~ msgstr ""

#~ msgid "Attributes of current convolution"
#~ msgstr ""

#~ msgid "4-D with shape [batch, in_channel, in_height, in_width]"
#~ msgstr ""

#~ msgid "4-D with shape [num_filter, in_channel, filter_height, filter_width]"
#~ msgstr ""

#~ msgid "stride size, or [stride_height, stride_width]"
#~ msgstr ""

#~ msgid ""
#~ "padding size, [pad_height, pad_width], "
#~ "[pad_top, pad_left, pad_down, pad_right]"
#~ msgstr ""

#~ msgid "number of bits used for activations/input elements"
#~ msgstr ""

#~ msgid "number of bits used for weight elements"
#~ msgstr ""

#~ msgid "return type of convolution"
#~ msgstr ""

#~ msgid "bit packing type"
#~ msgstr ""

#~ msgid ""
#~ "if binarization style is in unipolar "
#~ "1/0 format, instead of bipolar -1/+1 "
#~ "format"
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 4-D with shape [batch, "
#~ "out_channel, out_height, out_width]"
#~ msgstr ""

#~ msgid "4-D with shape [batch, in_height, in_width, in_channel]"
#~ msgstr ""

#~ msgid "4-D with shape [filter_height, filter_width, in_channel, num_filter]"
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 4-D with shape [batch, "
#~ "out_height, out_width, out_channel]"
#~ msgstr ""

#~ msgid "2-D with shape [batch, in_dim]"
#~ msgstr ""

#~ msgid ""
#~ "2-D with shape [out_dim, in_dim] or "
#~ "3-D with shape [out_dim, weight_bits, "
#~ "in_dim]"
#~ msgstr ""

#~ msgid "**output** -- 2-D with shape [batch, out_dim]"
#~ msgstr ""

#~ msgid "Supports 1D, 2D, 3D, ... and grouping."
#~ msgstr ""

#~ msgid ""
#~ "N-D with shape [batch, in_channel, "
#~ "in_height, in_width, ...] ordered by "
#~ "`order`"
#~ msgstr ""

#~ msgid ""
#~ "N-D with shape [num_filter, in_channel "
#~ "// groups, filter_height, filter_width, ...]"
#~ " for NCHW or [filter_height, filter_width,"
#~ " ..., in_channel // groups, num_filter] "
#~ "for NHWC"
#~ msgstr ""

#~ msgid ""
#~ "(where dim=2 for NCHW, dim=1 for "
#~ "NCH, etc.) Stride size, or "
#~ "[stride_height, stride_width, ...]"
#~ msgstr ""

#~ msgid ""
#~ "(where dim=2 for NCHW, dim=1 for "
#~ "NCH, etc.) padding size, or [pad_height,"
#~ " pad_width, ...] for dim ints, or "
#~ "[pad_top, pad_left, pad_bottom, pad_right] for"
#~ " 2*dim ints"
#~ msgstr ""

#~ msgid "dilation size, or [dilation_height, dilation_width]"
#~ msgstr ""

#~ msgid "number of groups"
#~ msgstr ""

#~ msgid ""
#~ "Ordering of dimensions. N indicates "
#~ "batch dimension, C indicates channels, "
#~ "any other character indicates HW (or "
#~ "H or HWD for 1D and 3D)."
#~ msgstr ""

#~ msgid ""
#~ "Elements are converted to this type "
#~ "before elementwise multiplication and "
#~ "summation."
#~ msgstr ""

#~ msgid ""
#~ "**Output** -- N-D with shape [batch, "
#~ "out_channel, out_height, out_width, ...] "
#~ "ordered by `order`."
#~ msgstr ""

#~ msgid ""
#~ "3-D input shape [batch, in_channel, "
#~ "in_width] for layout == 'NCW' and "
#~ "[batch, in_width, in_channel] for layout "
#~ "== 'NWC'"
#~ msgstr ""

#~ msgid ""
#~ "3-D kernel with shape [num_filter, "
#~ "in_channel, filter_size] for layout == "
#~ "'NCW' and [filter_size, in_channel, "
#~ "num_filter] for layout == 'NWC'"
#~ msgstr ""

#~ msgid "The spatial stride along width"
#~ msgstr ""

#~ msgid "Padding size, or ['VALID', 'SAME']"
#~ msgstr ""

#~ msgid "Dilation rate if convolution should be dilated."
#~ msgstr ""

#~ msgid "How input data is laid out, must be one of ['NCW', 'NWC']"
#~ msgstr ""

#~ msgid "The output data type. If None then output is same type as input."
#~ msgstr ""

#~ msgid ""
#~ "1D convolution in NCW layout. See "
#~ ":py:func:`conv` for details on parameters"
#~ msgstr ""

#~ msgid ""
#~ "1D convolution in NWC layout. See "
#~ ":py:func:`conv` for details on parameters"
#~ msgstr ""

#~ msgid "3-D with shape [batch, in_channel, in_width]"
#~ msgstr ""

#~ msgid "3-D with shape [in_channel, num_filter, filter_width]"
#~ msgstr ""

#~ msgid "The output data type. This is used for mixed precision."
#~ msgstr ""

#~ msgid ""
#~ "Used to recover the actual output "
#~ "shape in case there are more than"
#~ " one possible shape.  Must be smaller"
#~ " than stride."
#~ msgstr ""

#~ msgid "**output** -- 3-D with shape [batch, out_channel, out_width]"
#~ msgstr ""

#~ msgid ""
#~ "padding size, or [pad_height, pad_width] "
#~ "for 2 ints, or [pad_top, pad_left, "
#~ "pad_bottom, pad_right] for 4 ints"
#~ msgstr ""

#~ msgid "layout of data"
#~ msgstr ""

#~ msgid ""
#~ "5-D with shape [batch, in_channel_chunk, "
#~ "in_height, in_width, in_channel_block]"
#~ msgstr ""

#~ msgid ""
#~ "6-D with shape [num_filter_chunk, "
#~ "in_channel_chunk, filter_height, filter_width, "
#~ "in_channel_block, num_filter_block]"
#~ msgstr ""

#~ msgid "Input data layout"
#~ msgstr ""

#~ msgid "Output data layout"
#~ msgstr ""

#~ msgid "output data type"
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 5-D with shape [batch, "
#~ "out_channel_chunk, out_height, out_width, "
#~ "out_channel_block]"
#~ msgstr ""

#~ msgid ""
#~ "7-D with shape [num_filter_chunk, "
#~ "in_channel_chunk, filter_height, filter_width, "
#~ "in_channel_block/4, num_filter_block, 4]"
#~ msgstr ""

#~ msgid "numer of int8 elements accumulated"
#~ msgstr ""

#~ msgid "Grouped input symbols"
#~ msgstr ""

#~ msgid "Input shape and dtype"
#~ msgstr ""

#~ msgid "The output type"
#~ msgstr ""

#~ msgid ""
#~ "Unlike other TOPI functions, this "
#~ "function operates on both graph level"
#~ " and operator level."
#~ msgstr ""

#~ msgid "The raw kernel tensor with layout \"NHWC\"."
#~ msgstr ""

#~ msgid "Tile rows of the weight transformation for ConvGemm."
#~ msgstr ""

#~ msgid "Tile columns of the weight transformation for ConvGemm."
#~ msgstr ""

#~ msgid "**output** -- 2-D with shape [CI*KH*KW,CO]"
#~ msgstr ""

#~ msgid "4-D with shape [in_height, in_width, in_channel, batch]"
#~ msgstr ""

#~ msgid "Stride size, or [stride_height, stride_width]"
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 4-D with shape "
#~ "[out_height, out_width, out_channel, batch]"
#~ msgstr ""

#~ msgid "conv2d workload"
#~ msgstr ""

#~ msgid "tvm.autotvm config"
#~ msgstr ""

#~ msgid "**Output** -- Input shapes and layouts, and output shapes and layouts"
#~ msgstr ""

#~ msgid ""
#~ "**Output** -- 4-D with shape [batch, "
#~ "out_channel, out_height, out_width]"
#~ msgstr ""

#~ msgid "The type of output tensor"
#~ msgstr ""

#~ msgid "Attributes of current Transposed 2D convolution"
#~ msgstr ""

#~ msgid "4-D with shape [in_channel, num_filter, filter_height, filter_width]"
#~ msgstr ""

#~ msgid "The spatial stride along height and width"
#~ msgstr ""

#~ msgid "Used to get the right output shape for gradients"
#~ msgstr ""

#~ msgid ""
#~ "Conv2D Winograd in NHWC layout. This "
#~ "is a clean version to be used "
#~ "by the auto-scheduler for both CPU"
#~ " and GPU."
#~ msgstr ""

#~ msgid "padding size, or [pad_height, pad_width]"
#~ msgstr ""

#~ msgid "Specifies the output data type."
#~ msgstr ""

#~ msgid "Whether the kernel is precomputed"
#~ msgstr ""

#~ msgid ""
#~ "Conv2D Winograd without layout transform "
#~ "in NHWC layout. This is a clean"
#~ " version to be used by the "
#~ "auto-scheduler for both CPU and GPU."
#~ msgstr ""

#~ msgid ""
#~ "The raw kernel tensor with layout "
#~ "\"NCHW\". Only 3x3 kernel is supported"
#~ " for now."
#~ msgstr ""

#~ msgid "The convolution algorithm for Winograd NNPACK."
#~ msgstr ""

#~ msgid "**output** -- 4-D with shape [alpha, alpha, CO, CI]"
#~ msgstr ""

#~ msgid "The raw kernel tensor with layout \"NCHW\"."
#~ msgstr ""

#~ msgid ""
#~ "Tile size of winograd transform. e.g."
#~ " 2 for F(2x2, 3x3) and 4 for"
#~ " F(4x4, 3x3)"
#~ msgstr ""

#~ msgid "5-D with shape [batch, in_channel, in_depth, in_height, in_width]"
#~ msgstr ""

#~ msgid ""
#~ "5-D with shape [num_filter, in_channel, "
#~ "filter_depth, filter_height, filter_width]"
#~ msgstr ""

#~ msgid "Stride size, or [strid_depth, stride_height, stride_width]"
#~ msgstr ""

#~ msgid "dilation size, or [dilation_depth, dilation_height, dilation_width]"
#~ msgstr ""

#~ msgid ""
#~ "**Output** -- 5-D with shape [batch, "
#~ "out_channel, out_depth, out_height, out_width]"
#~ msgstr ""

#~ msgid "5-D with shape [batch, in_depth, in_height, in_width, in_channel]"
#~ msgstr ""

#~ msgid ""
#~ "5-D with shape [filter_depth, filter_height,"
#~ " filter_width, in_channel, num_filter]"
#~ msgstr ""

#~ msgid "Stride size, or [stride_depth, stride_height, stride_width]"
#~ msgstr ""

#~ msgid ""
#~ "**Output** -- 5-D with shape [batch, "
#~ "out_depth, out_height, out_width, out_channel]"
#~ msgstr ""

#~ msgid "Attributes of current Transposed 3D convolution"
#~ msgstr ""

#~ msgid ""
#~ "5-D with shape [in_channel, num_filter, "
#~ "filter_depth, filter_height, filter_width]"
#~ msgstr ""

#~ msgid "The spatial stride along depth,height and width"
#~ msgstr ""

#~ msgid "The raw kernel tensor with layout \"NCDHW\"."
#~ msgstr ""

#~ msgid "**output** -- 5-D with shape [alpha, alpha, alpha, CO, CI]"
#~ msgstr ""

#~ msgid "4-D with shape [batch, channel, height, width]"
#~ msgstr ""

#~ msgid "Kernel size for correlation, must be an odd number"
#~ msgstr ""

#~ msgid "Max displacement of Correlation"
#~ msgstr ""

#~ msgid "Stride for data1"
#~ msgstr ""

#~ msgid "Stride for data2 within the neightborhood centered around data1"
#~ msgstr ""

#~ msgid ""
#~ "Padding size, or [pad_height, pad_width] "
#~ "for 2 ints, or [pad_top, pad_left, "
#~ "pad_bottom, pad_right] for 4 ints"
#~ msgstr ""

#~ msgid "operation type is either multiplication or substraction"
#~ msgstr ""

#~ msgid ""
#~ "The deformable convolution operation is "
#~ "described in https://arxiv.org/abs/1703.06211"
#~ msgstr ""

#~ msgid ""
#~ "4-D with shape [batch, deformable_groups "
#~ "* filter_height * filter_width * 2, "
#~ "out_height, out_width]."
#~ msgstr ""

#~ msgid "number of deformable groups"
#~ msgstr ""

#~ msgid ""
#~ "4-D with shape [batch, out_height, "
#~ "out_width,                 deformable_groups * "
#~ "filter_height * filter_width * 2]."
#~ msgstr ""

#~ msgid "4-D with shape [batch, out_height, out_width,"
#~ msgstr ""

#~ msgid "deformable_groups * filter_height * filter_width * 2]."
#~ msgstr ""

#~ msgid ""
#~ "The default implementation of dense in"
#~ " topi. This is an alias of "
#~ "matmul_nt operator for data tensor in"
#~ " non-transposed format and weight "
#~ "tensor in transposed format."
#~ msgstr ""

#~ msgid "2-D with shape [out_dim, in_dim]"
#~ msgstr ""

#~ msgid "1-D with shape [out_dim]"
#~ msgstr ""

#~ msgid "The output type. This is used for mixed precision."
#~ msgstr ""

#~ msgid "Attributes of current dense"
#~ msgstr ""

#~ msgid "4-D tensor in either NCHW or NHWC layout."
#~ msgstr ""

#~ msgid "Size of blocks to compose from channel dimension."
#~ msgstr ""

#~ msgid "Either NCHW or NHWC, indicating data layout."
#~ msgstr ""

#~ msgid ""
#~ "Either DCR or CDR, indicates how "
#~ "channels should be accessed. In DCR, "
#~ "channels are interwoven in the "
#~ "Tensorflow style while in CDR channels"
#~ " are accessed sequentially as in "
#~ "Pytorch."
#~ msgstr ""

#~ msgid ""
#~ "**output** -- Output of shape [N, "
#~ "C / block_size**2, H * block_size, "
#~ "W * block_size]"
#~ msgstr ""

#~ msgid ""
#~ "6-D with shape [out_channel_chunk, 1, "
#~ "filter_height, filter_width, 1, out_channel_block]"
#~ " In NCHWc depthwise convolution, we "
#~ "group kernel's in_channel and "
#~ "channel_multiplier together then do the "
#~ "tiling."
#~ msgstr ""

#~ msgid "Output data type"
#~ msgstr ""

#~ msgid ""
#~ "**Output** -- 5-D with shape [batch, "
#~ "out_channel_chunk, out_height, out_width, "
#~ "out_channel_block]"
#~ msgstr ""

#~ msgid ""
#~ "4-D with shape [filter_height, filter_width,"
#~ " in_channel, channel_multiplier]"
#~ msgstr ""

#~ msgid "4-D with shape [batch, out_height, out_width, out_channel]"
#~ msgstr ""

#~ msgid "**Output** -- 4-D with shape [batch, in_height, in_width, in_channel]"
#~ msgstr ""

#~ msgid ""
#~ "**Output** -- 4-D with shape "
#~ "[filter_height, filter_width, in_channel, "
#~ "channel_multiplier]"
#~ msgstr ""

#~ msgid ""
#~ "4-D with shape [in_channel, "
#~ "channel_multiplier, filter_height, filter_width]"
#~ msgstr ""

#~ msgid "The spatial stride, or (stride_height, stride_width)."
#~ msgstr ""

#~ msgid ""
#~ "**Output** -- 4-D with shape [batch, "
#~ "out_height, out_width, out_channel]"
#~ msgstr ""

#~ msgid "n-D, can be any layout."
#~ msgstr ""

#~ msgid "Dilation stride on each dimension, 1 means no dilation."
#~ msgstr ""

#~ msgid "Value used to dilate the input."
#~ msgstr ""

#~ msgid "The name prefix operators generated"
#~ msgstr ""

#~ msgid "**Output** -- n-D, the same layout as data."
#~ msgstr ""

#~ msgid "The input expression."
#~ msgstr ""

#~ msgid "**equal** -- Whether they equals."
#~ msgstr ""

#~ msgid ""
#~ "Perform softmax activation on the data."
#~ " Use approximation to compute exponent "
#~ "for faster speed."
#~ msgstr ""

#~ msgid "can be any dimension"
#~ msgstr ""

#~ msgid "channel axis"
#~ msgstr ""

#~ msgid "**output** -- output shape is the same as input"
#~ msgstr ""

#~ msgid "Compute equivalent of"
#~ msgstr ""

#~ msgid "Useful for"
#~ msgstr ""

#~ msgid ""
#~ "Encoding explicit re-use of computation"
#~ " in convolution ops operated on a "
#~ "sliding window input"
#~ msgstr ""

#~ msgid ""
#~ "Implementing a FIFO queue to cache "
#~ "intermediate results, e.g. as in Fast"
#~ " WaveNet."
#~ msgstr ""

#~ msgid "Previous value of the FIFO buffer"
#~ msgstr ""

#~ msgid "Specify which axis should be used for buffering"
#~ msgstr ""

#~ msgid "**result** -- Updated value for the buffer"
#~ msgstr ""

#~ msgid "Input array."
#~ msgstr ""

#~ msgid "**output** -- 2-D array with collapsed higher dimensions."
#~ msgstr ""

#~ msgid "**out_value** -- The output."
#~ msgstr ""

#~ msgid "Conv kernel size"
#~ msgstr ""

#~ msgid ""
#~ "* **pad_top** (*int*) -- Padding size"
#~ " on top * **pad_left** (*int*) -- "
#~ "Padding size on left * **pad_down** "
#~ "(*int*) -- Padding size on down. *"
#~ " **pad_right** (*int*) -- Padding size "
#~ "on right."
#~ msgstr ""

#~ msgid "**pad_top** (*int*) -- Padding size on top"
#~ msgstr ""

#~ msgid "**pad_left** (*int*) -- Padding size on left"
#~ msgstr ""

#~ msgid "**pad_down** (*int*) -- Padding size on down."
#~ msgstr ""

#~ msgid "**pad_right** (*int*) -- Padding size on right."
#~ msgstr ""

#~ msgid ""
#~ "* **pad_left** (*int*) -- Padding size"
#~ " on left * **pad_right** (*int*) --"
#~ " Padding size on right."
#~ msgstr ""

#~ msgid ""
#~ "* **pad_front** (*int*) -- Padding size"
#~ " on front. * **pad_top** (*int*) --"
#~ " Padding size on top * **pad_left**"
#~ " (*int*) -- Padding size on left "
#~ "* **pad_back** (*int*) -- Padding size"
#~ " on back. * **pad_down** (*int*) --"
#~ " Padding size on down. * "
#~ "**pad_right** (*int*) -- Padding size on"
#~ " right."
#~ msgstr ""

#~ msgid "**pad_front** (*int*) -- Padding size on front."
#~ msgstr ""

#~ msgid "**pad_back** (*int*) -- Padding size on back."
#~ msgstr ""

#~ msgid ""
#~ "* **pad_top** (*int*) -- Padding size"
#~ " on top * **pad_down** (*int*) -- "
#~ "Padding size on down. * **pad_left** "
#~ "(*int*) -- Padding size on left *"
#~ " **pad_right** (*int*) -- Padding size "
#~ "on right."
#~ msgstr ""

#~ msgid ""
#~ "It decides the height and width "
#~ "dimension according to the layout "
#~ "string, in which 'W' and 'H' means"
#~ " width and height respectively. Width "
#~ "and height dimension cannot be split."
#~ " For example, NCHW, NCHW16c, etc. are"
#~ " valid for pool, while NCHW16w, "
#~ "NCHW16h are not. See parameter `layout`"
#~ " for more information of the layout"
#~ " string convention."
#~ msgstr ""

#~ msgid ""
#~ "**output** -- n-D in same layout "
#~ "with height and width dimension size "
#~ "of 1. e.g., for NCHW, the output"
#~ " shape will be [batch, channel, 1,"
#~ " 1]"
#~ msgstr ""

#~ msgid "3-D with shape [num_filter, in_channel, filter_size]"
#~ msgstr ""

#~ msgid ""
#~ "Padding size can be an integer for"
#~ " equal padding, a tuple of (left, "
#~ "right) or a string in ['VALID', "
#~ "'SAME']."
#~ msgstr ""

#~ msgid "Number of groups"
#~ msgstr ""

#~ msgid "3-D with shape [batch, in_width, in_channel]"
#~ msgstr ""

#~ msgid "3-D with shape [filter_size, in_channel, num_filter]"
#~ msgstr ""

#~ msgid ""
#~ "4-D with shape [num_filter, in_channel "
#~ "// groups, filter_height, filter_width]"
#~ msgstr ""

#~ msgid "4-D with shape [batch, in_height, in_width, in_channel, ...]"
#~ msgstr ""

#~ msgid ""
#~ "4-D with shape [filter_height, filter_width,"
#~ " in_channel // groups, num_filter]"
#~ msgstr ""

#~ msgid ""
#~ "4-D with shape [in_channel, out_channel "
#~ "// groups, filter_height, filter_width]"
#~ msgstr ""

#~ msgid ""
#~ "E.g. layout_transform(t, \"NCHW\", \"CNHW\") "
#~ "--> relay.transpose(t, [1, 0, 2, 3])"
#~ msgstr ""

#~ msgid "The Tensor to transpose"
#~ msgstr ""

#~ msgid "The current layout e.g. NCHW or OIHW"
#~ msgstr ""

#~ msgid "The desired layout, must be compatible with current_layout"
#~ msgstr ""

#~ msgid "The slope for the small gradient when x < 0"
#~ msgstr ""

#~ msgid "2-D input data"
#~ msgstr ""

#~ msgid "**output** -- 2-D output with same shape"
#~ msgstr ""

#~ msgid ""
#~ "sum_sqr_up^i{x, y} = (bias+((alpha/size)*"
#~ "                                 {sum_{j=max(0, "
#~ "i-size/2)}^{min(N-1,i+size/2)}"
#~ "                                      (data^j{x,y})^2}))^beta "
#~ "output^i{x, y} = data^i{x, y}/sum_sqr_up^i{x,"
#~ " y} N is the number for input"
#~ " channels"
#~ msgstr ""

#~ msgid "normalisation window size"
#~ msgstr ""

#~ msgid "input data layout channel axis default value is 1 for NCHW format"
#~ msgstr ""

#~ msgid "offset to avoid dividing by 0"
#~ msgstr ""

#~ msgid "to be divided"
#~ msgstr ""

#~ msgid "exponent"
#~ msgstr ""

#~ msgid "**output** -- 4-D output with same shape"
#~ msgstr ""

#~ msgid "Whether the tensor_a is in transposed format."
#~ msgstr ""

#~ msgid "Whether the tensor_b is in transposed format."
#~ msgstr ""

#~ msgid "Attributes of current matmul"
#~ msgstr ""

#~ msgid "Pad width on each dimension to pad the before the axis begin."
#~ msgstr ""

#~ msgid "Pad width each dimension to pad the after the axis end."
#~ msgstr ""

#~ msgid "Type of mirror padding to apply. Must be SYMMETRIC or REFLECT"
#~ msgstr ""

#~ msgid "**Output** -- n-D, the same layout as Input."
#~ msgstr ""

#~ msgid "output{n, i_1, i_2, ..., i_k} = -p * w"
#~ msgstr ""

#~ msgid "where t = target{n, i_1, i_2, ..., i_k}"
#~ msgstr ""

#~ msgid ""
#~ "p = predictions{n, t, i_1, i_2, "
#~ "i_k} w = weights{n, i_1, i_2, ...,"
#~ " i_k} if t != ignore_index else "
#~ "0"
#~ msgstr ""

#~ msgid "result = reduction(output)"
#~ msgstr ""

#~ msgid ""
#~ "(k+2)-D with shape (N, C, d_1, "
#~ "d_2, ..., d_k), where C is the "
#~ "number of target classes"
#~ msgstr ""

#~ msgid ""
#~ "(k+1)-D with shape (N, d_1, d_2, "
#~ "..., d_k) The target value of the"
#~ " input."
#~ msgstr ""

#~ msgid "1-D with shape (C,) The weight of each target value."
#~ msgstr ""

#~ msgid ""
#~ "The reduction method to apply to "
#~ "output. Can be \"mean\", \"sum\" or "
#~ "\"none\"."
#~ msgstr ""

#~ msgid "The target value to ignore."
#~ msgstr ""

#~ msgid ""
#~ "**output** -- a scalar if the "
#~ "reduction type is \"mean\" or \"sum\","
#~ " otherwise the same shape as "
#~ "`target`."
#~ msgstr ""

#~ msgid "The value to be padded."
#~ msgstr ""

#~ msgid ""
#~ "Width axis is determined according to"
#~ " the layout string. in which 'w' "
#~ "means width. Width dimension cannot be"
#~ " split. For example, NCW, NCW16c, "
#~ "etc. are valid for pool, while "
#~ "NCW16w is not. See parameter `layout`"
#~ " for more information of the layout"
#~ " string convention."
#~ msgstr ""

#~ msgid "Kernel size, [kernel_width]"
#~ msgstr ""

#~ msgid "Stride size, [stride_width]"
#~ msgstr ""

#~ msgid "Pad size, [pad_left, pad_right]"
#~ msgstr ""

#~ msgid "Whether to use ceil when calculating output size."
#~ msgstr ""

#~ msgid ""
#~ "Layout of the input data. The "
#~ "layout is supposed to be composed "
#~ "of upper cases, lower cases and "
#~ "numbers, where upper case indicates a"
#~ " dimension and the corresponding lower "
#~ "case with factor size indicates the "
#~ "split dimension. For example, NCW16c can"
#~ " describe a 4-D tensor of "
#~ "[batch_size, channel, width, channel_block], "
#~ "in which channel_block=16 is a split "
#~ "of dimension channel."
#~ msgstr ""

#~ msgid "Whether include padding in the calculation when pool_type is 'avg'"
#~ msgstr ""

#~ msgid "Kernel size, [kernel_height, kernel_width]"
#~ msgstr ""

#~ msgid "Stride size, [stride_height, stride_width]"
#~ msgstr ""

#~ msgid "Pad size, [pad_top, pad_left, pad_bottom, pad_right]]"
#~ msgstr ""

#~ msgid ""
#~ "It decides the depth, height and "
#~ "width dimension according to the layout"
#~ " string, in which 'D', 'W' and "
#~ "'H' means depth, width and height "
#~ "respectively. Depth, width and height "
#~ "dimension cannot be split. For example,"
#~ " NCDHW, NCDHW16c, etc. are valid for"
#~ " pool, while NCDHW16d, NCDHW16w, NCDHW16h"
#~ " are not. See parameter `layout` for"
#~ " more information of the layout "
#~ "string convention."
#~ msgstr ""

#~ msgid "Kernel size, [kernel_depth, kernel_height, kernel_width]"
#~ msgstr ""

#~ msgid "Stride size, [stride_depth, stride_height, stride_width]"
#~ msgstr ""

#~ msgid ""
#~ "Pad size, [pad_front, pad_top, pad_left, "
#~ "pad_back, pad_bottom, pad_right]"
#~ msgstr ""

#~ msgid ""
#~ "Layout of the input data. The "
#~ "layout is supposed to be composed "
#~ "of upper cases, lower cases and "
#~ "numbers, where upper case indicates a"
#~ " dimension and the corresponding lower "
#~ "case with factor size indicates the "
#~ "split dimension. For example, NCDHW16c "
#~ "can describe a 6-D tensor of "
#~ "[batch_size, channel, depth, height, width,"
#~ " channel_block], in which channel_block=16 "
#~ "is a split of dimension channel."
#~ msgstr ""

#~ msgid ""
#~ "PReLU. It accepts two arguments: an "
#~ "input ``x`` and a weight array "
#~ "``W`` and computes the output as "
#~ ":math:`PReLU(x) y = x > 0 ? "
#~ "x : W * x`, where :math:`*` "
#~ "is an elementwise multiplication for "
#~ "each sample in the batch."
#~ msgstr ""

#~ msgid "Channelised slope tensor for prelu"
#~ msgstr ""

#~ msgid "The axis where the channel data needs to be applied"
#~ msgstr ""

#~ msgid ""
#~ "* **y** (*tvm.te.Tensor*) -- The result."
#~ " * *Links* * *-----* * **[http** "
#~ "(*//arxiv.org/pdf/1502.01852v1.pdf]*)"
#~ msgstr ""

#~ msgid "**y** (*tvm.te.Tensor*) -- The result."
#~ msgstr ""

#~ msgid "*Links*"
#~ msgstr ""

#~ msgid "*-----*"
#~ msgstr ""

#~ msgid "**[http** (*//arxiv.org/pdf/1502.01852v1.pdf]*)"
#~ msgstr ""

#~ msgid "4-D input tensor, NCHW layout [batch, channel, height, width]"
#~ msgstr ""

#~ msgid "Scale tensor, 1-D of size channel number"
#~ msgstr ""

#~ msgid "Shift tensor, 1-D of size channel number"
#~ msgstr ""

#~ msgid "**Output** -- Output tensor, layout is NCHW"
#~ msgstr ""

#~ msgid ""
#~ "5-D input tensor, NCHWc layout [batch,"
#~ " channel_chunk, height, width, channel_block]"
#~ msgstr ""

#~ msgid "Scale tensor, 2-D of size [channel_chunk, channel_block]"
#~ msgstr ""

#~ msgid "Shift tensor, 2-D of size [channel_chunk, channel_block]"
#~ msgstr ""

#~ msgid "**Output** -- Output tensor, layout is NHWC"
#~ msgstr ""

#~ msgid "4-D input tensor, NHWC layout [batch, height, width, channel]"
#~ msgstr ""

#~ msgid "**out** -- The simplified output"
#~ msgstr ""

#~ msgid ""
#~ "Simulated QNN dequantize operator that "
#~ "mimics QNN outputs without changing "
#~ "datatype. The benefit of this operator"
#~ " over true QNN dequantize is that "
#~ "this operator allows dynamic datatype "
#~ "selection and can operate on both "
#~ "per-channel and scalar scales and "
#~ "zero points while QNN dequantize "
#~ "requires both of these to be fixed"
#~ " at compile time."
#~ msgstr ""

#~ msgid "An N-D input tensor to the operator."
#~ msgstr ""

#~ msgid ""
#~ "A scalar variable that indicates which"
#~ " datatype to simulate dequantization with."
#~ " Use SQNN_DTYPE_TO_CODE to convert a "
#~ "dtype string into the corresponding "
#~ "variable value."
#~ msgstr ""

#~ msgid ""
#~ "A scalar tensor representing the scale"
#~ " to use when dequantizing from "
#~ "integer datatypes. When it contains more"
#~ " than a single value, N must "
#~ "match the number of channels in "
#~ "data."
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor representing the zero "
#~ "point to use when dequantizing from "
#~ "integer datatypes. When it contains more"
#~ " than a single value, N must "
#~ "match the number of channels in "
#~ "data."
#~ msgstr ""

#~ msgid ""
#~ "The channel axis for quantization. "
#~ "Default value is -1 which corresponds"
#~ " to the last axis."
#~ msgstr ""

#~ msgid ""
#~ "Simulated QNN quantize operator that "
#~ "mimics QNN outputs without changing "
#~ "datatype. The benefit of this operator"
#~ " over true QNN quantize is that "
#~ "this operator allows dynamic datatype "
#~ "selection and can operate on both "
#~ "per-channel and scalar scales and "
#~ "zero points while QNN quantize requires"
#~ " both of these to be fixed at"
#~ " compile time."
#~ msgstr ""

#~ msgid ""
#~ "A scalar variable that indicates which"
#~ " datatype to simulate quantization with."
#~ " Use SQNN_DTYPE_TO_CODE to convert a "
#~ "dtype string into the corresponding "
#~ "variable value."
#~ msgstr ""

#~ msgid ""
#~ "A scalar tensor representing the scale"
#~ " to use when quantizing to integer"
#~ " datatypes. When it contains more "
#~ "than a single value, N must match"
#~ " the number of channels in data."
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor representing the zero "
#~ "point to use when quantizing to "
#~ "integer datatypes. When it contains more"
#~ " than a single value, N must "
#~ "match the number of channels in "
#~ "data."
#~ msgstr ""

#~ msgid ""
#~ "list of shape [M] where M is "
#~ "number of spatial dims, specifies "
#~ "zero-padding size before each spatial "
#~ "dimension."
#~ msgstr ""

#~ msgid ""
#~ "list of shape [M] where M is "
#~ "number of spatial dims, specifies "
#~ "zero-padding size after each spatial "
#~ "dimension."
#~ msgstr ""

#~ msgid "The value used for padding."
#~ msgstr ""

#~ msgid "Size of blocks to decompose into channel dimension."
#~ msgstr ""

#~ msgid ""
#~ "**output** -- Output of shape [N, "
#~ "C * block_size**2, H / block_size, "
#~ "W / block_size]"
#~ msgstr ""

#~ msgid "2-D with shape [M, N]"
#~ msgstr ""

#~ msgid "1-D with shape [nnz] (CSR)"
#~ msgstr ""

#~ msgid "1-D with shape [M + 1] (CSR)"
#~ msgstr ""

#~ msgid "**output** -- 2-D with shape [M, N]"
#~ msgstr ""

#~ msgid ""
#~ "4-D with shape ``[M, H, W, K]``"
#~ " (layout=NHWC)  4-D with shape ``[M, "
#~ "K, H, W]`` (layout=NCHW)"
#~ msgstr ""

#~ msgid "4-D with shape ``[M, H, W, K]`` (layout=NHWC)"
#~ msgstr ""

#~ msgid "4-D with shape ``[M, K, H, W]`` (layout=NCHW)"
#~ msgstr ""

#~ msgid ""
#~ "2-D with shape ``[num_blocks, bs_r]`` "
#~ "(BSR)  3-D with shape ``[num_blocks, "
#~ "bs_r, bs_c]`` (BSR)"
#~ msgstr ""

#~ msgid "2-D with shape ``[num_blocks, bs_r]`` (BSR)"
#~ msgstr ""

#~ msgid "3-D with shape ``[num_blocks, bs_r, bs_c]`` (BSR)"
#~ msgstr ""

#~ msgid "1-D with shape ``[num_blocks]`` (BSR)"
#~ msgstr ""

#~ msgid "1-D with shape ``[(N + 1) // bs_r]`` (BSR)"
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 4-D with shape [M, "
#~ "H, W, N] (layout=NHWC) 4-D with "
#~ "shape [M, N, H ,W] (layout=NCHW)"
#~ msgstr ""

#~ msgid "2-D with shape [M, K]"
#~ msgstr ""

#~ msgid ""
#~ "1-D with shape [nnz] (CSR) or 3-D"
#~ " with shape [num_blocks, bs_r, bs_c] "
#~ "(BSR)"
#~ msgstr ""

#~ msgid "1-D with shape [nnz] (CSR) or 1-D with shape [num_blocks] (BSR)"
#~ msgstr ""

#~ msgid "1-D with shape [N + 1] (CSR) or 1-D with shape [(N + 1) // bs_r] (BSR)"
#~ msgstr ""

#~ msgid "Indicates whether lhs or rhs matrix is sparse. Default value is False."
#~ msgstr ""

#~ msgid ""
#~ "This is used for modifying the "
#~ "inputs weights so they are more "
#~ "amenable for the target."
#~ msgstr ""

#~ msgid "1-D with shape [M + 1] (CSR) or 1-D with shape [(M + 1) // bs_r] (BSR)"
#~ msgstr ""

#~ msgid "2-D with shape [N, K]"
#~ msgstr ""

#~ msgid ""
#~ "Transpose a square sparse matrix, `A`"
#~ " is an n-by-n sparse matrix in "
#~ "the CSR format. ** Currently only "
#~ "support Square Matrices **"
#~ msgstr ""

#~ msgid "1-D with shape [nonzeros]"
#~ msgstr ""

#~ msgid "1-D with shape [nonzeros], dtype of 'int32'"
#~ msgstr ""

#~ msgid "1-D with shape [n+1], dtype of 'int32'"
#~ msgstr ""

#~ msgid ""
#~ "* **out_data** (*tvm.te.Tensor*) -- 1-D "
#~ "with shape [nonzeros] * **out_indices** "
#~ "(*tvm.te.Tensor*) -- 1-D with shape "
#~ "[nonzeros], dtype of 'int32' * "
#~ "**out_indptr** (*tvm.te.Tensor*) -- 1-D with"
#~ " shape [n+1], dtype of 'int32'"
#~ msgstr ""

#~ msgid "**out_data** (*tvm.te.Tensor*) -- 1-D with shape [nonzeros]"
#~ msgstr ""

#~ msgid ""
#~ "**out_indices** (*tvm.te.Tensor*) -- 1-D with"
#~ " shape [nonzeros], dtype of 'int32'"
#~ msgstr ""

#~ msgid ""
#~ "**out_indptr** (*tvm.te.Tensor*) -- 1-D with"
#~ " shape [n+1], dtype of 'int32'"
#~ msgstr ""

#~ msgid "Input/output Tensor of a TVM subgraph."
#~ msgstr ""

#~ msgid "Map from the input Tensor to its buffer name."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid ""
#~ "The buffer name is specially designed,"
#~ " and these buffer should be provided"
#~ " in `SearchTask(..., task_inputs={...})`."
#~ msgstr ""

#~ msgid "The output tensor of conv2d_NCHWc."
#~ msgstr ""

#~ msgid "The output dtype."
#~ msgstr ""

#~ msgid "**unpacked_out** -- The unpacked output tensor in NCHW layout."
#~ msgstr ""

#~ msgid "Nearest neighbor and bilinear upsampling are supported."
#~ msgstr ""

#~ msgid ""
#~ "inputs is a 4-D tensor with shape"
#~ " [batch, channel, in_height, in_width] or"
#~ "  [batch, in_height, in_width, channel]"
#~ msgstr ""

#~ msgid "Scaling factor for height"
#~ msgstr ""

#~ msgid "Scaling factor for width"
#~ msgstr ""

#~ msgid "either \"NCHW\" or \"NHWC\""
#~ msgstr ""

#~ msgid "Method to be used for upsampling."
#~ msgstr ""

#~ msgid ""
#~ "Shape to return. If left None will"
#~ " be inferred (If shape is determined"
#~ " dynamically, pass out_dtype.shape as "
#~ "output_shape)"
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 4-D with shape [batch, "
#~ "channel, in_height*scale_h, in_width*scale_w] or "
#~ "[batch, in_height*scale, in_width*scale, channel]"
#~ msgstr ""

#~ msgid ""
#~ "inputs is a 5-D tensor with shape"
#~ " [batch, channel, in_depth, in_height, "
#~ "in_width] or  [batch, in_depth, in_height, "
#~ "in_width, channel]"
#~ msgstr ""

#~ msgid "Scaling factor for depth"
#~ msgstr ""

#~ msgid "either \"NCDHW\" or \"NDHWC\""
#~ msgstr ""

#~ msgid ""
#~ "Describes how to transform the "
#~ "coordinate in the resized tensor to "
#~ "the coordinate in the original tensor."
#~ " Refer to the ONNX Resize operator"
#~ " specification for details. Available "
#~ "options are \"half_pixel\", \"align_corners\" "
#~ "and \"asymmetric\"."
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 5-D with shape [batch, "
#~ "channel, in_depth*scale, in_height*scale, "
#~ "in_width*scale] or [batch, in_depth*scale, "
#~ "in_height*scale, in_width*scale, channel]"
#~ msgstr ""

#~ msgid "IMAGE network operators"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`affine_grid "
#~ "<tvm.topi.image.tvm.topi.image.affine_grid>`\\ \\(data\\, "
#~ "target\\_shape\\)"
#~ msgstr ""

#~ msgid "affine_grid operator that generates 2D sampling grid."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`crop_and_resize "
#~ "<tvm.topi.image.tvm.topi.image.crop_and_resize>`\\ \\(data\\,"
#~ " boxes\\, box\\_indices\\, ...\\)"
#~ msgstr ""

#~ msgid "Perform crop and resize operation on the data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dilation2d_nchw "
#~ "<tvm.topi.image.tvm.topi.image.dilation2d_nchw>`\\ \\(input\\,"
#~ " filter\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid "Morphological dilation operator in NCHW layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dilation2d_nhwc "
#~ "<tvm.topi.image.tvm.topi.image.dilation2d_nhwc>`\\ \\(input\\,"
#~ " filter\\, stride\\, ...\\)"
#~ msgstr ""

#~ msgid "Morphological 2d dilation NHWC layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_1d_indices "
#~ "<tvm.topi.image.tvm.topi.image.get_1d_indices>`\\ "
#~ "\\(indices\\[\\, layout\\]\\)"
#~ msgstr ""

#~ msgid "Get 1d indices"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_1d_pixel "
#~ "<tvm.topi.image.tvm.topi.image.get_1d_pixel>`\\ \\(data\\, "
#~ "layout\\, image\\_width\\, n\\, ...\\)"
#~ msgstr ""

#~ msgid "Get 1d pixel"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_2d_indices "
#~ "<tvm.topi.image.tvm.topi.image.get_2d_indices>`\\ "
#~ "\\(indices\\[\\, layout\\]\\)"
#~ msgstr ""

#~ msgid "Get 2d indices"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_2d_pixel "
#~ "<tvm.topi.image.tvm.topi.image.get_2d_pixel>`\\ \\(data\\, "
#~ "layout\\, image\\_height\\, ...\\)"
#~ msgstr ""

#~ msgid "Get 2d pixel"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_3d_indices "
#~ "<tvm.topi.image.tvm.topi.image.get_3d_indices>`\\ "
#~ "\\(indices\\[\\, layout\\]\\)"
#~ msgstr ""

#~ msgid "Get 3d indices"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_3d_pixel "
#~ "<tvm.topi.image.tvm.topi.image.get_3d_pixel>`\\ \\(data\\, "
#~ "layout\\, image\\_depth\\, ...\\)"
#~ msgstr ""

#~ msgid "Get 3d pixel"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_closest_index "
#~ "<tvm.topi.image.tvm.topi.image.get_closest_index>`\\ "
#~ "\\(in\\_x\\, rounding\\_method\\, boxes\\)"
#~ msgstr ""

#~ msgid "get the closest index to a value based on a certain rounding method"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_inx <tvm.topi.image.tvm.topi.image.get_inx>`\\ "
#~ "\\(x\\, image\\_width\\, target\\_width\\, ...\\[\\,"
#~ " ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Infer input x from output x with"
#~ " various coordinate transformation methods"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple "
#~ "<tvm.topi.image.tvm.topi.image.get_pad_tuple>`\\ \\(padding\\,"
#~ " kernel\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`grid_sample "
#~ "<tvm.topi.image.tvm.topi.image.grid_sample>`\\ \\(data\\, "
#~ "grid\\[\\, method\\, layout\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Applies bilinear sampling to input feature map."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`nchw_pack_layout "
#~ "<tvm.topi.image.tvm.topi.image.nchw_pack_layout>`\\ "
#~ "\\(layout\\_info\\)"
#~ msgstr ""

#~ msgid "Check whether the layout type is NCHWinic"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`nchw_xc_layout "
#~ "<tvm.topi.image.tvm.topi.image.nchw_xc_layout>`\\ "
#~ "\\(layout\\_info\\)"
#~ msgstr ""

#~ msgid "Check whether the layout type is NCHWxc"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pad <tvm.topi.image.tvm.topi.image.pad>`\\ "
#~ "\\(data\\, pad\\_before\\[\\, pad\\_after\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`resize1d <tvm.topi.image.tvm.topi.image.resize1d>`\\"
#~ " \\(data\\, roi\\, size\\[\\, layout\\, "
#~ "method\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Perform resize operation on the data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`resize2d <tvm.topi.image.tvm.topi.image.resize2d>`\\"
#~ " \\(data\\, roi\\, size\\[\\, layout\\, "
#~ "method\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`resize3d <tvm.topi.image.tvm.topi.image.resize3d>`\\"
#~ " \\(data\\, roi\\, size\\[\\, layout\\, "
#~ "method\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`simplify <tvm.topi.image.tvm.topi.image.simplify>`\\"
#~ " \\(expr\\)"
#~ msgstr ""

#~ msgid ""
#~ "This operation is described in "
#~ "https://arxiv.org/pdf/1506.02025.pdf. It generates a"
#~ " uniform sampling grid within the "
#~ "target shape and normalizes it to "
#~ "[-1, 1]. The provided affine "
#~ "transformation is then applied on the"
#~ " sampling grid."
#~ msgstr ""

#~ msgid "3-D with shape [batch, 2, 3]. The affine matrix."
#~ msgstr ""

#~ msgid "Specifies the output shape (H, W)."
#~ msgstr ""

#~ msgid "**Output** -- 4-D with shape [batch, 2, target_height, target_width]"
#~ msgstr ""

#~ msgid ""
#~ "A 2-D tensor of shape [num_boxes, "
#~ "4]. Each row of the tensor "
#~ "specifies the coordinates of a box."
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor of shape [num_boxes], "
#~ "box_indices[i] specifies the data that "
#~ "the i-th box refers to."
#~ msgstr ""

#~ msgid "The target size of each box."
#~ msgstr ""

#~ msgid "\"NCHW\", \"NHWC\""
#~ msgstr ""

#~ msgid "Method to be used for resizing."
#~ msgstr ""

#~ msgid "Value used for extrapolation, when applicable."
#~ msgstr ""

#~ msgid "Type to return. If left None will be same as input type."
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 4-D with shape [num_boxes,"
#~ " channel, crop_height, crop_width] or "
#~ "[num_boxes, crop_height, crop_width, channel]"
#~ msgstr ""

#~ msgid "3-D with shape [ in_channel, filter_height, filter_width]"
#~ msgstr ""

#~ msgid "Padding size"
#~ msgstr ""

#~ msgid "**Output** -- 4-D with shape [batch, in_channel, out_height, out_width]"
#~ msgstr ""

#~ msgid "3-D with shape [filter_height, filter_width, in_channel]"
#~ msgstr ""

#~ msgid "**Output** -- 4-D with shape [batch, out_height, out_width, in_channel]"
#~ msgstr ""

#~ msgid ""
#~ "Given :math:`data` and :math:`grid`, assuming"
#~ " NCHW layout, then the output is "
#~ "computed by"
#~ msgstr ""

#~ msgid ""
#~ "x_{src} = grid[batch, 0, y_{dst}, x_{dst}] \\\n"
#~ "y_{src} = grid[batch, 1, y_{dst}, x_{dst}] \\\n"
#~ "output[batch, channel, y_{dst}, x_{dst}] = "
#~ "G(data[batch, channel, y_{src}, x_{src})"
#~ msgstr ""

#~ msgid ""
#~ ":math:`x_{dst}`, :math:`y_{dst}` enumerate all "
#~ "spatial locations in :math:`output`, and "
#~ ":math:`G()` denotes the interpolation method."
#~ " The out-boundary points will be "
#~ "padded with zeros if the padding_mode"
#~ " is \"zeros\". The shape of the "
#~ "output will be (data.shape[0], data.shape[1],"
#~ " grid.shape[2], grid.shape[3])."
#~ msgstr ""

#~ msgid "The operator assumes that :math:`grid` has been normalized to [-1, 1]."
#~ msgstr ""

#~ msgid ""
#~ "grid_sample often cooperates with affine_grid"
#~ " which generates sampling grids for "
#~ "grid_sample."
#~ msgstr ""

#~ msgid "4-D with shape [batch, 2, out_height, out_width]"
#~ msgstr ""

#~ msgid "The interpolation method. Only 'bilinear' is supported."
#~ msgstr ""

#~ msgid "The layout of input data and the output."
#~ msgstr ""

#~ msgid ""
#~ "inputs is a 3-D tensor with shape"
#~ " [batch, channel in_width] or  [batch "
#~ "in_width, channel]"
#~ msgstr ""

#~ msgid ""
#~ "The region of interest for cropping "
#~ "the input image. Expected to be of"
#~ " size 2, and format [start_w, end_w]."
#~ " Only used if coordinate_transformation_mode "
#~ "is tf_crop_and_resize."
#~ msgstr ""

#~ msgid "Output resolution scale to"
#~ msgstr ""

#~ msgid "\"NCW\", \"NWC\", or \"NCWc\"."
#~ msgstr ""

#~ msgid "method of interpolation (\"nearest\", \"linear\", \"bicubic\")"
#~ msgstr ""

#~ msgid ""
#~ "Describes how to transform the "
#~ "coordinate in the resized tensor to "
#~ "the coordinate in the original tensor."
#~ " [half_pixel, align_corners, asymmetric, "
#~ "pytorch_half_pixel, tf_half_pixel_for_nn, and "
#~ "tf_crop_and_resize]."
#~ msgstr ""

#~ msgid "Method for rounding coordinate locations"
#~ msgstr ""

#~ msgid "Bicubic spline coefficient"
#~ msgstr ""

#~ msgid "Exclude values outside the image fdor bicubic interpolation"
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 4-D with shape [batch, "
#~ "chananel, in_width*scale] or [batch, "
#~ "in_width*scale, channel] or 5-D with "
#~ "shape [batch, channel-major, in_width*scale,"
#~ " channel-minor]"
#~ msgstr ""

#~ msgid ""
#~ "The region of interest for cropping "
#~ "the input image. Expected to be of"
#~ " size 4, and format [start_h, "
#~ "start_w, end_h, end_w]. Only used if "
#~ "coordinate_transformation_mode is tf_crop_and_resize."
#~ msgstr ""

#~ msgid "\"NCHW\", \"NHWC\", or \"NCHWc\"."
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 4-D with shape [batch, "
#~ "channel, in_height*scale, in_width*scale] or "
#~ "[batch, in_height*scale, in_width*scale, channel]"
#~ " or 5-D with shape [batch, "
#~ "channel-major, in_height*scale, in_width*scale, "
#~ "channel-minor]"
#~ msgstr ""

#~ msgid ""
#~ "The region of interest for cropping "
#~ "the input image. Expected to be of"
#~ " size 6, and format [start_d, "
#~ "start_h, start_w, end_d, end_h, end_w]. "
#~ "Only used if coordinate_transformation_mode is"
#~ " tf_crop_and_resize."
#~ msgstr ""

#~ msgid "\"NCDHW\", \"NDHWC\", or \"NCDHWc\"."
#~ msgstr ""

#~ msgid ""
#~ "**output** -- 4-D with shape [batch, "
#~ "channel, in_depth*scale, in_height*scale, "
#~ "in_width*scale] or [batch, in_depth*scale, "
#~ "in_height*scale, in_width*scale, channel] or "
#~ "5-D with shape [batch, channel-major,"
#~ " in_depth*scale, in_height*scale, in_width*scale, "
#~ "channel-minor]"
#~ msgstr ""

#~ msgid "Sparse operators"
#~ msgstr ""

#~ msgid ":py:obj:`csrmm <tvm.topi.sparse.csrmm>`\\ \\(a\\, b\\[\\, c\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "The `csrmm` routine performs a "
#~ "matrix-matrix operation defined as :math:`C"
#~ " := A*B + C`, where `B` and "
#~ "`C` are dense matrices, `A` is an"
#~ " m-by-k sparse matrix in the CSR "
#~ "format."
#~ msgstr ""

#~ msgid ":py:obj:`csrmv <tvm.topi.sparse.csrmv>`\\ \\(a\\, x\\[\\, y\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "The `csrmv` routine performs a "
#~ "matrix-vector operation defined as :math:`y"
#~ " := A*x + y`, where `x` and "
#~ "`y` are vectors, `A` is an m-by-k"
#~ " sparse matrix in the CSR format."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dense <tvm.topi.sparse.dense>`\\ \\(data\\, "
#~ "weight\\[\\, bias\\]\\)"
#~ msgstr ""

#~ msgid "Applies a linear transformation: :math:`Y = XW^T + b`."
#~ msgstr ""

#~ msgid "2-D sparse matrix with shape [m, k]"
#~ msgstr ""

#~ msgid "2-D dense matrix with shape [k, n]"
#~ msgstr ""

#~ msgid "1-D dense vector with shape [n]"
#~ msgstr ""

#~ msgid "**output** -- 2-D with shape [m, n]"
#~ msgstr ""

#~ msgid "2-D dense matrix with shape [k, 1]"
#~ msgstr ""

#~ msgid "1-D dense vector with shape [1]"
#~ msgstr ""

#~ msgid "**output** -- 2-D dense matrix with shape [m, 1]"
#~ msgstr ""

#~ msgid ""
#~ "Applies a linear transformation: :math:`Y "
#~ "= XW^T + b`. Either data or "
#~ "weight should be tvm.contrib.sparse.CSRNDArray."
#~ msgstr ""

