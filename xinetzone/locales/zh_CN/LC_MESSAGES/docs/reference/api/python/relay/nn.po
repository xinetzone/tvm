# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-01-20 16:06+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../doc/docs/reference/api/python/relay/nn.rst:19
msgid "tvm.relay.nn"
msgstr ""

#: ../../docstring of tvm.relay.nn:1
msgid "Neural network related operators."
msgstr ""

#: ../../docstring of tvm.relay.nn:1
msgid "**Classes:**"
msgstr ""

#: ../../docstring of tvm.relay.nn:1:<autosummary>:1
msgid ":py:obj:`Constant <tvm.relay.nn.Constant>`\\ \\(data\\[\\, span\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1 tvm.relay.nn:1:<autosummary>:1
msgid "A constant expression in Relay."
msgstr ""

#: ../../docstring of tvm.relay.nn:1:<autosummary>:1
msgid ":py:obj:`Expr <tvm.relay.nn.Expr>`\\"
msgstr ""

#: ../../docstring of tvm.relay.nn:1:<autosummary>:1
msgid ":py:class:`~tvm.ir.expr.RelayExpr` 的别名"
msgstr ""

#: ../../docstring of tvm.relay.nn:1
msgid "**Functions:**"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_avg_pool1d <tvm.relay.nn.adaptive_avg_pool1d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "1D adaptive average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_avg_pool2d <tvm.relay.nn.adaptive_avg_pool2d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "2D adaptive average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_avg_pool3d <tvm.relay.nn.adaptive_avg_pool3d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "3D adaptive avg pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_max_pool1d <tvm.relay.nn.adaptive_max_pool1d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "1D adaptive max pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_max_pool2d <tvm.relay.nn.adaptive_max_pool2d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "2D adaptive max pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`adaptive_max_pool3d <tvm.relay.nn.adaptive_max_pool3d>`\\ "
"\\(data\\[\\, output\\_size\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "3D adaptive max pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`avg_pool1d <tvm.relay.nn.avg_pool1d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.avg_pool1d:1
msgid "1D average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`avg_pool2d <tvm.relay.nn.avg_pool2d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.avg_pool2d:1
msgid "2D average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`avg_pool2d_grad <tvm.relay.nn.avg_pool2d_grad>`\\ "
"\\(out\\_grad\\, data\\[\\, pool\\_size\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.avg_pool2d_grad:1
msgid "Gradient of 2D average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`avg_pool3d <tvm.relay.nn.avg_pool3d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.avg_pool3d:1
msgid "3D average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`batch_flatten <tvm.relay.nn.batch_flatten>`\\ \\(data\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.batch_flatten:1
msgid "BatchFlatten."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`batch_matmul <tvm.relay.nn.batch_matmul>`\\ \\(tensor\\_a\\, "
"tensor\\_b\\[\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.batch_matmul:1
msgid "Compute batch matrix multiplication of `tensor_a` and `tensor_b`."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`batch_norm <tvm.relay.nn.batch_norm>`\\ \\(data\\, gamma\\, "
"beta\\, moving\\_mean\\, ...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Batch normalization layer (Ioffe and Szegedy, 2014)."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`batch_to_space_nd <tvm.relay.nn.batch_to_space_nd>`\\ \\(data\\,"
" block\\_shape\\, crops\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.batch_to_space_nd:1
msgid "Reshape the batch dimension into spatial dimensions."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`bias_add <tvm.relay.nn.bias_add>`\\ \\(data\\, bias\\[\\, "
"axis\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.bias_add:1
msgid "add_bias operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`bitpack <tvm.relay.nn.bitpack>`\\ \\(data\\[\\, bits\\, "
"pack\\_axis\\, bit\\_axis\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.bitpack:1
msgid "Tensor packing for bitserial operations."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`bitserial_conv2d <tvm.relay.nn.bitserial_conv2d>`\\ \\(data\\, "
"weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.bitserial_conv2d:1
msgid "2D convolution using bitserial computation."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`bitserial_dense <tvm.relay.nn.bitserial_dense>`\\ \\(data\\, "
"weight\\[\\, units\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Bitserial Dense operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`const <tvm.relay.nn.const>`\\ \\(value\\[\\, dtype\\, span\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.expr.const:1
msgid "Create a constant value."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_gemm_weight_transform "
"<tvm.relay.nn.contrib_conv2d_gemm_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:1
msgid "Weight Transformation part for 2D convolution with gemm algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_gemm_without_weight_transform "
"<tvm.relay.nn.contrib_conv2d_gemm_without_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:1
msgid "2D convolution with gemm algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_nchwc <tvm.relay.nn.contrib_conv2d_nchwc>`\\ "
"\\(data\\, kernel\\[\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:1
msgid "Variant of 2D convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_winograd_nnpack_weight_transform "
"<tvm.relay.nn.contrib_conv2d_winograd_nnpack_weight_transform>`\\ "
"\\(...\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:1
msgid "Weight Transformation part for 2D convolution with winograd algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_winograd_weight_transform "
"<tvm.relay.nn.contrib_conv2d_winograd_weight_transform>`\\ \\(...\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv2d_winograd_without_weight_transform "
"<tvm.relay.nn.contrib_conv2d_winograd_without_weight_transform>`\\ "
"\\(...\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:1
msgid "2D convolution with winograd algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv3d_winograd_weight_transform "
"<tvm.relay.nn.contrib_conv3d_winograd_weight_transform>`\\ \\(...\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:1
msgid "Weight Transformation part for 3D convolution with winograd algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_conv3d_winograd_without_weight_transform "
"<tvm.relay.nn.contrib_conv3d_winograd_without_weight_transform>`\\ "
"\\(...\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:1
msgid "3D convolution with winograd algorithm."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_dense_pack <tvm.relay.nn.contrib_dense_pack>`\\ "
"\\(data\\, weight\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Dense operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`contrib_depthwise_conv2d_nchwc "
"<tvm.relay.nn.contrib_depthwise_conv2d_nchwc>`\\ \\(data\\, kernel\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:1
msgid "Variant of 2D depthwise convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv1d <tvm.relay.nn.conv1d>`\\ \\(data\\, weight\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv1d:1
msgid "1D convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv1d_transpose <tvm.relay.nn.conv1d_transpose>`\\ \\(data\\, "
"weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv1d_transpose:1
msgid "One dimensional transposed convolution operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv2d <tvm.relay.nn.conv2d>`\\ \\(data\\, weight\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv2d:1
msgid "2D convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_backward_weight <tvm.relay.nn.conv2d_backward_weight>`\\ "
"\\(grad\\, data\\[\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv2d_backward_weight:1
msgid "The gradient of conv2d with respect to weight."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv2d_transpose <tvm.relay.nn.conv2d_transpose>`\\ \\(data\\, "
"weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv2d_transpose:1
msgid "Two dimensional transposed convolution operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv3d <tvm.relay.nn.conv3d>`\\ \\(data\\, weight\\[\\, "
"strides\\, padding\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv3d:1
msgid "3D convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`conv3d_transpose <tvm.relay.nn.conv3d_transpose>`\\ \\(data\\, "
"weight\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.conv3d_transpose:1
msgid "3D transpose convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`correlation <tvm.relay.nn.correlation>`\\ \\(data1\\, data2\\, "
"kernel\\_size\\, ...\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.correlation:1
msgid "Applies correlation to inputs."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`cross_entropy <tvm.relay.nn.cross_entropy>`\\ \\(predictions\\, "
"targets\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.cross_entropy:1
msgid "CrossEntropy without logits."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`cross_entropy_with_logits "
"<tvm.relay.nn.cross_entropy_with_logits>`\\ \\(predictions\\, targets\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:1
msgid "CrossEntropy with logits."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`deformable_conv2d <tvm.relay.nn.deformable_conv2d>`\\ \\(data\\,"
" offset\\, weight\\[\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.deformable_conv2d:1
msgid "Deformable 2d convolution."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`dense <tvm.relay.nn.dense>`\\ \\(data\\, weight\\[\\, units\\, "
"out\\_dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`depth_to_space <tvm.relay.nn.depth_to_space>`\\ \\(data\\, "
"block\\_size\\[\\, layout\\, mode\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.depth_to_space:1
msgid "Convert channels into spatial blocks."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`dilate <tvm.relay.nn.dilate>`\\ \\(data\\, strides\\[\\, "
"dilation\\_value\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.dilate:1
msgid "Dilate data with given dilation value (0 by default)."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`dropout <tvm.relay.nn.dropout>`\\ \\(data\\[\\, rate\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.dropout:1 tvm.relay.op.nn.nn.dropout_raw:1
msgid "Applies the dropout operation to the input array."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`dropout_raw <tvm.relay.nn.dropout_raw>`\\ \\(data\\[\\, "
"rate\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`fast_softmax <tvm.relay.nn.fast_softmax>`\\ \\(data\\[\\, "
"axis\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.softmax:1
msgid "Computes softmax."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`fifo_buffer <tvm.relay.nn.fifo_buffer>`\\ \\(data\\, buffer\\, "
"axis\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.fifo_buffer:1
msgid "FIFO buffer to enable computation reuse in CNNs with sliding indow input"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`get_pad_tuple1d <tvm.relay.nn.get_pad_tuple1d>`\\ \\(padding\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
"Common code to get the 1 dimensional pad option Parameters ---------- "
"padding : Union[int, Tuple[int, ...]]     Padding size Returns ------- "
"pad_left : int     Padding size on left pad_right : int     Padding size "
"on right."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`get_pad_tuple2d <tvm.relay.nn.get_pad_tuple2d>`\\ \\(padding\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
"Common code to get the pad option Parameters ---------- padding : "
"Union[int, Tuple[int, ...]]     Padding size Returns ------- pad_top : "
"int     Padding size on top pad_left : int     Padding size on left "
"pad_down : int     Padding size on down. pad_right : int     Padding size"
" on right."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`get_pad_tuple3d <tvm.relay.nn.get_pad_tuple3d>`\\ \\(padding\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
"Common code to get the pad option Parameters ---------- padding : "
"Union[int, Tuple[int, ...]]     Padding size Returns ------- pad_front : "
"int     Padding size on front pad_top : int     Padding size on top "
"pad_left : int     Padding size on left pad_back : int     Padding size "
"on back pad_down : int     Padding size on down. pad_right : int     "
"Padding size on right."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_avg_pool1d <tvm.relay.nn.global_avg_pool1d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_avg_pool1d:1
msgid "1D global average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_avg_pool2d <tvm.relay.nn.global_avg_pool2d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_avg_pool2d:1
msgid "2D global average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_avg_pool3d <tvm.relay.nn.global_avg_pool3d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_avg_pool3d:1
msgid "3D global average pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_max_pool1d <tvm.relay.nn.global_max_pool1d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_max_pool1d:1
msgid "1D global maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_max_pool2d <tvm.relay.nn.global_max_pool2d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_max_pool2d:1
msgid "2D global maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`global_max_pool3d <tvm.relay.nn.global_max_pool3d>`\\ "
"\\(data\\[\\, layout\\, out\\_layout\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.global_max_pool3d:1
msgid "3D global maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`group_norm <tvm.relay.nn.group_norm>`\\ \\(data\\, gamma\\, "
"beta\\, num\\_groups\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
"Group normalization normalizes over group of channels for each training "
"examples."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`instance_norm <tvm.relay.nn.instance_norm>`\\ \\(data\\, "
"gamma\\, beta\\[\\, axis\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.instance_norm:1
msgid ""
"Instance Normalization (Ulyanov and et al., 2016) Applies instance "
"normalization to the n-dimensional input array."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`l2_normalize <tvm.relay.nn.l2_normalize>`\\ \\(data\\, eps\\[\\,"
" axis\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.l2_normalize:1
msgid "Perform L2 normalization on the input data"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`layer_norm <tvm.relay.nn.layer_norm>`\\ \\(data\\, gamma\\, "
"beta\\[\\, axis\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Layer normalization (Lei Ba and et al., 2016)."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`leaky_relu <tvm.relay.nn.leaky_relu>`\\ \\(data\\[\\, alpha\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.leaky_relu:1 tvm.relay.op.nn.nn.prelu:1
msgid ""
"This operator takes data as input and does Leaky version of a Rectified "
"Linear Unit."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`log_softmax <tvm.relay.nn.log_softmax>`\\ \\(data\\[\\, "
"axis\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.log_softmax:1
msgid "Computes log softmax."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`lrn <tvm.relay.nn.lrn>`\\ \\(data\\[\\, size\\, axis\\, bias\\, "
"alpha\\, beta\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.lrn:1
msgid "This operator takes data as input and does local response normalization."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`matmul <tvm.relay.nn.matmul>`\\ \\(tensor\\_a\\, "
"tensor\\_b\\[\\, units\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid "Matmul operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`max_pool1d <tvm.relay.nn.max_pool1d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.max_pool1d:1
msgid "1D maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`max_pool2d <tvm.relay.nn.max_pool2d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.max_pool2d:1
msgid "2D maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`max_pool2d_grad <tvm.relay.nn.max_pool2d_grad>`\\ "
"\\(out\\_grad\\, data\\[\\, pool\\_size\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.max_pool2d_grad:1
msgid "Gradient of 2D maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`max_pool3d <tvm.relay.nn.max_pool3d>`\\ \\(data\\[\\, "
"pool\\_size\\, strides\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.max_pool3d:1
msgid "3D maximum pooling operator."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`mirror_pad <tvm.relay.nn.mirror_pad>`\\ \\(data\\, "
"pad\\_width\\[\\, mode\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.mirror_pad:1
msgid "MirrorPadding"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`nll_loss <tvm.relay.nn.nll_loss>`\\ \\(predictions\\, targets\\,"
" weights\\[\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.nll_loss:1
msgid "Negative log likelihood loss."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`pad <tvm.relay.nn.pad>`\\ \\(data\\, pad\\_width\\[\\, "
"pad\\_value\\, pad\\_mode\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.pad:1
msgid "Padding"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`prelu <tvm.relay.nn.prelu>`\\ \\(data\\, alpha\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`relu <tvm.relay.nn.relu>`\\ \\(data\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.relu:1
msgid "Rectified linear unit."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`softmax <tvm.relay.nn.softmax>`\\ \\(data\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`space_to_batch_nd <tvm.relay.nn.space_to_batch_nd>`\\ \\(data\\,"
" block\\_shape\\, paddings\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.space_to_batch_nd:1
msgid ""
"Divide spatial dimensions of the data into a grid of blocks and "
"interleave them into batch dim."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`space_to_depth <tvm.relay.nn.space_to_depth>`\\ \\(data\\, "
"block\\_size\\[\\, layout\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.space_to_depth:1
msgid "Convert spatial blocks into channels."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`sparse_add <tvm.relay.nn.sparse_add>`\\ \\(dense\\_mat\\, "
"sparse\\_mat\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.sparse_add:1
msgid ""
"Computes the matrix addition of `dense_mat` and `sparse_mat`, where "
"`dense_mat` is a dense matrix and `sparse_mat` is a sparse (CSR) "
"namedtuple with fields `data`, `indices`, and `indptr`."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`sparse_dense <tvm.relay.nn.sparse_dense>`\\ \\(dense\\_mat\\, "
"sparse\\_mat\\[\\, sparse\\_lhs\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.sparse_dense:1
msgid ""
"Computes the matrix multiplication of `dense_mat` and `sparse_mat`, where"
" `dense_mat` is a dense matrix and `sparse_mat` is a sparse (either BSR "
"or CSR) namedtuple with fields `data`, `indices`, and `indptr`."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ":py:obj:`sparse_transpose <tvm.relay.nn.sparse_transpose>`\\ \\(x\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.sparse_transpose:1
msgid ""
"Computes the fast matrix transpose of x, where x is a sparse tensor in "
"CSR format (represented as a namedtuple with fields `data`, `indices`, "
"and `indptr`)."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`upsampling <tvm.relay.nn.upsampling>`\\ \\(data\\[\\, "
"scale\\_h\\, scale\\_w\\, layout\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.upsampling:1
msgid "Upsampling."
msgstr ""

#: of tvm.relay.expr.Constant:1:<autosummary>:1
msgid ""
":py:obj:`upsampling3d <tvm.relay.nn.upsampling3d>`\\ \\(data\\[\\, "
"scale\\_d\\, scale\\_h\\, ...\\]\\)"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:1:<autosummary>:1
#: tvm.relay.op.nn.nn.upsampling3d:1
msgid "3D Upsampling."
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:4 tvm.relay.expr.const:4
#: tvm.relay.op.nn.nn.adaptive_avg_pool1d:23
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:26
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:25
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:23
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:26
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:25 tvm.relay.op.nn.nn.avg_pool1d:15
#: tvm.relay.op.nn.nn.avg_pool2d:24 tvm.relay.op.nn.nn.avg_pool2d_grad:6
#: tvm.relay.op.nn.nn.avg_pool3d:16 tvm.relay.op.nn.nn.batch_flatten:11
#: tvm.relay.op.nn.nn.batch_matmul:11 tvm.relay.op.nn.nn.batch_norm:41
#: tvm.relay.op.nn.nn.batch_to_space_nd:4 tvm.relay.op.nn.nn.bias_add:8
#: tvm.relay.op.nn.nn.bitpack:14 tvm.relay.op.nn.nn.bitserial_conv2d:4
#: tvm.relay.op.nn.nn.bitserial_dense:10
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:7
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:7
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:8
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:7
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:7
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:7
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:7
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:7
#: tvm.relay.op.nn.nn.contrib_dense_pack:9
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:8
#: tvm.relay.op.nn.nn.conv1d:27 tvm.relay.op.nn.nn.conv1d_transpose:4
#: tvm.relay.op.nn.nn.conv2d:27 tvm.relay.op.nn.nn.conv2d_transpose:4
#: tvm.relay.op.nn.nn.conv3d:27 tvm.relay.op.nn.nn.conv3d_transpose:4
#: tvm.relay.op.nn.nn.correlation:41 tvm.relay.op.nn.nn.cross_entropy:4
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:4
#: tvm.relay.op.nn.nn.deformable_conv2d:6 tvm.relay.op.nn.nn.dense:9
#: tvm.relay.op.nn.nn.depth_to_space:4 tvm.relay.op.nn.nn.dilate:4
#: tvm.relay.op.nn.nn.dropout:8 tvm.relay.op.nn.nn.dropout_raw:8
#: tvm.relay.op.nn.nn.fast_softmax:9 tvm.relay.op.nn.nn.fifo_buffer:18
#: tvm.relay.op.nn.nn.global_avg_pool1d:17
#: tvm.relay.op.nn.nn.global_avg_pool2d:19
#: tvm.relay.op.nn.nn.global_avg_pool3d:18
#: tvm.relay.op.nn.nn.global_max_pool1d:16
#: tvm.relay.op.nn.nn.global_max_pool2d:19
#: tvm.relay.op.nn.nn.global_max_pool3d:17 tvm.relay.op.nn.nn.group_norm:27
#: tvm.relay.op.nn.nn.instance_norm:26 tvm.relay.op.nn.nn.l2_normalize:7
#: tvm.relay.op.nn.nn.layer_norm:20 tvm.relay.op.nn.nn.leaky_relu:9
#: tvm.relay.op.nn.nn.log_softmax:11 tvm.relay.op.nn.nn.lrn:12
#: tvm.relay.op.nn.nn.matmul:9 tvm.relay.op.nn.nn.max_pool1d:15
#: tvm.relay.op.nn.nn.max_pool2d:23 tvm.relay.op.nn.nn.max_pool2d_grad:6
#: tvm.relay.op.nn.nn.max_pool3d:16 tvm.relay.op.nn.nn.mirror_pad:7
#: tvm.relay.op.nn.nn.nll_loss:11 tvm.relay.op.nn.nn.pad:7
#: tvm.relay.op.nn.nn.prelu:9 tvm.relay.op.nn.nn.relu:7
#: tvm.relay.op.nn.nn.softmax:9 tvm.relay.op.nn.nn.space_to_batch_nd:5
#: tvm.relay.op.nn.nn.space_to_depth:4 tvm.relay.op.nn.nn.sparse_add:13
#: tvm.relay.op.nn.nn.sparse_dense:27 tvm.relay.op.nn.nn.sparse_transpose:15
#: tvm.relay.op.nn.nn.upsampling:12 tvm.relay.op.nn.nn.upsampling3d:12
msgid "Parameters"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:6
#: tvm.relay.op.nn.nn.adaptive_avg_pool1d:25
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:28
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:27
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:25
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:28
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:27 tvm.relay.op.nn.nn.avg_pool1d:17
#: tvm.relay.op.nn.nn.avg_pool2d:26 tvm.relay.op.nn.nn.avg_pool2d_grad:11
#: tvm.relay.op.nn.nn.avg_pool3d:18 tvm.relay.op.nn.nn.batch_flatten:13
#: tvm.relay.op.nn.nn.batch_norm:43 tvm.relay.op.nn.nn.batch_to_space_nd:6
#: tvm.relay.op.nn.nn.bias_add:10 tvm.relay.op.nn.nn.bitpack:16
#: tvm.relay.op.nn.nn.bitserial_conv2d:6 tvm.relay.op.nn.nn.bitserial_dense:12
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:10
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_dense_pack:12
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:10
#: tvm.relay.op.nn.nn.conv1d:29 tvm.relay.op.nn.nn.conv1d_transpose:6
#: tvm.relay.op.nn.nn.conv2d:29 tvm.relay.op.nn.nn.conv2d_transpose:6
#: tvm.relay.op.nn.nn.conv3d:29 tvm.relay.op.nn.nn.conv3d_transpose:6
#: tvm.relay.op.nn.nn.deformable_conv2d:8 tvm.relay.op.nn.nn.dense:12
#: tvm.relay.op.nn.nn.depth_to_space:6 tvm.relay.op.nn.nn.dilate:6
#: tvm.relay.op.nn.nn.dropout:10 tvm.relay.op.nn.nn.dropout_raw:10
#: tvm.relay.op.nn.nn.fifo_buffer:19 tvm.relay.op.nn.nn.global_avg_pool1d:19
#: tvm.relay.op.nn.nn.global_avg_pool2d:21
#: tvm.relay.op.nn.nn.global_avg_pool3d:20
#: tvm.relay.op.nn.nn.global_max_pool1d:18
#: tvm.relay.op.nn.nn.global_max_pool2d:21
#: tvm.relay.op.nn.nn.global_max_pool3d:19 tvm.relay.op.nn.nn.group_norm:29
#: tvm.relay.op.nn.nn.instance_norm:28 tvm.relay.op.nn.nn.l2_normalize:9
#: tvm.relay.op.nn.nn.layer_norm:22 tvm.relay.op.nn.nn.leaky_relu:11
#: tvm.relay.op.nn.nn.lrn:14 tvm.relay.op.nn.nn.matmul:12
#: tvm.relay.op.nn.nn.max_pool1d:17 tvm.relay.op.nn.nn.max_pool2d:25
#: tvm.relay.op.nn.nn.max_pool2d_grad:11 tvm.relay.op.nn.nn.max_pool3d:18
#: tvm.relay.op.nn.nn.prelu:11 tvm.relay.op.nn.nn.relu:9
#: tvm.relay.op.nn.nn.space_to_batch_nd:7 tvm.relay.op.nn.nn.space_to_depth:6
#: tvm.relay.op.nn.nn.upsampling:14 tvm.relay.op.nn.nn.upsampling3d:14
msgid "data"
msgstr ""

#: of tvm.relay.expr.Constant:-1
msgid "tvm.nd.NDArray"
msgstr ""

#: of tvm.relay.expr.Constant:6
msgid "The data content of the constant expression."
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:8 tvm.relay.expr.const:12
msgid "span: Optional[tvm.relay.Span]"
msgstr ""

#: ../../docstring of tvm.relay.expr.Constant:9 tvm.relay.expr.const:12
msgid "Span that points to original source code."
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid ":py:obj:`checked_type <tvm.relay.nn.Expr.checked_type>`\\"
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid "Get the checked type of tvm.relay.Expr."
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid ":py:obj:`struct_info <tvm.relay.nn.Expr.struct_info>`\\"
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid "Get the struct info field"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:1
msgid "1D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:3
#: tvm.relay.op.nn.nn.global_avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation "
"across each window represented by W."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:7
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:7
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, in_channels, width)`, to produce an output Tensor "
"with shape (batch_size, in_channels, output_width)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:12
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:12
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:11
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:12
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:12
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:11
msgid ""
"The pooling kernel and stride sizes are automatically chosen for desired "
"output sizes."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:20
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:23
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:22
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:20
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:23
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:22
msgid "For output_size:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output width."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:19
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size) for any input (NCW)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:-1
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:-1
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:-1
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:-1
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:-1
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:-1 tvm.relay.op.nn.nn.avg_pool1d:-1
#: tvm.relay.op.nn.nn.avg_pool2d:-1 tvm.relay.op.nn.nn.avg_pool2d_grad:-1
#: tvm.relay.op.nn.nn.avg_pool3d:-1 tvm.relay.op.nn.nn.batch_flatten:-1
#: tvm.relay.op.nn.nn.batch_matmul:-1 tvm.relay.op.nn.nn.batch_norm:-1
#: tvm.relay.op.nn.nn.bias_add:-1 tvm.relay.op.nn.nn.bitpack:-1
#: tvm.relay.op.nn.nn.bitserial_conv2d:-1 tvm.relay.op.nn.nn.bitserial_dense:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_dense_pack:-1
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:-1
#: tvm.relay.op.nn.nn.conv1d:-1 tvm.relay.op.nn.nn.conv1d_transpose:-1
#: tvm.relay.op.nn.nn.conv2d:-1 tvm.relay.op.nn.nn.conv2d_transpose:-1
#: tvm.relay.op.nn.nn.conv3d:-1 tvm.relay.op.nn.nn.conv3d_transpose:-1
#: tvm.relay.op.nn.nn.cross_entropy:-1
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:-1
#: tvm.relay.op.nn.nn.deformable_conv2d:-1 tvm.relay.op.nn.nn.dense:-1
#: tvm.relay.op.nn.nn.depth_to_space:-1 tvm.relay.op.nn.nn.dilate:-1
#: tvm.relay.op.nn.nn.dropout:-1 tvm.relay.op.nn.nn.dropout_raw:-1
#: tvm.relay.op.nn.nn.fast_softmax:-1 tvm.relay.op.nn.nn.fifo_buffer:-1
#: tvm.relay.op.nn.nn.global_avg_pool1d:-1
#: tvm.relay.op.nn.nn.global_avg_pool2d:-1
#: tvm.relay.op.nn.nn.global_avg_pool3d:-1
#: tvm.relay.op.nn.nn.global_max_pool1d:-1
#: tvm.relay.op.nn.nn.global_max_pool2d:-1
#: tvm.relay.op.nn.nn.global_max_pool3d:-1 tvm.relay.op.nn.nn.group_norm:-1
#: tvm.relay.op.nn.nn.instance_norm:-1 tvm.relay.op.nn.nn.l2_normalize:-1
#: tvm.relay.op.nn.nn.layer_norm:-1 tvm.relay.op.nn.nn.leaky_relu:-1
#: tvm.relay.op.nn.nn.log_softmax:-1 tvm.relay.op.nn.nn.lrn:-1
#: tvm.relay.op.nn.nn.matmul:-1 tvm.relay.op.nn.nn.max_pool1d:-1
#: tvm.relay.op.nn.nn.max_pool2d:-1 tvm.relay.op.nn.nn.max_pool2d_grad:-1
#: tvm.relay.op.nn.nn.max_pool3d:-1 tvm.relay.op.nn.nn.mirror_pad:-1
#: tvm.relay.op.nn.nn.nll_loss:-1 tvm.relay.op.nn.nn.pad:-1
#: tvm.relay.op.nn.nn.prelu:-1 tvm.relay.op.nn.nn.relu:-1
#: tvm.relay.op.nn.nn.softmax:-1 tvm.relay.op.nn.nn.space_to_depth:-1
#: tvm.relay.op.nn.nn.sparse_add:-1 tvm.relay.op.nn.nn.sparse_dense:-1
#: tvm.relay.op.nn.nn.upsampling:-1 tvm.relay.op.nn.nn.upsampling3d:-1
msgid "tvm.relay.Expr"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:25
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:28
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:27
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:25
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:28
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:27 tvm.relay.op.nn.nn.avg_pool1d:17
#: tvm.relay.op.nn.nn.avg_pool2d:26 tvm.relay.op.nn.nn.avg_pool2d_grad:11
#: tvm.relay.op.nn.nn.avg_pool3d:18 tvm.relay.op.nn.nn.batch_flatten:13
#: tvm.relay.op.nn.nn.bias_add:10 tvm.relay.op.nn.nn.bitserial_conv2d:6
#: tvm.relay.op.nn.nn.bitserial_dense:12
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:10
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:10
#: tvm.relay.op.nn.nn.conv1d:29 tvm.relay.op.nn.nn.conv1d_transpose:6
#: tvm.relay.op.nn.nn.conv2d:29 tvm.relay.op.nn.nn.conv2d_transpose:6
#: tvm.relay.op.nn.nn.conv3d:29 tvm.relay.op.nn.nn.conv3d_transpose:6
#: tvm.relay.op.nn.nn.deformable_conv2d:8 tvm.relay.op.nn.nn.dropout:10
#: tvm.relay.op.nn.nn.dropout_raw:10 tvm.relay.op.nn.nn.fast_softmax:11
#: tvm.relay.op.nn.nn.global_avg_pool1d:19
#: tvm.relay.op.nn.nn.global_avg_pool2d:21
#: tvm.relay.op.nn.nn.global_avg_pool3d:20
#: tvm.relay.op.nn.nn.global_max_pool1d:18
#: tvm.relay.op.nn.nn.global_max_pool2d:21
#: tvm.relay.op.nn.nn.global_max_pool3d:19 tvm.relay.op.nn.nn.l2_normalize:9
#: tvm.relay.op.nn.nn.leaky_relu:11 tvm.relay.op.nn.nn.log_softmax:13
#: tvm.relay.op.nn.nn.lrn:14 tvm.relay.op.nn.nn.max_pool1d:17
#: tvm.relay.op.nn.nn.max_pool2d:25 tvm.relay.op.nn.nn.max_pool2d_grad:11
#: tvm.relay.op.nn.nn.max_pool3d:18 tvm.relay.op.nn.nn.prelu:11
#: tvm.relay.op.nn.nn.softmax:11 tvm.relay.op.nn.nn.upsampling:14
#: tvm.relay.op.nn.nn.upsampling3d:14
msgid "The input data to the operator."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:28
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:31
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:30
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:28
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:31
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:30
msgid "output_size"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:-1
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:-1
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:-1
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:-1
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:-1
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:-1
msgid "tuple of int. optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:28
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:31
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:30
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:28
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:31
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:30
msgid "Output height and width."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:31
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:34
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:33
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:31
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:34
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:33 tvm.relay.op.nn.nn.avg_pool1d:32
#: tvm.relay.op.nn.nn.avg_pool2d:41 tvm.relay.op.nn.nn.avg_pool2d_grad:23
#: tvm.relay.op.nn.nn.avg_pool3d:33 tvm.relay.op.nn.nn.depth_to_space:12
#: tvm.relay.op.nn.nn.global_avg_pool1d:22
#: tvm.relay.op.nn.nn.global_avg_pool2d:24
#: tvm.relay.op.nn.nn.global_avg_pool3d:23
#: tvm.relay.op.nn.nn.global_max_pool1d:21
#: tvm.relay.op.nn.nn.global_max_pool2d:24
#: tvm.relay.op.nn.nn.global_max_pool3d:22 tvm.relay.op.nn.nn.max_pool1d:32
#: tvm.relay.op.nn.nn.max_pool2d:40 tvm.relay.op.nn.nn.max_pool2d_grad:23
#: tvm.relay.op.nn.nn.max_pool3d:33 tvm.relay.op.nn.nn.space_to_depth:12
#: tvm.relay.op.nn.nn.upsampling:23 tvm.relay.op.nn.nn.upsampling3d:26
msgid "layout"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:-1
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:-1
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:-1
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:-1
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:-1
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:-1 tvm.relay.op.nn.nn.avg_pool1d:-1
#: tvm.relay.op.nn.nn.avg_pool2d:-1 tvm.relay.op.nn.nn.avg_pool2d_grad:-1
#: tvm.relay.op.nn.nn.avg_pool3d:-1 tvm.relay.op.nn.nn.bitpack:-1
#: tvm.relay.op.nn.nn.bitserial_conv2d:-1 tvm.relay.op.nn.nn.bitserial_dense:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_dense_pack:-1
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:-1
#: tvm.relay.op.nn.nn.conv1d_transpose:-1
#: tvm.relay.op.nn.nn.conv2d_transpose:-1
#: tvm.relay.op.nn.nn.deformable_conv2d:-1 tvm.relay.op.nn.nn.dense:-1
#: tvm.relay.op.nn.nn.global_avg_pool1d:-1
#: tvm.relay.op.nn.nn.global_avg_pool2d:-1
#: tvm.relay.op.nn.nn.global_avg_pool3d:-1
#: tvm.relay.op.nn.nn.global_max_pool1d:-1
#: tvm.relay.op.nn.nn.global_max_pool2d:-1
#: tvm.relay.op.nn.nn.global_max_pool3d:-1 tvm.relay.op.nn.nn.max_pool1d:-1
#: tvm.relay.op.nn.nn.max_pool2d:-1 tvm.relay.op.nn.nn.max_pool2d_grad:-1
#: tvm.relay.op.nn.nn.max_pool3d:-1 tvm.relay.op.nn.nn.upsampling:-1
#: tvm.relay.op.nn.nn.upsampling3d:-1
msgid "str, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:31
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:34
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:33
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:31
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:34
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:33 tvm.relay.op.nn.nn.avg_pool1d:32
#: tvm.relay.op.nn.nn.avg_pool2d:41 tvm.relay.op.nn.nn.avg_pool2d_grad:23
#: tvm.relay.op.nn.nn.avg_pool3d:33 tvm.relay.op.nn.nn.bitserial_conv2d:30
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:33
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:34
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:36
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:36
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:34
#: tvm.relay.op.nn.nn.conv1d:53 tvm.relay.op.nn.nn.conv1d_transpose:30
#: tvm.relay.op.nn.nn.conv2d:53 tvm.relay.op.nn.nn.conv2d_transpose:30
#: tvm.relay.op.nn.nn.conv3d:53 tvm.relay.op.nn.nn.conv3d_transpose:30
#: tvm.relay.op.nn.nn.deformable_conv2d:38
#: tvm.relay.op.nn.nn.global_avg_pool1d:22
#: tvm.relay.op.nn.nn.global_avg_pool2d:24
#: tvm.relay.op.nn.nn.global_avg_pool3d:23
#: tvm.relay.op.nn.nn.global_max_pool1d:21
#: tvm.relay.op.nn.nn.global_max_pool2d:24
#: tvm.relay.op.nn.nn.global_max_pool3d:22 tvm.relay.op.nn.nn.max_pool1d:32
#: tvm.relay.op.nn.nn.max_pool2d:40 tvm.relay.op.nn.nn.max_pool2d_grad:23
#: tvm.relay.op.nn.nn.max_pool3d:33 tvm.relay.op.nn.nn.upsampling:23
#: tvm.relay.op.nn.nn.upsampling3d:26
msgid "Layout of the input."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:34
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:37
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:36
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:34
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:37
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:36 tvm.relay.op.nn.nn.avg_pool1d:35
#: tvm.relay.op.nn.nn.avg_pool2d:44 tvm.relay.op.nn.nn.avg_pool2d_grad:26
#: tvm.relay.op.nn.nn.avg_pool3d:36
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:39
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:40
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:42
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:42
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:40
#: tvm.relay.op.nn.nn.conv1d:59 tvm.relay.op.nn.nn.conv1d_transpose:36
#: tvm.relay.op.nn.nn.conv2d:59 tvm.relay.op.nn.nn.conv2d_transpose:36
#: tvm.relay.op.nn.nn.conv3d:59 tvm.relay.op.nn.nn.conv3d_transpose:36
#: tvm.relay.op.nn.nn.deformable_conv2d:44
#: tvm.relay.op.nn.nn.global_avg_pool1d:25
#: tvm.relay.op.nn.nn.global_avg_pool2d:27
#: tvm.relay.op.nn.nn.global_avg_pool3d:26
#: tvm.relay.op.nn.nn.global_max_pool1d:24
#: tvm.relay.op.nn.nn.global_max_pool2d:27
#: tvm.relay.op.nn.nn.global_max_pool3d:25 tvm.relay.op.nn.nn.max_pool1d:35
#: tvm.relay.op.nn.nn.max_pool2d:43 tvm.relay.op.nn.nn.max_pool2d_grad:26
#: tvm.relay.op.nn.nn.max_pool3d:36
msgid "out_layout"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:34
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:37
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:36
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:34
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:37
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:36
#: tvm.relay.op.nn.nn.global_avg_pool1d:25
#: tvm.relay.op.nn.nn.global_avg_pool3d:26
#: tvm.relay.op.nn.nn.global_max_pool1d:24
#: tvm.relay.op.nn.nn.global_max_pool3d:25
msgid "Layout of the output."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:37
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:40
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:39
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:37
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:40
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:39 tvm.relay.op.nn.nn.avg_pool1d:44
#: tvm.relay.op.nn.nn.avg_pool2d:53 tvm.relay.op.nn.nn.avg_pool2d_grad:35
#: tvm.relay.op.nn.nn.avg_pool3d:45 tvm.relay.op.nn.nn.batch_flatten:16
#: tvm.relay.op.nn.nn.batch_matmul:28 tvm.relay.op.nn.nn.batch_norm:73
#: tvm.relay.op.nn.nn.batch_to_space_nd:17 tvm.relay.op.nn.nn.bias_add:19
#: tvm.relay.op.nn.nn.bitpack:34 tvm.relay.op.nn.nn.bitserial_conv2d:42
#: tvm.relay.op.nn.nn.bitserial_dense:36
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:16
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:45
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:46
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:15
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:15
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:48
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:15
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:48
#: tvm.relay.op.nn.nn.contrib_dense_pack:28
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:46
#: tvm.relay.op.nn.nn.conv1d:65 tvm.relay.op.nn.nn.conv1d_transpose:45
#: tvm.relay.op.nn.nn.conv2d:65 tvm.relay.op.nn.nn.conv2d_transpose:45
#: tvm.relay.op.nn.nn.conv3d:65 tvm.relay.op.nn.nn.conv3d_transpose:42
#: tvm.relay.op.nn.nn.correlation:72 tvm.relay.op.nn.nn.cross_entropy:12
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:12
#: tvm.relay.op.nn.nn.deformable_conv2d:50 tvm.relay.op.nn.nn.dense:26
#: tvm.relay.op.nn.nn.depth_to_space:19 tvm.relay.op.nn.nn.dilate:15
#: tvm.relay.op.nn.nn.dropout:16 tvm.relay.op.nn.nn.dropout_raw:16
#: tvm.relay.op.nn.nn.fast_softmax:16 tvm.relay.op.nn.nn.fifo_buffer:27
#: tvm.relay.op.nn.nn.global_avg_pool1d:28
#: tvm.relay.op.nn.nn.global_avg_pool2d:30
#: tvm.relay.op.nn.nn.global_avg_pool3d:29
#: tvm.relay.op.nn.nn.global_max_pool1d:27
#: tvm.relay.op.nn.nn.global_max_pool2d:30
#: tvm.relay.op.nn.nn.global_max_pool3d:28 tvm.relay.op.nn.nn.group_norm:54
#: tvm.relay.op.nn.nn.instance_norm:50 tvm.relay.op.nn.nn.l2_normalize:18
#: tvm.relay.op.nn.nn.layer_norm:44 tvm.relay.op.nn.nn.leaky_relu:17
#: tvm.relay.op.nn.nn.log_softmax:19 tvm.relay.op.nn.nn.lrn:32
#: tvm.relay.op.nn.nn.matmul:32 tvm.relay.op.nn.nn.max_pool1d:41
#: tvm.relay.op.nn.nn.max_pool2d:49 tvm.relay.op.nn.nn.max_pool2d_grad:32
#: tvm.relay.op.nn.nn.max_pool3d:42 tvm.relay.op.nn.nn.mirror_pad:17
#: tvm.relay.op.nn.nn.nll_loss:29 tvm.relay.op.nn.nn.pad:20
#: tvm.relay.op.nn.nn.prelu:20 tvm.relay.op.nn.nn.relu:12
#: tvm.relay.op.nn.nn.softmax:17 tvm.relay.op.nn.nn.space_to_batch_nd:21
#: tvm.relay.op.nn.nn.space_to_depth:15 tvm.relay.op.nn.nn.sparse_add:21
#: tvm.relay.op.nn.nn.sparse_dense:38 tvm.relay.op.nn.nn.sparse_transpose:20
#: tvm.relay.op.nn.nn.upsampling:32 tvm.relay.op.nn.nn.upsampling3d:38
#: tvm.relay.op.nn.utils.get_pad_tuple1d:7
#: tvm.relay.op.nn.utils.get_pad_tuple2d:7
#: tvm.relay.op.nn.utils.get_pad_tuple3d:7
msgid "Returns"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:38
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:41
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:40
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:38
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:41
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:40 tvm.relay.op.nn.nn.avg_pool1d:45
#: tvm.relay.op.nn.nn.avg_pool2d:54 tvm.relay.op.nn.nn.avg_pool2d_grad:36
#: tvm.relay.op.nn.nn.avg_pool3d:46 tvm.relay.op.nn.nn.batch_flatten:17
#: tvm.relay.op.nn.nn.batch_norm:76 tvm.relay.op.nn.nn.batch_to_space_nd:22
#: tvm.relay.op.nn.nn.bias_add:20 tvm.relay.op.nn.nn.bitpack:35
#: tvm.relay.op.nn.nn.bitserial_conv2d:43 tvm.relay.op.nn.nn.bitserial_dense:37
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:17
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:46
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:47
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:16
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:16
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:49
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:16
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:49
#: tvm.relay.op.nn.nn.contrib_dense_pack:29
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:47
#: tvm.relay.op.nn.nn.conv1d:66 tvm.relay.op.nn.nn.conv1d_transpose:46
#: tvm.relay.op.nn.nn.conv2d:66 tvm.relay.op.nn.nn.conv2d_transpose:46
#: tvm.relay.op.nn.nn.conv3d:66 tvm.relay.op.nn.nn.conv3d_transpose:43
#: tvm.relay.op.nn.nn.cross_entropy:13
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:13
#: tvm.relay.op.nn.nn.deformable_conv2d:51 tvm.relay.op.nn.nn.dense:27
#: tvm.relay.op.nn.nn.depth_to_space:21 tvm.relay.op.nn.nn.dropout:17
#: tvm.relay.op.nn.nn.dropout_raw:17 tvm.relay.op.nn.nn.fast_softmax:17
#: tvm.relay.op.nn.nn.fifo_buffer:28 tvm.relay.op.nn.nn.global_avg_pool1d:29
#: tvm.relay.op.nn.nn.global_avg_pool2d:31
#: tvm.relay.op.nn.nn.global_avg_pool3d:30
#: tvm.relay.op.nn.nn.global_max_pool1d:28
#: tvm.relay.op.nn.nn.global_max_pool2d:31
#: tvm.relay.op.nn.nn.global_max_pool3d:29 tvm.relay.op.nn.nn.group_norm:55
#: tvm.relay.op.nn.nn.instance_norm:52 tvm.relay.op.nn.nn.l2_normalize:19
#: tvm.relay.op.nn.nn.layer_norm:45 tvm.relay.op.nn.nn.leaky_relu:18
#: tvm.relay.op.nn.nn.log_softmax:20 tvm.relay.op.nn.nn.lrn:33
#: tvm.relay.op.nn.nn.matmul:33 tvm.relay.op.nn.nn.max_pool1d:42
#: tvm.relay.op.nn.nn.max_pool2d:50 tvm.relay.op.nn.nn.max_pool2d_grad:33
#: tvm.relay.op.nn.nn.max_pool3d:43 tvm.relay.op.nn.nn.mirror_pad:18
#: tvm.relay.op.nn.nn.nll_loss:30 tvm.relay.op.nn.nn.pad:21
#: tvm.relay.op.nn.nn.prelu:21 tvm.relay.op.nn.nn.relu:13
#: tvm.relay.op.nn.nn.softmax:18 tvm.relay.op.nn.nn.space_to_batch_nd:25
#: tvm.relay.op.nn.nn.space_to_depth:17 tvm.relay.op.nn.nn.sparse_transpose:22
#: tvm.relay.op.nn.nn.upsampling:33 tvm.relay.op.nn.nn.upsampling3d:39
msgid "result"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool1d:39
#: tvm.relay.op.nn.nn.adaptive_avg_pool2d:42
#: tvm.relay.op.nn.nn.adaptive_avg_pool3d:41
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:39
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:42
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:41 tvm.relay.op.nn.nn.avg_pool1d:46
#: tvm.relay.op.nn.nn.avg_pool2d:55 tvm.relay.op.nn.nn.avg_pool2d_grad:37
#: tvm.relay.op.nn.nn.avg_pool3d:47 tvm.relay.op.nn.nn.batch_matmul:30
#: tvm.relay.op.nn.nn.bitserial_conv2d:44 tvm.relay.op.nn.nn.bitserial_dense:38
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:47
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:48
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:17
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:17
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:50
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:17
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:50
#: tvm.relay.op.nn.nn.contrib_dense_pack:30
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:48
#: tvm.relay.op.nn.nn.conv1d:67 tvm.relay.op.nn.nn.conv1d_transpose:47
#: tvm.relay.op.nn.nn.conv2d:67 tvm.relay.op.nn.nn.conv2d_transpose:47
#: tvm.relay.op.nn.nn.conv3d:67 tvm.relay.op.nn.nn.conv3d_transpose:44
#: tvm.relay.op.nn.nn.cross_entropy:14
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:14
#: tvm.relay.op.nn.nn.deformable_conv2d:52 tvm.relay.op.nn.nn.dense:28
#: tvm.relay.op.nn.nn.fast_softmax:18 tvm.relay.op.nn.nn.global_avg_pool1d:30
#: tvm.relay.op.nn.nn.global_avg_pool2d:32
#: tvm.relay.op.nn.nn.global_avg_pool3d:31
#: tvm.relay.op.nn.nn.global_max_pool1d:29
#: tvm.relay.op.nn.nn.global_max_pool2d:32
#: tvm.relay.op.nn.nn.global_max_pool3d:30 tvm.relay.op.nn.nn.l2_normalize:20
#: tvm.relay.op.nn.nn.leaky_relu:19 tvm.relay.op.nn.nn.log_softmax:21
#: tvm.relay.op.nn.nn.lrn:34 tvm.relay.op.nn.nn.matmul:34
#: tvm.relay.op.nn.nn.max_pool1d:43 tvm.relay.op.nn.nn.max_pool2d:51
#: tvm.relay.op.nn.nn.max_pool2d_grad:34 tvm.relay.op.nn.nn.max_pool3d:44
#: tvm.relay.op.nn.nn.mirror_pad:19 tvm.relay.op.nn.nn.nll_loss:31
#: tvm.relay.op.nn.nn.pad:22 tvm.relay.op.nn.nn.prelu:22
#: tvm.relay.op.nn.nn.relu:14 tvm.relay.op.nn.nn.softmax:19
#: tvm.relay.op.nn.nn.sparse_add:23 tvm.relay.op.nn.nn.sparse_dense:40
#: tvm.relay.op.nn.nn.upsampling:34 tvm.relay.op.nn.nn.upsampling3d:40
msgid "The computed result."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:1
msgid "2D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:3
#: tvm.relay.op.nn.nn.global_avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D average value calculation "
"across each window represented by WxH."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:7
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with shape (batch_size, in_channels, output_height, output_width)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:16
#: tvm.relay.op.nn.nn.adaptive_max_pool1d:16
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output height and width."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:19
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size x output_size) for any input (NCHW)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool2d:22
#: tvm.relay.op.nn.nn.adaptive_max_pool2d:22
msgid ""
"If a tuple of integers (height, width) are provided for output_size, the "
"output size is (N x C x height x width) for any input (NCHW)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:1
msgid "3D adaptive avg pooling operator. This operator is experimental."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D avg value calculation "
"across each window represented by DxWxH."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:6
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:6
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, in_channels, depth, height, width)`, to produce an "
"output Tensor with shape (batch_size, in_channels, output_depth, "
"output_height, output_width)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:15
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:15
msgid ""
"If this argument is not provided, input depth, height and width will be "
"used as output depth, height and width."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:18
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:18
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size x output_size x output_size) for any input (NCDHW)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_avg_pool3d:21
#: tvm.relay.op.nn.nn.adaptive_max_pool3d:21
msgid ""
"If a tuple of integers (depth, height, width) are provided for "
"output_size, the output size is (N x C x depth x height x width) for any "
"input (NCDHW)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:1
msgid "1D adaptive max pooling operator. This operator is experimental."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_max_pool1d:3
#: tvm.relay.op.nn.nn.global_max_pool1d:3
msgid ""
"This operator takes data as input and does 1D max value calculation "
"across each window represented by W."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:1
msgid "2D adaptive max pooling operator. This operator is experimental."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_max_pool2d:3
#: tvm.relay.op.nn.nn.global_max_pool2d:3
msgid ""
"This operator takes data as input and does 2D max value calculation "
"across each window represented by WxH."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:1
msgid "3D adaptive max pooling operator. This operator is experimental."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.adaptive_max_pool3d:3
#: tvm.relay.op.nn.nn.global_max_pool3d:3
msgid ""
"This operator takes data as input and does 3D max value calculation "
"across each window represented by DxWxH."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:6
#: tvm.relay.op.nn.nn.max_pool1d:6
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, channels, width)`, to produce an output Tensor."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:10
#: tvm.relay.op.nn.nn.avg_pool3d:11 tvm.relay.op.nn.nn.max_pool1d:10
#: tvm.relay.op.nn.nn.max_pool3d:11
msgid ""
"The ceil_mode is used to take ceil or floor while computing out shape. "
"count_include_pad indicates including or excluding padded input values in"
" computation. This operator accepts data layout specification."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:20
#: tvm.relay.op.nn.nn.avg_pool2d:29 tvm.relay.op.nn.nn.avg_pool2d_grad:14
#: tvm.relay.op.nn.nn.avg_pool3d:21 tvm.relay.op.nn.nn.max_pool1d:20
#: tvm.relay.op.nn.nn.max_pool2d:28 tvm.relay.op.nn.nn.max_pool2d_grad:14
#: tvm.relay.op.nn.nn.max_pool3d:21
msgid "pool_size"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:-1
#: tvm.relay.op.nn.nn.avg_pool2d:-1 tvm.relay.op.nn.nn.avg_pool2d_grad:-1
#: tvm.relay.op.nn.nn.avg_pool3d:-1 tvm.relay.op.nn.nn.max_pool1d:-1
#: tvm.relay.op.nn.nn.max_pool2d:-1 tvm.relay.op.nn.nn.max_pool2d_grad:-1
#: tvm.relay.op.nn.nn.max_pool3d:-1
msgid "int or tuple of int, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:20
#: tvm.relay.op.nn.nn.avg_pool2d:29 tvm.relay.op.nn.nn.avg_pool2d_grad:14
#: tvm.relay.op.nn.nn.avg_pool3d:21 tvm.relay.op.nn.nn.max_pool1d:20
#: tvm.relay.op.nn.nn.max_pool2d:28 tvm.relay.op.nn.nn.max_pool2d_grad:14
#: tvm.relay.op.nn.nn.max_pool3d:21
msgid "The size of window for pooling."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:23
#: tvm.relay.op.nn.nn.avg_pool2d:32 tvm.relay.op.nn.nn.avg_pool2d_grad:17
#: tvm.relay.op.nn.nn.avg_pool3d:24 tvm.relay.op.nn.nn.bitserial_conv2d:12
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:15
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:16
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:16
#: tvm.relay.op.nn.nn.conv1d:35 tvm.relay.op.nn.nn.conv1d_transpose:12
#: tvm.relay.op.nn.nn.conv2d:35 tvm.relay.op.nn.nn.conv2d_transpose:12
#: tvm.relay.op.nn.nn.conv3d:35 tvm.relay.op.nn.nn.conv3d_transpose:12
#: tvm.relay.op.nn.nn.deformable_conv2d:17 tvm.relay.op.nn.nn.dilate:9
#: tvm.relay.op.nn.nn.max_pool1d:23 tvm.relay.op.nn.nn.max_pool2d:31
#: tvm.relay.op.nn.nn.max_pool2d_grad:17 tvm.relay.op.nn.nn.max_pool3d:24
msgid "strides"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:23
#: tvm.relay.op.nn.nn.avg_pool2d:32 tvm.relay.op.nn.nn.avg_pool2d_grad:17
#: tvm.relay.op.nn.nn.avg_pool3d:24 tvm.relay.op.nn.nn.max_pool1d:23
#: tvm.relay.op.nn.nn.max_pool2d:31 tvm.relay.op.nn.nn.max_pool2d_grad:17
#: tvm.relay.op.nn.nn.max_pool3d:24
msgid "The strides of pooling."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:26
#: tvm.relay.op.nn.nn.avg_pool2d:35 tvm.relay.op.nn.nn.avg_pool3d:27
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:21
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:22
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:24
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:24
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:22
#: tvm.relay.op.nn.nn.conv1d:41 tvm.relay.op.nn.nn.conv1d_transpose:18
#: tvm.relay.op.nn.nn.conv2d:41 tvm.relay.op.nn.nn.conv2d_transpose:18
#: tvm.relay.op.nn.nn.conv3d:41 tvm.relay.op.nn.nn.conv3d_transpose:18
#: tvm.relay.op.nn.nn.deformable_conv2d:23 tvm.relay.op.nn.nn.max_pool1d:26
#: tvm.relay.op.nn.nn.max_pool2d:34 tvm.relay.op.nn.nn.max_pool3d:27
msgid "dilation"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:26
#: tvm.relay.op.nn.nn.avg_pool2d:35 tvm.relay.op.nn.nn.avg_pool3d:27
#: tvm.relay.op.nn.nn.max_pool1d:26 tvm.relay.op.nn.nn.max_pool2d:34
#: tvm.relay.op.nn.nn.max_pool3d:27
msgid "The dilation of pooling."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:29
#: tvm.relay.op.nn.nn.avg_pool2d:38 tvm.relay.op.nn.nn.avg_pool2d_grad:20
#: tvm.relay.op.nn.nn.avg_pool3d:30 tvm.relay.op.nn.nn.bitserial_conv2d:15
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:19
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:21
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:21
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:19
#: tvm.relay.op.nn.nn.conv1d:38 tvm.relay.op.nn.nn.conv1d_transpose:15
#: tvm.relay.op.nn.nn.conv2d:38 tvm.relay.op.nn.nn.conv2d_transpose:15
#: tvm.relay.op.nn.nn.conv3d:38 tvm.relay.op.nn.nn.conv3d_transpose:15
#: tvm.relay.op.nn.nn.correlation:63 tvm.relay.op.nn.nn.deformable_conv2d:20
#: tvm.relay.op.nn.nn.max_pool1d:29 tvm.relay.op.nn.nn.max_pool2d:37
#: tvm.relay.op.nn.nn.max_pool2d_grad:20 tvm.relay.op.nn.nn.max_pool3d:30
msgid "padding"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:29
#: tvm.relay.op.nn.nn.avg_pool2d:38 tvm.relay.op.nn.nn.avg_pool2d_grad:20
#: tvm.relay.op.nn.nn.avg_pool3d:30 tvm.relay.op.nn.nn.max_pool1d:29
#: tvm.relay.op.nn.nn.max_pool2d:37 tvm.relay.op.nn.nn.max_pool2d_grad:20
#: tvm.relay.op.nn.nn.max_pool3d:30
msgid "The padding for pooling."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:-1
#: tvm.relay.op.nn.nn.avg_pool2d:-1 tvm.relay.op.nn.nn.avg_pool2d_grad:-1
#: tvm.relay.op.nn.nn.avg_pool3d:-1 tvm.relay.op.nn.nn.batch_matmul:-1
#: tvm.relay.op.nn.nn.conv1d:-1 tvm.relay.op.nn.nn.conv1d_transpose:-1
#: tvm.relay.op.nn.nn.conv2d:-1 tvm.relay.op.nn.nn.conv2d_transpose:-1
#: tvm.relay.op.nn.nn.conv3d:-1 tvm.relay.op.nn.nn.conv3d_transpose:-1
#: tvm.relay.op.nn.nn.global_avg_pool2d:-1
#: tvm.relay.op.nn.nn.global_max_pool2d:-1 tvm.relay.op.nn.nn.matmul:-1
#: tvm.relay.op.nn.nn.max_pool1d:-1 tvm.relay.op.nn.nn.max_pool2d:-1
#: tvm.relay.op.nn.nn.max_pool2d_grad:-1 tvm.relay.op.nn.nn.max_pool3d:-1
msgid "Optional[str]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:35
#: tvm.relay.op.nn.nn.avg_pool2d:44 tvm.relay.op.nn.nn.avg_pool2d_grad:26
#: tvm.relay.op.nn.nn.avg_pool3d:36 tvm.relay.op.nn.nn.global_avg_pool2d:27
#: tvm.relay.op.nn.nn.global_max_pool2d:27 tvm.relay.op.nn.nn.max_pool1d:35
#: tvm.relay.op.nn.nn.max_pool2d:43 tvm.relay.op.nn.nn.max_pool2d_grad:26
#: tvm.relay.op.nn.nn.max_pool3d:36
msgid "Layout of the output"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:38
#: tvm.relay.op.nn.nn.avg_pool2d:47 tvm.relay.op.nn.nn.avg_pool2d_grad:29
#: tvm.relay.op.nn.nn.avg_pool3d:39 tvm.relay.op.nn.nn.max_pool1d:38
#: tvm.relay.op.nn.nn.max_pool2d:46 tvm.relay.op.nn.nn.max_pool2d_grad:29
#: tvm.relay.op.nn.nn.max_pool3d:39
msgid "ceil_mode"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:-1
#: tvm.relay.op.nn.nn.avg_pool2d:-1 tvm.relay.op.nn.nn.avg_pool2d_grad:-1
#: tvm.relay.op.nn.nn.avg_pool3d:-1 tvm.relay.op.nn.nn.bitserial_dense:-1
#: tvm.relay.op.nn.nn.max_pool1d:-1 tvm.relay.op.nn.nn.max_pool2d:-1
#: tvm.relay.op.nn.nn.max_pool2d_grad:-1 tvm.relay.op.nn.nn.max_pool3d:-1
#: tvm.relay.op.nn.nn.sparse_dense:-1 tvm.relay.op.nn.nn.upsampling:-1
msgid "bool, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:38
#: tvm.relay.op.nn.nn.avg_pool2d:47 tvm.relay.op.nn.nn.avg_pool2d_grad:29
#: tvm.relay.op.nn.nn.avg_pool3d:39 tvm.relay.op.nn.nn.max_pool1d:38
#: tvm.relay.op.nn.nn.max_pool2d:46 tvm.relay.op.nn.nn.max_pool2d_grad:29
#: tvm.relay.op.nn.nn.max_pool3d:39
msgid "To enable or disable ceil while pooling."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:41
#: tvm.relay.op.nn.nn.avg_pool2d:50 tvm.relay.op.nn.nn.avg_pool2d_grad:32
#: tvm.relay.op.nn.nn.avg_pool3d:42
msgid "count_include_pad"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool1d:41
#: tvm.relay.op.nn.nn.avg_pool2d:50 tvm.relay.op.nn.nn.avg_pool2d_grad:32
#: tvm.relay.op.nn.nn.avg_pool3d:42
msgid "To include padding to compute the average."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool2d:7
#: tvm.relay.op.nn.nn.global_avg_pool2d:7
#: tvm.relay.op.nn.nn.global_max_pool2d:7 tvm.relay.op.nn.nn.max_pool2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with the following rule:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool2d:11
msgid "with data of shape (b, c, h, w), pool_size (kh, kw)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool2d:13
msgid ""
"\\mbox{out}(b, c, y, x)  = \\frac{1}{kh * kw} \\sum_{m=0}^{kh-1} "
"\\sum_{n=0}^{kw-1}\n"
"     \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x "
"+ n)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool2d:18
msgid ""
"Padding is applied to data before the computation. ceil_mode is used to "
"take ceil or floor while computing out shape. count_include_pad indicates"
" including or excluding padded input values in computation. This operator"
" accepts data layout specification."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool2d:-1
#: tvm.relay.op.nn.nn.avg_pool2d_grad:-1 tvm.relay.op.nn.nn.avg_pool3d:-1
#: tvm.relay.op.nn.nn.bitserial_conv2d:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:-1
#: tvm.relay.op.nn.nn.conv1d_transpose:-1
#: tvm.relay.op.nn.nn.conv2d_transpose:-1
#: tvm.relay.op.nn.nn.deformable_conv2d:-1 tvm.relay.op.nn.nn.max_pool2d:-1
#: tvm.relay.op.nn.nn.max_pool2d_grad:-1 tvm.relay.op.nn.nn.max_pool3d:-1
msgid "tuple of int, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:3
msgid ""
"This operator takes out_grad and data as input and calculates gradient of"
" avg_pool2d."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:8
#: tvm.relay.op.nn.nn.max_pool2d_grad:8
msgid "out_grad"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool2d_grad:8
#: tvm.relay.op.nn.nn.max_pool2d_grad:8
msgid "The output gradient"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.avg_pool3d:7
#: tvm.relay.op.nn.nn.max_pool3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, channels, depth, height, width)`, to produce an "
"output Tensor."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_flatten:3
msgid ""
"This operator flattens all the dimensions except for the batch dimension."
" which results a 2D output."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_flatten:6
msgid ""
"For data with shape ``(d1, d2, ..., dk)`` batch_flatten(data) returns "
"reshaped output of shape ``(d1, d2*...*dk)``."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_flatten:18
msgid "The Flattened result."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:3
msgid ""
"Both `tensor_a` and `tensor_b` can be transposed. For legacy reason, we "
"use NT format (transpose_a=False, transpose_b=True) by default."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:6
msgid ""
"\\mbox{batch_matmul}(A, B)[i, :, :] = \\mbox{matmul}(A[i, :, :], B[i, :, "
":])"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:13
msgid "tensor_a"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:13
msgid "The first input."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:16
msgid "tensor_b"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:16
msgid "The second input."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:19
#: tvm.relay.op.nn.nn.bitserial_conv2d:39 tvm.relay.op.nn.nn.bitserial_dense:30
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:42
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:43
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:45
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:45
#: tvm.relay.op.nn.nn.contrib_dense_pack:25
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:43
#: tvm.relay.op.nn.nn.conv1d:62 tvm.relay.op.nn.nn.conv1d_transpose:42
#: tvm.relay.op.nn.nn.conv2d:62 tvm.relay.op.nn.nn.conv2d_transpose:42
#: tvm.relay.op.nn.nn.conv3d:62 tvm.relay.op.nn.nn.conv3d_transpose:39
#: tvm.relay.op.nn.nn.deformable_conv2d:47 tvm.relay.op.nn.nn.dense:23
#: tvm.relay.op.nn.nn.matmul:23
msgid "out_dtype"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:19
msgid "Specifies the output data type for mixed precision batch matmul."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:22
#: tvm.relay.op.nn.nn.matmul:26
msgid "transpose_a"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:-1
#: tvm.relay.op.nn.nn.matmul:-1
msgid "Optional[bool] = False"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:22
msgid "Whether the first tensor is in transposed format."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:25
#: tvm.relay.op.nn.nn.matmul:29
msgid "transpose_b"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:-1
msgid "Optional[bool] = True"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:25
msgid "Whether the second tensor is in transposed format."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_matmul:29
#: tvm.relay.op.nn.nn.sparse_add:23 tvm.relay.op.nn.nn.sparse_dense:39
msgid "result: tvm.relay.Expr"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:1
msgid ""
"Batch normalization layer (Ioffe and Szegedy, 2014). Normalizes the input"
" at each batch, i.e. applies a transformation that maintains the mean "
"activation close to 0 and the activation standard deviation close to 1."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:6
msgid ""
"data\\_mean[i] = mean(data[:,i,:,...]) \\\\\n"
"data\\_var[i] = var(data[:,i,:,...])"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:11
msgid ""
"Then compute the normalized output, which has the same shape as input, as"
" following:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:13
msgid ""
"out[:,i,:,...] = \\frac{data[:,i,:,...] - "
"data\\_mean[i]}{\\sqrt{data\\_var[i]+\\epsilon}}\n"
"    * gamma[i] + beta[i]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:18
msgid "Both *mean* and *var* returns a scalar by treating the input as a vector."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:20
#: tvm.relay.op.nn.nn.instance_norm:14
msgid ""
"Assume the input has size *k* on axis 1, then both ``gamma`` and ``beta``"
" have shape *(k,)*."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:23
msgid ""
"Besides the inputs and the outputs, this operator accepts two auxiliary "
"states, ``moving_mean`` and ``moving_var``, which are *k*-length vectors."
" They are global statistics for the whole dataset, which are updated by"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:32
msgid ""
"The parameter ``axis`` specifies which axis of the input shape denotes "
"the 'channel' (separately normalized groups).  The default is 1. "
"Specifying -1 sets the channel axis to be the last item in the input "
"shape."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:38
#: tvm.relay.op.nn.nn.fast_softmax:6 tvm.relay.op.nn.nn.group_norm:24
#: tvm.relay.op.nn.nn.instance_norm:23 tvm.relay.op.nn.nn.layer_norm:17
#: tvm.relay.op.nn.nn.log_softmax:8 tvm.relay.op.nn.nn.softmax:6
msgid "This operator can be optimized away for inference."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:43
msgid "Input to which batch_norm will be applied."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:46
#: tvm.relay.op.nn.nn.group_norm:32 tvm.relay.op.nn.nn.instance_norm:31
#: tvm.relay.op.nn.nn.layer_norm:25
msgid "gamma"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:46
#: tvm.relay.op.nn.nn.group_norm:32 tvm.relay.op.nn.nn.instance_norm:31
#: tvm.relay.op.nn.nn.layer_norm:25
msgid "The gamma scale factor."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:49
#: tvm.relay.op.nn.nn.group_norm:35 tvm.relay.op.nn.nn.instance_norm:34
#: tvm.relay.op.nn.nn.layer_norm:28 tvm.relay.op.nn.nn.lrn:29
msgid "beta"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:49
#: tvm.relay.op.nn.nn.group_norm:35 tvm.relay.op.nn.nn.instance_norm:34
#: tvm.relay.op.nn.nn.layer_norm:28
msgid "The beta offset factor."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:52
msgid "moving_mean"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:52
msgid "Running mean of input,"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:55
msgid "moving_var"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:55
msgid "Running variance of input."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:58
#: tvm.relay.op.nn.nn.bias_add:16 tvm.relay.op.nn.nn.fifo_buffer:24
#: tvm.relay.op.nn.nn.group_norm:41 tvm.relay.op.nn.nn.instance_norm:37
#: tvm.relay.op.nn.nn.l2_normalize:15 tvm.relay.op.nn.nn.layer_norm:31
#: tvm.relay.op.nn.nn.lrn:20 tvm.relay.op.nn.nn.prelu:17
msgid "axis"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:-1
#: tvm.relay.op.nn.nn.group_norm:-1 tvm.relay.op.nn.nn.instance_norm:-1
msgid "int, optional, default=1"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:58
#: tvm.relay.op.nn.nn.instance_norm:37
msgid "Specify along which shape axis the channel is specified."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:61
#: tvm.relay.op.nn.nn.group_norm:44 tvm.relay.op.nn.nn.instance_norm:40
#: tvm.relay.op.nn.nn.layer_norm:34
msgid "epsilon"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:-1
#: tvm.relay.op.nn.nn.group_norm:-1 tvm.relay.op.nn.nn.instance_norm:-1
#: tvm.relay.op.nn.nn.layer_norm:-1
msgid "double, optional, default=1e-5"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:61
#: tvm.relay.op.nn.nn.group_norm:44 tvm.relay.op.nn.nn.instance_norm:40
#: tvm.relay.op.nn.nn.layer_norm:34
msgid "Small float added to variance to avoid dividing by zero."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:65
#: tvm.relay.op.nn.nn.group_norm:48 tvm.relay.op.nn.nn.instance_norm:44
#: tvm.relay.op.nn.nn.layer_norm:38
msgid "center"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:-1
#: tvm.relay.op.nn.nn.group_norm:-1 tvm.relay.op.nn.nn.instance_norm:-1
#: tvm.relay.op.nn.nn.layer_norm:-1
msgid "boolean, optional, default=True"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:64
#: tvm.relay.op.nn.nn.group_norm:47 tvm.relay.op.nn.nn.instance_norm:43
#: tvm.relay.op.nn.nn.layer_norm:37
msgid ""
"If True, add offset of beta to normalized tensor, If False, beta is "
"ignored."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:70
#: tvm.relay.op.nn.nn.group_norm:51 tvm.relay.op.nn.nn.instance_norm:47
#: tvm.relay.op.nn.nn.layer_norm:41
msgid "scale"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:68
msgid ""
"If true, multiply by gamma. If False, gamma is not used. When the next "
"layer is piecewise linear (also e.g. nn.relu), this can be disabled since"
" the scaling will be done by the next layer."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:-1
#: tvm.relay.op.nn.nn.sparse_transpose:-1
msgid "relay.Tuple([tvm.relay.Expr, tvm.relay.Expr, tvm.relay.Expr])"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_norm:75
msgid ""
"Tuple of normed data (same shape as input), new running mean (k-length "
"vector), and new running variance (k-length vector)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_to_space_nd:-1
#: tvm.relay.op.nn.nn.correlation:-1 tvm.relay.op.nn.nn.space_to_batch_nd:-1
msgid "tvm.te.Tensor"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_to_space_nd:6
#: tvm.relay.op.nn.nn.space_to_batch_nd:7
msgid "N-D with shape [batch, spatial_shape, remaining_shape]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_to_space_nd:10
#: tvm.relay.op.nn.nn.space_to_batch_nd:11
msgid "block_shape"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_to_space_nd:-1
#: tvm.relay.op.nn.nn.space_to_batch_nd:-1
msgid "relay.Expr"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_to_space_nd:9
#: tvm.relay.op.nn.nn.space_to_batch_nd:10
msgid ""
"1-D of size [M] where M is number of spatial dims, specifies block size "
"for each spatial dimension."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_to_space_nd:14
msgid "crops"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_to_space_nd:13
msgid ""
"2-D of shape [M, 2] where M is number of spatial dims, specifies [begin, "
"end] crop size for each spatial dimension."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.batch_to_space_nd:19
msgid ""
"N-D Tensor with shape [batch / prod(block_shape), in_shape[1] * "
"block_shape[0] - crops[0,0] - crops[0,1], ..., in_shape[M] * "
"block_shape[M-1] - crops[M-1, 0] - crops[M-1, 1], remaining_shape]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bias_add:3
msgid ""
"Add 1D bias to the axis of data. This function is a special case of add "
"which allows inference of shape of the bias from data."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bias_add:13 tvm.relay.op.nn.nn.lrn:23
msgid "bias"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bias_add:13
msgid "The bias to be added."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bias_add:-1
#: tvm.relay.op.nn.nn.bitserial_conv2d:-1 tvm.relay.op.nn.nn.bitserial_dense:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_dense_pack:-1
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:-1
#: tvm.relay.op.nn.nn.conv1d_transpose:-1
#: tvm.relay.op.nn.nn.conv2d_transpose:-1
#: tvm.relay.op.nn.nn.deformable_conv2d:-1 tvm.relay.op.nn.nn.dense:-1
#: tvm.relay.op.nn.nn.lrn:-1 tvm.relay.op.nn.nn.prelu:-1
msgid "int, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bias_add:16
msgid "The axis to add the bias."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bias_add:21
msgid "The final result."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:3
msgid ""
"The values along the input tensor's pack_axis are quantized and packed "
"together into the specified pack_type in a new bit axis."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:6
msgid ""
"For example, consider bitpacking with data to be a tensor with shape `[1,"
" 64, 128, 128]`, pack_axis=1, bit_axis=4, pack_type=uint8, and bits=2. "
"The output in this case will be of shape `[1, 8, 128, 128, 2]`. The "
"dimension of axis 1 has been reduced by a factor of 8 since each value is"
" packed into an 8-bit uint8. Axis 4 is now two bitplanes representing the"
" quantized value of the incoming data. The output tensor is now ready to "
"be used in a bitserial operation."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:-1
msgid "tvm.relay.expr"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:16
msgid "The incoming tensor to be packed."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:19
msgid "bits"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:-1
#: tvm.relay.op.nn.nn.bitserial_conv2d:-1 tvm.relay.op.nn.nn.bitserial_dense:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:-1
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:-1
#: tvm.relay.op.nn.nn.depth_to_space:-1 tvm.relay.op.nn.nn.fifo_buffer:-1
#: tvm.relay.op.nn.nn.group_norm:-1 tvm.relay.op.nn.nn.nll_loss:-1
#: tvm.relay.op.nn.nn.space_to_depth:-1
#: tvm.relay.op.nn.utils.get_pad_tuple1d:-1
#: tvm.relay.op.nn.utils.get_pad_tuple2d:-1
#: tvm.relay.op.nn.utils.get_pad_tuple3d:-1
msgid "int"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:19
msgid "Number of bits that should be packed."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:22
msgid "pack_axis"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:22
msgid "Axis that should be decomposed and packed."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:25
msgid "bit_axis"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:25
msgid "New axis containing bitplane."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:28
msgid "pack_type"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:-1
msgid "str"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:28
#: tvm.relay.op.nn.nn.bitserial_conv2d:36
msgid "Datatype to pack bits into."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:31
msgid "name"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:31
msgid "Name of the operation."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitpack:36
msgid "The packed tensor."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:9
#: tvm.relay.op.nn.nn.bitserial_dense:15
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_dense_pack:16 tvm.relay.op.nn.nn.conv1d:32
#: tvm.relay.op.nn.nn.conv1d_transpose:9 tvm.relay.op.nn.nn.conv2d:32
#: tvm.relay.op.nn.nn.conv2d_transpose:9 tvm.relay.op.nn.nn.conv3d:32
#: tvm.relay.op.nn.nn.conv3d_transpose:9
#: tvm.relay.op.nn.nn.deformable_conv2d:14 tvm.relay.op.nn.nn.dense:16
#: tvm.relay.op.nn.nn.matmul:16
msgid "weight"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:9
#: tvm.relay.op.nn.nn.bitserial_dense:15
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:9
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:12
#: tvm.relay.op.nn.nn.conv1d:32 tvm.relay.op.nn.nn.conv1d_transpose:9
#: tvm.relay.op.nn.nn.conv2d:32 tvm.relay.op.nn.nn.conv2d_transpose:9
#: tvm.relay.op.nn.nn.conv3d:32 tvm.relay.op.nn.nn.conv3d_transpose:9
#: tvm.relay.op.nn.nn.deformable_conv2d:14
msgid "The weight expressions."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:12
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:15
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:16
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:16
#: tvm.relay.op.nn.nn.conv1d:35 tvm.relay.op.nn.nn.conv1d_transpose:12
#: tvm.relay.op.nn.nn.conv2d:35 tvm.relay.op.nn.nn.conv2d_transpose:12
#: tvm.relay.op.nn.nn.conv3d:35 tvm.relay.op.nn.nn.conv3d_transpose:12
#: tvm.relay.op.nn.nn.deformable_conv2d:17
msgid "The strides of convolution."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:15
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:18
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:19
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:21
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:21
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:19
#: tvm.relay.op.nn.nn.conv2d:38 tvm.relay.op.nn.nn.conv3d:38
#: tvm.relay.op.nn.nn.conv3d_transpose:15
#: tvm.relay.op.nn.nn.deformable_conv2d:20
msgid "The padding of convolution on both sides of inputs before convolution."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:18
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:27
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:28
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:30
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:30
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:28
#: tvm.relay.op.nn.nn.conv1d:47 tvm.relay.op.nn.nn.conv1d_transpose:21
#: tvm.relay.op.nn.nn.conv2d:47 tvm.relay.op.nn.nn.conv2d_transpose:21
#: tvm.relay.op.nn.nn.conv3d:47 tvm.relay.op.nn.nn.conv3d_transpose:24
#: tvm.relay.op.nn.nn.deformable_conv2d:32
msgid "channels"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:18
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:27
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:28
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:30
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:30
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:28
#: tvm.relay.op.nn.nn.conv1d:47 tvm.relay.op.nn.nn.conv1d_transpose:21
#: tvm.relay.op.nn.nn.conv2d:47 tvm.relay.op.nn.nn.conv2d_transpose:21
#: tvm.relay.op.nn.nn.conv3d:47 tvm.relay.op.nn.nn.conv3d_transpose:24
#: tvm.relay.op.nn.nn.deformable_conv2d:32
msgid "Number of output channels of this convolution."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:21
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:30
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:31
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:33
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:33
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:31
#: tvm.relay.op.nn.nn.conv1d:50 tvm.relay.op.nn.nn.conv1d_transpose:24
#: tvm.relay.op.nn.nn.conv2d:50 tvm.relay.op.nn.nn.conv2d_transpose:24
#: tvm.relay.op.nn.nn.conv3d:50 tvm.relay.op.nn.nn.conv3d_transpose:27
#: tvm.relay.op.nn.nn.deformable_conv2d:35
msgid "kernel_size"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:21
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:30
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:31
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:33
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:33
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:31
#: tvm.relay.op.nn.nn.conv1d_transpose:24 tvm.relay.op.nn.nn.conv2d:50
#: tvm.relay.op.nn.nn.conv2d_transpose:24 tvm.relay.op.nn.nn.conv3d:50
#: tvm.relay.op.nn.nn.conv3d_transpose:27
#: tvm.relay.op.nn.nn.deformable_conv2d:35
msgid "The spatial of the convolution kernel."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:24
msgid "activation_bits"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:24
msgid "Number of bits to pack for activations."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:27
#: tvm.relay.op.nn.nn.bitserial_dense:24
msgid "weight_bits"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:27
msgid "Number of bits to pack for weights."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:30
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:33
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:34
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:36
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:36
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:34
#: tvm.relay.op.nn.nn.conv1d:53 tvm.relay.op.nn.nn.conv1d_transpose:30
#: tvm.relay.op.nn.nn.conv2d:53 tvm.relay.op.nn.nn.conv2d_transpose:30
#: tvm.relay.op.nn.nn.conv3d:53 tvm.relay.op.nn.nn.conv3d_transpose:30
#: tvm.relay.op.nn.nn.deformable_conv2d:38
msgid "data_layout"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:33
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:36
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:37
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:39
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:39
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:37
#: tvm.relay.op.nn.nn.conv1d:56 tvm.relay.op.nn.nn.conv1d_transpose:33
#: tvm.relay.op.nn.nn.conv2d:56 tvm.relay.op.nn.nn.conv2d_transpose:33
#: tvm.relay.op.nn.nn.conv3d:56 tvm.relay.op.nn.nn.conv3d_transpose:33
#: tvm.relay.op.nn.nn.deformable_conv2d:41
msgid "kernel_layout"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:33
msgid "Layout of the kernel"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:36
msgid "pack_dtype: str, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_conv2d:39
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:42
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:43
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:45
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:45
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:43
#: tvm.relay.op.nn.nn.conv1d:62 tvm.relay.op.nn.nn.conv1d_transpose:42
#: tvm.relay.op.nn.nn.conv2d:62 tvm.relay.op.nn.nn.conv2d_transpose:42
#: tvm.relay.op.nn.nn.conv3d:62 tvm.relay.op.nn.nn.deformable_conv2d:47
msgid "Specifies the output data type for mixed precision conv2d."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:1
msgid ""
"Bitserial Dense operator. Applies matrix multiplication of two quantized "
"matrices using a fast bitserial algorithm."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:7
msgid "`Y = X * W`"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:18
#: tvm.relay.op.nn.nn.contrib_dense_pack:22 tvm.relay.op.nn.nn.dense:19
#: tvm.relay.op.nn.nn.matmul:19
msgid "units"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:18
#: tvm.relay.op.nn.nn.contrib_dense_pack:22 tvm.relay.op.nn.nn.dense:19
msgid "Number of hidden units of the dense transformation."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:21
msgid "data_bits"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:21
msgid "Number of bits incoming tensor should be packed with."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:24
msgid "Number of bits weight tensor should be packed with."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:27
msgid "pack_dtype"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:27
msgid "Datatype to pack individual bits into before computation."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:30
#: tvm.relay.op.nn.nn.contrib_dense_pack:25
msgid "Specifies the output data type for mixed precision dense."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:33
msgid "unipolar"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.bitserial_dense:33
msgid "Whether to use unipolar or bipolar quantization for inputs."
msgstr ""

#: ../../docstring of tvm.relay.expr.const:6
msgid "value: Union[bool, int, float, numpy.ndarray, tvm.nd.NDArray]"
msgstr ""

#: ../../docstring of tvm.relay.expr.const:6
msgid "The constant value."
msgstr ""

#: ../../docstring of tvm.relay.expr.const:9
msgid "dtype: str, optional"
msgstr ""

#: ../../docstring of tvm.relay.expr.const:9
msgid "The data type of the resulting constant."
msgstr ""

#: ../../docstring of tvm.relay.expr.const:15
msgid "Note"
msgstr ""

#: ../../docstring of tvm.relay.expr.const:16
msgid "When dtype is None, we use the following rule:"
msgstr ""

#: ../../docstring of tvm.relay.expr.const:18
msgid "int maps to \"int32\""
msgstr ""

#: ../../docstring of tvm.relay.expr.const:19
msgid "float maps to \"float32\""
msgstr ""

#: ../../docstring of tvm.relay.expr.const:20
msgid "bool maps to \"bool\""
msgstr ""

#: ../../docstring of tvm.relay.expr.const:21
msgid "other using the same default rule as numpy."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:3
msgid ""
"We separate this as a single op to enable pre-compute for inference. Use "
"this together with nn.contrib_conv2d_gemm_without_weight_transform"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:8
#: tvm.relay.op.nn.nn.nll_loss:19
msgid "weights"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:10
msgid "tile_N: int"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:11
msgid ""
"Tile size across N axis of the weight transformation for ConvGemm. (N = "
"OC)"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:13
msgid "tile_K: int"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_weight_transform:13
msgid ""
"Tile size across K axis of the weight transformation for ConvGemm. (K = "
"KW * KH * IC)"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:3
msgid ""
"The basic parameters are the same as the ones in vanilla conv2d. It "
"assumes the weight is pre-transformed by "
"nn.contrib_conv2d_gemm_weight_transform"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:21
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:22
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:24
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:24
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:22
#: tvm.relay.op.nn.nn.conv1d:41 tvm.relay.op.nn.nn.conv1d_transpose:18
#: tvm.relay.op.nn.nn.conv2d:41 tvm.relay.op.nn.nn.conv2d_transpose:18
#: tvm.relay.op.nn.nn.conv3d:41 tvm.relay.op.nn.nn.conv3d_transpose:18
#: tvm.relay.op.nn.nn.deformable_conv2d:23
msgid "Specifies the dilation rate to be used for dilated convolution."
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:24
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:25
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:27
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:27
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:25
#: tvm.relay.op.nn.nn.conv1d:44 tvm.relay.op.nn.nn.conv1d_transpose:27
#: tvm.relay.op.nn.nn.conv2d:44 tvm.relay.op.nn.nn.conv2d_transpose:27
#: tvm.relay.op.nn.nn.conv3d:44 tvm.relay.op.nn.nn.conv3d_transpose:21
#: tvm.relay.op.nn.nn.deformable_conv2d:29
msgid "groups"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:24
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:25
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:27
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:27
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:25
#: tvm.relay.op.nn.nn.conv1d_transpose:27 tvm.relay.op.nn.nn.conv2d:44
#: tvm.relay.op.nn.nn.conv2d_transpose:27 tvm.relay.op.nn.nn.conv3d:44
#: tvm.relay.op.nn.nn.conv3d_transpose:21
#: tvm.relay.op.nn.nn.deformable_conv2d:29
msgid "Number of groups for grouped convolution."
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:36
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:37
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:39
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:39
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:37
#: tvm.relay.op.nn.nn.conv1d:56 tvm.relay.op.nn.nn.conv1d_transpose:33
#: tvm.relay.op.nn.nn.conv2d:56 tvm.relay.op.nn.nn.conv2d_transpose:33
#: tvm.relay.op.nn.nn.conv3d:56 tvm.relay.op.nn.nn.conv3d_transpose:33
#: tvm.relay.op.nn.nn.deformable_conv2d:41
msgid "Layout of the weight."
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_gemm_without_weight_transform:39
#: tvm.relay.op.nn.nn.contrib_conv2d_nchwc:40
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:42
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:42
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:40
#: tvm.relay.op.nn.nn.conv1d:59 tvm.relay.op.nn.nn.conv1d_transpose:36
#: tvm.relay.op.nn.nn.conv2d:59 tvm.relay.op.nn.nn.conv2d_transpose:36
#: tvm.relay.op.nn.nn.conv3d:59 tvm.relay.op.nn.nn.conv3d_transpose:36
#: tvm.relay.op.nn.nn.deformable_conv2d:44
msgid "Layout of the output, by default, out_layout is the same as data_layout"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:3
msgid ""
"This operator takes the weight as the convolution kernel and convolves it"
" with data to produce an output, following a specialized NCHWc data "
"layout."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:13
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:13
msgid "kernel"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_conv2d_nchwc:13
#: tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:13
msgid "The kernel expressions."
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:3
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:3
msgid ""
"We separate this as a single op to enable pre-compute for inference. Use "
"this together with nn.contrib_conv2d_winograd_without_weight_transform"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:12
msgid "convolution_algorithm"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_nnpack_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:15
msgid "The Tile size of winograd. E.g. 2 for F(2x2, 3x3) and 4 for F(4x4, 3x3)"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:15
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:15
msgid "tile_size"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv2d_winograd_without_weight_transform:3
msgid ""
"The basic parameters are the same as the ones in vanilla conv2d. It "
"assumes the weight is pre-transformed by "
"nn.contrib_conv2d_winograd_weight_transform"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:3
msgid ""
"We separate this as a single op to enable pre-compute for inference. Use "
"this together with nn.contrib_conv3d_winograd_without_weight_transform"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_weight_transform:12
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:15
msgid ""
"The Tile size of winograd. E.g. 2 for F(2x2x2, 3x3x3) and 4 for F(4x4x4, "
"3x3x3)"
msgstr ""

#: ../../docstring of
#: tvm.relay.op.nn.nn.contrib_conv3d_winograd_without_weight_transform:3
msgid ""
"The basic parameters are the same as the ones in vanilla conv3d. It "
"assumes the weight is pre-transformed by "
"nn.contrib_conv3d_winograd_weight_transform"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_dense_pack:1
msgid "Dense operator. Applies a linear transformation with packed weight"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_dense_pack:6
#: tvm.relay.op.nn.nn.dense:6
msgid "`Y = X * W^T`"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_dense_pack:11
msgid "The input data to the operator, of shape `(batch, units_in)`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_dense_pack:15
msgid ""
"The transformed weight expressions, 3-D matrix, of shape `(units // "
"pack_weight_tile, units_in, pack_weight_tile)`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_dense_pack:19
msgid "weight_layout: str"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_dense_pack:19
msgid "The layout of weight, such as \"NC\" or \"NC8n\"."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.contrib_depthwise_conv2d_nchwc:3
msgid ""
"This operator takes the weight as the depthwise convolution kernel and "
"depthwise convolves it with data to produce an output, following a "
"specialized NCHWc data layout."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d:3 tvm.relay.op.nn.nn.conv2d:3
#: tvm.relay.op.nn.nn.conv3d:3
msgid ""
"This operator takes the weight as the convolution kernel and convolves it"
" with data to produce an output."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d:7
msgid ""
"In the default case, where the data_layout is `NCW` and kernel_layout is "
"`OIW`, conv1d takes in a data Tensor with shape `(batch_size, "
"in_channels, width)`, and a weight Tensor with shape `(channels, "
"in_channels, kernel_size)` to produce an output Tensor with the following"
" rule:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d:13
msgid ""
"\\mbox{out}[b, c, w] = \\sum_{dw, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * w + dw] *\n"
"   \\mbox{weight}[c, k, dw]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d:19
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCW` for data and `OIW` for weight), perform the computation, "
"then convert to the out_layout."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d:-1 tvm.relay.op.nn.nn.conv2d:-1
#: tvm.relay.op.nn.nn.conv3d:-1 tvm.relay.op.nn.nn.conv3d_transpose:-1
msgid "Optional[int, Tuple[int]]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d:38
msgid "The padding of convolution on both sides of the input before convolution."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d:-1 tvm.relay.op.nn.nn.conv2d:-1
#: tvm.relay.op.nn.nn.conv3d:-1 tvm.relay.op.nn.nn.conv3d_transpose:-1
#: tvm.relay.op.nn.nn.matmul:-1
msgid "Optional[int]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d:44
msgid "Currently unused for 1D convolution."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d:50
msgid "The spatial dimension of the convolution kernel."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d_transpose:-1
#: tvm.relay.op.nn.nn.conv2d_transpose:-1
msgid "Tuple[int], optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d_transpose:15
#: tvm.relay.op.nn.nn.conv2d_transpose:15
msgid "The padding of convolution on both sides of inputs."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d_transpose:39
#: tvm.relay.op.nn.nn.conv2d_transpose:39
msgid "output_padding"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv1d_transpose:39
#: tvm.relay.op.nn.nn.conv2d_transpose:39
msgid "Used to disambiguate the output shape."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` and kernel_layout is"
" `OIHW`, conv2d takes in a data Tensor with shape `(batch_size, "
"in_channels, height, width)`, and a weight Tensor with shape `(channels, "
"in_channels, kernel_size[0], kernel_size[1])` to produce an output Tensor"
" with the following rule:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv2d:13
msgid ""
"\\mbox{out}[b, c, y, x] = \\sum_{dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * y  + dy, \\mbox{strides}[1] * "
"x + dx] *\n"
"   \\mbox{weight}[c, k, dy, dx]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv2d:19
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCHW` for data and `OIHW` for weight), perform the computation, "
"then convert to the out_layout."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv2d_backward_weight:3
msgid ""
"This operator takes the output gradient `grad` and convolves it with "
"`data` as the convolution kernel, to produce the gradient with respect to"
" weight."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv2d_backward_weight:6
msgid ""
"Note that the parameter `kernel_size` is the spatial size of the "
"corresponding forward convolution kernel, not that of `data`. "
"`grad_layout` and `kernel_layout` are the layouts of `grad` and the "
"weight gradient respectively."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv2d_backward_weight:10
msgid ""
"Other parameters are the same as the conv2d op. See its documentation for"
" more details."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` and kernel_layout "
"is `OIDHW`, conv3d takes in a data Tensor with shape `(batch_size, "
"in_channels, depth, height, width)`, and a weight Tensor with shape "
"`(channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])`"
" to produce an output Tensor with the following rule:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv3d:13
msgid ""
"\\mbox{out}[b, c, z, y, x] = \\sum_{dz, dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * z  + dz, \\mbox{strides}[1] * "
"y  + dy,\n"
"   \\mbox{strides}[2] * x + dx] * \\mbox{weight}[c, k, dz, dy, dx]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv3d:19
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCDHW` for data and `OIDHW` for weight), perform the "
"computation, then convert to the out_layout."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv3d:-1
#: tvm.relay.op.nn.nn.conv3d_transpose:-1
msgid "Optional[Tuple[int]]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.conv3d_transpose:39
msgid "Specifies the output data type for mixed precision conv3d."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:3
msgid ""
"The correlation layer performs multiplicative patch comparisons between "
"two feature maps. Given two multi-channel feature maps :math:`f_{1}, "
"f_{2}`, with :math:`w`, :math:`h`, and :math:`c` being their width, "
"height, and number of channels, the correlation layer lets the network "
"compare each patch from :math:`f_{1}` with each patch from :math:`f_{2}`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:8
msgid ""
"For now we consider only a single comparison of two patches. The "
"'correlation' of two patches centered at :math:`x_{1}` in the first map "
"and :math:`x_{2}` in the second map is then defined as:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:12
msgid ""
"c(x_{1}, x_{2}) = \\sum_{o \\in [-k,k] \\times [-k,k]} <f_{1}(x_{1} + o),"
" f_{2}(x_{2} + o)>"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:16
msgid "for a square patch of size :math:`K:=2k+1`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:18
msgid ""
"Note that the equation above is identical to one step of a convolution in"
" neural networks, but instead of convolving data with a filter, it "
"convolves data with other    data. For this reason, it has no training "
"weights."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:22
msgid ""
"Computing :math:`c(x_{1}, x_{2})` involves :math:`c * K^{2}` "
"multiplications. Comparing all patch combinations involves "
":math:`w^{2}*h^{2}` such computations."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:25
msgid ""
"Given a maximum displacement :math:`d`, for each location :math:`x_{1}` "
"it computes correlations :math:`c(x_{1}, x_{2})` only in a neighborhood "
"of size :math:`D:=2d+1`, by limiting the range of :math:`x_{2}`. We use "
"strides :math:`s_{1}, s_{2}`, to quantize :math:`x_{1}` globally and to "
"quantize :math:`x_{2}` within the neighborhood centered around "
":math:`x_{1}`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:31
msgid "The final output is defined by the following expression:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:33
msgid "out[n, q, i, j] = c(x_{i, j}, x_{q})"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:37
msgid ""
"where :math:`i` and :math:`j` enumerate spatial locations in "
":math:`f_{1}`, and :math:`q` denotes the :math:`q^{th}` neighborhood of "
":math:`x_{i,j}`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:43
msgid "data1"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:43
#: tvm.relay.op.nn.nn.correlation:46
msgid "4-D with shape [batch, channel, height, width]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:46
msgid "data2"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:49
msgid "kernel_size: int"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:49
msgid "Kernel size for correlation, must be an odd number"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:52
msgid "max_displacement: int"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:52
msgid "Max displacement of Correlation"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:55
msgid "stride1: int"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:55
msgid "Stride for data1"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:58
msgid "stride2: int"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:58
msgid "Stride for data2 within the neightborhood centered around data1"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:-1
msgid "int or a list/tuple of 2 or 4 ints"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:61
msgid ""
"Padding size, or [pad_height, pad_width] for 2 ints, or [pad_top, "
"pad_left, pad_bottom, pad_right] for 4 ints"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:66
msgid "is_multiply: bool"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:66
msgid "operation type is either multiplication or substraction"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:69
msgid "layout: str"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:69
msgid "layout of data1, data2 and the output"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:73
#: tvm.relay.op.nn.nn.dilate:16
msgid "Output"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.correlation:74
msgid "4-D with shape [batch, out_channel, out_height, out_width]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.cross_entropy:6
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:6
#: tvm.relay.op.nn.nn.nll_loss:13
msgid "predictions"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.cross_entropy:6
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:6
#: tvm.relay.op.nn.nn.nll_loss:13
msgid "The predictions."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.cross_entropy:9
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:9
#: tvm.relay.op.nn.nn.nll_loss:16
msgid "targets"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.cross_entropy:9
#: tvm.relay.op.nn.nn.cross_entropy_with_logits:9
msgid "The targets."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.deformable_conv2d:3
msgid ""
"The deformable convolution operation is described in "
"https://arxiv.org/abs/1703.06211"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.deformable_conv2d:11
msgid "offset"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.deformable_conv2d:11
msgid "The offset expressions."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.deformable_conv2d:26
msgid "deformable_groups"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.deformable_conv2d:26
msgid "Number of deformable groups."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dense:1
msgid "Dense operator. Applies a linear transformation"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dense:11
msgid "The input data to the operator, of shape `(d_1, d_2, ..., d_n, units_in)`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dense:15
msgid "The weight expressions, 2-D matrix, of shape `(units, units_in)`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dense:22
msgid ""
"Specifies the output data type for mixed precision dense, of shape `(d_1,"
" d_2, ..., d_n, units)`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.depth_to_space:6
msgid "Input data with channels divisible by block_size**2"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.depth_to_space:9
#: tvm.relay.op.nn.nn.space_to_depth:9
msgid "block_size"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.depth_to_space:9
msgid "Size of blocks to convert channels into."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.depth_to_space:-1
#: tvm.relay.op.nn.nn.nll_loss:-1 tvm.relay.op.nn.nn.space_to_depth:-1
msgid "string"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.depth_to_space:12
#: tvm.relay.op.nn.nn.space_to_depth:12
msgid "One of NCHW or NHWC, indicates channel axis."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.depth_to_space:16
msgid "mode"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.depth_to_space:15
msgid "One of DCR or CDR, indicates which order channels are accessed in."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.depth_to_space:21
msgid "Tensor with shape [in_batch, in_channel / block_size * block_size,"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.depth_to_space:22
msgid "in_height * block_size, in_width * block_size]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dilate:6
msgid "n-D, can be any layout."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dilate:-1
msgid "tuple of <int>"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dilate:9
msgid "Dilation stride on each dimension, 1 means no dilation."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dilate:12
msgid "dilation_value"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dilate:-1
msgid "int/float, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dilate:12
msgid "Value used to dilate the input."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dilate:17
msgid "The computed result"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dropout:3
#: tvm.relay.op.nn.nn.dropout_raw:3
msgid ""
"During training, each element of the input is set to zero with "
"probability ``p``. The whole array is rescaled by ``1/(1-p)`` to keep the"
" expected sum of the input unchanged."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dropout:13
#: tvm.relay.op.nn.nn.dropout_raw:13
msgid "rate"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dropout:-1
#: tvm.relay.op.nn.nn.dropout_raw:-1
msgid "float, optional (default=0.5)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dropout:13
#: tvm.relay.op.nn.nn.dropout_raw:13
msgid "The probability for an element to be reset to 0."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.dropout:18
#: tvm.relay.op.nn.nn.dropout_raw:18
msgid "The result of dropout"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fast_softmax:1
msgid "Computes softmax. Use approximation to compute exponent for faster speed."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fast_softmax:4
#: tvm.relay.op.nn.nn.softmax:3
msgid ""
"\\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n"
"\n"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fast_softmax:10
#: tvm.relay.op.nn.nn.log_softmax:13 tvm.relay.op.nn.nn.mirror_pad:8
#: tvm.relay.op.nn.nn.pad:8 tvm.relay.op.nn.nn.softmax:11
msgid "data: tvm.relay.Expr"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fast_softmax:13
#: tvm.relay.op.nn.nn.log_softmax:16 tvm.relay.op.nn.nn.softmax:14
msgid "axis: int, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fast_softmax:13
#: tvm.relay.op.nn.nn.softmax:14
msgid "The axis to sum over when computing softmax"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fifo_buffer:3
msgid "Compute equivalent of"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fifo_buffer:12
msgid "Useful for"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fifo_buffer:14
msgid ""
"Encoding explicit re-use of computation in convolution ops operated on a "
"sliding window input"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fifo_buffer:15
msgid ""
"Implementing a FIFO queue to cache intermediate results, e.g. as in Fast "
"WaveNet."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fifo_buffer:20
#: tvm.relay.op.nn.nn.relu:9
msgid "The input data"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fifo_buffer:21
msgid "buffer"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fifo_buffer:22
msgid "Previous value of the FIFO buffer"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fifo_buffer:24
msgid "Specify which axis should be used for buffering"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.fifo_buffer:29
msgid "Updated value for the buffer"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:1
msgid ""
"Common code to get the 1 dimensional pad option Parameters ---------- "
"padding : Union[int, Tuple[int, ...]]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:5
#: tvm.relay.op.nn.utils.get_pad_tuple2d:5
#: tvm.relay.op.nn.utils.get_pad_tuple3d:5
msgid "Padding size"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:8
#: tvm.relay.op.nn.utils.get_pad_tuple2d:10
#: tvm.relay.op.nn.utils.get_pad_tuple3d:12
msgid "pad_left"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:9
#: tvm.relay.op.nn.utils.get_pad_tuple2d:11
#: tvm.relay.op.nn.utils.get_pad_tuple3d:13
msgid "Padding size on left"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:10
#: tvm.relay.op.nn.utils.get_pad_tuple2d:14
#: tvm.relay.op.nn.utils.get_pad_tuple3d:18
msgid "pad_right"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple1d:11
#: tvm.relay.op.nn.utils.get_pad_tuple2d:15
#: tvm.relay.op.nn.utils.get_pad_tuple3d:19
msgid "Padding size on right."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:1
#: tvm.relay.op.nn.utils.get_pad_tuple3d:1
msgid ""
"Common code to get the pad option Parameters ---------- padding : "
"Union[int, Tuple[int, ...]]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:8
#: tvm.relay.op.nn.utils.get_pad_tuple3d:10
msgid "pad_top"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:9
#: tvm.relay.op.nn.utils.get_pad_tuple3d:11
msgid "Padding size on top"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:12
#: tvm.relay.op.nn.utils.get_pad_tuple3d:16
msgid "pad_down"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple2d:13
#: tvm.relay.op.nn.utils.get_pad_tuple3d:17
msgid "Padding size on down."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:8
msgid "pad_front"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:9
msgid "Padding size on front"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:14
msgid "pad_back"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.utils.get_pad_tuple3d:15
msgid "Padding size on back"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_avg_pool1d:6
#: tvm.relay.op.nn.nn.global_max_pool1d:6
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, in_channels, width)`, to produce an output Tensor "
"with the following rule:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_avg_pool1d:10
msgid "with data of shape (b, c, w)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_avg_pool1d:12
msgid ""
"\\mbox{out}(b, c, 1)  = \\frac{1}{w} \\sum_{n=0}^{w-1} \\mbox{data}(b, c,"
" n)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_avg_pool2d:11
#: tvm.relay.op.nn.nn.global_max_pool2d:11
msgid "with data of shape (b, c, h, w)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_avg_pool2d:13
msgid ""
"\\mbox{out}(b, c, 1, 1)  = \\frac{1}{h * w} \\sum_{m=0}^{h-1} "
"\\sum_{n=0}^{w-1}\n"
"     \\mbox{data}(b, c, m, n)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation "
"across each window represented by DxWxH."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_avg_pool3d:6
#: tvm.relay.op.nn.nn.global_max_pool3d:6
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, in_channels, depth, height, width)`, to produce an "
"output Tensor with the following rule:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_avg_pool3d:10
msgid "with data of shape (b, c, d, h, w)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_avg_pool3d:12
msgid ""
"\\mbox{out}(b, c, 1, 1, 1)  = \\frac{1}{d * h * w} \\sum_{l=0}^{d-1}  "
"\\sum_{m=0}^{h-1}\n"
"     \\sum_{n=0}^{w-1} \\mbox{data}(b, c, l, m, n)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_max_pool1d:10
msgid "with data of shape (b, c, w) .. math::"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_max_pool2d:13
msgid ""
"\\mbox{out}(b, c, 1, 1)  = \\max_{m=0, \\ldots, h} \\max_{n=0, \\ldots, "
"w}\n"
"     \\mbox{data}(b, c, m, n)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.global_max_pool3d:10
msgid "with data of shape (b, c, d, h, w) .. math::"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:1
msgid ""
"Group normalization normalizes over group of channels for each training "
"examples. We can say that, Group Norm is in between Instance Norm and "
"Layer Norm. When we put all the channels into a single group, group "
"normalization becomes Layer normalization. And, when we put each channel "
"into different groups it becomes Instance normalization"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:6
msgid "https://arxiv.org/pdf/1803.08494.pdf"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:8
msgid ""
"Applies group normalization to the n-dimensional input array by "
"seperating the input channels into 'num_groups' groups, each containing "
"'num_channels / num_groups' channels. The mean and standard-deviation are"
" calculated separately over the each group. gamma and beta are learnable "
"per-channel affine transform parameter vectors of size num_channels."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:13
#: tvm.relay.op.nn.nn.layer_norm:6
msgid ""
"out = \\frac{data - mean(data, axis)}{\\sqrt{var(data, axis)+\\epsilon}}\n"
"    * gamma + beta"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:18
msgid ""
"Unlike batch normalization, the mean and var are computed along a group "
"of channels."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:20
msgid ""
"If the input has size k on axis 1, then both gamma and beta have shape "
"(k,)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:29
msgid "Input to which group_norm will be applied."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:38
msgid "num_groups"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:38
msgid "The number of groups to separate the channels into."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:41
msgid "The axis of the channels."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:51
#: tvm.relay.op.nn.nn.instance_norm:47 tvm.relay.op.nn.nn.layer_norm:41
msgid "If True, multiply by gamma. If False, gamma is not used."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.group_norm:56
#: tvm.relay.op.nn.nn.instance_norm:52 tvm.relay.op.nn.nn.layer_norm:46
msgid "The normalized data."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.instance_norm:4
msgid ""
"out = \\frac{data - mean(data)}{\\sqrt{var(data)+\\epsilon}}\n"
"    * gamma + beta"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.instance_norm:9
msgid ""
"The instance normalization is similar to batch normalization, but unlike "
"batch normalization, the mean and var are calculated per-dimension "
"separately for each object(instance) in a mini-batch, not over a batch. "
"And the same normalization is applied both at test and train time."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.instance_norm:17
msgid ""
"The parameter ``axis`` specifies which axis of the input shape denotes "
"the 'channel'.  The default is 1. Specifying -1 sets the channel axis to "
"be the last item in the input shape."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.instance_norm:28
msgid "Input to which instance_norm will be applied."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.l2_normalize:3
msgid ""
"y(i, j) = x(i, j) / sqrt(max(sum(x^2), eps))\n"
"\n"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.l2_normalize:12
msgid "eps"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.l2_normalize:-1
#: tvm.relay.op.nn.nn.leaky_relu:-1
msgid "float"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.l2_normalize:12
msgid "epsilon value"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.l2_normalize:-1
msgid "list of int, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.l2_normalize:15
msgid "axis over the normalization applied"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.layer_norm:1
msgid ""
"Layer normalization (Lei Ba and et al., 2016). Applies layer "
"normalization to the n-dimensional input array. This operator takes an "
"n-dimensional input array and normalizes the input using the given axis:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.layer_norm:11
msgid ""
"Unlike batch normalization, the mean and var are computed along the "
"channel dimension."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.layer_norm:13
msgid ""
"Assume the input has size k on axis 1, then both gamma and beta have "
"shape (k,)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.layer_norm:22
msgid "Input to which layer_norm will be applied."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.layer_norm:-1
msgid "int, optional, default=-1"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.layer_norm:31
msgid "The axis that should be normalized, typically the axis of the channels."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.leaky_relu:4
msgid "`y = x > 0 ? x : alpha * x`"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.leaky_relu:14
#: tvm.relay.op.nn.nn.lrn:26 tvm.relay.op.nn.nn.prelu:14
msgid "alpha"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.leaky_relu:14
#: tvm.relay.op.nn.nn.prelu:14
msgid "Slope coefficient for the negative half axis."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.log_softmax:3
msgid "\\text{log_softmax}(x)_i = \\log \\frac{exp(x_i)}{\\sum_j exp(x_j)}"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.log_softmax:16
msgid "The axis to sum over when computing log softmax"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.lrn:3
msgid ""
"Normalize the input in a local region across or within feature maps. Each"
" input value is divided by (data / (bias + (alpha * sum_data ^2 "
"/size))^beta) where n is the size of each local region, and the sum is "
"taken over the region centered at that value (zero padding is added where"
" necessary)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.lrn:8
msgid ""
"(data / (bias + (alpha * sum_data ^2 /size))^beta)\n"
"\n"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.lrn:17
msgid "size"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.lrn:17
msgid "The size of the local region to be considered for normalization."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.lrn:20
msgid "Input data layout channel axis. Default value is 1 for NCHW format"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.lrn:-1
msgid "float, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.lrn:23
msgid "The offset parameter to avoid dividing by 0."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.lrn:26
msgid "The scaling parameter."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.lrn:29
msgid "The exponent parameter."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.matmul:1
msgid ""
"Matmul operator. Applies a linear transformation. The A & B can be "
"transposed."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.matmul:4
msgid "`C = A * B`"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.matmul:11
msgid ""
"The first input of the operator, of shape `(d_1, d_2, ..., d_n, "
"units_in)` or `(d_1, d_2, ..., units_in, d_n)`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.matmul:15
msgid ""
"The second input expressions, 2-D matrix, of shape `(units_in, units)` or"
" `(units, units_in)`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.matmul:19
msgid "Number of hidden units of the matmul transformation."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.matmul:22
msgid ""
"Specifies the output data type for mixed precision matmul, of shape "
"`(d_1, d_2, ..., d_n, units)`."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.matmul:26
msgid "Whether the data tensor is in transposed format."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.matmul:29
msgid "Whether the weight tensor is in transposed format."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.max_pool1d:3
msgid ""
"This operator takes data as input and does 1D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.max_pool2d:3
msgid ""
"This operator takes data as input and does 2D max value calculation with "
"in pool_size sized window by striding defined by stride"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.max_pool2d:11
msgid "with data of shape (b, c, h, w) and pool_size (kh, kw)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.max_pool2d:13
msgid ""
"\\mbox{out}(b, c, y, x)  = \\max_{m=0, \\ldots, kh-1} \\max_{n=0, "
"\\ldots, kw-1}\n"
"     \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x "
"+ n)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.max_pool2d:18
msgid ""
"Padding is applied to data before the computation. ceil_mode is used to "
"take ceil or floor while computing out shape. This operator accepts data "
"layout specification."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.max_pool2d_grad:3
msgid ""
"This operator takes out_grad and data as input and calculates gradient of"
" max_pool2d."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.max_pool3d:3
msgid ""
"This operator takes data as input and does 3D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.mirror_pad:3
msgid ""
"This operator takes in a tensor and pads each axis by the specified "
"widths using mirroring of the border pixels."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.mirror_pad:9 tvm.relay.op.nn.nn.pad:9
msgid "The input data to the operator"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.mirror_pad:11
msgid "pad_width: tuple of <tuple of <int>>, required"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.mirror_pad:11
#: tvm.relay.op.nn.nn.pad:11
msgid ""
"Number of values padded to the edges of each axis, in the format of "
"((before_1, after_1), ..., (before_N, after_N))"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.mirror_pad:14
msgid "mode: string, optional, default='SYMMETRIC'"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.mirror_pad:14
msgid "What type of mirroring to use, must be SYMMETRIC or REFLECT."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:6
msgid "output{n, i_1, i_2, ..., i_k} = -p * w"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:6
msgid "where t = target{n, i_1, i_2, ..., i_k}"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:5
msgid ""
"p = predictions{n, t, i_1, i_2, i_k} w = weights{n, i_1, i_2, ..., i_k} "
"if t != ignore_index else 0"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:8
msgid "result = reduction(output)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:16
msgid "The target value of each prediction."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:19
msgid "The weight of each target value."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:23
msgid "reduction"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:22
msgid ""
"The reduction method to apply to the output. Possible values are "
"\"mean\", \"sum\" and \"none\"."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:26
msgid "ignore_index"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.nll_loss:26
msgid "The target value to ignore."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.pad:3
msgid ""
"This operator takes in a tensor and pads each axis by the specified "
"widths using the specified value."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.pad:11
msgid "pad_width: tuple of <tuple of <int>>, or tvm.relay.Expr, required"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.pad:13
msgid "pad_value: float, or tvm.relay.Expr, optional, default=0"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.pad:14
msgid "The value used for padding"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.pad:17
msgid "pad_mode: 'constant', 'edge', 'reflect'"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.pad:16
msgid ""
"'constant' pads with constant_value pad_value 'edge' pads using the edge "
"values of the input array 'reflect' pads by reflecting values with "
"respect to the edge"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.prelu:4
msgid "y = x > 0 ? x : alpha * x"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.prelu:17
msgid "Specify which shape axis the channel is specified."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.relu:3
msgid ""
"out = max(x, 0)\n"
"\n"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_batch_nd:15
msgid "paddings"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_batch_nd:14
msgid ""
"2-D of shape [M, 2] where M is number of spatial dims, specifies [before,"
" after] paddings for each spatial dimension."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_batch_nd:18
msgid "pad_value"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_batch_nd:-1
msgid "float, or relay.Expr, optional, default=0"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_batch_nd:18
msgid "The value used for padding."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_batch_nd:23
msgid ""
"N-D Tensor with shape [in_batch * prod(block_shape), padded_data[1] / "
"block_shape[0], ..., padded_data[M] / block_shape[M-1], remaining_shape]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_depth:6
msgid "Input data with spatial dimensions divisible by block_size"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_depth:9
msgid "Size of blocks to decompose into channels."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_depth:17
msgid "Tensor with shape [in_batch, in_channel * block_size * block_size,"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.space_to_depth:18
msgid "in_height / block_size, in_width / block_size]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_add:5
msgid ""
"\\mbox{sparse_add}(dense_mat, sparse_mat)[m, n] = "
"\\mbox{add}(\\mbox{as_dense}(S), (D))[m, n]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_add:9
msgid ""
"where `as_dense` returns dense equivalent of the given S(sparse matrix) "
"while performing addition with given D(dense matrix)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_add:15
#: tvm.relay.op.nn.nn.sparse_dense:29
msgid "dense_mat"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_add:15
msgid "The input dense matrix for the matrix addition"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_add:18
#: tvm.relay.op.nn.nn.sparse_dense:32
msgid "sparse_mat"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_add:-1
#: tvm.relay.op.nn.nn.sparse_dense:-1 tvm.relay.op.nn.nn.sparse_transpose:-1
msgid "Union[namedtuple, Tuple[ndarray, ndarray, ndarray]]."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_add:18
msgid "The input sparse matrix(CSR) for the matrix addition."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_add:26
msgid "Examples"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:9
msgid "\\if sparse_lhs=False:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:6
msgid ""
"\\mbox{sparse_dense}(dense_mat, sparse_mat)[m, n]\n"
"= \\mbox{matmul}(D, \\mbox{as_dense}(S)^T)[m, n]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:15
msgid "\\if sparse_lhs=True:"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:12
msgid ""
"\\mbox{sparse_dense}(dense_mat, sparse_mat)[m, n]\n"
"= \\mbox{matmul}(\\mbox{as_dense}(S), (D)^T)[m, n]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:17
msgid ""
"where `as_dense` returns dense equivalent of the given S(sparse matrix) "
"while performing matmul with given D(dense matrix)."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:20
msgid ""
"See "
"https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
" and "
"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.bsr_matrix.html"
" for more detail on the sparse matrix representation."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:29
msgid "The input dense matrix for the matrix multiplication"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:32
msgid "The input sparse matrix for the matrix multiplication."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:35
msgid "sparse_lhs"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_dense:35
msgid "Indicates whether lhs or rhs matrix is sparse. Default value is False."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_transpose:5
msgid "** Currently only support Square Matrices **"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_transpose:7
msgid "\\mbox{sparse_transpose}(x)[n, n] = (x^T)[n, n]"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_transpose:11
msgid ""
"Please refer to "
"https://github.com/scipy/scipy/blob/v1.3.0/scipy/sparse/csr.py for the "
"algorithm implemented in this operator."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_transpose:17
msgid "x"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_transpose:17
msgid "The sparse weight matrix for the fast matrix transpose."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.sparse_transpose:22
msgid ""
"Tuple of output sparse tensor (same shape and format as input), i.e. if "
"CSR then output is in ([data, indices, indptr]) form"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:3
msgid ""
"This operator takes data as input and does 2D scaling to the given scale "
"factor. In the default case, where the data_layout is `NCHW` with data of"
" shape (n, c, h, w) out will have a shape (n, c, h*scale_h, w*scale_w)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:8
msgid ""
"method indicates the algorithm to be used while calculating the out value"
" and method can be one of (\"bilinear\", \"nearest_neighbor\", "
"\"bicubic\")"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:17
#: tvm.relay.op.nn.nn.upsampling3d:20
msgid "scale_h"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:-1
msgid "tvm.relay.Expr or int or float"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:17
#: tvm.relay.op.nn.nn.upsampling3d:20
msgid "The scale factor for height upsampling."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:20
#: tvm.relay.op.nn.nn.upsampling3d:23
msgid "scale_w"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:20
#: tvm.relay.op.nn.nn.upsampling3d:23
msgid "The scale factor for width upsampling."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:26
#: tvm.relay.op.nn.nn.upsampling3d:29
msgid "method"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:26
msgid "Scale method to used [nearest_neighbor, bilinear, bicubic]."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:29
msgid "align_corners"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling:29
msgid "Whether to keep corners in proper place."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling3d:3
msgid ""
"This operator takes data as input and does 3D scaling to the given scale "
"factor. In the default case, where the data_layout is `NCDHW` with data "
"of shape (n, c, d, h, w) out will have a shape (n, c, d*scale_d, "
"h*scale_h, w*scale_w)"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling3d:8
msgid ""
"method indicates the algorithm to be used while calculating the out value"
" and method can be one of (\"trilinear\", \"nearest_neighbor\")"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling3d:17
msgid "scale_d"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling3d:17
msgid "The scale factor for depth upsampling."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling3d:29
msgid "Scale method to used [nearest_neighbor, trilinear]."
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling3d:35
msgid "coordinate_transformation_mode: string, optional"
msgstr ""

#: ../../docstring of tvm.relay.op.nn.nn.upsampling3d:32
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. Refer to the ONNX Resize operator "
"specification for details. Available options are \"half_pixel\", "
"\"align_corners\" and \"asymmetric\"."
msgstr ""

#~ msgid ":py:obj:`Constant <tvm.relay.nn.Constant>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Expr <tvm.relay.nn.Expr>`\\"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_avg_pool1d "
#~ "<tvm.relay.nn.adaptive_avg_pool1d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_avg_pool2d "
#~ "<tvm.relay.nn.adaptive_avg_pool2d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_avg_pool3d "
#~ "<tvm.relay.nn.adaptive_avg_pool3d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_max_pool1d "
#~ "<tvm.relay.nn.adaptive_max_pool1d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_max_pool2d "
#~ "<tvm.relay.nn.adaptive_max_pool2d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_max_pool3d "
#~ "<tvm.relay.nn.adaptive_max_pool3d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`avg_pool1d <tvm.relay.nn.avg_pool1d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`avg_pool2d <tvm.relay.nn.avg_pool2d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`avg_pool2d_grad <tvm.relay.nn.avg_pool2d_grad>`\\ "
#~ "\\(out\\_grad\\, data\\[\\, pool\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`avg_pool3d <tvm.relay.nn.avg_pool3d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`batch_flatten <tvm.relay.nn.batch_flatten>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_matmul <tvm.relay.nn.batch_matmul>`\\ "
#~ "\\(tensor\\_a\\, tensor\\_b\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_norm <tvm.relay.nn.batch_norm>`\\ "
#~ "\\(data\\, gamma\\, beta\\, moving\\_mean\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_to_space_nd <tvm.relay.nn.batch_to_space_nd>`\\"
#~ " \\(data\\, block\\_shape\\, crops\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bias_add <tvm.relay.nn.bias_add>`\\ \\(data\\,"
#~ " bias\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitpack <tvm.relay.nn.bitpack>`\\ \\(data\\[\\,"
#~ " bits\\, pack\\_axis\\, bit\\_axis\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitserial_conv2d <tvm.relay.nn.bitserial_conv2d>`\\ "
#~ "\\(data\\, weight\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitserial_dense <tvm.relay.nn.bitserial_dense>`\\ "
#~ "\\(data\\, weight\\[\\, units\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`const <tvm.relay.nn.const>`\\ \\(value\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_gemm_weight_transform "
#~ "<tvm.relay.nn.contrib_conv2d_gemm_weight_transform>`\\ \\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_gemm_without_weight_transform "
#~ "<tvm.relay.nn.contrib_conv2d_gemm_without_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_nchwc "
#~ "<tvm.relay.nn.contrib_conv2d_nchwc>`\\ \\(data\\, "
#~ "kernel\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_winograd_nnpack_weight_transform "
#~ "<tvm.relay.nn.contrib_conv2d_winograd_nnpack_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_winograd_weight_transform "
#~ "<tvm.relay.nn.contrib_conv2d_winograd_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_winograd_without_weight_transform "
#~ "<tvm.relay.nn.contrib_conv2d_winograd_without_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv3d_winograd_weight_transform "
#~ "<tvm.relay.nn.contrib_conv3d_winograd_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv3d_winograd_without_weight_transform "
#~ "<tvm.relay.nn.contrib_conv3d_winograd_without_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_dense_pack <tvm.relay.nn.contrib_dense_pack>`\\"
#~ " \\(data\\, weight\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_depthwise_conv2d_nchwc "
#~ "<tvm.relay.nn.contrib_depthwise_conv2d_nchwc>`\\ \\(data\\, "
#~ "kernel\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv1d <tvm.relay.nn.conv1d>`\\ \\(data\\, "
#~ "weight\\[\\, strides\\, padding\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv1d_transpose <tvm.relay.nn.conv1d_transpose>`\\ "
#~ "\\(data\\, weight\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d <tvm.relay.nn.conv2d>`\\ \\(data\\, "
#~ "weight\\[\\, strides\\, padding\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_transpose <tvm.relay.nn.conv2d_transpose>`\\ "
#~ "\\(data\\, weight\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d <tvm.relay.nn.conv3d>`\\ \\(data\\, "
#~ "weight\\[\\, strides\\, padding\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d_transpose <tvm.relay.nn.conv3d_transpose>`\\ "
#~ "\\(data\\, weight\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`correlation <tvm.relay.nn.correlation>`\\ "
#~ "\\(data1\\, data2\\, kernel\\_size\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cross_entropy <tvm.relay.nn.cross_entropy>`\\ "
#~ "\\(predictions\\, targets\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cross_entropy_with_logits "
#~ "<tvm.relay.nn.cross_entropy_with_logits>`\\ \\(predictions\\,"
#~ " targets\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`deformable_conv2d <tvm.relay.nn.deformable_conv2d>`\\"
#~ " \\(data\\, offset\\, weight\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dense <tvm.relay.nn.dense>`\\ \\(data\\, "
#~ "weight\\[\\, units\\, out\\_dtype\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`depth_to_space <tvm.relay.nn.depth_to_space>`\\ "
#~ "\\(data\\, block\\_size\\[\\, layout\\, mode\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dilate <tvm.relay.nn.dilate>`\\ \\(data\\, "
#~ "strides\\[\\, dilation\\_value\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`dropout <tvm.relay.nn.dropout>`\\ \\(data\\[\\, rate\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dropout_raw <tvm.relay.nn.dropout_raw>`\\ "
#~ "\\(data\\[\\, rate\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`fast_softmax <tvm.relay.nn.fast_softmax>`\\ "
#~ "\\(data\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`fifo_buffer <tvm.relay.nn.fifo_buffer>`\\ "
#~ "\\(data\\, buffer\\, axis\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple1d <tvm.relay.nn.get_pad_tuple1d>`\\ "
#~ "\\(padding\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple2d <tvm.relay.nn.get_pad_tuple2d>`\\ "
#~ "\\(padding\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple3d <tvm.relay.nn.get_pad_tuple3d>`\\ "
#~ "\\(padding\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_avg_pool1d <tvm.relay.nn.global_avg_pool1d>`\\"
#~ " \\(data\\[\\, layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_avg_pool2d <tvm.relay.nn.global_avg_pool2d>`\\"
#~ " \\(data\\[\\, layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_avg_pool3d <tvm.relay.nn.global_avg_pool3d>`\\"
#~ " \\(data\\[\\, layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_max_pool1d <tvm.relay.nn.global_max_pool1d>`\\"
#~ " \\(data\\[\\, layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_max_pool2d <tvm.relay.nn.global_max_pool2d>`\\"
#~ " \\(data\\[\\, layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_max_pool3d <tvm.relay.nn.global_max_pool3d>`\\"
#~ " \\(data\\[\\, layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`group_norm <tvm.relay.nn.group_norm>`\\ "
#~ "\\(data\\, gamma\\, beta\\, num\\_groups\\[\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`instance_norm <tvm.relay.nn.instance_norm>`\\ "
#~ "\\(data\\, gamma\\, beta\\[\\, axis\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`l2_normalize <tvm.relay.nn.l2_normalize>`\\ "
#~ "\\(data\\, eps\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`layer_norm <tvm.relay.nn.layer_norm>`\\ "
#~ "\\(data\\, gamma\\, beta\\[\\, axis\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`leaky_relu <tvm.relay.nn.leaky_relu>`\\ "
#~ "\\(data\\[\\, alpha\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`log_softmax <tvm.relay.nn.log_softmax>`\\ "
#~ "\\(data\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`lrn <tvm.relay.nn.lrn>`\\ \\(data\\[\\, "
#~ "size\\, axis\\, bias\\, alpha\\, beta\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`matmul <tvm.relay.nn.matmul>`\\ \\(tensor\\_a\\,"
#~ " tensor\\_b\\[\\, units\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max_pool1d <tvm.relay.nn.max_pool1d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max_pool2d <tvm.relay.nn.max_pool2d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max_pool2d_grad <tvm.relay.nn.max_pool2d_grad>`\\ "
#~ "\\(out\\_grad\\, data\\[\\, pool\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max_pool3d <tvm.relay.nn.max_pool3d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`mirror_pad <tvm.relay.nn.mirror_pad>`\\ "
#~ "\\(data\\, pad\\_width\\[\\, mode\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`nll_loss <tvm.relay.nn.nll_loss>`\\ "
#~ "\\(predictions\\, targets\\, weights\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pad <tvm.relay.nn.pad>`\\ \\(data\\, "
#~ "pad\\_width\\[\\, pad\\_value\\, pad\\_mode\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`prelu <tvm.relay.nn.prelu>`\\ \\(data\\, "
#~ "alpha\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`relu <tvm.relay.nn.relu>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid ":py:obj:`softmax <tvm.relay.nn.softmax>`\\ \\(data\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`space_to_batch_nd <tvm.relay.nn.space_to_batch_nd>`\\"
#~ " \\(data\\, block\\_shape\\, paddings\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`space_to_depth <tvm.relay.nn.space_to_depth>`\\ "
#~ "\\(data\\, block\\_size\\[\\, layout\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_add <tvm.relay.nn.sparse_add>`\\ "
#~ "\\(dense\\_mat\\, sparse\\_mat\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_dense <tvm.relay.nn.sparse_dense>`\\ "
#~ "\\(dense\\_mat\\, sparse\\_mat\\[\\, sparse\\_lhs\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`sparse_transpose <tvm.relay.nn.sparse_transpose>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`upsampling <tvm.relay.nn.upsampling>`\\ "
#~ "\\(data\\[\\, scale\\_h\\, scale\\_w\\, layout\\,"
#~ " ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`upsampling3d <tvm.relay.nn.upsampling3d>`\\ "
#~ "\\(data\\[\\, scale\\_d\\, scale\\_h\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`checked_type <tvm.relay.nn.Expr.checked_type>`\\"
#~ msgstr ""

#~ msgid "Neural network related operators."
#~ msgstr ""

#~ msgid "**Classes:**"
#~ msgstr ""

#~ msgid ":py:obj:`Constant <tvm.relay.op.nn.Constant>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "A constant expression in Relay."
#~ msgstr ""

#~ msgid ":py:obj:`Expr <tvm.relay.op.nn.Expr>`\\"
#~ msgstr ""

#~ msgid "alias of :py:class:`tvm.ir.expr.RelayExpr`"
#~ msgstr ""

#~ msgid "**Functions:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_avg_pool1d "
#~ "<tvm.relay.op.nn.adaptive_avg_pool1d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "1D adaptive average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_avg_pool2d "
#~ "<tvm.relay.op.nn.adaptive_avg_pool2d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "2D adaptive average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_avg_pool3d "
#~ "<tvm.relay.op.nn.adaptive_avg_pool3d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "3D adaptive avg pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_max_pool1d "
#~ "<tvm.relay.op.nn.adaptive_max_pool1d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "1D adaptive max pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_max_pool2d "
#~ "<tvm.relay.op.nn.adaptive_max_pool2d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "2D adaptive max pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`adaptive_max_pool3d "
#~ "<tvm.relay.op.nn.adaptive_max_pool3d>`\\ \\(data\\[\\, "
#~ "output\\_size\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "3D adaptive max pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`avg_pool1d <tvm.relay.op.nn.avg_pool1d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "1D average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`avg_pool2d <tvm.relay.op.nn.avg_pool2d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "2D average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`avg_pool2d_grad <tvm.relay.op.nn.avg_pool2d_grad>`\\"
#~ " \\(out\\_grad\\, data\\[\\, pool\\_size\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Gradient of 2D average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`avg_pool3d <tvm.relay.op.nn.avg_pool3d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "3D average pooling operator."
#~ msgstr ""

#~ msgid ":py:obj:`batch_flatten <tvm.relay.op.nn.batch_flatten>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "BatchFlatten."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_matmul <tvm.relay.op.nn.batch_matmul>`\\ "
#~ "\\(tensor\\_a\\, tensor\\_b\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Compute batch matrix multiplication of `tensor_a` and `tensor_b`."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_norm <tvm.relay.op.nn.batch_norm>`\\ "
#~ "\\(data\\, gamma\\, beta\\, moving\\_mean\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Batch normalization layer (Ioffe and Szegedy, 2014)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`batch_to_space_nd "
#~ "<tvm.relay.op.nn.batch_to_space_nd>`\\ \\(data\\, "
#~ "block\\_shape\\, crops\\)"
#~ msgstr ""

#~ msgid "Reshape the batch dimension into spatial dimensions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bias_add <tvm.relay.op.nn.bias_add>`\\ \\(data\\,"
#~ " bias\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "add_bias operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitpack <tvm.relay.op.nn.bitpack>`\\ "
#~ "\\(data\\[\\, bits\\, pack\\_axis\\, bit\\_axis\\,"
#~ " ...\\]\\)"
#~ msgstr ""

#~ msgid "Tensor packing for bitserial operations."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitserial_conv2d <tvm.relay.op.nn.bitserial_conv2d>`\\"
#~ " \\(data\\, weight\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "2D convolution using bitserial computation."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bitserial_dense <tvm.relay.op.nn.bitserial_dense>`\\"
#~ " \\(data\\, weight\\[\\, units\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Bitserial Dense operator."
#~ msgstr ""

#~ msgid ":py:obj:`const <tvm.relay.op.nn.const>`\\ \\(value\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Create a constant value."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_gemm_weight_transform "
#~ "<tvm.relay.op.nn.contrib_conv2d_gemm_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid "Weight Transformation part for 2D convolution with gemm algorithm."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_gemm_without_weight_transform "
#~ "<tvm.relay.op.nn.contrib_conv2d_gemm_without_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid "2D convolution with gemm algorithm."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_nchwc "
#~ "<tvm.relay.op.nn.contrib_conv2d_nchwc>`\\ \\(data\\, "
#~ "kernel\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Variant of 2D convolution."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_winograd_nnpack_weight_transform "
#~ "<tvm.relay.op.nn.contrib_conv2d_winograd_nnpack_weight_transform>`\\"
#~ " \\(...\\)"
#~ msgstr ""

#~ msgid "Weight Transformation part for 2D convolution with winograd algorithm."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_winograd_weight_transform "
#~ "<tvm.relay.op.nn.contrib_conv2d_winograd_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv2d_winograd_without_weight_transform "
#~ "<tvm.relay.op.nn.contrib_conv2d_winograd_without_weight_transform>`\\"
#~ " \\(...\\)"
#~ msgstr ""

#~ msgid "2D convolution with winograd algorithm."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv3d_winograd_weight_transform "
#~ "<tvm.relay.op.nn.contrib_conv3d_winograd_weight_transform>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid "Weight Transformation part for 3D convolution with winograd algorithm."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_conv3d_winograd_without_weight_transform "
#~ "<tvm.relay.op.nn.contrib_conv3d_winograd_without_weight_transform>`\\"
#~ " \\(...\\)"
#~ msgstr ""

#~ msgid "3D convolution with winograd algorithm."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_dense_pack "
#~ "<tvm.relay.op.nn.contrib_dense_pack>`\\ \\(data\\, "
#~ "weight\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Dense operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`contrib_depthwise_conv2d_nchwc "
#~ "<tvm.relay.op.nn.contrib_depthwise_conv2d_nchwc>`\\ \\(data\\,"
#~ " kernel\\)"
#~ msgstr ""

#~ msgid "Variant of 2D depthwise convolution."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv1d <tvm.relay.op.nn.conv1d>`\\ \\(data\\, "
#~ "weight\\[\\, strides\\, padding\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "1D convolution."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv1d_transpose <tvm.relay.op.nn.conv1d_transpose>`\\"
#~ " \\(data\\, weight\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "One dimensional transposed convolution operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d <tvm.relay.op.nn.conv2d>`\\ \\(data\\, "
#~ "weight\\[\\, strides\\, padding\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "2D convolution."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_backward_weight "
#~ "<tvm.relay.op.nn.conv2d_backward_weight>`\\ \\(grad\\, "
#~ "data\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "The gradient of conv2d with respect to weight."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_transpose <tvm.relay.op.nn.conv2d_transpose>`\\"
#~ " \\(data\\, weight\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Two dimensional transposed convolution operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d <tvm.relay.op.nn.conv3d>`\\ \\(data\\, "
#~ "weight\\[\\, strides\\, padding\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "3D convolution."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv3d_transpose <tvm.relay.op.nn.conv3d_transpose>`\\"
#~ " \\(data\\, weight\\[\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "3D transpose convolution."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`correlation <tvm.relay.op.nn.correlation>`\\ "
#~ "\\(data1\\, data2\\, kernel\\_size\\, ...\\)"
#~ msgstr ""

#~ msgid "Applies correlation to inputs."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cross_entropy <tvm.relay.op.nn.cross_entropy>`\\ "
#~ "\\(predictions\\, targets\\)"
#~ msgstr ""

#~ msgid "CrossEntropy without logits."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cross_entropy_with_logits "
#~ "<tvm.relay.op.nn.cross_entropy_with_logits>`\\ \\(predictions\\,"
#~ " targets\\)"
#~ msgstr ""

#~ msgid "CrossEntropy with logits."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`deformable_conv2d "
#~ "<tvm.relay.op.nn.deformable_conv2d>`\\ \\(data\\, "
#~ "offset\\, weight\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Deformable 2d convolution."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dense <tvm.relay.op.nn.dense>`\\ \\(data\\, "
#~ "weight\\[\\, units\\, out\\_dtype\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`depth_to_space <tvm.relay.op.nn.depth_to_space>`\\ "
#~ "\\(data\\, block\\_size\\[\\, layout\\, mode\\]\\)"
#~ msgstr ""

#~ msgid "Convert channels into spatial blocks."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dilate <tvm.relay.op.nn.dilate>`\\ \\(data\\, "
#~ "strides\\[\\, dilation\\_value\\]\\)"
#~ msgstr ""

#~ msgid "Dilate data with given dilation value (0 by default)."
#~ msgstr ""

#~ msgid ":py:obj:`dropout <tvm.relay.op.nn.dropout>`\\ \\(data\\[\\, rate\\]\\)"
#~ msgstr ""

#~ msgid "Applies the dropout operation to the input array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dropout_raw <tvm.relay.op.nn.dropout_raw>`\\ "
#~ "\\(data\\[\\, rate\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`fast_softmax <tvm.relay.op.nn.fast_softmax>`\\ "
#~ "\\(data\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Computes softmax."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`fifo_buffer <tvm.relay.op.nn.fifo_buffer>`\\ "
#~ "\\(data\\, buffer\\, axis\\)"
#~ msgstr ""

#~ msgid ""
#~ "FIFO buffer to enable computation reuse"
#~ " in CNNs with sliding indow input"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple1d <tvm.relay.op.nn.get_pad_tuple1d>`\\"
#~ " \\(padding\\)"
#~ msgstr ""

#~ msgid ""
#~ "Common code to get the 1 "
#~ "dimensional pad option :param padding: "
#~ "Padding size :type padding: Union[int, "
#~ "Tuple[int, ...]]"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple2d <tvm.relay.op.nn.get_pad_tuple2d>`\\"
#~ " \\(padding\\)"
#~ msgstr ""

#~ msgid ""
#~ "Common code to get the pad option"
#~ " :param padding: Padding size :type "
#~ "padding: Union[int, Tuple[int, ...]]"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_pad_tuple3d <tvm.relay.op.nn.get_pad_tuple3d>`\\"
#~ " \\(padding\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_avg_pool1d "
#~ "<tvm.relay.op.nn.global_avg_pool1d>`\\ \\(data\\[\\, "
#~ "layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid "1D global average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_avg_pool2d "
#~ "<tvm.relay.op.nn.global_avg_pool2d>`\\ \\(data\\[\\, "
#~ "layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid "2D global average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_avg_pool3d "
#~ "<tvm.relay.op.nn.global_avg_pool3d>`\\ \\(data\\[\\, "
#~ "layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid "3D global average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_max_pool1d "
#~ "<tvm.relay.op.nn.global_max_pool1d>`\\ \\(data\\[\\, "
#~ "layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid "1D global maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_max_pool2d "
#~ "<tvm.relay.op.nn.global_max_pool2d>`\\ \\(data\\[\\, "
#~ "layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid "2D global maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`global_max_pool3d "
#~ "<tvm.relay.op.nn.global_max_pool3d>`\\ \\(data\\[\\, "
#~ "layout\\, out\\_layout\\]\\)"
#~ msgstr ""

#~ msgid "3D global maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`group_norm <tvm.relay.op.nn.group_norm>`\\ "
#~ "\\(data\\, gamma\\, beta\\, num\\_groups\\[\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Group normalization normalizes over group "
#~ "of channels for each training examples."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`instance_norm <tvm.relay.op.nn.instance_norm>`\\ "
#~ "\\(data\\, gamma\\, beta\\[\\, axis\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Instance Normalization (Ulyanov and et "
#~ "al., 2016) Applies instance normalization "
#~ "to the n-dimensional input array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`l2_normalize <tvm.relay.op.nn.l2_normalize>`\\ "
#~ "\\(data\\, eps\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Perform L2 normalization on the input data"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`layer_norm <tvm.relay.op.nn.layer_norm>`\\ "
#~ "\\(data\\, gamma\\, beta\\[\\, axis\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Layer normalization (Lei Ba and et al., 2016)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`leaky_relu <tvm.relay.op.nn.leaky_relu>`\\ "
#~ "\\(data\\[\\, alpha\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does Leaky version of a "
#~ "Rectified Linear Unit."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`log_softmax <tvm.relay.op.nn.log_softmax>`\\ "
#~ "\\(data\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Computes log softmax."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`lrn <tvm.relay.op.nn.lrn>`\\ \\(data\\[\\, "
#~ "size\\, axis\\, bias\\, alpha\\, beta\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does local response normalization."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`matmul <tvm.relay.op.nn.matmul>`\\ "
#~ "\\(tensor\\_a\\, tensor\\_b\\[\\, units\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Matmul operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max_pool1d <tvm.relay.op.nn.max_pool1d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "1D maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max_pool2d <tvm.relay.op.nn.max_pool2d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "2D maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max_pool2d_grad <tvm.relay.op.nn.max_pool2d_grad>`\\"
#~ " \\(out\\_grad\\, data\\[\\, pool\\_size\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Gradient of 2D maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max_pool3d <tvm.relay.op.nn.max_pool3d>`\\ "
#~ "\\(data\\[\\, pool\\_size\\, strides\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "3D maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`mirror_pad <tvm.relay.op.nn.mirror_pad>`\\ "
#~ "\\(data\\, pad\\_width\\[\\, mode\\]\\)"
#~ msgstr ""

#~ msgid "MirrorPadding"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`nll_loss <tvm.relay.op.nn.nll_loss>`\\ "
#~ "\\(predictions\\, targets\\, weights\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Negative log likelihood loss."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pad <tvm.relay.op.nn.pad>`\\ \\(data\\, "
#~ "pad\\_width\\[\\, pad\\_value\\, pad\\_mode\\]\\)"
#~ msgstr ""

#~ msgid "Padding"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`prelu <tvm.relay.op.nn.prelu>`\\ \\(data\\, "
#~ "alpha\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`relu <tvm.relay.op.nn.relu>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Rectified linear unit."
#~ msgstr ""

#~ msgid ":py:obj:`softmax <tvm.relay.op.nn.softmax>`\\ \\(data\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`space_to_batch_nd "
#~ "<tvm.relay.op.nn.space_to_batch_nd>`\\ \\(data\\, "
#~ "block\\_shape\\, paddings\\)"
#~ msgstr ""

#~ msgid ""
#~ "Divide spatial dimensions of the data"
#~ " into a grid of blocks and "
#~ "interleave them into batch dim."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`space_to_depth <tvm.relay.op.nn.space_to_depth>`\\ "
#~ "\\(data\\, block\\_size\\[\\, layout\\]\\)"
#~ msgstr ""

#~ msgid "Convert spatial blocks into channels."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_add <tvm.relay.op.nn.sparse_add>`\\ "
#~ "\\(dense\\_mat\\, sparse\\_mat\\)"
#~ msgstr ""

#~ msgid ""
#~ "Computes the matrix addition of "
#~ "`dense_mat` and `sparse_mat`, where "
#~ "`dense_mat` is a dense matrix and "
#~ "`sparse_mat` is a sparse (CSR) "
#~ "namedtuple with fields `data`, `indices`, "
#~ "and `indptr`."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_dense <tvm.relay.op.nn.sparse_dense>`\\ "
#~ "\\(dense\\_mat\\, sparse\\_mat\\[\\, sparse\\_lhs\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Computes the matrix multiplication of "
#~ "`dense_mat` and `sparse_mat`, where "
#~ "`dense_mat` is a dense matrix and "
#~ "`sparse_mat` is a sparse (either BSR "
#~ "or CSR) namedtuple with fields `data`,"
#~ " `indices`, and `indptr`."
#~ msgstr ""

#~ msgid ":py:obj:`sparse_transpose <tvm.relay.op.nn.sparse_transpose>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid ""
#~ "Computes the fast matrix transpose of"
#~ " x, where x is a sparse tensor"
#~ " in CSR format (represented as a "
#~ "namedtuple with fields `data`, `indices`, "
#~ "and `indptr`)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`upsampling <tvm.relay.op.nn.upsampling>`\\ "
#~ "\\(data\\[\\, scale\\_h\\, scale\\_w\\, layout\\,"
#~ " ...\\]\\)"
#~ msgstr ""

#~ msgid "Upsampling."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`upsampling3d <tvm.relay.op.nn.upsampling3d>`\\ "
#~ "\\(data\\[\\, scale\\_d\\, scale\\_h\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "3D Upsampling."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "The data content of the constant expression."
#~ msgstr ""

#~ msgid ":py:obj:`checked_type <tvm.relay.op.nn.Expr.checked_type>`\\"
#~ msgstr ""

#~ msgid "Get the checked type of tvm.relay.Expr."
#~ msgstr ""

#~ msgid "1D adaptive average pooling operator. This operator is experimental."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 1D average value calculation"
#~ " across each window represented by W."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCW` a data Tensor "
#~ "with shape `(batch_size, in_channels, width)`,"
#~ " to produce an output Tensor with "
#~ "shape (batch_size, in_channels, output_width)."
#~ msgstr ""

#~ msgid ""
#~ "The pooling kernel and stride sizes "
#~ "are automatically chosen for desired "
#~ "output sizes."
#~ msgstr ""

#~ msgid "For output_size:"
#~ msgstr ""

#~ msgid ""
#~ "If this argument is not provided, "
#~ "input height and width will be "
#~ "used as output width."
#~ msgstr ""

#~ msgid ""
#~ "If a single integer is provided "
#~ "for output_size, the output size is "
#~ "(N x C x output_size) for any "
#~ "input (NCW)."
#~ msgstr ""

#~ msgid "The input data to the operator."
#~ msgstr ""

#~ msgid "Output height and width."
#~ msgstr ""

#~ msgid "Layout of the input."
#~ msgstr ""

#~ msgid "Layout of the output."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "**result** -- The computed result."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "2D adaptive average pooling operator. This operator is experimental."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 2D average value calculation"
#~ " across each window represented by "
#~ "WxH."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCHW` a data Tensor "
#~ "with shape `(batch_size, in_channels, height,"
#~ " width)`, to produce an output Tensor"
#~ " with shape (batch_size, in_channels, "
#~ "output_height, output_width)."
#~ msgstr ""

#~ msgid ""
#~ "If this argument is not provided, "
#~ "input height and width will be "
#~ "used as output height and width."
#~ msgstr ""

#~ msgid ""
#~ "If a single integer is provided "
#~ "for output_size, the output size is "
#~ "(N x C x output_size x "
#~ "output_size) for any input (NCHW)."
#~ msgstr ""

#~ msgid ""
#~ "If a tuple of integers (height, "
#~ "width) are provided for output_size, the"
#~ " output size is (N x C x "
#~ "height x width) for any input "
#~ "(NCHW)."
#~ msgstr ""

#~ msgid "3D adaptive avg pooling operator. This operator is experimental."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 3D avg value calculation "
#~ "across each window represented by DxWxH."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCDHW` a data Tensor "
#~ "with shape `(batch_size, in_channels, depth,"
#~ " height, width)`, to produce an "
#~ "output Tensor with shape (batch_size, "
#~ "in_channels, output_depth, output_height, "
#~ "output_width)."
#~ msgstr ""

#~ msgid ""
#~ "If this argument is not provided, "
#~ "input depth, height and width will "
#~ "be used as output depth, height "
#~ "and width."
#~ msgstr ""

#~ msgid ""
#~ "If a single integer is provided "
#~ "for output_size, the output size is "
#~ "(N x C x output_size x output_size"
#~ " x output_size) for any input "
#~ "(NCDHW)."
#~ msgstr ""

#~ msgid ""
#~ "If a tuple of integers (depth, "
#~ "height, width) are provided for "
#~ "output_size, the output size is (N "
#~ "x C x depth x height x "
#~ "width) for any input (NCDHW)."
#~ msgstr ""

#~ msgid "1D adaptive max pooling operator. This operator is experimental."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 1D max value calculation "
#~ "across each window represented by W."
#~ msgstr ""

#~ msgid "2D adaptive max pooling operator. This operator is experimental."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 2D max value calculation "
#~ "across each window represented by WxH."
#~ msgstr ""

#~ msgid "3D adaptive max pooling operator. This operator is experimental."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 3D max value calculation "
#~ "across each window represented by DxWxH."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 1D average value calculation"
#~ " with in pool_size sized window by"
#~ " striding defined by stride"
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCW` a data Tensor "
#~ "with shape `(batch_size, channels, width)`,"
#~ " to produce an output Tensor."
#~ msgstr ""

#~ msgid ""
#~ "The ceil_mode is used to take ceil"
#~ " or floor while computing out shape."
#~ " count_include_pad indicates including or "
#~ "excluding padded input values in "
#~ "computation. This operator accepts data "
#~ "layout specification."
#~ msgstr ""

#~ msgid "The size of window for pooling."
#~ msgstr ""

#~ msgid "The strides of pooling."
#~ msgstr ""

#~ msgid "The dilation of pooling."
#~ msgstr ""

#~ msgid "The padding for pooling."
#~ msgstr ""

#~ msgid "Layout of the output"
#~ msgstr ""

#~ msgid "To enable or disable ceil while pooling."
#~ msgstr ""

#~ msgid "To include padding to compute the average."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 2D average value calculation"
#~ " with in pool_size sized window by"
#~ " striding defined by stride"
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCHW` a data Tensor "
#~ "with shape `(batch_size, in_channels, height,"
#~ " width)`, to produce an output Tensor"
#~ " with the following rule:"
#~ msgstr ""

#~ msgid "with data of shape (b, c, h, w), pool_size (kh, kw)"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}(b, c, y, x)  = \\frac{1}{kh"
#~ " * kw} \\sum_{m=0}^{kh-1} \\sum_{n=0}^{kw-1}\n"
#~ ""
#~ "     \\mbox{data}(b, c, \\mbox{stride}[0] *"
#~ " y + m, \\mbox{stride}[1] * x +"
#~ " n)"
#~ msgstr ""

#~ msgid ""
#~ "Padding is applied to data before "
#~ "the computation. ceil_mode is used to"
#~ " take ceil or floor while computing"
#~ " out shape. count_include_pad indicates "
#~ "including or excluding padded input "
#~ "values in computation. This operator "
#~ "accepts data layout specification."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes out_grad and data"
#~ " as input and calculates gradient of"
#~ " avg_pool2d."
#~ msgstr ""

#~ msgid "The output gradient"
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 3D average value calculation"
#~ " with in pool_size sized window by"
#~ " striding defined by stride"
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCDHW` a data Tensor "
#~ "with shape `(batch_size, channels, depth, "
#~ "height, width)`, to produce an output"
#~ " Tensor."
#~ msgstr ""

#~ msgid ""
#~ "This operator flattens all the "
#~ "dimensions except for the batch "
#~ "dimension. which results a 2D output."
#~ msgstr ""

#~ msgid ""
#~ "For data with shape ``(d1, d2, "
#~ "..., dk)`` batch_flatten(data) returns "
#~ "reshaped output of shape ``(d1, "
#~ "d2*...*dk)``."
#~ msgstr ""

#~ msgid "**result** -- The Flattened result."
#~ msgstr ""

#~ msgid ""
#~ "Both `tensor_a` and `tensor_b` can be"
#~ " transposed. For legacy reason, we "
#~ "use NT format (transpose_a=False, "
#~ "transpose_b=True) by default."
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{batch_matmul}(A, B)[i, :, :] = "
#~ "\\mbox{matmul}(A[i, :, :], B[i, :, :])"
#~ msgstr ""

#~ msgid "The first input."
#~ msgstr ""

#~ msgid "The second input."
#~ msgstr ""

#~ msgid "Specifies the output data type for mixed precision batch matmul."
#~ msgstr ""

#~ msgid "Whether the first tensor is in transposed format."
#~ msgstr ""

#~ msgid "Whether the second tensor is in transposed format."
#~ msgstr ""

#~ msgid ""
#~ "Batch normalization layer (Ioffe and "
#~ "Szegedy, 2014). Normalizes the input at"
#~ " each batch, i.e. applies a "
#~ "transformation that maintains the mean "
#~ "activation close to 0 and the "
#~ "activation standard deviation close to "
#~ "1."
#~ msgstr ""

#~ msgid ""
#~ "data\\_mean[i] = mean(data[:,i,:,...]) \\\\\n"
#~ "data\\_var[i] = var(data[:,i,:,...])"
#~ msgstr ""

#~ msgid ""
#~ "Then compute the normalized output, "
#~ "which has the same shape as input,"
#~ " as following:"
#~ msgstr ""

#~ msgid ""
#~ "out[:,i,:,...] = \\frac{data[:,i,:,...] - "
#~ "data\\_mean[i]}{\\sqrt{data\\_var[i]+\\epsilon}}\n"
#~ "    * gamma[i] + beta[i]"
#~ msgstr ""

#~ msgid ""
#~ "Both *mean* and *var* returns a "
#~ "scalar by treating the input as a"
#~ " vector."
#~ msgstr ""

#~ msgid ""
#~ "Assume the input has size *k* on"
#~ " axis 1, then both ``gamma`` and "
#~ "``beta`` have shape *(k,)*."
#~ msgstr ""

#~ msgid ""
#~ "Besides the inputs and the outputs, "
#~ "this operator accepts two auxiliary "
#~ "states, ``moving_mean`` and ``moving_var``, "
#~ "which are *k*-length vectors. They are"
#~ " global statistics for the whole "
#~ "dataset, which are updated by"
#~ msgstr ""

#~ msgid ""
#~ "The parameter ``axis`` specifies which "
#~ "axis of the input shape denotes "
#~ "the 'channel' (separately normalized groups)."
#~ "  The default is 1. Specifying -1 "
#~ "sets the channel axis to be the"
#~ " last item in the input shape."
#~ msgstr ""

#~ msgid "This operator can be optimized away for inference."
#~ msgstr ""

#~ msgid "Input to which batch_norm will be applied."
#~ msgstr ""

#~ msgid "The gamma scale factor."
#~ msgstr ""

#~ msgid "The beta offset factor."
#~ msgstr ""

#~ msgid "Running mean of input,"
#~ msgstr ""

#~ msgid "Running variance of input."
#~ msgstr ""

#~ msgid "Specify along which shape axis the channel is specified."
#~ msgstr ""

#~ msgid "Small float added to variance to avoid dividing by zero."
#~ msgstr ""

#~ msgid ""
#~ "If True, add offset of beta to "
#~ "normalized tensor, If False, beta is "
#~ "ignored."
#~ msgstr ""

#~ msgid ""
#~ "If true, multiply by gamma. If "
#~ "False, gamma is not used. When the"
#~ " next layer is piecewise linear (also"
#~ " e.g. nn.relu), this can be disabled"
#~ " since the scaling will be done "
#~ "by the next layer."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- Tuple of normed data "
#~ "(same shape as input), new running "
#~ "mean (k-length vector), and new running"
#~ " variance (k-length vector)"
#~ msgstr ""

#~ msgid "N-D with shape [batch, spatial_shape, remaining_shape]"
#~ msgstr ""

#~ msgid ""
#~ "1-D of size [M] where M is "
#~ "number of spatial dims, specifies block"
#~ " size for each spatial dimension."
#~ msgstr ""

#~ msgid ""
#~ "2-D of shape [M, 2] where M "
#~ "is number of spatial dims, specifies "
#~ "[begin, end] crop size for each "
#~ "spatial dimension."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- N-D Tensor with shape "
#~ "[batch / prod(block_shape), in_shape[1] * "
#~ "block_shape[0] - crops[0,0] - crops[0,1], "
#~ "..., in_shape[M] * block_shape[M-1] - "
#~ "crops[M-1, 0] - crops[M-1, 1], "
#~ "remaining_shape]"
#~ msgstr ""

#~ msgid ""
#~ "Add 1D bias to the axis of "
#~ "data. This function is a special "
#~ "case of add which allows inference "
#~ "of shape of the bias from data."
#~ msgstr ""

#~ msgid "The bias to be added."
#~ msgstr ""

#~ msgid "The axis to add the bias."
#~ msgstr ""

#~ msgid "**result** -- The final result."
#~ msgstr ""

#~ msgid ""
#~ "The values along the input tensor's "
#~ "pack_axis are quantized and packed "
#~ "together into the specified pack_type in"
#~ " a new bit axis."
#~ msgstr ""

#~ msgid ""
#~ "For example, consider bitpacking with "
#~ "data to be a tensor with shape "
#~ "`[1, 64, 128, 128]`, pack_axis=1, "
#~ "bit_axis=4, pack_type=uint8, and bits=2. The"
#~ " output in this case will be of"
#~ " shape `[1, 8, 128, 128, 2]`. "
#~ "The dimension of axis 1 has been"
#~ " reduced by a factor of 8 since"
#~ " each value is packed into an "
#~ "8-bit uint8. Axis 4 is now two "
#~ "bitplanes representing the quantized value "
#~ "of the incoming data. The output "
#~ "tensor is now ready to be used "
#~ "in a bitserial operation."
#~ msgstr ""

#~ msgid "The incoming tensor to be packed."
#~ msgstr ""

#~ msgid "Number of bits that should be packed."
#~ msgstr ""

#~ msgid "Axis that should be decomposed and packed."
#~ msgstr ""

#~ msgid "New axis containing bitplane."
#~ msgstr ""

#~ msgid "Datatype to pack bits into."
#~ msgstr ""

#~ msgid "Name of the operation."
#~ msgstr ""

#~ msgid "**result** -- The packed tensor."
#~ msgstr ""

#~ msgid "The weight expressions."
#~ msgstr ""

#~ msgid "The strides of convolution."
#~ msgstr ""

#~ msgid "The padding of convolution on both sides of inputs before convolution."
#~ msgstr ""

#~ msgid "Number of output channels of this convolution."
#~ msgstr ""

#~ msgid "The spatial of the convolution kernel."
#~ msgstr ""

#~ msgid "Number of bits to pack for activations."
#~ msgstr ""

#~ msgid "Number of bits to pack for weights."
#~ msgstr ""

#~ msgid "Layout of the kernel"
#~ msgstr ""

#~ msgid "Specifies the output data type for mixed precision conv2d."
#~ msgstr ""

#~ msgid ""
#~ "Bitserial Dense operator. Applies matrix "
#~ "multiplication of two quantized matrices "
#~ "using a fast bitserial algorithm."
#~ msgstr ""

#~ msgid "`Y = X * W`"
#~ msgstr ""

#~ msgid "Number of hidden units of the dense transformation."
#~ msgstr ""

#~ msgid "Number of bits incoming tensor should be packed with."
#~ msgstr ""

#~ msgid "Number of bits weight tensor should be packed with."
#~ msgstr ""

#~ msgid "Datatype to pack individual bits into before computation."
#~ msgstr ""

#~ msgid "Specifies the output data type for mixed precision dense."
#~ msgstr ""

#~ msgid "Whether to use unipolar or bipolar quantization for inputs."
#~ msgstr ""

#~ msgid "The constant value."
#~ msgstr ""

#~ msgid "The data type of the resulting constant."
#~ msgstr ""

#~ msgid "When dtype is None, we use the following rule:"
#~ msgstr ""

#~ msgid "int maps to \"int32\""
#~ msgstr ""

#~ msgid "float maps to \"float32\""
#~ msgstr ""

#~ msgid "bool maps to \"bool\""
#~ msgstr ""

#~ msgid "other using the same default rule as numpy."
#~ msgstr ""

#~ msgid ""
#~ "We separate this as a single op"
#~ " to enable pre-compute for inference."
#~ " Use this together with "
#~ "nn.contrib_conv2d_gemm_without_weight_transform"
#~ msgstr ""

#~ msgid "Tile rows of the weight transformation for ConvGemm."
#~ msgstr ""

#~ msgid "Tile columns of the weight transformation for ConvGemm."
#~ msgstr ""

#~ msgid ""
#~ "The basic parameters are the same "
#~ "as the ones in vanilla conv2d. It"
#~ " assumes the weight is pre-"
#~ "transformed by nn.contrib_conv2d_gemm_weight_transform"
#~ msgstr ""

#~ msgid "Specifies the dilation rate to be used for dilated convolution."
#~ msgstr ""

#~ msgid "Number of groups for grouped convolution."
#~ msgstr ""

#~ msgid "Layout of the weight."
#~ msgstr ""

#~ msgid "Layout of the output, by default, out_layout is the same as data_layout"
#~ msgstr ""

#~ msgid ""
#~ "This operator takes the weight as "
#~ "the convolution kernel and convolves it"
#~ " with data to produce an output, "
#~ "following a specialized NCHWc data "
#~ "layout."
#~ msgstr ""

#~ msgid "The kernel expressions."
#~ msgstr ""

#~ msgid ""
#~ "We separate this as a single op"
#~ " to enable pre-compute for inference."
#~ " Use this together with "
#~ "nn.contrib_conv2d_winograd_without_weight_transform"
#~ msgstr ""

#~ msgid "The Tile size of winograd. E.g. 2 for F(2x2, 3x3) and 4 for F(4x4, 3x3)"
#~ msgstr ""

#~ msgid ""
#~ "The basic parameters are the same "
#~ "as the ones in vanilla conv2d. It"
#~ " assumes the weight is pre-"
#~ "transformed by nn.contrib_conv2d_winograd_weight_transform"
#~ msgstr ""

#~ msgid ""
#~ "We separate this as a single op"
#~ " to enable pre-compute for inference."
#~ " Use this together with "
#~ "nn.contrib_conv3d_winograd_without_weight_transform"
#~ msgstr ""

#~ msgid ""
#~ "The Tile size of winograd. E.g. 2"
#~ " for F(2x2x2, 3x3x3) and 4 for "
#~ "F(4x4x4, 3x3x3)"
#~ msgstr ""

#~ msgid ""
#~ "The basic parameters are the same "
#~ "as the ones in vanilla conv3d. It"
#~ " assumes the weight is pre-"
#~ "transformed by nn.contrib_conv3d_winograd_weight_transform"
#~ msgstr ""

#~ msgid "Dense operator. Applies a linear transformation with packed weight"
#~ msgstr ""

#~ msgid "`Y = X * W^T`"
#~ msgstr ""

#~ msgid "The input data to the operator, of shape `(batch, units_in)`."
#~ msgstr ""

#~ msgid ""
#~ "The transformed weight expressions, 3-D "
#~ "matrix, of shape `(units // "
#~ "pack_weight_tile, units_in, pack_weight_tile)`."
#~ msgstr ""

#~ msgid "The layout of weight, such as \"NC\" or \"NC8n\"."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes the weight as "
#~ "the depthwise convolution kernel and "
#~ "depthwise convolves it with data to "
#~ "produce an output, following a "
#~ "specialized NCHWc data layout."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes the weight as "
#~ "the convolution kernel and convolves it"
#~ " with data to produce an output."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCW` and kernel_layout "
#~ "is `OIW`, conv1d takes in a data"
#~ " Tensor with shape `(batch_size, "
#~ "in_channels, width)`, and a weight "
#~ "Tensor with shape `(channels, in_channels, "
#~ "kernel_size)` to produce an output "
#~ "Tensor with the following rule:"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}[b, c, w] = \\sum_{dw, k}\n"
#~ "   \\mbox{data}[b, k, \\mbox{strides}[0] * w + dw] *\n"
#~ "   \\mbox{weight}[c, k, dw]"
#~ msgstr ""

#~ msgid ""
#~ "Padding and dilation are applied to "
#~ "data and weight respectively before the"
#~ " computation. This operator accepts data"
#~ " layout specification. Semantically, the "
#~ "operator will convert the layout to "
#~ "the canonical layout (`NCW` for data "
#~ "and `OIW` for weight), perform the "
#~ "computation, then convert to the "
#~ "out_layout."
#~ msgstr ""

#~ msgid ""
#~ "The padding of convolution on both "
#~ "sides of the input before convolution."
#~ msgstr ""

#~ msgid "Currently unused for 1D convolution."
#~ msgstr ""

#~ msgid "The spatial dimension of the convolution kernel."
#~ msgstr ""

#~ msgid "The padding of convolution on both sides of inputs."
#~ msgstr ""

#~ msgid "Used to disambiguate the output shape."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCHW` and kernel_layout "
#~ "is `OIHW`, conv2d takes in a data"
#~ " Tensor with shape `(batch_size, "
#~ "in_channels, height, width)`, and a "
#~ "weight Tensor with shape `(channels, "
#~ "in_channels, kernel_size[0], kernel_size[1])` to "
#~ "produce an output Tensor with the "
#~ "following rule:"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}[b, c, y, x] = \\sum_{dy, dx, k}\n"
#~ "   \\mbox{data}[b, k, \\mbox{strides}[0] * "
#~ "y  + dy, \\mbox{strides}[1] * x +"
#~ " dx] *\n"
#~ "   \\mbox{weight}[c, k, dy, dx]"
#~ msgstr ""

#~ msgid ""
#~ "Padding and dilation are applied to "
#~ "data and weight respectively before the"
#~ " computation. This operator accepts data"
#~ " layout specification. Semantically, the "
#~ "operator will convert the layout to "
#~ "the canonical layout (`NCHW` for data"
#~ " and `OIHW` for weight), perform the"
#~ " computation, then convert to the "
#~ "out_layout."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes the output gradient"
#~ " `grad` and convolves it with `data`"
#~ " as the convolution kernel, to "
#~ "produce the gradient with respect to "
#~ "weight."
#~ msgstr ""

#~ msgid ""
#~ "Note that the parameter `kernel_size` is"
#~ " the spatial size of the "
#~ "corresponding forward convolution kernel, not"
#~ " that of `data`. `grad_layout` and "
#~ "`kernel_layout` are the layouts of "
#~ "`grad` and the weight gradient "
#~ "respectively."
#~ msgstr ""

#~ msgid ""
#~ "Other parameters are the same as "
#~ "the conv2d op. See its documentation "
#~ "for more details."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCDHW` and kernel_layout "
#~ "is `OIDHW`, conv3d takes in a data"
#~ " Tensor with shape `(batch_size, "
#~ "in_channels, depth, height, width)`, and "
#~ "a weight Tensor with shape `(channels,"
#~ " in_channels, kernel_size[0], kernel_size[1], "
#~ "kernel_size[2])` to produce an output "
#~ "Tensor with the following rule:"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}[b, c, z, y, x] = \\sum_{dz, dy, dx, k}\n"
#~ "   \\mbox{data}[b, k, \\mbox{strides}[0] * "
#~ "z  + dz, \\mbox{strides}[1] * y  +"
#~ " dy,\n"
#~ "   \\mbox{strides}[2] * x + dx] * \\mbox{weight}[c, k, dz, dy, dx]"
#~ msgstr ""

#~ msgid ""
#~ "Padding and dilation are applied to "
#~ "data and weight respectively before the"
#~ " computation. This operator accepts data"
#~ " layout specification. Semantically, the "
#~ "operator will convert the layout to "
#~ "the canonical layout (`NCDHW` for data"
#~ " and `OIDHW` for weight), perform the"
#~ " computation, then convert to the "
#~ "out_layout."
#~ msgstr ""

#~ msgid "Specifies the output data type for mixed precision conv3d."
#~ msgstr ""

#~ msgid ""
#~ "The correlation layer performs multiplicative"
#~ " patch comparisons between two feature "
#~ "maps. Given two multi-channel feature"
#~ " maps :math:`f_{1}, f_{2}`, with :math:`w`,"
#~ " :math:`h`, and :math:`c` being their "
#~ "width, height, and number of channels,"
#~ " the correlation layer lets the "
#~ "network compare each patch from "
#~ ":math:`f_{1}` with each patch from "
#~ ":math:`f_{2}`."
#~ msgstr ""

#~ msgid ""
#~ "For now we consider only a single"
#~ " comparison of two patches. The "
#~ "'correlation' of two patches centered at"
#~ " :math:`x_{1}` in the first map and"
#~ " :math:`x_{2}` in the second map is"
#~ " then defined as:"
#~ msgstr ""

#~ msgid ""
#~ "c(x_{1}, x_{2}) = \\sum_{o \\in [-k,k]"
#~ " \\times [-k,k]} <f_{1}(x_{1} + o), "
#~ "f_{2}(x_{2} + o)>"
#~ msgstr ""

#~ msgid "for a square patch of size :math:`K:=2k+1`."
#~ msgstr ""

#~ msgid ""
#~ "Note that the equation above is "
#~ "identical to one step of a "
#~ "convolution in neural networks, but "
#~ "instead of convolving data with a "
#~ "filter, it convolves data with other"
#~ "    data. For this reason, it has "
#~ "no training weights."
#~ msgstr ""

#~ msgid ""
#~ "Computing :math:`c(x_{1}, x_{2})` involves "
#~ ":math:`c * K^{2}` multiplications. Comparing"
#~ " all patch combinations involves "
#~ ":math:`w^{2}*h^{2}` such computations."
#~ msgstr ""

#~ msgid ""
#~ "Given a maximum displacement :math:`d`, "
#~ "for each location :math:`x_{1}` it "
#~ "computes correlations :math:`c(x_{1}, x_{2})` "
#~ "only in a neighborhood of size "
#~ ":math:`D:=2d+1`, by limiting the range "
#~ "of :math:`x_{2}`. We use strides "
#~ ":math:`s_{1}, s_{2}`, to quantize "
#~ ":math:`x_{1}` globally and to quantize "
#~ ":math:`x_{2}` within the neighborhood centered"
#~ " around :math:`x_{1}`."
#~ msgstr ""

#~ msgid "The final output is defined by the following expression:"
#~ msgstr ""

#~ msgid "out[n, q, i, j] = c(x_{i, j}, x_{q})"
#~ msgstr ""

#~ msgid ""
#~ "where :math:`i` and :math:`j` enumerate "
#~ "spatial locations in :math:`f_{1}`, and "
#~ ":math:`q` denotes the :math:`q^{th}` "
#~ "neighborhood of :math:`x_{i,j}`."
#~ msgstr ""

#~ msgid "4-D with shape [batch, channel, height, width]"
#~ msgstr ""

#~ msgid "Kernel size for correlation, must be an odd number"
#~ msgstr ""

#~ msgid "Max displacement of Correlation"
#~ msgstr ""

#~ msgid "Stride for data1"
#~ msgstr ""

#~ msgid "Stride for data2 within the neightborhood centered around data1"
#~ msgstr ""

#~ msgid ""
#~ "Padding size, or [pad_height, pad_width] "
#~ "for 2 ints, or [pad_top, pad_left, "
#~ "pad_bottom, pad_right] for 4 ints"
#~ msgstr ""

#~ msgid "operation type is either multiplication or substraction"
#~ msgstr ""

#~ msgid "layout of data1, data2 and the output"
#~ msgstr ""

#~ msgid ""
#~ "**Output** -- 4-D with shape [batch, "
#~ "out_channel, out_height, out_width]"
#~ msgstr ""

#~ msgid "The predictions."
#~ msgstr ""

#~ msgid "The targets."
#~ msgstr ""

#~ msgid ""
#~ "The deformable convolution operation is "
#~ "described in https://arxiv.org/abs/1703.06211"
#~ msgstr ""

#~ msgid "The offset expressions."
#~ msgstr ""

#~ msgid "Number of deformable groups."
#~ msgstr ""

#~ msgid "Dense operator. Applies a linear transformation"
#~ msgstr ""

#~ msgid ""
#~ "The input data to the operator, of"
#~ " shape `(d_1, d_2, ..., d_n, "
#~ "units_in)`."
#~ msgstr ""

#~ msgid "The weight expressions, 2-D matrix, of shape `(units, units_in)`."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the output data type for "
#~ "mixed precision dense, of shape `(d_1,"
#~ " d_2, ..., d_n, units)`."
#~ msgstr ""

#~ msgid "Input data with channels divisible by block_size**2"
#~ msgstr ""

#~ msgid "Size of blocks to convert channels into."
#~ msgstr ""

#~ msgid "One of NCHW or NHWC, indicates channel axis."
#~ msgstr ""

#~ msgid "One of DCR or CDR, indicates which order channels are accessed in."
#~ msgstr ""

#~ msgid ""
#~ "**result** --  Tensor with shape "
#~ "[in_batch, in_channel / block_size * "
#~ "block_size,                    in_height * "
#~ "block_size, in_width * block_size]"
#~ msgstr ""

#~ msgid "**result** --"
#~ msgstr ""

#~ msgid "Tensor with shape [in_batch, in_channel / block_size * block_size,"
#~ msgstr ""

#~ msgid "in_height * block_size, in_width * block_size]"
#~ msgstr ""

#~ msgid "n-D, can be any layout."
#~ msgstr ""

#~ msgid "Dilation stride on each dimension, 1 means no dilation."
#~ msgstr ""

#~ msgid "Value used to dilate the input."
#~ msgstr ""

#~ msgid "**Output** -- The computed result"
#~ msgstr ""

#~ msgid ""
#~ "During training, each element of the "
#~ "input is set to zero with "
#~ "probability ``p``. The whole array is"
#~ " rescaled by ``1/(1-p)`` to keep the"
#~ " expected sum of the input unchanged."
#~ msgstr ""

#~ msgid "The probability for an element to be reset to 0."
#~ msgstr ""

#~ msgid "**result** -- The result of dropout"
#~ msgstr ""

#~ msgid ""
#~ "Computes softmax. Use approximation to "
#~ "compute exponent for faster speed."
#~ msgstr ""

#~ msgid ""
#~ "\\text{softmax}(x)_i = \\frac{exp(x_i)}{\\sum_j exp(x_j)}\n"
#~ "\n"
#~ msgstr ""

#~ msgid "The axis to sum over when computing softmax"
#~ msgstr ""

#~ msgid "Compute equivalent of"
#~ msgstr ""

#~ msgid "Useful for"
#~ msgstr ""

#~ msgid ""
#~ "Encoding explicit re-use of computation"
#~ " in convolution ops operated on a "
#~ "sliding window input"
#~ msgstr ""

#~ msgid ""
#~ "Implementing a FIFO queue to cache "
#~ "intermediate results, e.g. as in Fast"
#~ " WaveNet."
#~ msgstr ""

#~ msgid "The input data"
#~ msgstr ""

#~ msgid "Previous value of the FIFO buffer"
#~ msgstr ""

#~ msgid "Specify which axis should be used for buffering"
#~ msgstr ""

#~ msgid "**result** -- Updated value for the buffer"
#~ msgstr ""

#~ msgid ""
#~ "* **pad_left** (*int*) -- Padding size"
#~ " on left * **pad_right** (*int*) --"
#~ " Padding size on right."
#~ msgstr ""

#~ msgid "**pad_left** (*int*) -- Padding size on left"
#~ msgstr ""

#~ msgid "**pad_right** (*int*) -- Padding size on right."
#~ msgstr ""

#~ msgid ""
#~ "* **pad_top** (*int*) -- Padding size"
#~ " on top * **pad_left** (*int*) -- "
#~ "Padding size on left * **pad_down** "
#~ "(*int*) -- Padding size on down. *"
#~ " **pad_right** (*int*) -- Padding size "
#~ "on right."
#~ msgstr ""

#~ msgid "**pad_top** (*int*) -- Padding size on top"
#~ msgstr ""

#~ msgid "**pad_down** (*int*) -- Padding size on down."
#~ msgstr ""

#~ msgid ""
#~ "* **pad_front** (*int*) -- Padding size"
#~ " on front * **pad_top** (*int*) --"
#~ " Padding size on top * **pad_left**"
#~ " (*int*) -- Padding size on left "
#~ "* **pad_back** (*int*) -- Padding size"
#~ " on back * **pad_down** (*int*) --"
#~ " Padding size on down. * "
#~ "**pad_right** (*int*) -- Padding size on"
#~ " right."
#~ msgstr ""

#~ msgid "**pad_front** (*int*) -- Padding size on front"
#~ msgstr ""

#~ msgid "**pad_back** (*int*) -- Padding size on back"
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCW` a data Tensor "
#~ "with shape `(batch_size, in_channels, width)`,"
#~ " to produce an output Tensor with "
#~ "the following rule:"
#~ msgstr ""

#~ msgid "with data of shape (b, c, w)"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}(b, c, 1)  = \\frac{1}{w} "
#~ "\\sum_{n=0}^{w-1} \\mbox{data}(b, c, n)"
#~ msgstr ""

#~ msgid "with data of shape (b, c, h, w)"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}(b, c, 1, 1)  = \\frac{1}{h"
#~ " * w} \\sum_{m=0}^{h-1} \\sum_{n=0}^{w-1}\n"
#~ "     \\mbox{data}(b, c, m, n)"
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 3D average value calculation"
#~ " across each window represented by "
#~ "DxWxH."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCDHW` a data Tensor "
#~ "with shape `(batch_size, in_channels, depth,"
#~ " height, width)`, to produce an "
#~ "output Tensor with the following rule:"
#~ msgstr ""

#~ msgid "with data of shape (b, c, d, h, w)"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}(b, c, 1, 1, 1)  = "
#~ "\\frac{1}{d * h * w} \\sum_{l=0}^{d-1}"
#~ "  \\sum_{m=0}^{h-1}\n"
#~ "     \\sum_{n=0}^{w-1} \\mbox{data}(b, c, l, m, n)"
#~ msgstr ""

#~ msgid "with data of shape (b, c, w) .. math::"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}(b, c, 1, 1)  = \\max_{m=0,"
#~ " \\ldots, h} \\max_{n=0, \\ldots, w}\n"
#~ ""
#~ "     \\mbox{data}(b, c, m, n)"
#~ msgstr ""

#~ msgid "with data of shape (b, c, d, h, w) .. math::"
#~ msgstr ""

#~ msgid ""
#~ "Group normalization normalizes over group "
#~ "of channels for each training examples."
#~ " We can say that, Group Norm is"
#~ " in between Instance Norm and Layer"
#~ " Norm. When we put all the "
#~ "channels into a single group, group "
#~ "normalization becomes Layer normalization. "
#~ "And, when we put each channel into"
#~ " different groups it becomes Instance "
#~ "normalization"
#~ msgstr ""

#~ msgid "https://arxiv.org/pdf/1803.08494.pdf"
#~ msgstr ""

#~ msgid ""
#~ "Applies group normalization to the "
#~ "n-dimensional input array by seperating "
#~ "the input channels into 'num_groups' "
#~ "groups, each containing 'num_channels / "
#~ "num_groups' channels. The mean and "
#~ "standard-deviation are calculated separately "
#~ "over the each group. gamma and "
#~ "beta are learnable per-channel affine"
#~ " transform parameter vectors of size "
#~ "num_channels."
#~ msgstr ""

#~ msgid ""
#~ "out = \\frac{data - mean(data, "
#~ "axis)}{\\sqrt{var(data, axis)+\\epsilon}}\n"
#~ "    * gamma + beta"
#~ msgstr ""

#~ msgid ""
#~ "Unlike batch normalization, the mean and"
#~ " var are computed along a group "
#~ "of channels."
#~ msgstr ""

#~ msgid ""
#~ "If the input has size k on "
#~ "axis 1, then both gamma and beta"
#~ " have shape (k,)."
#~ msgstr ""

#~ msgid "Input to which group_norm will be applied."
#~ msgstr ""

#~ msgid "The number of groups to separate the channels into."
#~ msgstr ""

#~ msgid "The axis of the channels."
#~ msgstr ""

#~ msgid "If True, multiply by gamma. If False, gamma is not used."
#~ msgstr ""

#~ msgid "**result** -- The normalized data."
#~ msgstr ""

#~ msgid ""
#~ "out = \\frac{data - mean(data)}{\\sqrt{var(data)+\\epsilon}}\n"
#~ "    * gamma + beta"
#~ msgstr ""

#~ msgid ""
#~ "The instance normalization is similar to"
#~ " batch normalization, but unlike batch "
#~ "normalization, the mean and var are "
#~ "calculated per-dimension separately for "
#~ "each object(instance) in a mini-batch,"
#~ " not over a batch. And the same"
#~ " normalization is applied both at "
#~ "test and train time."
#~ msgstr ""

#~ msgid ""
#~ "The parameter ``axis`` specifies which "
#~ "axis of the input shape denotes "
#~ "the 'channel'.  The default is 1. "
#~ "Specifying -1 sets the channel axis "
#~ "to be the last item in the "
#~ "input shape."
#~ msgstr ""

#~ msgid "Input to which instance_norm will be applied."
#~ msgstr ""

#~ msgid ""
#~ "* **result** (*tvm.relay.Expr*) -- The "
#~ "normalized data. * **.. _`Instance "
#~ "Normalization** (The Missing Ingredient for"
#~ " Fast Stylization`:) -- "
#~ "https://arxiv.org/abs/1607.08022"
#~ msgstr ""

#~ msgid "**result** (*tvm.relay.Expr*) -- The normalized data."
#~ msgstr ""

#~ msgid ""
#~ "**.. _`Instance Normalization** (The Missing"
#~ " Ingredient for Fast Stylization`:) -- "
#~ "https://arxiv.org/abs/1607.08022"
#~ msgstr ""

#~ msgid ""
#~ "y(i, j) = x(i, j) / sqrt(max(sum(x^2), eps))\n"
#~ "\n"
#~ msgstr ""

#~ msgid "epsilon value"
#~ msgstr ""

#~ msgid "axis over the normalization applied"
#~ msgstr ""

#~ msgid ""
#~ "Layer normalization (Lei Ba and et "
#~ "al., 2016). Applies layer normalization "
#~ "to the n-dimensional input array. This"
#~ " operator takes an n-dimensional input "
#~ "array and normalizes the input using "
#~ "the given axis:"
#~ msgstr ""

#~ msgid ""
#~ "Unlike batch normalization, the mean and"
#~ " var are computed along the channel"
#~ " dimension."
#~ msgstr ""

#~ msgid ""
#~ "Assume the input has size k on "
#~ "axis 1, then both gamma and beta"
#~ " have shape (k,)."
#~ msgstr ""

#~ msgid "Input to which layer_norm will be applied."
#~ msgstr ""

#~ msgid "The axis that should be normalized, typically the axis of the channels."
#~ msgstr ""

#~ msgid "`y = x > 0 ? x : alpha * x`"
#~ msgstr ""

#~ msgid "Slope coefficient for the negative half axis."
#~ msgstr ""

#~ msgid "\\text{log_softmax}(x)_i = \\log \\frac{exp(x_i)}{\\sum_j exp(x_j)}"
#~ msgstr ""

#~ msgid "The axis to sum over when computing log softmax"
#~ msgstr ""

#~ msgid ""
#~ "Normalize the input in a local "
#~ "region across or within feature maps."
#~ " Each input value is divided by "
#~ "(data / (bias + (alpha * sum_data"
#~ " ^2 /size))^beta) where n is the "
#~ "size of each local region, and the"
#~ " sum is taken over the region "
#~ "centered at that value (zero padding "
#~ "is added where necessary)."
#~ msgstr ""

#~ msgid ""
#~ "(data / (bias + (alpha * sum_data ^2 /size))^beta)\n"
#~ "\n"
#~ msgstr ""

#~ msgid "The size of the local region to be considered for normalization."
#~ msgstr ""

#~ msgid "Input data layout channel axis. Default value is 1 for NCHW format"
#~ msgstr ""

#~ msgid "The offset parameter to avoid dividing by 0."
#~ msgstr ""

#~ msgid "The scaling parameter."
#~ msgstr ""

#~ msgid "The exponent parameter."
#~ msgstr ""

#~ msgid ""
#~ "Matmul operator. Applies a linear "
#~ "transformation. The A & B can be"
#~ " transposed."
#~ msgstr ""

#~ msgid "`C = A * B`"
#~ msgstr ""

#~ msgid ""
#~ "The first input of the operator, "
#~ "of shape `(d_1, d_2, ..., d_n, "
#~ "units_in)` or `(d_1, d_2, ..., units_in,"
#~ " d_n)`."
#~ msgstr ""

#~ msgid ""
#~ "The second input expressions, 2-D "
#~ "matrix, of shape `(units_in, units)` or"
#~ " `(units, units_in)`."
#~ msgstr ""

#~ msgid "Number of hidden units of the matmul transformation."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the output data type for "
#~ "mixed precision matmul, of shape `(d_1,"
#~ " d_2, ..., d_n, units)`."
#~ msgstr ""

#~ msgid "Whether the data tensor is in transposed format."
#~ msgstr ""

#~ msgid "Whether the weight tensor is in transposed format."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 1D max value calculation "
#~ "with in pool_size sized window by "
#~ "striding defined by stride."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 2D max value calculation "
#~ "with in pool_size sized window by "
#~ "striding defined by stride"
#~ msgstr ""

#~ msgid "with data of shape (b, c, h, w) and pool_size (kh, kw)"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}(b, c, y, x)  = \\max_{m=0,"
#~ " \\ldots, kh-1} \\max_{n=0, \\ldots, kw-1}"
#~ "\n"
#~ "     \\mbox{data}(b, c, \\mbox{stride}[0] *"
#~ " y + m, \\mbox{stride}[1] * x +"
#~ " n)"
#~ msgstr ""

#~ msgid ""
#~ "Padding is applied to data before "
#~ "the computation. ceil_mode is used to"
#~ " take ceil or floor while computing"
#~ " out shape. This operator accepts "
#~ "data layout specification."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes out_grad and data"
#~ " as input and calculates gradient of"
#~ " max_pool2d."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 3D max value calculation "
#~ "with in pool_size sized window by "
#~ "striding defined by stride."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes in a tensor "
#~ "and pads each axis by the "
#~ "specified widths using mirroring of the"
#~ " border pixels."
#~ msgstr ""

#~ msgid "The input data to the operator"
#~ msgstr ""

#~ msgid ""
#~ "Number of values padded to the "
#~ "edges of each axis, in the format"
#~ " of ((before_1, after_1), ..., (before_N,"
#~ " after_N))"
#~ msgstr ""

#~ msgid "What type of mirroring to use, must be SYMMETRIC or REFLECT."
#~ msgstr ""

#~ msgid "output{n, i_1, i_2, ..., i_k} = -p * w"
#~ msgstr ""

#~ msgid "where t = target{n, i_1, i_2, ..., i_k}"
#~ msgstr ""

#~ msgid ""
#~ "p = predictions{n, t, i_1, i_2, "
#~ "i_k} w = weights{n, i_1, i_2, ...,"
#~ " i_k} if t != ignore_index else "
#~ "0"
#~ msgstr ""

#~ msgid "result = reduction(output)"
#~ msgstr ""

#~ msgid "The target value of each prediction."
#~ msgstr ""

#~ msgid "The weight of each target value."
#~ msgstr ""

#~ msgid ""
#~ "The reduction method to apply to "
#~ "the output. Possible values are "
#~ "\"mean\", \"sum\" and \"none\"."
#~ msgstr ""

#~ msgid "The target value to ignore."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes in a tensor "
#~ "and pads each axis by the "
#~ "specified widths using the specified "
#~ "value."
#~ msgstr ""

#~ msgid "The value used for padding"
#~ msgstr ""

#~ msgid ""
#~ "'constant' pads with constant_value pad_value"
#~ " 'edge' pads using the edge values"
#~ " of the input array 'reflect' pads"
#~ " by reflecting values with respect to"
#~ " the edge"
#~ msgstr ""

#~ msgid "y = x > 0 ? x : alpha * x"
#~ msgstr ""

#~ msgid "Specify which shape axis the channel is specified."
#~ msgstr ""

#~ msgid ""
#~ "out = max(x, 0)\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "2-D of shape [M, 2] where M "
#~ "is number of spatial dims, specifies "
#~ "[before, after] paddings for each "
#~ "spatial dimension."
#~ msgstr ""

#~ msgid "The value used for padding."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- N-D Tensor with shape "
#~ "[in_batch * prod(block_shape), padded_data[1] "
#~ "/ block_shape[0], ..., padded_data[M] / "
#~ "block_shape[M-1], remaining_shape]"
#~ msgstr ""

#~ msgid "Input data with spatial dimensions divisible by block_size"
#~ msgstr ""

#~ msgid "Size of blocks to decompose into channels."
#~ msgstr ""

#~ msgid ""
#~ "**result** --  Tensor with shape "
#~ "[in_batch, in_channel * block_size * "
#~ "block_size,                    in_height / "
#~ "block_size, in_width / block_size]"
#~ msgstr ""

#~ msgid "Tensor with shape [in_batch, in_channel * block_size * block_size,"
#~ msgstr ""

#~ msgid "in_height / block_size, in_width / block_size]"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{sparse_add}(dense_mat, sparse_mat)[m, n] ="
#~ " \\mbox{add}(\\mbox{as_dense}(S), (D))[m, n]"
#~ msgstr ""

#~ msgid ""
#~ "where `as_dense` returns dense equivalent "
#~ "of the given S(sparse matrix) while "
#~ "performing addition with given D(dense "
#~ "matrix)."
#~ msgstr ""

#~ msgid "The input dense matrix for the matrix addition"
#~ msgstr ""

#~ msgid "The input sparse matrix(CSR) for the matrix addition."
#~ msgstr ""

#~ msgid "实际案例"
#~ msgstr ""

#~ msgid "\\if sparse_lhs=False:"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{sparse_dense}(dense_mat, sparse_mat)[m, n]\n"
#~ "= \\mbox{matmul}(D, \\mbox{as_dense}(S)^T)[m, n]"
#~ msgstr ""

#~ msgid "\\if sparse_lhs=True:"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{sparse_dense}(dense_mat, sparse_mat)[m, n]\n"
#~ "= \\mbox{matmul}(\\mbox{as_dense}(S), (D)^T)[m, n]"
#~ msgstr ""

#~ msgid ""
#~ "where `as_dense` returns dense equivalent "
#~ "of the given S(sparse matrix) while "
#~ "performing matmul with given D(dense "
#~ "matrix)."
#~ msgstr ""

#~ msgid ""
#~ "See "
#~ "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
#~ " and "
#~ "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.bsr_matrix.html"
#~ " for more detail on the sparse "
#~ "matrix representation."
#~ msgstr ""

#~ msgid "The input dense matrix for the matrix multiplication"
#~ msgstr ""

#~ msgid "The input sparse matrix for the matrix multiplication."
#~ msgstr ""

#~ msgid "Indicates whether lhs or rhs matrix is sparse. Default value is False."
#~ msgstr ""

#~ msgid "** Currently only support Square Matrices **"
#~ msgstr ""

#~ msgid "\\mbox{sparse_transpose}(x)[n, n] = (x^T)[n, n]"
#~ msgstr ""

#~ msgid ""
#~ "Please refer to "
#~ "https://github.com/scipy/scipy/blob/v1.3.0/scipy/sparse/csr.py "
#~ "for the algorithm implemented in this"
#~ " operator."
#~ msgstr ""

#~ msgid "The sparse weight matrix for the fast matrix transpose."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- Tuple of output sparse "
#~ "tensor (same shape and format as "
#~ "input), i.e. if CSR then output is"
#~ " in ([data, indices, indptr]) form"
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 2D scaling to the given"
#~ " scale factor. In the default case,"
#~ " where the data_layout is `NCHW` with"
#~ " data of shape (n, c, h, w) "
#~ "out will have a shape (n, c, "
#~ "h*scale_h, w*scale_w)"
#~ msgstr ""

#~ msgid ""
#~ "method indicates the algorithm to be "
#~ "used while calculating the out value "
#~ "and method can be one of "
#~ "(\"bilinear\", \"nearest_neighbor\", \"bicubic\")"
#~ msgstr ""

#~ msgid "The scale factor for height upsampling."
#~ msgstr ""

#~ msgid "The scale factor for width upsampling."
#~ msgstr ""

#~ msgid "Scale method to used [nearest_neighbor, bilinear, bicubic]."
#~ msgstr ""

#~ msgid "Whether to keep corners in proper place."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 3D scaling to the given"
#~ " scale factor. In the default case,"
#~ " where the data_layout is `NCDHW` "
#~ "with data of shape (n, c, d, "
#~ "h, w) out will have a shape "
#~ "(n, c, d*scale_d, h*scale_h, w*scale_w)"
#~ msgstr ""

#~ msgid ""
#~ "method indicates the algorithm to be "
#~ "used while calculating the out value "
#~ "and method can be one of "
#~ "(\"trilinear\", \"nearest_neighbor\")"
#~ msgstr ""

#~ msgid "The scale factor for depth upsampling."
#~ msgstr ""

#~ msgid "Scale method to used [nearest_neighbor, trilinear]."
#~ msgstr ""

#~ msgid ""
#~ "Describes how to transform the "
#~ "coordinate in the resized tensor to "
#~ "the coordinate in the original tensor."
#~ " Refer to the ONNX Resize operator"
#~ " specification for details. Available "
#~ "options are \"half_pixel\", \"align_corners\" "
#~ "and \"asymmetric\"."
#~ msgstr ""

#~ msgid ":py:obj:`Constant <tvm.relay.nn.Constant>`\\ \\(data\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ":py:class:`~tvm.ir.expr.RelayExpr` 的别名"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`const <tvm.relay.nn.const>`\\ \\(value\\[\\, "
#~ "dtype\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`conv2d_backward_weight "
#~ "<tvm.relay.nn.conv2d_backward_weight>`\\ \\(grad\\, "
#~ "data\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Common code to get the 1 "
#~ "dimensional pad option Parameters ----------"
#~ " padding : Union[int, Tuple[int, ...]]"
#~ "     Padding size Returns ------- pad_left"
#~ " : int     Padding size on left "
#~ "pad_right : int     Padding size on "
#~ "right."
#~ msgstr ""

#~ msgid ""
#~ "Common code to get the pad option"
#~ " Parameters ---------- padding : Union[int,"
#~ " Tuple[int, ...]]     Padding size Returns"
#~ " ------- pad_top : int     Padding "
#~ "size on top pad_left : int     "
#~ "Padding size on left pad_down : "
#~ "int     Padding size on down. pad_right"
#~ " : int     Padding size on right."
#~ msgstr ""

#~ msgid ""
#~ "Common code to get the pad option"
#~ " Parameters ---------- padding : Union[int,"
#~ " Tuple[int, ...]]     Padding size Returns"
#~ " ------- pad_front : int     Padding "
#~ "size on front pad_top : int     "
#~ "Padding size on top pad_left : int"
#~ "     Padding size on left pad_back :"
#~ " int     Padding size on back "
#~ "pad_down : int     Padding size on "
#~ "down. pad_right : int     Padding size"
#~ " on right."
#~ msgstr ""

#~ msgid "Parameters"
#~ msgstr ""

#~ msgid "data"
#~ msgstr ""

#~ msgid "tvm.nd.NDArray"
#~ msgstr ""

#~ msgid "span: Optional[tvm.relay.Span]"
#~ msgstr ""

#~ msgid "Span that points to original source code."
#~ msgstr ""

#~ msgid ":py:obj:`struct_info <tvm.relay.nn.Expr.struct_info>`\\"
#~ msgstr ""

#~ msgid "Get the struct info field"
#~ msgstr ""

#~ msgid "tvm.relay.Expr"
#~ msgstr ""

#~ msgid "output_size"
#~ msgstr ""

#~ msgid "tuple of int. optional"
#~ msgstr ""

#~ msgid "layout"
#~ msgstr ""

#~ msgid "str, optional"
#~ msgstr ""

#~ msgid "out_layout"
#~ msgstr ""

#~ msgid "Returns"
#~ msgstr ""

#~ msgid "result"
#~ msgstr ""

#~ msgid "The computed result."
#~ msgstr ""

#~ msgid "pool_size"
#~ msgstr ""

#~ msgid "int or tuple of int, optional"
#~ msgstr ""

#~ msgid "strides"
#~ msgstr ""

#~ msgid "dilation"
#~ msgstr ""

#~ msgid "padding"
#~ msgstr ""

#~ msgid "Optional[str]"
#~ msgstr ""

#~ msgid "ceil_mode"
#~ msgstr ""

#~ msgid "bool, optional"
#~ msgstr ""

#~ msgid "count_include_pad"
#~ msgstr ""

#~ msgid "tuple of int, optional"
#~ msgstr ""

#~ msgid "out_grad"
#~ msgstr ""

#~ msgid "The Flattened result."
#~ msgstr ""

#~ msgid "tensor_a"
#~ msgstr ""

#~ msgid "tensor_b"
#~ msgstr ""

#~ msgid "out_dtype"
#~ msgstr ""

#~ msgid "transpose_a"
#~ msgstr ""

#~ msgid "Optional[bool] = False"
#~ msgstr ""

#~ msgid "transpose_b"
#~ msgstr ""

#~ msgid "Optional[bool] = True"
#~ msgstr ""

#~ msgid "result: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "gamma"
#~ msgstr ""

#~ msgid "beta"
#~ msgstr ""

#~ msgid "moving_mean"
#~ msgstr ""

#~ msgid "moving_var"
#~ msgstr ""

#~ msgid "axis"
#~ msgstr ""

#~ msgid "int, optional, default=1"
#~ msgstr ""

#~ msgid "epsilon"
#~ msgstr ""

#~ msgid "double, optional, default=1e-5"
#~ msgstr ""

#~ msgid "center"
#~ msgstr ""

#~ msgid "boolean, optional, default=True"
#~ msgstr ""

#~ msgid "scale"
#~ msgstr ""

#~ msgid "relay.Tuple([tvm.relay.Expr, tvm.relay.Expr, tvm.relay.Expr])"
#~ msgstr ""

#~ msgid ""
#~ "Tuple of normed data (same shape "
#~ "as input), new running mean (k-length"
#~ " vector), and new running variance "
#~ "(k-length vector)"
#~ msgstr ""

#~ msgid "tvm.te.Tensor"
#~ msgstr ""

#~ msgid "block_shape"
#~ msgstr ""

#~ msgid "relay.Expr"
#~ msgstr ""

#~ msgid "crops"
#~ msgstr ""

#~ msgid ""
#~ "N-D Tensor with shape [batch / "
#~ "prod(block_shape), in_shape[1] * block_shape[0] "
#~ "- crops[0,0] - crops[0,1], ..., "
#~ "in_shape[M] * block_shape[M-1] - crops[M-1,"
#~ " 0] - crops[M-1, 1], remaining_shape]"
#~ msgstr ""

#~ msgid "bias"
#~ msgstr ""

#~ msgid "int, optional"
#~ msgstr ""

#~ msgid "The final result."
#~ msgstr ""

#~ msgid "tvm.relay.expr"
#~ msgstr ""

#~ msgid "bits"
#~ msgstr ""

#~ msgid "int"
#~ msgstr ""

#~ msgid "pack_axis"
#~ msgstr ""

#~ msgid "bit_axis"
#~ msgstr ""

#~ msgid "pack_type"
#~ msgstr ""

#~ msgid "str"
#~ msgstr ""

#~ msgid "name"
#~ msgstr ""

#~ msgid "The packed tensor."
#~ msgstr ""

#~ msgid "weight"
#~ msgstr ""

#~ msgid "channels"
#~ msgstr ""

#~ msgid "kernel_size"
#~ msgstr ""

#~ msgid "activation_bits"
#~ msgstr ""

#~ msgid "weight_bits"
#~ msgstr ""

#~ msgid "data_layout"
#~ msgstr ""

#~ msgid "kernel_layout"
#~ msgstr ""

#~ msgid "pack_dtype: str, optional"
#~ msgstr ""

#~ msgid "units"
#~ msgstr ""

#~ msgid "data_bits"
#~ msgstr ""

#~ msgid "pack_dtype"
#~ msgstr ""

#~ msgid "unipolar"
#~ msgstr ""

#~ msgid "value: Union[bool, int, float, numpy.ndarray, tvm.nd.NDArray]"
#~ msgstr ""

#~ msgid "dtype: str, optional"
#~ msgstr ""

#~ msgid "Note"
#~ msgstr ""

#~ msgid "weights"
#~ msgstr ""

#~ msgid "tile_rows: int"
#~ msgstr ""

#~ msgid "tile_cols: int"
#~ msgstr ""

#~ msgid "groups"
#~ msgstr ""

#~ msgid "kernel"
#~ msgstr ""

#~ msgid "convolution_algorithm"
#~ msgstr ""

#~ msgid "tile_size"
#~ msgstr ""

#~ msgid "weight_layout: str"
#~ msgstr ""

#~ msgid "Optional[int, Tuple[int]]"
#~ msgstr ""

#~ msgid "Optional[int]"
#~ msgstr ""

#~ msgid "Tuple[int], optional"
#~ msgstr ""

#~ msgid "output_padding"
#~ msgstr ""

#~ msgid "Optional[Tuple[int]]"
#~ msgstr ""

#~ msgid "data1"
#~ msgstr ""

#~ msgid "data2"
#~ msgstr ""

#~ msgid "kernel_size: int"
#~ msgstr ""

#~ msgid "max_displacement: int"
#~ msgstr ""

#~ msgid "stride1: int"
#~ msgstr ""

#~ msgid "stride2: int"
#~ msgstr ""

#~ msgid "int or a list/tuple of 2 or 4 ints"
#~ msgstr ""

#~ msgid "is_multiply: bool"
#~ msgstr ""

#~ msgid "layout: str"
#~ msgstr ""

#~ msgid "Output"
#~ msgstr ""

#~ msgid "4-D with shape [batch, out_channel, out_height, out_width]"
#~ msgstr ""

#~ msgid "predictions"
#~ msgstr ""

#~ msgid "targets"
#~ msgstr ""

#~ msgid "offset"
#~ msgstr ""

#~ msgid "deformable_groups"
#~ msgstr ""

#~ msgid "block_size"
#~ msgstr ""

#~ msgid "string"
#~ msgstr ""

#~ msgid "mode"
#~ msgstr ""

#~ msgid "tuple of <int>"
#~ msgstr ""

#~ msgid "dilation_value"
#~ msgstr ""

#~ msgid "int/float, optional"
#~ msgstr ""

#~ msgid "The computed result"
#~ msgstr ""

#~ msgid "rate"
#~ msgstr ""

#~ msgid "float, optional (default=0.5)"
#~ msgstr ""

#~ msgid "The result of dropout"
#~ msgstr ""

#~ msgid "data: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "axis: int, optional"
#~ msgstr ""

#~ msgid "buffer"
#~ msgstr ""

#~ msgid "Updated value for the buffer"
#~ msgstr ""

#~ msgid ""
#~ "Common code to get the 1 "
#~ "dimensional pad option Parameters ----------"
#~ " padding : Union[int, Tuple[int, ...]]"
#~ msgstr ""

#~ msgid "Padding size"
#~ msgstr ""

#~ msgid "pad_left"
#~ msgstr ""

#~ msgid "Padding size on left"
#~ msgstr ""

#~ msgid "pad_right"
#~ msgstr ""

#~ msgid "Padding size on right."
#~ msgstr ""

#~ msgid ""
#~ "Common code to get the pad option"
#~ " Parameters ---------- padding : Union[int,"
#~ " Tuple[int, ...]]"
#~ msgstr ""

#~ msgid "pad_top"
#~ msgstr ""

#~ msgid "Padding size on top"
#~ msgstr ""

#~ msgid "pad_down"
#~ msgstr ""

#~ msgid "Padding size on down."
#~ msgstr ""

#~ msgid "pad_front"
#~ msgstr ""

#~ msgid "Padding size on front"
#~ msgstr ""

#~ msgid "pad_back"
#~ msgstr ""

#~ msgid "Padding size on back"
#~ msgstr ""

#~ msgid "num_groups"
#~ msgstr ""

#~ msgid "The normalized data."
#~ msgstr ""

#~ msgid "eps"
#~ msgstr ""

#~ msgid "float"
#~ msgstr ""

#~ msgid "list of int, optional"
#~ msgstr ""

#~ msgid "int, optional, default=-1"
#~ msgstr ""

#~ msgid "alpha"
#~ msgstr ""

#~ msgid "size"
#~ msgstr ""

#~ msgid "float, optional"
#~ msgstr ""

#~ msgid "pad_width: tuple of <tuple of <int>>, required"
#~ msgstr ""

#~ msgid "mode: string, optional, default='SYMMETRIC'"
#~ msgstr ""

#~ msgid "reduction"
#~ msgstr ""

#~ msgid "ignore_index"
#~ msgstr ""

#~ msgid "pad_width: tuple of <tuple of <int>>, or tvm.relay.Expr, required"
#~ msgstr ""

#~ msgid "pad_value: float, or tvm.relay.Expr, optional, default=0"
#~ msgstr ""

#~ msgid "pad_mode: 'constant', 'edge', 'reflect'"
#~ msgstr ""

#~ msgid "paddings"
#~ msgstr ""

#~ msgid "pad_value"
#~ msgstr ""

#~ msgid "float, or relay.Expr, optional, default=0"
#~ msgstr ""

#~ msgid ""
#~ "N-D Tensor with shape [in_batch * "
#~ "prod(block_shape), padded_data[1] / block_shape[0],"
#~ " ..., padded_data[M] / block_shape[M-1], "
#~ "remaining_shape]"
#~ msgstr ""

#~ msgid "dense_mat"
#~ msgstr ""

#~ msgid "sparse_mat"
#~ msgstr ""

#~ msgid "Union[namedtuple, Tuple[ndarray, ndarray, ndarray]]."
#~ msgstr ""

#~ msgid "Examples"
#~ msgstr ""

#~ msgid "sparse_lhs"
#~ msgstr ""

#~ msgid "x"
#~ msgstr ""

#~ msgid ""
#~ "Tuple of output sparse tensor (same "
#~ "shape and format as input), i.e. "
#~ "if CSR then output is in ([data,"
#~ " indices, indptr]) form"
#~ msgstr ""

#~ msgid "scale_h"
#~ msgstr ""

#~ msgid "tvm.relay.Expr or int or float"
#~ msgstr ""

#~ msgid "scale_w"
#~ msgstr ""

#~ msgid "method"
#~ msgstr ""

#~ msgid "align_corners"
#~ msgstr ""

#~ msgid "scale_d"
#~ msgstr ""

#~ msgid "coordinate_transformation_mode: string, optional"
#~ msgstr ""

