# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-13 12:27+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../notebook/docs/reference/api/python/tir.rst:21
msgid "tvm.tir"
msgstr ""

#: of tvm.tir:1
msgid "Namespace for Tensor-level IR"
msgstr ""

#: of tvm.tir:1 tvm.tir.analysis:1 tvm.tir.transform:1
msgid "**Classes:**"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Add <tvm.tir.Add>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1 tvm.tir:1:<autosummary>:1
msgid "Add node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`Allocate <tvm.tir.Allocate>`\\ \\(buffer\\_var\\, dtype\\, "
"extents\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.stmt.Allocate:1 tvm.tir:1:<autosummary>:1
msgid "Allocate node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`AllocateConst <tvm.tir.AllocateConst>`\\ \\(buffer\\_var\\, "
"dtype\\, extents\\, ...\\)"
msgstr ""

#: of tvm.tir.stmt.AllocateConst:1 tvm.tir:1:<autosummary>:1
msgid "Allocate constant node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`And <tvm.tir.And>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.And:1 tvm.tir:1:<autosummary>:1
msgid "And node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Any <tvm.tir.Any>`\\ \\(\\[span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Any:1 tvm.tir:1:<autosummary>:1
msgid "Any node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`AssertStmt <tvm.tir.AssertStmt>`\\ \\(condition\\, message\\, "
"body\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.AssertStmt:1 tvm.tir:1:<autosummary>:1
msgid "AssertStmt node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`AttrStmt <tvm.tir.AttrStmt>`\\ \\(node\\, attr\\_key\\, value\\,"
" body\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.AttrStmt:1 tvm.tir:1:<autosummary>:1
msgid "AttrStmt node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`BijectiveLayout <tvm.tir.BijectiveLayout>`\\ \\(\\)"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid "Bijective mapping for two layouts (src-layout and dst-layout)."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`Block <tvm.tir.Block>`\\ \\(iter\\_vars\\, reads\\, writes\\, "
"name\\_hint\\, body\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1 tvm.tir.stmt.Block:1
#: tvm.tir:1:<autosummary>:1
msgid "Block node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`BlockDependenceInfo <tvm.tir.BlockDependenceInfo>`\\ \\(mod\\)"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo:1
#: tvm.tir:1:<autosummary>:1
msgid ""
"An object that helps build and query block level dependences using the 2 "
"core objects BlockScope and StmtSRef"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`BlockRealize <tvm.tir.BlockRealize>`\\ \\(iter\\_values\\, "
"predicate\\, block\\)"
msgstr ""

#: of tvm.tir.stmt.BlockRealize:1 tvm.tir:1:<autosummary>:1
msgid "BlockRealize node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`BlockScope <tvm.tir.BlockScope>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:1 tvm.tir:1:<autosummary>:1
msgid ""
"An object corresponds to each block sref in the sref tree, which tracks "
"the producer-consumer dependency between blocks."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`Broadcast <tvm.tir.Broadcast>`\\ \\(value\\, lanes\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Broadcast:1 tvm.tir:1:<autosummary>:1
msgid "Broadcast node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Buffer <tvm.tir.Buffer>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1 tvm.tir.buffer.Buffer:1
#: tvm.tir:1:<autosummary>:1
msgid "Symbolic data buffer in TVM."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`BufferLoad <tvm.tir.BufferLoad>`\\ \\(buffer\\, indices\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.expr.BufferLoad:1 tvm.tir:1:<autosummary>:1
msgid "Buffer load node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`BufferRealize <tvm.tir.BufferRealize>`\\ \\(buffer\\, bounds\\, "
"condition\\, body\\)"
msgstr ""

#: of tvm.tir.stmt.BufferRealize:1 tvm.tir:1:<autosummary>:1
msgid "Buffer realize node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`BufferRegion <tvm.tir.BufferRegion>`\\ \\(buffer\\, region\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1 tvm.tir.stmt.BufferRegion:1
#: tvm.tir:1:<autosummary>:1
msgid "BufferRegion node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`BufferStore <tvm.tir.BufferStore>`\\ \\(buffer\\, value\\, "
"indices\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.BufferStore:1 tvm.tir:1:<autosummary>:1
msgid "Buffer store node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Call <tvm.tir.Call>`\\ \\(dtype\\, op\\, args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Call:1 tvm.tir:1:<autosummary>:1
msgid "Call node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`CallEffectKind <tvm.tir.CallEffectKind>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.expr.CallEffectKind:1 tvm.tir:1:<autosummary>:1
msgid "Possible kinds of Call effects."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Cast <tvm.tir.Cast>`\\ \\(dtype\\, value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Cast:1 tvm.tir:1:<autosummary>:1
msgid "Cast expression."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`CommReducer <tvm.tir.CommReducer>`\\ \\(lhs\\, rhs\\, result\\, "
"identity\\_element\\)"
msgstr ""

#: of tvm.tir.expr.CommReducer:1 tvm.tir:1:<autosummary>:1
msgid "Commutative reduce operator"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`DataProducer <tvm.tir.DataProducer>`\\ \\(\\)"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`DeclBuffer <tvm.tir.DeclBuffer>`\\ \\(buffer\\, body\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.DeclBuffer:1 tvm.tir:1:<autosummary>:1
msgid "DeclBuffer node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Div <tvm.tir.Div>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Div:1 tvm.tir:1:<autosummary>:1
msgid "Div node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`EQ <tvm.tir.EQ>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.EQ:1 tvm.tir:1:<autosummary>:1
msgid "EQ node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Evaluate <tvm.tir.Evaluate>`\\ \\(value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.Evaluate:1 tvm.tir:1:<autosummary>:1
msgid "Evaluate node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`FloatImm <tvm.tir.FloatImm>`\\ \\(dtype\\, value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.FloatImm:1 tvm.tir:1:<autosummary>:1
msgid "Float constant."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`FloorDiv <tvm.tir.FloorDiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.FloorDiv:1 tvm.tir:1:<autosummary>:1
msgid "FloorDiv node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`FloorMod <tvm.tir.FloorMod>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.FloorMod:1 tvm.tir:1:<autosummary>:1
msgid "FloorMod node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`For <tvm.tir.For>`\\ \\(loop\\_var\\, min\\_val\\, extent\\, "
"kind\\, body\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.stmt.For:1 tvm.tir:1:<autosummary>:1
msgid "For node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`ForKind <tvm.tir.ForKind>`\\ \\(value\\)"
msgstr ""

#: of tvm.tir.stmt.ForKind:1 tvm.tir:1:<autosummary>:1
msgid "The kind of the for loop."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`GE <tvm.tir.GE>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.GE:1 tvm.tir:1:<autosummary>:1
msgid "GE node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`GT <tvm.tir.GT>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.GT:1 tvm.tir:1:<autosummary>:1
msgid "GT node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`IfThenElse <tvm.tir.IfThenElse>`\\ \\(condition\\, "
"then\\_case\\, else\\_case\\)"
msgstr ""

#: of tvm.tir.stmt.IfThenElse:1 tvm.tir:1:<autosummary>:1
msgid "IfThenElse node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`IndexMap <tvm.tir.IndexMap>`\\ \\(initial\\_indices\\, "
"final\\_indices\\, ...\\)"
msgstr ""

#: of tvm.tir.function.IndexMap:1 tvm.tir:1:<autosummary>:1
msgid ""
"A mapping from multi-dimensional indices to another set of multi-"
"dimensional indices"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`IntImm <tvm.tir.IntImm>`\\ \\(dtype\\, value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.IntImm:1 tvm.tir:1:<autosummary>:1
msgid "Int constant."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`IterVar <tvm.tir.IterVar>`\\ \\(dom\\, var\\, iter\\_type\\[\\, "
"thread\\_tag\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.IterVar:1 tvm.tir:1:<autosummary>:1
msgid "Represent iteration variable."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`LE <tvm.tir.LE>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.LE:1 tvm.tir:1:<autosummary>:1
msgid "LE node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`LT <tvm.tir.LT>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.LT:1 tvm.tir:1:<autosummary>:1
msgid "LT node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Layout <tvm.tir.Layout>`\\ \\(\\)"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
"Layout is composed of upper cases, lower cases and numbers, where upper "
"case indicates a primal axis and the corresponding lower case with factor"
" size indicates the subordinate axis."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Let <tvm.tir.Let>`\\ \\(var\\, value\\, body\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Let:1 tvm.tir:1:<autosummary>:1
msgid "Let node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`LetStmt <tvm.tir.LetStmt>`\\ \\(var\\, value\\, body\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.LetStmt:1 tvm.tir:1:<autosummary>:1
msgid "LetStmt node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`MatchBufferRegion <tvm.tir.MatchBufferRegion>`\\ \\(buffer\\, "
"source\\)"
msgstr ""

#: of tvm.tir.stmt.MatchBufferRegion:1 tvm.tir:1:<autosummary>:1
msgid "MatchBufferRegion node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Max <tvm.tir.Max>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Max:1 tvm.tir:1:<autosummary>:1
msgid "Max node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Min <tvm.tir.Min>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Min:1 tvm.tir:1:<autosummary>:1
msgid "Min node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Mod <tvm.tir.Mod>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Mod:1 tvm.tir:1:<autosummary>:1
msgid "Mod node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Mul <tvm.tir.Mul>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Mul:1 tvm.tir:1:<autosummary>:1
msgid "Mul node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`NE <tvm.tir.NE>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.NE:1 tvm.tir:1:<autosummary>:1
msgid "NE node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Not <tvm.tir.Not>`\\ \\(a\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Not:1 tvm.tir:1:<autosummary>:1
msgid "Not node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Or <tvm.tir.Or>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Or:1 tvm.tir:1:<autosummary>:1
msgid "Or node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`Prefetch <tvm.tir.Prefetch>`\\ \\(buffer\\, bounds\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.Prefetch:1 tvm.tir:1:<autosummary>:1
msgid "Prefetch node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`PrimFunc <tvm.tir.PrimFunc>`\\ \\(params\\, body\\[\\, "
"ret\\_type\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1 tvm.tir.function.PrimFunc:1
#: tvm.tir:1:<autosummary>:1
msgid "A function declaration expression."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`ProducerLoad <tvm.tir.ProducerLoad>`\\ \\(producer\\, "
"indices\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.ProducerLoad:1 tvm.tir:1:<autosummary>:1
msgid "Producer load node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`ProducerRealize <tvm.tir.ProducerRealize>`\\ \\(producer\\, "
"bounds\\, condition\\, ...\\)"
msgstr ""

#: of tvm.tir.stmt.ProducerRealize:1 tvm.tir:1:<autosummary>:1
msgid "ProducerRealize node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`ProducerStore <tvm.tir.ProducerStore>`\\ \\(producer\\, value\\,"
" indices\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.ProducerStore:1 tvm.tir:1:<autosummary>:1
msgid "ProducerStore node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`Ramp <tvm.tir.Ramp>`\\ \\(base\\, stride\\, lanes\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Ramp:1 tvm.tir:1:<autosummary>:1
msgid "Ramp node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`Reduce <tvm.tir.Reduce>`\\ \\(combiner\\, src\\, rdom\\, "
"condition\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Reduce:1 tvm.tir:1:<autosummary>:1
msgid "Reduce node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`Schedule <tvm.tir.Schedule>`\\ \\(mod\\, \\*\\[\\, seed\\, "
"debug\\_mask\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1 tvm.tir:1:<autosummary>:1
msgid "The user-facing schedule class"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`ScheduleState <tvm.tir.ScheduleState>`\\ \\(mod\\, \\*\\[\\, "
"debug\\_mask\\, enable\\_check\\]\\)"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:1 tvm.tir:1:<autosummary>:1
msgid ""
"The state of scheduling, which exposes a `Replace` method as the primary "
"resort for all the scheduling primitives to manipulate the TensorIR."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`Select <tvm.tir.Select>`\\ \\(condition\\, true\\_value\\, "
"false\\_value\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.expr.Select:1 tvm.tir:1:<autosummary>:1
msgid "Select node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`SeqStmt <tvm.tir.SeqStmt>`\\ \\(seq\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.SeqStmt:1 tvm.tir:1:<autosummary>:1
msgid "Sequence of statements."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ""
":py:obj:`Shuffle <tvm.tir.Shuffle>`\\ \\(vectors\\, indices\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Shuffle:1 tvm.tir:1:<autosummary>:1
msgid "Shuffle node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`SizeVar <tvm.tir.SizeVar>`\\ \\(name\\, dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.SizeVar:2 tvm.tir:1:<autosummary>:1
msgid "Symbolic variable to represent a tensor index size"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Stmt <tvm.tir.Stmt>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1 tvm.tir.stmt.Stmt:1
#: tvm.tir:1:<autosummary>:1
msgid "Base class of all the statements."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`StmtSRef <tvm.tir.StmtSRef>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.block_scope.StmtSRef:1 tvm.tir:1:<autosummary>:1
msgid ""
"An object that refers to schedulable elements in the TensorIR, aka "
"\"sref\"."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`StringImm <tvm.tir.StringImm>`\\ \\(value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.StringImm:1 tvm.tir:1:<autosummary>:1
msgid "String constant."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Sub <tvm.tir.Sub>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Sub:1 tvm.tir:1:<autosummary>:1
msgid "Sub node."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`TensorIntrin <tvm.tir.TensorIntrin>`\\ \\(desc\\, impl\\)"
msgstr ""

#: of tvm.tir.function.TensorIntrin:1 tvm.tir:1:<autosummary>:1
msgid "A tensor intrinsic."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`Var <tvm.tir.Var>`\\ \\(name\\, dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1 tvm.tir.expr.Var:1
#: tvm.tir:1:<autosummary>:1
msgid "Symbolic variable."
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`While <tvm.tir.While>`\\ \\(condition\\, body\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.While:1 tvm.tir:1:<autosummary>:1
msgid "While node."
msgstr ""

#: of tvm.tir:1
msgid "**Exceptions:**"
msgstr ""

#: of tvm.tir:1:<autosummary>:1
msgid ":py:obj:`ScheduleError <tvm.tir.ScheduleError>`\\"
msgstr ""

#: of tvm.tir.schedule.schedule.ScheduleError:1 tvm.tir:1:<autosummary>:1
msgid "Error that happens during TensorIR scheduling."
msgstr ""

#: of tvm.tir:1 tvm.tir.analysis:1 tvm.tir.stmt_functor:1 tvm.tir.transform:1
msgid "**Functions:**"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`TVMBackendAllocWorkspace <tvm.tir.TVMBackendAllocWorkspace>`\\ "
"\\(device\\_type\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.TVMBackendAllocWorkspace:1
msgid "Backend function to allocate temporal workspace"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`TVMBackendFreeWorkspace <tvm.tir.TVMBackendFreeWorkspace>`\\ "
"\\(device\\_type\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.TVMBackendFreeWorkspace:1
msgid "Backend function to free temporal workspace."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`abs <tvm.tir.abs>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.abs:1
msgid "Get absolute value of the input element-wise."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`acos <tvm.tir.acos>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.acos:1 tvm.tir.op.acosh:1
msgid "Take acos of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`acosh <tvm.tir.acosh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`add <tvm.tir.add>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.generic.add:1
msgid "Generic add operator."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`address_of <tvm.tir.address_of>`\\ \\(buffer\\_load\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.address_of:1
msgid "Returns the address of an element in the buffer"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`all <tvm.tir.all>`\\ \\(\\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.all:2
msgid "Create a new expression of the intersection of all conditions in the"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`any <tvm.tir.any>`\\ \\(\\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.any:1
msgid "Create a new experssion of the union of all conditions in the arguments"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`asin <tvm.tir.asin>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.asin:1
msgid "Take asin of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`asinh <tvm.tir.asinh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.asinh:1
msgid "Take asinh of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`assume <tvm.tir.assume>`\\ \\(\\[cond\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.assume:1
msgid "Provide a true statement that can be used for simplifications"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`atan <tvm.tir.atan>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.atan:1
msgid "Take atan of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`atan2 <tvm.tir.atan2>`\\ \\(x1\\, x2\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.atan2:1
msgid "Take arctan2(x1, x2)."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`atanh <tvm.tir.atanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.atanh:1
msgid "Take atanh of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`bijective_layout <tvm.tir.bijective_layout>`\\ "
"\\(src\\_layout\\, dst\\_layout\\)"
msgstr ""

#: of tvm.tir.data_layout.bijective_layout:1 tvm.tir.expr.Add:1:<autosummary>:1
msgid "Create a bijective layout mapping."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`bitwise_and <tvm.tir.bitwise_and>`\\ \\(x\\, y\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.bitwise_and:1
msgid "Take bitwise and of two values"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`bitwise_not <tvm.tir.bitwise_not>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.bitwise_not:1
msgid "Take bitwise not of input value"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`bitwise_or <tvm.tir.bitwise_or>`\\ \\(x\\, y\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.bitwise_or:1
msgid "Take bitwise or of two values"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`bitwise_xor <tvm.tir.bitwise_xor>`\\ \\(x\\, y\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.bitwise_xor:1
msgid "Take bitwise xor of two values"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`call_cpacked <tvm.tir.call_cpacked>`\\ \\(\\*args\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.call_cpacked:1
#: tvm.tir.op.call_packed:1
msgid "Build expression by call an external packed function."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`call_cpacked_lowered <tvm.tir.call_cpacked_lowered>`\\ "
"\\(\\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid "Lowered version of call c-packed."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`call_extern <tvm.tir.call_extern>`\\ \\(dtype\\, func\\_name\\, "
"\\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.call_extern:1
msgid "Build expression by calling a extern function."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`call_intrin <tvm.tir.call_intrin>`\\ \\(dtype\\, func\\_name\\, "
"\\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.call_intrin:1
msgid "Build expression by calling an intrinsic function."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`call_llvm_intrin <tvm.tir.call_llvm_intrin>`\\ \\(dtype\\, "
"name\\, \\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.call_llvm_intrin:1
msgid "Build expression by calling a llvm intrinsic function"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`call_llvm_pure_intrin <tvm.tir.call_llvm_pure_intrin>`\\ "
"\\(dtype\\, name\\, \\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.call_llvm_pure_intrin:1
msgid "Build expression by calling a pure llvm intrinsic function"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`call_packed <tvm.tir.call_packed>`\\ \\(\\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`call_packed_lowered <tvm.tir.call_packed_lowered>`\\ "
"\\(\\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid "Lowered version of call packed."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`call_pure_extern <tvm.tir.call_pure_extern>`\\ \\(dtype\\, "
"func\\_name\\, \\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.call_pure_extern:1
msgid "Build expression by calling a pure extern function."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`call_tir <tvm.tir.call_tir>`\\ \\(global\\_var\\, \\*args\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.call_tir:1
msgid "Performs a call into another PrimFunc in the same IRModule"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`ceil <tvm.tir.ceil>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ceil:1
msgid "Take ceil of float input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`ceildiv <tvm.tir.ceildiv>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ceildiv:1
msgid "Generic ceildiv operator."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`clz <tvm.tir.clz>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.clz:1
msgid "Count leading zero bits of an integer x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`comm_reducer <tvm.tir.comm_reducer>`\\ \\(fcombine\\, "
"fidentity\\[\\, name\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.comm_reducer:1
msgid "Create a commutative reducer for reduction."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`copysign <tvm.tir.copysign>`\\ \\(x1\\, x2\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.copysign:1
msgid "Change the sign of x1 to that of x2, element-wise."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`cos <tvm.tir.cos>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.cos:1
msgid "Take cos of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`cosh <tvm.tir.cosh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.cosh:1
msgid "Take cosh of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`create_barriers <tvm.tir.create_barriers>`\\ "
"\\(barrier\\_count\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.create_barriers:1
msgid "TVM intrinsic to create N barriers"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`decl_buffer <tvm.tir.decl_buffer>`\\ \\(shape\\[\\, dtype\\, "
"name\\, data\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:1 tvm.tir.expr.Add:1:<autosummary>:1
msgid "Declare a new symbolic buffer."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`div <tvm.tir.div>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.div:1
msgid "Compute a / b as in C/C++ semantics."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`end_profile_intrinsic <tvm.tir.end_profile_intrinsic>`\\ \\(id\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
"End profile intrinsic. Parameters ---------- id : int     The intrinsic "
"id. Returns ------- call : PrimExpr     The call expression."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`erf <tvm.tir.erf>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.erf:1
msgid "Take gauss error function of the input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`exp <tvm.tir.exp>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.exp:1
msgid "Take exponential of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`exp10 <tvm.tir.exp10>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.exp10:1
msgid "Calculate 10**x"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`exp2 <tvm.tir.exp2>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.exp2:1
msgid "Calculate 2**x"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`floor <tvm.tir.floor>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.floor:1
msgid "Take floor of float input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`floordiv <tvm.tir.floordiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.floordiv:1
msgid "Compute the floordiv of two expressions."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`floormod <tvm.tir.floormod>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.floormod:1
msgid "Compute the floormod of two expressions."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`fmod <tvm.tir.fmod>`\\ \\(x\\, y\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.fmod:1
msgid "Return the remainder of x divided by y with the same sign as x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`hypot <tvm.tir.hypot>`\\ \\(x1\\, x2\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.hypot:1
msgid "Equivalent to sqrt(x1**2 + x2**2), element-wise."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`if_then_else <tvm.tir.if_then_else>`\\ \\(cond\\, t\\, f\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.if_then_else:1
msgid "Conditional selection expression."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`indexdiv <tvm.tir.indexdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.indexdiv:1
msgid "Compute floor(a / b) where a and b are non-negative."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`indexmod <tvm.tir.indexmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid "Compute the remainder of indexdiv."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`infinity <tvm.tir.infinity>`\\ \\(dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.infinity:1
#: tvm.tir.op.reinterpret:1
msgid "infinity value of dtype"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`isfinite <tvm.tir.isfinite>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.isfinite:1
msgid "Check if input value is finite."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`isinf <tvm.tir.isinf>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.isinf:1
msgid "Check if input value is infinite."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`isnan <tvm.tir.isnan>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.isnan:1
msgid "Check if input value is Nan."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`isnullptr <tvm.tir.isnullptr>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.isnullptr:1
msgid "Check if input value is nullptr."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`layout <tvm.tir.layout>`\\ \\(layout\\_str\\[\\, dtype\\]\\)"
msgstr ""

#: of tvm.tir.data_layout.layout:1 tvm.tir.expr.Add:1:<autosummary>:1
msgid "Create a layout node from a string."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`ldexp <tvm.tir.ldexp>`\\ \\(x1\\, x2\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ldexp:1
msgid "Returns x1 * (2 ** x2)."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`likely <tvm.tir.likely>`\\ \\(cond\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.likely:1
msgid "Mark condition as likely."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`log <tvm.tir.log>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.log:1
msgid "Take log of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`log10 <tvm.tir.log10>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.log10:1
msgid "Take log10 of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`log1p <tvm.tir.log1p>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.log1p:1
msgid "Take log(x + 1) with respect to input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`log2 <tvm.tir.log2>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.log2:1
msgid "Take log2 of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`lookup_param <tvm.tir.lookup_param>`\\ \\(param\\_name\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.lookup_param:1
msgid "Returns the param by name"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`max <tvm.tir.max>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
#: tvm.tir.op.comm_reducer.<locals>.reducer:1
msgid "Create a max expression over axis."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`max_value <tvm.tir.max_value>`\\ \\(dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.max_value:1
msgid "maximum value of dtype"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`min <tvm.tir.min>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
#: tvm.tir.op.comm_reducer.<locals>.reducer:1
msgid "Create a min expression over axis."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`min_value <tvm.tir.min_value>`\\ \\(dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.min_value:1
msgid "minimum value of dtype"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`mma_fill <tvm.tir.mma_fill>`\\ \\(dtype\\, local\\_size\\, "
"local\\_ptr\\, offset\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.mma_fill:1
msgid "TVM intrinsic for zero-initalizing an MMA accumulation registor"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`mma_store <tvm.tir.mma_store>`\\ \\(dtype\\, m\\, n\\, "
"dst\\_ptr\\, src\\_ptr\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.mma_store:1
msgid "TVM intrinsic for storing the result of PTX MMA into a destination pointer"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`multiply <tvm.tir.multiply>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.generic.multiply:1
msgid "Generic multiply operator."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`nearbyint <tvm.tir.nearbyint>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.round:1
msgid "Round elements of the array to the nearest integer."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`nextafter <tvm.tir.nextafter>`\\ \\(x1\\, x2\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.nextafter:1
msgid "Return the next floating-point value after x1 towards x2."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`popcount <tvm.tir.popcount>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.popcount:1
msgid "Count the number of set bits in input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`pow <tvm.tir.pow>`\\ \\(x\\, y\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.pow:1 tvm.tir.op.power:1
msgid "x power y"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`power <tvm.tir.power>`\\ \\(x\\, y\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`ptx_arrive_barrier <tvm.tir.ptx_arrive_barrier>`\\ "
"\\(barrier\\_id\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_arrive_barrier:1
msgid ""
"TVM intrinsic for ptx barrier arrival using mbarrier.arrive "
"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html"
"#parallel-synchronization-and-communication-instructions-mbarrier-arrive"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`ptx_arrive_barrier_expect_tx "
"<tvm.tir.ptx_arrive_barrier_expect_tx>`\\ \\(barrier\\_id\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
#: tvm.tir.op.ptx_arrive_barrier_expect_tx:1
msgid ""
"TVM intrinsic for ptx barrier arrival with expect tx using "
"mbarrier.arrive.expect_tx https://docs.nvidia.com/cuda/parallel-thread-"
"execution/index.html#parallel-synchronization-and-communication-"
"instructions-mbarrier-arrive https://docs.nvidia.com/cuda/parallel-"
"thread-execution/index.html#parallel-synchronization-and-communication-"
"instructions-mbarrier-expect-tx-operation"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`ptx_commit_group <tvm.tir.ptx_commit_group>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_commit_group:1
msgid ""
"TVM intrinsic for ptx async copy commit https://docs.nvidia.com/cuda"
"/parallel-thread-execution/index.html#data-movement-and-conversion-"
"instructions-cp-async-commit-group"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`ptx_cp_async <tvm.tir.ptx_cp_async>`\\ \\(dtype\\, "
"shared\\_ptr\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_cp_async:1
msgid ""
"TVM intrinsic for ptx async copy from global to shared memory using "
"cp.async https://docs.nvidia.com/cuda/parallel-thread-"
"execution/index.html#data-movement-and-conversion-instructions-cp-async"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`ptx_cp_async_barrier <tvm.tir.ptx_cp_async_barrier>`\\ "
"\\(barrier\\_id\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_cp_async_barrier:1
msgid ""
"TVM intrinsic for ptx async copy barrier using cp.async.mbarrier.arrive "
"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html"
"#parallel-synchronization-and-communication-instructions-cp-async-"
"mbarrier-arrive"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`ptx_cp_async_bulk <tvm.tir.ptx_cp_async_bulk>`\\ \\(dtype\\, "
"shared\\_ptr\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_cp_async_bulk:1
msgid ""
"TVM intrinsic for ptx async copy from global to shared memory using "
"cp.async.bulk https://docs.nvidia.com/cuda/parallel-thread-"
"execution/index.html#data-movement-and-conversion-instructions-cp-async-"
"bulk"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`ptx_init_barrier_thread_count "
"<tvm.tir.ptx_init_barrier_thread_count>`\\ \\(barrier\\_id\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
#: tvm.tir.op.ptx_init_barrier_thread_count:1
msgid ""
"TVM intrinsic for ptx barrier initialization of thread count using "
"mbarrier.init https://docs.nvidia.com/cuda/parallel-thread-"
"execution/index.html#parallel-synchronization-and-communication-"
"instructions-mbarrier-init"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`ptx_ldmatrix <tvm.tir.ptx_ldmatrix>`\\ \\(dtype\\, trans\\, "
"num\\, type\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_ldmatrix:1
msgid ""
"TVM intrinsic for ptx load matrix from shared memory "
"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-"
"level-matrix-instructions-ldmatrix"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`ptx_mma <tvm.tir.ptx_mma>`\\ \\(dtype\\, shape\\, A\\_layout\\, "
"B\\_layout\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_mma:1
msgid ""
"TVM intrinsic for ptx tensor core mma instructions "
"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-"
"level-matrix-instructions-for-mma"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`ptx_mma_sp <tvm.tir.ptx_mma_sp>`\\ \\(dtype\\, shape\\, "
"A\\_layout\\, B\\_layout\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_mma_sp:1
msgid ""
"TVM intrinsic for sparse tensor core ptx instructions "
"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-"
"level-matrix-instructions-for-sparse-mma"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`ptx_wait_barrier <tvm.tir.ptx_wait_barrier>`\\ \\(barrier\\_id\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_wait_barrier:1
msgid ""
"TVM intrinsic for ptx barrier wait using mbarrier.try_wait "
"https://docs.nvidia.com/cuda/parallel-thread-execution/index.html"
"#parallel-synchronization-and-communication-instructions-mbarrier-test-"
"wait-mbarrier-try-wait"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`ptx_wait_group <tvm.tir.ptx_wait_group>`\\ \\(num\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ptx_wait_group:1
msgid ""
"TVM intrinsic for ptx async copy wait https://docs.nvidia.com/cuda"
"/parallel-thread-execution/index.html#data-movement-and-conversion-"
"instructions-cp-async-wait-group"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`q_multiply_shift <tvm.tir.q_multiply_shift>`\\ \\(x\\, y\\, q\\,"
" s\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
"Execute a multiplication between two Q-numbers x and y followed by a "
"right shift s."
msgstr ""
"执行两个 Q-numbers x 和 y 之间的乘法运算，然后进行右移 s 位。数学表达式为（$(x * y) >> s$）"

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`q_multiply_shift_per_axis <tvm.tir.q_multiply_shift_per_axis>`\\"
" \\(x\\, y\\, ls\\, rs\\, q\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.q_multiply_shift_per_axis:1
msgid "Execute a multiplication between two Q-numbers x and y"
msgstr "执行两个 Q-numbers x 和 y 之间的乘法运算"

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`reinterpret <tvm.tir.reinterpret>`\\ \\(dtype\\, value\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`ret <tvm.tir.ret>`\\ \\(val\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.ret:1
msgid "Create a tir return expression"
msgstr "创建 tir 返回表达式"

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`round <tvm.tir.round>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`rsqrt <tvm.tir.rsqrt>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.rsqrt:1
msgid "Take reciprocal of square root of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`shift_left <tvm.tir.shift_left>`\\ \\(x\\, y\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.shift_left:1
msgid "Return the result of x left shifted by y bits."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`shift_right <tvm.tir.shift_right>`\\ \\(x\\, y\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.shift_right:1
msgid "Return the result of x right shifted by y bits."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`sigmoid <tvm.tir.sigmoid>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.sigmoid:1
msgid "Quick function to get sigmoid"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`sin <tvm.tir.sin>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.sin:1
msgid "Take sin of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`sinh <tvm.tir.sinh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.sinh:1
msgid "Take sinh of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`sqrt <tvm.tir.sqrt>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.sqrt:1
msgid "Take square root of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`start_profile_intrinsic <tvm.tir.start_profile_intrinsic>`\\ "
"\\(id\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
"Start profile intrinsic. Parameters ---------- id : int     The intrinsic"
" id. Returns ------- call : PrimExpr     The call expression."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`stmt_list <tvm.tir.stmt_list>`\\ \\(stmt\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.stmt.stmt_list:1
msgid "Make list of stmt from blocks."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`stmt_seq <tvm.tir.stmt_seq>`\\ \\(\\*args\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.stmt.stmt_seq:1
msgid "Make sequence of statements"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`subtract <tvm.tir.subtract>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.generic.subtract:1
msgid "Generic subtract operator."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`sum <tvm.tir.sum>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
#: tvm.tir.op.comm_reducer.<locals>.reducer:1
msgid "Create a sum expression over axis."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`tan <tvm.tir.tan>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tan:1
msgid "Take tan of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`tanh <tvm.tir.tanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tanh:1
msgid "Take hyperbolic tanh of input x."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`trace <tvm.tir.trace>`\\ \\(args\\[\\, trace\\_action\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.trace:1
msgid "Trace tensor data at the runtime."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`trunc <tvm.tir.trunc>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.trunc:1
msgid "Get truncated value of the input."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`truncdiv <tvm.tir.truncdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.truncdiv:1
msgid "Compute the truncdiv of two expressions."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`truncmod <tvm.tir.truncmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.truncmod:1
msgid "Compute the truncmod of two expressions."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_access_ptr <tvm.tir.tvm_access_ptr>`\\ \\(ptype\\, data\\, "
"offset\\, extent\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_access_ptr:1
msgid "Get head access address with memory access pattern info"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_bmma_sync <tvm.tir.tvm_bmma_sync>`\\ \\(fragment\\_d\\, "
"index\\_d\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_bmma_sync:1
msgid "TVM intrinsic for tensor core bmma_sync operators"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_check_return <tvm.tir.tvm_check_return>`\\ \\(expected\\, "
"...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
"Return new on stack dtype[num] Parameters ---------- expected : int     "
"The expected return code. return_unexpected : int     The unexpected "
"return code. nested_call : PrimExpr     The call expression to check "
"return. Returns ------- call : PrimExpr     The call expression."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_fill_fragment <tvm.tir.tvm_fill_fragment>`\\ \\(fragment\\, "
"m\\, n\\, k\\, index\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_fill_fragment:1
msgid "TVM intrinsic for tensor core fill_fragment operators"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_load_matrix_sync <tvm.tir.tvm_load_matrix_sync>`\\ "
"\\(fragment\\, m\\, n\\, k\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_load_matrix_sync:1
msgid "TVM intrinsic for tensor core load operators"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_mma_sync <tvm.tir.tvm_mma_sync>`\\ \\(fragment\\_d\\, "
"index\\_d\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_mma_sync:1
msgid "TVM intrinsic for tensor core mma_sync operators"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_stack_alloca <tvm.tir.tvm_stack_alloca>`\\ \\(dtype\\_str\\,"
" num\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_stack_alloca:1
msgid "Return new on stack dtype[num]"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_stack_make_array <tvm.tir.tvm_stack_make_array>`\\ "
"\\(data\\, shape\\, strides\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_stack_make_array:1
msgid "Allocate a NDArray(DLTensor) on stack, return the handle"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_stack_make_shape <tvm.tir.tvm_stack_make_shape>`\\ "
"\\(\\*args\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_stack_make_shape:1
msgid "Allocate a shape tuple on stack, return the handle"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_store_matrix_sync <tvm.tir.tvm_store_matrix_sync>`\\ "
"\\(fragment\\, m\\, n\\, k\\, ...\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_store_matrix_sync:1
msgid "TVM intrinsic for tensor core store operators"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_struct_get <tvm.tir.tvm_struct_get>`\\ \\(arr\\, index\\, "
"field\\, dtype\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_struct_get:1
msgid "Get struct field value in array"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_struct_set <tvm.tir.tvm_struct_set>`\\ \\(arr\\, index\\, "
"field\\, value\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_struct_set:1
msgid "Set value in struct field in array"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`tvm_thread_allreduce <tvm.tir.tvm_thread_allreduce>`\\ "
"\\(\\*freduce\\_args\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_thread_allreduce:1
msgid "Perform allreduce inside threadblock."
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`tvm_throw_last_error <tvm.tir.tvm_throw_last_error>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_throw_last_error:1
msgid "Throw TVMGetLastError()"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`tvm_tuple <tvm.tir.tvm_tuple>`\\ \\(\\*value\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.tvm_tuple:1
msgid "Create a tuple structure in value field of AttrStmt"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`type_annotation <tvm.tir.type_annotation>`\\ \\(dtype\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.type_annotation:1
msgid "Create a type annotation expression"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`undef <tvm.tir.undef>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.undef:1
msgid "Returns an initialized but arbitrary value"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ""
":py:obj:`vectorcombine <tvm.tir.vectorcombine>`\\ \\(dtype\\, vec1\\, "
"vec2\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.vectorcombine:1
msgid "Concat two vectors"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`vectorhigh <tvm.tir.vectorhigh>`\\ \\(dtype\\, vec\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.vectorhigh:1
msgid "Get the high level half of the vector"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1
msgid ":py:obj:`vectorlow <tvm.tir.vectorlow>`\\ \\(dtype\\, vec\\)"
msgstr ""

#: of tvm.tir.expr.Add:1:<autosummary>:1 tvm.tir.op.vectorlow:1
msgid "Get the low level half of the vector"
msgstr ""

#: of tvm.ir.module.IRModule:6 tvm.ir.module.IRModule.__getitem__:4
#: tvm.ir.module.IRModule.__setitem__:4 tvm.ir.module.IRModule.astext:4
#: tvm.ir.module.IRModule.from_expr:4 tvm.ir.module.IRModule.get_attr:4
#: tvm.ir.module.IRModule.get_constructor:4
#: tvm.ir.module.IRModule.get_global_type_var:4
#: tvm.ir.module.IRModule.get_global_var:4 tvm.ir.module.IRModule.update:4
#: tvm.ir.module.IRModule.update_func:5
#: tvm.ir.module.IRModule.update_global_info:4
#: tvm.ir.module.IRModule.with_attr:4
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:12
#: tvm.tir.analysis.analysis.calculate_allocated_bytes:4
#: tvm.tir.analysis.analysis.calculate_constant_bytes:5
#: tvm.tir.analysis.analysis.calculate_workspace_bytes:5
#: tvm.tir.analysis.analysis.detect_buffer_access_lca:7
#: tvm.tir.analysis.analysis.estimate_tir_flops:4
#: tvm.tir.analysis.analysis.expr_deep_equal:4
#: tvm.tir.analysis.analysis.find_anchor_block:16
#: tvm.tir.analysis.analysis.get_block_access_region:5
#: tvm.tir.analysis.analysis.get_block_read_write_region:5
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:8
#: tvm.tir.analysis.analysis.undefined_vars:4
#: tvm.tir.analysis.analysis.verify_gpu_code:4
#: tvm.tir.analysis.analysis.verify_memory:4
#: tvm.tir.analysis.analysis.verify_ssa:4
#: tvm.tir.analysis.analysis.verify_well_formed:5
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:4
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_sref:4
#: tvm.tir.block_scope.BlockScope.get_deps_by_dst:4
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:4
#: tvm.tir.buffer.Buffer.access_ptr:7 tvm.tir.buffer.Buffer.offset_of:4
#: tvm.tir.buffer.Buffer.vload:4 tvm.tir.buffer.Buffer.vstore:4
#: tvm.tir.buffer.decl_buffer:9 tvm.tir.data_layout.BijectiveLayout:8
#: tvm.tir.data_layout.BijectiveLayout.backward_index:4
#: tvm.tir.data_layout.BijectiveLayout.backward_shape:4
#: tvm.tir.data_layout.BijectiveLayout.forward_index:4
#: tvm.tir.data_layout.BijectiveLayout.forward_shape:4
#: tvm.tir.data_layout.Layout.factor_of:4 tvm.tir.data_layout.Layout.index_of:4
#: tvm.tir.data_layout.bijective_layout:4 tvm.tir.data_layout.layout:4
#: tvm.tir.expr.Add:4 tvm.tir.expr.And:4 tvm.tir.expr.Broadcast:4
#: tvm.tir.expr.BufferLoad:4 tvm.tir.expr.Call:4 tvm.tir.expr.Cast:4
#: tvm.tir.expr.CommReducer:4 tvm.tir.expr.Div:4 tvm.tir.expr.EQ:4
#: tvm.tir.expr.FloatImm:4 tvm.tir.expr.FloorDiv:4 tvm.tir.expr.FloorMod:4
#: tvm.tir.expr.GE:4 tvm.tir.expr.GT:4 tvm.tir.expr.IntImm:4
#: tvm.tir.expr.IterVar:6 tvm.tir.expr.LE:4 tvm.tir.expr.LT:4
#: tvm.tir.expr.Let:4 tvm.tir.expr.Max:4 tvm.tir.expr.Min:4 tvm.tir.expr.Mod:4
#: tvm.tir.expr.Mul:4 tvm.tir.expr.NE:4 tvm.tir.expr.Not:4 tvm.tir.expr.Or:4
#: tvm.tir.expr.ProducerLoad:4 tvm.tir.expr.Ramp:4 tvm.tir.expr.Reduce:4
#: tvm.tir.expr.Select:11 tvm.tir.expr.Shuffle:4 tvm.tir.expr.SizeVar:5
#: tvm.tir.expr.StringImm:4 tvm.tir.expr.Sub:4 tvm.tir.expr.Var:4
#: tvm.tir.function.IndexMap:4 tvm.tir.function.IndexMap.from_func:4
#: tvm.tir.function.IndexMap.from_func_with_separators:4
#: tvm.tir.function.IndexMap.inverse:6
#: tvm.tir.function.IndexMap.is_equivalent_to:4
#: tvm.tir.function.IndexMap.map_indices:4
#: tvm.tir.function.IndexMap.map_ndarray:4
#: tvm.tir.function.IndexMap.map_shape:4
#: tvm.tir.function.IndexMap.non_surjective_inverse:6
#: tvm.tir.function.PrimFunc:4 tvm.tir.function.PrimFunc.specialize:4
#: tvm.tir.function.PrimFunc.with_body:4 tvm.tir.function.TensorIntrin:4
#: tvm.tir.function.TensorIntrin.get:4 tvm.tir.function.TensorIntrin.register:4
#: tvm.tir.generic.add:4 tvm.tir.generic.multiply:4 tvm.tir.generic.subtract:4
#: tvm.tir.op.TVMBackendAllocWorkspace:4 tvm.tir.op.TVMBackendFreeWorkspace:4
#: tvm.tir.op.abs:4 tvm.tir.op.acos:4 tvm.tir.op.acosh:4
#: tvm.tir.op.address_of:4 tvm.tir.op.all:5 tvm.tir.op.any:4 tvm.tir.op.asin:4
#: tvm.tir.op.asinh:4 tvm.tir.op.assume:4 tvm.tir.op.atan:4 tvm.tir.op.atan2:4
#: tvm.tir.op.atanh:4 tvm.tir.op.bitwise_and:4 tvm.tir.op.bitwise_not:4
#: tvm.tir.op.bitwise_or:4 tvm.tir.op.bitwise_xor:4 tvm.tir.op.call_cpacked:7
#: tvm.tir.op.call_cpacked_lowered:6 tvm.tir.op.call_extern:4
#: tvm.tir.op.call_intrin:7 tvm.tir.op.call_llvm_intrin:4
#: tvm.tir.op.call_llvm_pure_intrin:4 tvm.tir.op.call_packed:11
#: tvm.tir.op.call_packed_lowered:9 tvm.tir.op.call_pure_extern:4
#: tvm.tir.op.ceil:4 tvm.tir.op.ceildiv:4 tvm.tir.op.clz:4
#: tvm.tir.op.comm_reducer:4 tvm.tir.op.comm_reducer.<locals>.reducer:4
#: tvm.tir.op.copysign:4 tvm.tir.op.cos:4 tvm.tir.op.cosh:4
#: tvm.tir.op.create_barriers:4 tvm.tir.op.div:4 tvm.tir.op.erf:4
#: tvm.tir.op.exp:4 tvm.tir.op.exp10:4 tvm.tir.op.exp2:4 tvm.tir.op.floor:4
#: tvm.tir.op.floordiv:4 tvm.tir.op.floormod:4 tvm.tir.op.fmod:4
#: tvm.tir.op.hypot:4 tvm.tir.op.if_then_else:4 tvm.tir.op.indexdiv:4
#: tvm.tir.op.indexmod:4 tvm.tir.op.infinity:4 tvm.tir.op.isfinite:4
#: tvm.tir.op.isinf:4 tvm.tir.op.isnan:4 tvm.tir.op.isnullptr:4
#: tvm.tir.op.ldexp:4 tvm.tir.op.likely:4 tvm.tir.op.log:4 tvm.tir.op.log10:4
#: tvm.tir.op.log1p:4 tvm.tir.op.log2:4 tvm.tir.op.lookup_param:4
#: tvm.tir.op.max_value:4 tvm.tir.op.min_value:4 tvm.tir.op.mma_fill:4
#: tvm.tir.op.mma_store:4 tvm.tir.op.nearbyint:11 tvm.tir.op.nextafter:4
#: tvm.tir.op.popcount:4 tvm.tir.op.pow:4 tvm.tir.op.power:4
#: tvm.tir.op.ptx_arrive_barrier:5 tvm.tir.op.ptx_arrive_barrier_expect_tx:6
#: tvm.tir.op.ptx_cp_async:5 tvm.tir.op.ptx_cp_async_barrier:5
#: tvm.tir.op.ptx_cp_async_bulk:5 tvm.tir.op.ptx_init_barrier_thread_count:5
#: tvm.tir.op.ptx_ldmatrix:5 tvm.tir.op.ptx_mma:5 tvm.tir.op.ptx_mma_sp:5
#: tvm.tir.op.ptx_wait_barrier:5 tvm.tir.op.ptx_wait_group:5
#: tvm.tir.op.q_multiply_shift:11 tvm.tir.op.q_multiply_shift_per_axis:4
#: tvm.tir.op.reinterpret:4 tvm.tir.op.ret:4 tvm.tir.op.round:4
#: tvm.tir.op.rsqrt:4 tvm.tir.op.shift_left:4 tvm.tir.op.shift_right:4
#: tvm.tir.op.sigmoid:4 tvm.tir.op.sin:4 tvm.tir.op.sinh:4 tvm.tir.op.sqrt:4
#: tvm.tir.op.tan:4 tvm.tir.op.tanh:4 tvm.tir.op.trace:9 tvm.tir.op.trunc:7
#: tvm.tir.op.truncdiv:4 tvm.tir.op.truncmod:4 tvm.tir.op.tvm_access_ptr:4
#: tvm.tir.op.tvm_bmma_sync:4 tvm.tir.op.tvm_fill_fragment:4
#: tvm.tir.op.tvm_load_matrix_sync:4 tvm.tir.op.tvm_mma_sync:4
#: tvm.tir.op.tvm_stack_alloca:4 tvm.tir.op.tvm_stack_make_array:4
#: tvm.tir.op.tvm_stack_make_shape:4 tvm.tir.op.tvm_store_matrix_sync:4
#: tvm.tir.op.tvm_struct_get:4 tvm.tir.op.tvm_struct_set:4
#: tvm.tir.op.tvm_thread_allreduce:4 tvm.tir.op.tvm_tuple:4
#: tvm.tir.op.type_annotation:4 tvm.tir.op.vectorcombine:4
#: tvm.tir.op.vectorhigh:4 tvm.tir.op.vectorlow:4
#: tvm.tir.schedule.schedule.Schedule.__init__:4
#: tvm.tir.schedule.schedule.Schedule.add_unit_loop:4
#: tvm.tir.schedule.schedule.Schedule.annotate:4
#: tvm.tir.schedule.schedule.Schedule.bind:10
#: tvm.tir.schedule.schedule.Schedule.blockize:4
#: tvm.tir.schedule.schedule.Schedule.cache_index:5
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:6
#: tvm.tir.schedule.schedule.Schedule.cache_read:8
#: tvm.tir.schedule.schedule.Schedule.cache_write:8
#: tvm.tir.schedule.schedule.Schedule.compute_at:19
#: tvm.tir.schedule.schedule.Schedule.compute_inline:14
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:18
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:18
#: tvm.tir.schedule.schedule.Schedule.fuse:8
#: tvm.tir.schedule.schedule.Schedule.get:9
#: tvm.tir.schedule.schedule.Schedule.get_block:8
#: tvm.tir.schedule.schedule.Schedule.get_child_blocks:4
#: tvm.tir.schedule.schedule.Schedule.get_consumers:4
#: tvm.tir.schedule.schedule.Schedule.get_loops:4
#: tvm.tir.schedule.schedule.Schedule.get_output_blocks:6
#: tvm.tir.schedule.schedule.Schedule.get_producers:4
#: tvm.tir.schedule.schedule.Schedule.get_sref:8
#: tvm.tir.schedule.schedule.Schedule.merge:8
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:12
#: tvm.tir.schedule.schedule.Schedule.parallel:9
#: tvm.tir.schedule.schedule.Schedule.reindex:8
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:13
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:13
#: tvm.tir.schedule.schedule.Schedule.remove_rv:4
#: tvm.tir.schedule.schedule.Schedule.reorder:12
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:4
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:16
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:17
#: tvm.tir.schedule.schedule.Schedule.rfactor:61
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:17
#: tvm.tir.schedule.schedule.Schedule.sample_categorical:4
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:4
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:4
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:4
#: tvm.tir.schedule.schedule.Schedule.seed:4
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:5
#: tvm.tir.schedule.schedule.Schedule.set_scope:5
#: tvm.tir.schedule.schedule.Schedule.split:9
#: tvm.tir.schedule.schedule.Schedule.storage_align:7
#: tvm.tir.schedule.schedule.Schedule.tensorize:4
#: tvm.tir.schedule.schedule.Schedule.transform_block_layout:4
#: tvm.tir.schedule.schedule.Schedule.transform_layout:4
#: tvm.tir.schedule.schedule.Schedule.unannotate:4
#: tvm.tir.schedule.schedule.Schedule.unroll:4
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:4
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:8
#: tvm.tir.schedule.schedule.Schedule.vectorize:9
#: tvm.tir.schedule.schedule.Schedule.work_on:11
#: tvm.tir.schedule.state.ScheduleState:13
#: tvm.tir.schedule.state.ScheduleState.__init__:4
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:4
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:4
#: tvm.tir.schedule.state.ScheduleState.get_sref:4
#: tvm.tir.schedule.state.ScheduleState.replace:12 tvm.tir.stmt.Allocate:4
#: tvm.tir.stmt.AllocateConst:4 tvm.tir.stmt.AssertStmt:4
#: tvm.tir.stmt.AttrStmt:4 tvm.tir.stmt.Block:4 tvm.tir.stmt.BlockRealize:4
#: tvm.tir.stmt.BufferRealize:4 tvm.tir.stmt.BufferRegion:4
#: tvm.tir.stmt.BufferStore:4 tvm.tir.stmt.DeclBuffer:4 tvm.tir.stmt.Evaluate:4
#: tvm.tir.stmt.For:4 tvm.tir.stmt.IfThenElse:4 tvm.tir.stmt.LetStmt:4
#: tvm.tir.stmt.MatchBufferRegion:4 tvm.tir.stmt.Prefetch:4
#: tvm.tir.stmt.ProducerRealize:4 tvm.tir.stmt.ProducerStore:4
#: tvm.tir.stmt.SeqStmt:4 tvm.tir.stmt.While:4 tvm.tir.stmt.stmt_list:4
#: tvm.tir.stmt.stmt_seq:4 tvm.tir.stmt_functor.ir_transform:4
#: tvm.tir.stmt_functor.post_order_visit:5
#: tvm.tir.stmt_functor.pre_order_visit:5 tvm.tir.stmt_functor.renew_defs:6
#: tvm.tir.stmt_functor.substitute:4
#: tvm.tir.transform.function_pass.prim_func_pass:8
#: tvm.tir.transform.transform.Apply:6
#: tvm.tir.transform.transform.CompactBufferAllocation:37
#: tvm.tir.transform.transform.FP8ComputeLegalize:4
#: tvm.tir.transform.transform.HoistIfThenElse:4
#: tvm.tir.transform.transform.InjectCopyIntrin:4
#: tvm.tir.transform.transform.LiftAttrScope:4
#: tvm.tir.transform.transform.NarrowDataType:4
#: tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:4
#: tvm.tir.transform.transform.StorageFlatten:5
#: tvm.tir.transform.transform.TextureFlatten:5
#: tvm.tir.transform.transform.ThreadSync:4
#: tvm.tir.transform.transform.VectorizeLoop:4
msgid "Parameters"
msgstr ""

#: of tvm.tir.expr.Add:6 tvm.tir.expr.And:6 tvm.tir.expr.Div:6
#: tvm.tir.expr.EQ:6 tvm.tir.expr.FloorDiv:6 tvm.tir.expr.FloorMod:6
#: tvm.tir.expr.GE:6 tvm.tir.expr.GT:6 tvm.tir.expr.LE:6 tvm.tir.expr.LT:6
#: tvm.tir.expr.Max:6 tvm.tir.expr.Min:6 tvm.tir.expr.Mod:6 tvm.tir.expr.Mul:6
#: tvm.tir.expr.NE:6 tvm.tir.expr.Not:6 tvm.tir.expr.Or:6 tvm.tir.expr.Sub:6
#: tvm.tir.op.div:6 tvm.tir.op.floordiv:6 tvm.tir.op.floormod:6
#: tvm.tir.op.indexdiv:6 tvm.tir.op.indexmod:6 tvm.tir.op.truncdiv:6
#: tvm.tir.op.truncmod:6
msgid "a"
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:-1 tvm.tir.expr.Add:-1
#: tvm.tir.expr.And:-1 tvm.tir.expr.Broadcast:-1 tvm.tir.expr.Cast:-1
#: tvm.tir.expr.Div:-1 tvm.tir.expr.EQ:-1 tvm.tir.expr.FloorDiv:-1
#: tvm.tir.expr.FloorMod:-1 tvm.tir.expr.GE:-1 tvm.tir.expr.GT:-1
#: tvm.tir.expr.LE:-1 tvm.tir.expr.LT:-1 tvm.tir.expr.Let:-1
#: tvm.tir.expr.Max:-1 tvm.tir.expr.Min:-1 tvm.tir.expr.Mod:-1
#: tvm.tir.expr.Mul:-1 tvm.tir.expr.NE:-1 tvm.tir.expr.Not:-1
#: tvm.tir.expr.Or:-1 tvm.tir.expr.Ramp:-1 tvm.tir.expr.Reduce:-1
#: tvm.tir.expr.Select:-1 tvm.tir.expr.Sub:-1
#: tvm.tir.op.TVMBackendAllocWorkspace:-1 tvm.tir.op.TVMBackendFreeWorkspace:-1
#: tvm.tir.op.abs:-1 tvm.tir.op.acos:-1 tvm.tir.op.acosh:-1
#: tvm.tir.op.address_of:-1 tvm.tir.op.asin:-1 tvm.tir.op.asinh:-1
#: tvm.tir.op.assume:-1 tvm.tir.op.atan:-1 tvm.tir.op.atan2:-1
#: tvm.tir.op.atanh:-1 tvm.tir.op.bitwise_and:-1 tvm.tir.op.bitwise_not:-1
#: tvm.tir.op.bitwise_or:-1 tvm.tir.op.bitwise_xor:-1
#: tvm.tir.op.call_cpacked:-1 tvm.tir.op.call_cpacked_lowered:-1
#: tvm.tir.op.call_extern:-1 tvm.tir.op.call_intrin:-1
#: tvm.tir.op.call_llvm_intrin:-1 tvm.tir.op.call_llvm_pure_intrin:-1
#: tvm.tir.op.call_packed:-1 tvm.tir.op.call_packed_lowered:-1
#: tvm.tir.op.call_pure_extern:-1 tvm.tir.op.call_tir:-1 tvm.tir.op.ceil:-1
#: tvm.tir.op.clz:-1 tvm.tir.op.comm_reducer.<locals>.reducer:-1
#: tvm.tir.op.copysign:-1 tvm.tir.op.cos:-1 tvm.tir.op.cosh:-1
#: tvm.tir.op.create_barriers:-1 tvm.tir.op.div:-1
#: tvm.tir.op.end_profile_intrinsic:-1 tvm.tir.op.erf:-1 tvm.tir.op.exp:-1
#: tvm.tir.op.exp10:-1 tvm.tir.op.exp2:-1 tvm.tir.op.floor:-1
#: tvm.tir.op.floordiv:-1 tvm.tir.op.floormod:-1 tvm.tir.op.fmod:-1
#: tvm.tir.op.hypot:-1 tvm.tir.op.if_then_else:-1 tvm.tir.op.indexdiv:-1
#: tvm.tir.op.indexmod:-1 tvm.tir.op.isfinite:-1 tvm.tir.op.isinf:-1
#: tvm.tir.op.isnan:-1 tvm.tir.op.isnullptr:-1 tvm.tir.op.ldexp:-1
#: tvm.tir.op.likely:-1 tvm.tir.op.log:-1 tvm.tir.op.log10:-1
#: tvm.tir.op.log1p:-1 tvm.tir.op.log2:-1 tvm.tir.op.lookup_param:-1
#: tvm.tir.op.mma_fill:-1 tvm.tir.op.mma_store:-1 tvm.tir.op.nearbyint:-1
#: tvm.tir.op.nextafter:-1 tvm.tir.op.popcount:-1 tvm.tir.op.pow:-1
#: tvm.tir.op.power:-1 tvm.tir.op.ptx_arrive_barrier:-1
#: tvm.tir.op.ptx_arrive_barrier_expect_tx:-1 tvm.tir.op.ptx_commit_group:-1
#: tvm.tir.op.ptx_cp_async:-1 tvm.tir.op.ptx_cp_async_barrier:-1
#: tvm.tir.op.ptx_cp_async_bulk:-1 tvm.tir.op.ptx_init_barrier_thread_count:-1
#: tvm.tir.op.ptx_ldmatrix:-1 tvm.tir.op.ptx_mma:-1 tvm.tir.op.ptx_mma_sp:-1
#: tvm.tir.op.ptx_wait_barrier:-1 tvm.tir.op.ptx_wait_group:-1
#: tvm.tir.op.q_multiply_shift:-1 tvm.tir.op.q_multiply_shift_per_axis:-1
#: tvm.tir.op.reinterpret:-1 tvm.tir.op.ret:-1 tvm.tir.op.round:-1
#: tvm.tir.op.rsqrt:-1 tvm.tir.op.shift_left:-1 tvm.tir.op.shift_right:-1
#: tvm.tir.op.sigmoid:-1 tvm.tir.op.sin:-1 tvm.tir.op.sinh:-1
#: tvm.tir.op.sqrt:-1 tvm.tir.op.start_profile_intrinsic:-1 tvm.tir.op.tan:-1
#: tvm.tir.op.tanh:-1 tvm.tir.op.trace:-1 tvm.tir.op.trunc:-1
#: tvm.tir.op.truncdiv:-1 tvm.tir.op.truncmod:-1 tvm.tir.op.tvm_access_ptr:-1
#: tvm.tir.op.tvm_bmma_sync:-1 tvm.tir.op.tvm_check_return:-1
#: tvm.tir.op.tvm_fill_fragment:-1 tvm.tir.op.tvm_load_matrix_sync:-1
#: tvm.tir.op.tvm_mma_sync:-1 tvm.tir.op.tvm_stack_alloca:-1
#: tvm.tir.op.tvm_stack_make_array:-1 tvm.tir.op.tvm_stack_make_shape:-1
#: tvm.tir.op.tvm_store_matrix_sync:-1 tvm.tir.op.tvm_struct_get:-1
#: tvm.tir.op.tvm_struct_set:-1 tvm.tir.op.tvm_thread_allreduce:-1
#: tvm.tir.op.tvm_throw_last_error:-1 tvm.tir.op.tvm_tuple:-1
#: tvm.tir.op.type_annotation:-1 tvm.tir.op.undef:-1
#: tvm.tir.op.vectorcombine:-1 tvm.tir.op.vectorhigh:-1 tvm.tir.op.vectorlow:-1
#: tvm.tir.stmt.Allocate:-1 tvm.tir.stmt.AssertStmt:-1 tvm.tir.stmt.AttrStmt:-1
#: tvm.tir.stmt.BufferRealize:-1 tvm.tir.stmt.BufferStore:-1
#: tvm.tir.stmt.Evaluate:-1 tvm.tir.stmt.For:-1 tvm.tir.stmt.IfThenElse:-1
#: tvm.tir.stmt.LetStmt:-1 tvm.tir.stmt.ProducerRealize:-1
#: tvm.tir.stmt.ProducerStore:-1 tvm.tir.stmt.While:-1
msgid "PrimExpr"
msgstr ""

#: of tvm.tir.expr.Add:6 tvm.tir.expr.And:6 tvm.tir.expr.Div:6
#: tvm.tir.expr.EQ:6 tvm.tir.expr.FloorDiv:6 tvm.tir.expr.FloorMod:6
#: tvm.tir.expr.GE:6 tvm.tir.expr.GT:6 tvm.tir.expr.LE:6 tvm.tir.expr.LT:6
#: tvm.tir.expr.Max:6 tvm.tir.expr.Min:6 tvm.tir.expr.Mod:6 tvm.tir.expr.Mul:6
#: tvm.tir.expr.NE:6 tvm.tir.expr.Or:6 tvm.tir.expr.Sub:6
msgid "The left hand operand."
msgstr ""

#: of tvm.tir.expr.Add:9 tvm.tir.expr.And:9 tvm.tir.expr.Div:9
#: tvm.tir.expr.EQ:9 tvm.tir.expr.FloorDiv:9 tvm.tir.expr.FloorMod:9
#: tvm.tir.expr.GE:9 tvm.tir.expr.GT:9 tvm.tir.expr.LE:9 tvm.tir.expr.LT:9
#: tvm.tir.expr.Max:9 tvm.tir.expr.Min:9 tvm.tir.expr.Mod:9 tvm.tir.expr.Mul:9
#: tvm.tir.expr.NE:9 tvm.tir.expr.Or:9 tvm.tir.expr.Sub:9 tvm.tir.op.div:9
#: tvm.tir.op.floordiv:9 tvm.tir.op.floormod:9 tvm.tir.op.indexdiv:9
#: tvm.tir.op.indexmod:9 tvm.tir.op.truncdiv:9 tvm.tir.op.truncmod:9
msgid "b"
msgstr ""

#: of tvm.tir.expr.Add:9 tvm.tir.expr.And:9 tvm.tir.expr.Div:9
#: tvm.tir.expr.EQ:9 tvm.tir.expr.FloorDiv:9 tvm.tir.expr.FloorMod:9
#: tvm.tir.expr.GE:9 tvm.tir.expr.GT:9 tvm.tir.expr.LE:9 tvm.tir.expr.LT:9
#: tvm.tir.expr.Max:9 tvm.tir.expr.Min:9 tvm.tir.expr.Mod:9 tvm.tir.expr.Mul:9
#: tvm.tir.expr.NE:9 tvm.tir.expr.Or:9 tvm.tir.expr.Sub:9
msgid "The right hand operand."
msgstr ""

#: of tvm.tir.expr.Add:11 tvm.tir.expr.And:11 tvm.tir.expr.Any:3
#: tvm.tir.expr.Broadcast:11 tvm.tir.expr.BufferLoad:11 tvm.tir.expr.Call:15
#: tvm.tir.expr.Cast:11 tvm.tir.expr.CommReducer:17 tvm.tir.expr.Div:11
#: tvm.tir.expr.EQ:11 tvm.tir.expr.FloatImm:11 tvm.tir.expr.FloorDiv:11
#: tvm.tir.expr.FloorMod:11 tvm.tir.expr.GE:11 tvm.tir.expr.GT:11
#: tvm.tir.expr.IntImm:11 tvm.tir.expr.IterVar:20 tvm.tir.expr.LE:11
#: tvm.tir.expr.LT:11 tvm.tir.expr.Let:14 tvm.tir.expr.Max:11
#: tvm.tir.expr.Min:11 tvm.tir.expr.Mod:11 tvm.tir.expr.Mul:11
#: tvm.tir.expr.NE:11 tvm.tir.expr.Not:8 tvm.tir.expr.Or:11
#: tvm.tir.expr.ProducerLoad:11 tvm.tir.expr.Ramp:14 tvm.tir.expr.Reduce:23
#: tvm.tir.expr.Select:21 tvm.tir.expr.Shuffle:11 tvm.tir.expr.SizeVar:12
#: tvm.tir.expr.StringImm:8 tvm.tir.expr.Sub:11 tvm.tir.expr.Var:11
#: tvm.tir.function.PrimFunc:21 tvm.tir.function.PrimFunc.with_body:9
#: tvm.tir.generic.add:10 tvm.tir.generic.multiply:10
#: tvm.tir.generic.subtract:10 tvm.tir.op.abs:9 tvm.tir.op.address_of:9
#: tvm.tir.op.all:10 tvm.tir.op.any:9 tvm.tir.op.bitwise_and:12
#: tvm.tir.op.bitwise_not:9 tvm.tir.op.bitwise_or:12 tvm.tir.op.bitwise_xor:12
#: tvm.tir.op.call_cpacked:12 tvm.tir.op.call_cpacked_lowered:11
#: tvm.tir.op.call_extern:15 tvm.tir.op.call_intrin:18
#: tvm.tir.op.call_llvm_intrin:15 tvm.tir.op.call_llvm_pure_intrin:15
#: tvm.tir.op.call_packed:16 tvm.tir.op.call_packed_lowered:14
#: tvm.tir.op.call_pure_extern:15 tvm.tir.op.ceil:9 tvm.tir.op.ceildiv:10
#: tvm.tir.op.div:12 tvm.tir.op.floor:9 tvm.tir.op.floordiv:12
#: tvm.tir.op.floormod:12 tvm.tir.op.if_then_else:15 tvm.tir.op.indexdiv:12
#: tvm.tir.op.indexmod:12 tvm.tir.op.infinity:9 tvm.tir.op.isfinite:9
#: tvm.tir.op.isinf:9 tvm.tir.op.isnan:9 tvm.tir.op.isnullptr:9
#: tvm.tir.op.likely:10 tvm.tir.op.lookup_param:9 tvm.tir.op.max_value:9
#: tvm.tir.op.min_value:9 tvm.tir.op.nearbyint:16 tvm.tir.op.pow:12
#: tvm.tir.op.power:12 tvm.tir.op.reinterpret:12 tvm.tir.op.round:9
#: tvm.tir.op.trunc:12 tvm.tir.op.truncdiv:12 tvm.tir.op.truncmod:12
#: tvm.tir.stmt.Allocate:23 tvm.tir.stmt.AllocateConst:26
#: tvm.tir.stmt.AssertStmt:14 tvm.tir.stmt.AttrStmt:17 tvm.tir.stmt.Block:32
#: tvm.tir.stmt.BlockRealize:14 tvm.tir.stmt.BufferRealize:17
#: tvm.tir.stmt.BufferStore:14 tvm.tir.stmt.Evaluate:8 tvm.tir.stmt.For:27
#: tvm.tir.stmt.IfThenElse:14 tvm.tir.stmt.LetStmt:14 tvm.tir.stmt.Prefetch:11
#: tvm.tir.stmt.ProducerRealize:20 tvm.tir.stmt.ProducerStore:14
#: tvm.tir.stmt.SeqStmt:8 tvm.tir.stmt.While:11
msgid "span"
msgstr ""

#: of tvm.tir.expr.Add:-1 tvm.tir.expr.And:-1 tvm.tir.expr.Any:-1
#: tvm.tir.expr.Broadcast:-1 tvm.tir.expr.BufferLoad:-1 tvm.tir.expr.Call:-1
#: tvm.tir.expr.Cast:-1 tvm.tir.expr.CommReducer:-1 tvm.tir.expr.Div:-1
#: tvm.tir.expr.EQ:-1 tvm.tir.expr.FloatImm:-1 tvm.tir.expr.FloorDiv:-1
#: tvm.tir.expr.FloorMod:-1 tvm.tir.expr.GE:-1 tvm.tir.expr.GT:-1
#: tvm.tir.expr.IntImm:-1 tvm.tir.expr.IterVar:-1 tvm.tir.expr.LE:-1
#: tvm.tir.expr.LT:-1 tvm.tir.expr.Let:-1 tvm.tir.expr.Max:-1
#: tvm.tir.expr.Min:-1 tvm.tir.expr.Mod:-1 tvm.tir.expr.Mul:-1
#: tvm.tir.expr.NE:-1 tvm.tir.expr.Not:-1 tvm.tir.expr.Or:-1
#: tvm.tir.expr.ProducerLoad:-1 tvm.tir.expr.Ramp:-1 tvm.tir.expr.Reduce:-1
#: tvm.tir.expr.Select:-1 tvm.tir.expr.Shuffle:-1 tvm.tir.expr.SizeVar:-1
#: tvm.tir.expr.StringImm:-1 tvm.tir.expr.Sub:-1 tvm.tir.expr.Var:-1
#: tvm.tir.function.PrimFunc:-1 tvm.tir.function.PrimFunc.with_body:-1
#: tvm.tir.generic.add:-1 tvm.tir.generic.multiply:-1
#: tvm.tir.generic.subtract:-1 tvm.tir.op.abs:-1 tvm.tir.op.address_of:-1
#: tvm.tir.op.all:-1 tvm.tir.op.any:-1 tvm.tir.op.bitwise_and:-1
#: tvm.tir.op.bitwise_not:-1 tvm.tir.op.bitwise_or:-1 tvm.tir.op.bitwise_xor:-1
#: tvm.tir.op.call_cpacked:-1 tvm.tir.op.call_cpacked_lowered:-1
#: tvm.tir.op.call_extern:-1 tvm.tir.op.call_intrin:-1
#: tvm.tir.op.call_llvm_intrin:-1 tvm.tir.op.call_llvm_pure_intrin:-1
#: tvm.tir.op.call_packed:-1 tvm.tir.op.call_packed_lowered:-1
#: tvm.tir.op.call_pure_extern:-1 tvm.tir.op.ceil:-1 tvm.tir.op.ceildiv:-1
#: tvm.tir.op.div:-1 tvm.tir.op.floor:-1 tvm.tir.op.floordiv:-1
#: tvm.tir.op.floormod:-1 tvm.tir.op.if_then_else:-1 tvm.tir.op.indexdiv:-1
#: tvm.tir.op.indexmod:-1 tvm.tir.op.infinity:-1 tvm.tir.op.isfinite:-1
#: tvm.tir.op.isinf:-1 tvm.tir.op.isnan:-1 tvm.tir.op.isnullptr:-1
#: tvm.tir.op.likely:-1 tvm.tir.op.lookup_param:-1 tvm.tir.op.max_value:-1
#: tvm.tir.op.min_value:-1 tvm.tir.op.nearbyint:-1 tvm.tir.op.pow:-1
#: tvm.tir.op.power:-1 tvm.tir.op.reinterpret:-1 tvm.tir.op.round:-1
#: tvm.tir.op.trunc:-1 tvm.tir.op.truncdiv:-1 tvm.tir.op.truncmod:-1
#: tvm.tir.stmt.Allocate:-1 tvm.tir.stmt.AllocateConst:-1
#: tvm.tir.stmt.AssertStmt:-1 tvm.tir.stmt.AttrStmt:-1 tvm.tir.stmt.Block:-1
#: tvm.tir.stmt.BlockRealize:-1 tvm.tir.stmt.BufferRealize:-1
#: tvm.tir.stmt.BufferStore:-1 tvm.tir.stmt.Evaluate:-1 tvm.tir.stmt.For:-1
#: tvm.tir.stmt.IfThenElse:-1 tvm.tir.stmt.LetStmt:-1 tvm.tir.stmt.Prefetch:-1
#: tvm.tir.stmt.ProducerRealize:-1 tvm.tir.stmt.ProducerStore:-1
#: tvm.tir.stmt.SeqStmt:-1 tvm.tir.stmt.While:-1
msgid "Optional[Span]"
msgstr ""

#: of tvm.tir.expr.Add:12 tvm.tir.expr.And:12 tvm.tir.expr.Any:4
#: tvm.tir.expr.Broadcast:12 tvm.tir.expr.BufferLoad:12 tvm.tir.expr.Call:16
#: tvm.tir.expr.Cast:12 tvm.tir.expr.CommReducer:18 tvm.tir.expr.Div:12
#: tvm.tir.expr.EQ:12 tvm.tir.expr.FloatImm:12 tvm.tir.expr.FloorDiv:12
#: tvm.tir.expr.FloorMod:12 tvm.tir.expr.GE:12 tvm.tir.expr.GT:12
#: tvm.tir.expr.IntImm:12 tvm.tir.expr.IterVar:20 tvm.tir.expr.LE:12
#: tvm.tir.expr.LT:12 tvm.tir.expr.Let:15 tvm.tir.expr.Max:12
#: tvm.tir.expr.Min:12 tvm.tir.expr.Mod:12 tvm.tir.expr.Mul:12
#: tvm.tir.expr.NE:12 tvm.tir.expr.Not:9 tvm.tir.expr.Or:12
#: tvm.tir.expr.ProducerLoad:12 tvm.tir.expr.Ramp:15 tvm.tir.expr.Reduce:24
#: tvm.tir.expr.Select:22 tvm.tir.expr.Shuffle:12 tvm.tir.expr.SizeVar:13
#: tvm.tir.expr.StringImm:9 tvm.tir.expr.Sub:12 tvm.tir.expr.Var:12
#: tvm.tir.function.PrimFunc:21 tvm.tir.function.PrimFunc.with_body:9
#: tvm.tir.stmt.Allocate:24 tvm.tir.stmt.AllocateConst:27
#: tvm.tir.stmt.AssertStmt:15 tvm.tir.stmt.AttrStmt:18
#: tvm.tir.stmt.BufferRealize:18 tvm.tir.stmt.BufferStore:15
#: tvm.tir.stmt.Evaluate:9 tvm.tir.stmt.For:28 tvm.tir.stmt.IfThenElse:15
#: tvm.tir.stmt.LetStmt:15 tvm.tir.stmt.Prefetch:12
#: tvm.tir.stmt.ProducerRealize:21 tvm.tir.stmt.ProducerStore:15
#: tvm.tir.stmt.SeqStmt:9 tvm.tir.stmt.While:12
msgid "The location of this itervar in the source code."
msgstr ""

#: of tvm.tir.stmt.Allocate:6 tvm.tir.stmt.AllocateConst:6
msgid "buffer_var"
msgstr ""

#: of tvm.tir.expr.Let:-1 tvm.tir.op.TVMBackendFreeWorkspace:-1
#: tvm.tir.op.mma_fill:-1 tvm.tir.op.mma_store:-1 tvm.tir.op.ptx_cp_async:-1
#: tvm.tir.op.ptx_cp_async_bulk:-1 tvm.tir.op.ptx_ldmatrix:-1
#: tvm.tir.op.ptx_mma:-1 tvm.tir.op.ptx_mma_sp:-1 tvm.tir.op.tvm_bmma_sync:-1
#: tvm.tir.op.tvm_fill_fragment:-1 tvm.tir.op.tvm_load_matrix_sync:-1
#: tvm.tir.op.tvm_mma_sync:-1 tvm.tir.op.tvm_store_matrix_sync:-1
#: tvm.tir.stmt.Allocate:-1 tvm.tir.stmt.AllocateConst:-1 tvm.tir.stmt.For:-1
#: tvm.tir.stmt.LetStmt:-1
msgid "Var"
msgstr ""

#: of tvm.tir.stmt.Allocate:6 tvm.tir.stmt.AllocateConst:6
msgid "The buffer variable."
msgstr ""

#: of tvm.tir.buffer.Buffer.vload:10 tvm.tir.buffer.decl_buffer:14
#: tvm.tir.data_layout.layout:16 tvm.tir.expr.Call:6 tvm.tir.expr.Cast:6
#: tvm.tir.expr.FloatImm:6 tvm.tir.expr.IntImm:6 tvm.tir.expr.SizeVar:10
#: tvm.tir.expr.Var:9 tvm.tir.op.call_extern:6 tvm.tir.op.call_intrin:9
#: tvm.tir.op.call_llvm_intrin:6 tvm.tir.op.call_llvm_pure_intrin:6
#: tvm.tir.op.call_pure_extern:6 tvm.tir.op.infinity:6 tvm.tir.op.max_value:6
#: tvm.tir.op.min_value:6 tvm.tir.op.mma_fill:6 tvm.tir.op.mma_store:6
#: tvm.tir.op.ptx_cp_async:7 tvm.tir.op.ptx_cp_async_bulk:7
#: tvm.tir.op.ptx_ldmatrix:7 tvm.tir.op.ptx_mma:7 tvm.tir.op.ptx_mma_sp:7
#: tvm.tir.op.reinterpret:6 tvm.tir.op.tvm_struct_get:6
#: tvm.tir.op.type_annotation:6 tvm.tir.op.vectorhigh:6 tvm.tir.op.vectorlow:6
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:14
#: tvm.tir.stmt.Allocate:9 tvm.tir.stmt.AllocateConst:9
msgid "dtype"
msgstr ""

#: of tvm.ir.module.IRModule.astext:-1 tvm.ir.module.IRModule.get_attr:-1
#: tvm.ir.module.IRModule.with_attr:-1 tvm.tir.buffer.Buffer.vload:-1
#: tvm.tir.data_layout.Layout.factor_of:-1
#: tvm.tir.data_layout.Layout.index_of:-1 tvm.tir.data_layout.layout:-1
#: tvm.tir.expr.Call:-1 tvm.tir.expr.Cast:-1 tvm.tir.expr.FloatImm:-1
#: tvm.tir.expr.IntImm:-1 tvm.tir.expr.IterVar:-1 tvm.tir.expr.SizeVar:-1
#: tvm.tir.expr.StringImm:-1 tvm.tir.expr.Var:-1
#: tvm.tir.function.IndexMap.from_func_with_separators:-1
#: tvm.tir.function.TensorIntrin.get:-1
#: tvm.tir.function.TensorIntrin.register:-1 tvm.tir.op.call_extern:-1
#: tvm.tir.op.call_intrin:-1 tvm.tir.op.call_llvm_intrin:-1
#: tvm.tir.op.call_llvm_pure_intrin:-1 tvm.tir.op.call_pure_extern:-1
#: tvm.tir.op.infinity:-1 tvm.tir.op.lookup_param:-1 tvm.tir.op.max_value:-1
#: tvm.tir.op.min_value:-1 tvm.tir.op.mma_fill:-1 tvm.tir.op.mma_store:-1
#: tvm.tir.op.ptx_cp_async:-1 tvm.tir.op.ptx_cp_async_bulk:-1
#: tvm.tir.op.ptx_ldmatrix:-1 tvm.tir.op.ptx_mma:-1 tvm.tir.op.ptx_mma_sp:-1
#: tvm.tir.op.reinterpret:-1 tvm.tir.op.tvm_stack_alloca:-1
#: tvm.tir.op.tvm_struct_get:-1 tvm.tir.op.vectorhigh:-1
#: tvm.tir.op.vectorlow:-1 tvm.tir.schedule.schedule.Schedule.annotate:-1
#: tvm.tir.schedule.schedule.Schedule.bind:-1
#: tvm.tir.schedule.schedule.Schedule.get_block:-1
#: tvm.tir.schedule.schedule.Schedule.set_scope:-1
#: tvm.tir.schedule.schedule.Schedule.tensorize:-1
#: tvm.tir.schedule.schedule.Schedule.unannotate:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:-1
#: tvm.tir.schedule.schedule.Schedule.work_on:-1 tvm.tir.stmt.Allocate:-1
#: tvm.tir.stmt.AllocateConst:-1 tvm.tir.stmt.AttrStmt:-1
#: tvm.tir.stmt.ProducerRealize:-1
#: tvm.tir.transform.transform.FP8ComputeLegalize:-1
#: tvm.tir.transform.transform.InjectCopyIntrin:-1
#: tvm.tir.transform.transform.LiftAttrScope:-1
msgid "str"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:14 tvm.tir.stmt.Allocate:9
#: tvm.tir.stmt.AllocateConst:9
msgid "The data type of the buffer."
msgstr ""

#: of tvm.tir.stmt.Allocate:12 tvm.tir.stmt.AllocateConst:12
msgid "extents"
msgstr ""

#: of tvm.tir.expr.Call:-1 tvm.tir.expr.Reduce:-1 tvm.tir.stmt.Allocate:-1
#: tvm.tir.stmt.AllocateConst:-1 tvm.tir.stmt.ProducerStore:-1
msgid "list of Expr"
msgstr ""

#: of tvm.tir.stmt.Allocate:12 tvm.tir.stmt.AllocateConst:12
msgid "The extents of the allocate"
msgstr ""

#: of tvm.tir.expr.Reduce:15 tvm.tir.expr.Select:13 tvm.tir.stmt.Allocate:15
#: tvm.tir.stmt.AssertStmt:6 tvm.tir.stmt.BufferRealize:12
#: tvm.tir.stmt.IfThenElse:6 tvm.tir.stmt.ProducerRealize:12
#: tvm.tir.stmt.While:6
msgid "condition"
msgstr ""

#: of tvm.tir.stmt.Allocate:15
msgid "The condition."
msgstr ""

#: of tvm.tir.expr.Let:12 tvm.tir.stmt.Allocate:18
#: tvm.tir.stmt.AllocateConst:21 tvm.tir.stmt.AssertStmt:12
#: tvm.tir.stmt.AttrStmt:15 tvm.tir.stmt.BufferRealize:15 tvm.tir.stmt.For:18
#: tvm.tir.stmt.LetStmt:12 tvm.tir.stmt.ProducerRealize:15 tvm.tir.stmt.While:9
msgid "body"
msgstr ""

#: of tvm.tir.buffer.Buffer.vstore:-1 tvm.tir.function.PrimFunc.with_body:-1
#: tvm.tir.stmt.Allocate:-1 tvm.tir.stmt.AllocateConst:-1
#: tvm.tir.stmt.AttrStmt:-1 tvm.tir.stmt.BufferRealize:-1 tvm.tir.stmt.For:-1
#: tvm.tir.stmt.IfThenElse:-1 tvm.tir.stmt.LetStmt:-1
#: tvm.tir.stmt.ProducerRealize:-1 tvm.tir.stmt.While:-1
#: tvm.tir.stmt.stmt_seq:-1
msgid "Stmt"
msgstr ""

#: of tvm.tir.stmt.Allocate:18 tvm.tir.stmt.AllocateConst:21
#: tvm.tir.stmt.AssertStmt:12 tvm.tir.stmt.AttrStmt:15 tvm.tir.stmt.For:18
#: tvm.tir.stmt.LetStmt:12 tvm.tir.stmt.While:9
msgid "The body statement."
msgstr ""

#: of tvm.tir.stmt.Allocate:21 tvm.tir.stmt.Block:30
msgid "annotations: Optional[Mapping[str, Object]]"
msgstr ""

#: of tvm.tir.stmt.Allocate:21
msgid "Additional annotation hints"
msgstr ""

#: of tvm.tir.stmt.AllocateConst:18
msgid "data_or_idx"
msgstr ""

#: of tvm.tir.stmt.AllocateConst:-1
msgid "Union[NDArray, int]"
msgstr ""

#: of tvm.tir.stmt.AllocateConst:15
msgid ""
"If an NDArray, this is the const data associated with the constant.  If "
"an integer, this is the index into the \"constants\" attribute of the "
"`IRModule` that contains the `AllocateConst`."
msgstr ""

#: of tvm.tir.stmt.AllocateConst:24
msgid "annotations"
msgstr ""

#: of tvm.tir.stmt.AllocateConst:-1
msgid "Optional[Map]"
msgstr ""

#: of tvm.tir.stmt.AllocateConst:24
msgid "Additional annotations about the allocation."
msgstr ""

#: of tvm.tir.stmt.AssertStmt:6
msgid "The assert condition."
msgstr ""

#: of tvm.tir.stmt.AssertStmt:9
msgid "message"
msgstr ""

#: of tvm.tir.stmt.AssertStmt:9
msgid "The error message."
msgstr ""

#: of tvm.tir.stmt.AssertStmt:-1 tvm.tir.stmt_functor.ir_transform:-1
#: tvm.tir.stmt_functor.substitute:-1
msgid "tvm.tir.Stmt"
msgstr ""

#: of tvm.tir.stmt.AttrStmt:6
msgid "node"
msgstr ""

#: of tvm.tir.op.if_then_else:-1 tvm.tir.stmt.AttrStmt:-1
msgid "Node"
msgstr ""

#: of tvm.tir.stmt.AttrStmt:6
msgid "The node to annotate the attribute"
msgstr ""

#: of tvm.ir.module.IRModule.get_attr:6 tvm.ir.module.IRModule.with_attr:6
#: tvm.tir.stmt.AttrStmt:9 tvm.tir.transform.transform.LiftAttrScope:6
msgid "attr_key"
msgstr ""

#: of tvm.tir.stmt.AttrStmt:9
msgid "Attribute type key."
msgstr ""

#: of tvm.tir.buffer.Buffer.vstore:9 tvm.tir.expr.Broadcast:6
#: tvm.tir.expr.Cast:9 tvm.tir.expr.FloatImm:9 tvm.tir.expr.IntImm:9
#: tvm.tir.expr.Let:9 tvm.tir.expr.StringImm:6
#: tvm.tir.op.comm_reducer.<locals>.reducer:14 tvm.tir.op.infinity:13
#: tvm.tir.op.max_value:13 tvm.tir.op.min_value:13 tvm.tir.op.reinterpret:9
#: tvm.tir.op.reinterpret:16 tvm.tir.op.tvm_fill_fragment:21
#: tvm.tir.op.tvm_struct_set:15 tvm.tir.op.tvm_tuple:6 tvm.tir.stmt.AttrStmt:12
#: tvm.tir.stmt.BufferStore:9 tvm.tir.stmt.Evaluate:6 tvm.tir.stmt.LetStmt:9
#: tvm.tir.stmt.ProducerStore:9
msgid "value"
msgstr ""

#: of tvm.tir.stmt.AttrStmt:12
msgid "The value of the attribute"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout:1
msgid ""
"Bijective mapping for two layouts (src-layout and dst-layout). It "
"provides shape and index conversion between each other."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout:4
msgid ""
"Do not construct directly, use :any:`bijective_layout` instead. See the "
"documentation of :any:`bijective_layout` for more details."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout:10
#: tvm.tir.data_layout.bijective_layout:6
msgid "src_layout"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout:-1
#: tvm.tir.data_layout.bijective_layout:-1
msgid "str or Layout"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout:10
#: tvm.tir.data_layout.bijective_layout:6
msgid "source layout."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout:13
#: tvm.tir.data_layout.bijective_layout:9
msgid "dst_layout"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout:13
#: tvm.tir.data_layout.bijective_layout:9
msgid "destination layout."
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:31 tvm.tir.buffer.Buffer:10
#: tvm.tir.data_layout.BijectiveLayout:16 tvm.tir.data_layout.Layout:9
#: tvm.tir.expr.IterVar:23 tvm.tir.op.call_cpacked:20
#: tvm.tir.op.call_cpacked_lowered:19 tvm.tir.op.call_packed:24
#: tvm.tir.op.call_packed_lowered:22 tvm.tir.op.trace:22
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:71
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:74
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:58
msgid "See Also"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout:17
msgid "bijective_layout : Declare a layout"
msgstr ""

#: of tvm.ir.module.IRModule:1 tvm.runtime.object.Object:1
#: tvm.tir.block_dependence_info.BlockDependenceInfo:1
#: tvm.tir.block_scope.BlockScope:1 tvm.tir.block_scope.StmtSRef:1
#: tvm.tir.buffer.Buffer:1 tvm.tir.data_layout.BijectiveLayout:1
#: tvm.tir.data_layout.Layout:1 tvm.tir.function.IndexMap:1
#: tvm.tir.function.PrimFunc:1 tvm.tir.function.TensorIntrin:1
#: tvm.tir.schedule.schedule.Schedule:1 tvm.tir.schedule.state.ScheduleState:1
msgid "**Methods:**"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:1:<autosummary>:1
msgid ""
":py:obj:`backward_index <tvm.tir.BijectiveLayout.backward_index>`\\ "
"\\(index\\)"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:1
#: tvm.tir.data_layout.BijectiveLayout.backward_index:1:<autosummary>:1
msgid "Given the indices of the dst-layout, infer the src index."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:1:<autosummary>:1
msgid ""
":py:obj:`backward_shape <tvm.tir.BijectiveLayout.backward_shape>`\\ "
"\\(shape\\)"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:1:<autosummary>:1
#: tvm.tir.data_layout.BijectiveLayout.backward_shape:1
msgid "Given the shape of the dst-layout, infer the src shape."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:1:<autosummary>:1
msgid ""
":py:obj:`forward_index <tvm.tir.BijectiveLayout.forward_index>`\\ "
"\\(index\\)"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:1:<autosummary>:1
#: tvm.tir.data_layout.BijectiveLayout.forward_index:1
msgid "Given the indices of the src-layout, infer the dst index."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:1:<autosummary>:1
msgid ""
":py:obj:`forward_shape <tvm.tir.BijectiveLayout.forward_shape>`\\ "
"\\(shape\\)"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:1:<autosummary>:1
#: tvm.tir.data_layout.BijectiveLayout.forward_shape:1
msgid "Given the shape of the src-layout, infer the dst shape."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:6
#: tvm.tir.data_layout.BijectiveLayout.forward_index:6
msgid "index: Array of Expr"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:6
msgid "The indices in dst-layout."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:9 tvm.ir.module.IRModule.astext:14
#: tvm.ir.module.IRModule.from_expr:15 tvm.ir.module.IRModule.get_attr:9
#: tvm.ir.module.IRModule.get_constructor:9
#: tvm.ir.module.IRModule.get_global_type_var:9
#: tvm.ir.module.IRModule.get_global_type_vars:4
#: tvm.ir.module.IRModule.get_global_var:9
#: tvm.ir.module.IRModule.get_global_vars:4 tvm.ir.module.IRModule.with_attr:12
#: tvm.ir.module.IRModule.with_attrs:7 tvm.ir.module.IRModule.without_attr:7
#: tvm.runtime.object.Object._move:25 tvm.tir.analysis.analysis.OOBChecker:4
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:24
#: tvm.tir.analysis.analysis.calculate_allocated_bytes:10
#: tvm.tir.analysis.analysis.calculate_constant_bytes:12
#: tvm.tir.analysis.analysis.calculate_workspace_bytes:12
#: tvm.tir.analysis.analysis.detect_buffer_access_lca:12
#: tvm.tir.analysis.analysis.estimate_tir_flops:9
#: tvm.tir.analysis.analysis.expr_deep_equal:12
#: tvm.tir.analysis.analysis.find_anchor_block:20
#: tvm.tir.analysis.analysis.get_block_access_region:13
#: tvm.tir.analysis.analysis.get_block_read_write_region:13
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:16
#: tvm.tir.analysis.analysis.get_vtcm_compaction_passes:5
#: tvm.tir.analysis.analysis.undefined_vars:12
#: tvm.tir.analysis.analysis.verify_gpu_code:12
#: tvm.tir.analysis.analysis.verify_memory:9
#: tvm.tir.analysis.analysis.verify_ssa:9
#: tvm.tir.analysis.analysis.verify_well_formed:13
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:9
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_sref:9
#: tvm.tir.block_scope.BlockScope.get_deps_by_dst:9
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:9
#: tvm.tir.buffer.Buffer.get_flattened_buffer:4
#: tvm.tir.buffer.Buffer.offset_of:10 tvm.tir.buffer.Buffer.vload:13
#: tvm.tir.buffer.Buffer.vstore:12 tvm.tir.buffer.decl_buffer:57
#: tvm.tir.data_layout.BijectiveLayout.backward_index:9
#: tvm.tir.data_layout.BijectiveLayout.backward_shape:9
#: tvm.tir.data_layout.BijectiveLayout.forward_index:9
#: tvm.tir.data_layout.BijectiveLayout.forward_shape:9
#: tvm.tir.data_layout.Layout.factor_of:9 tvm.tir.data_layout.Layout.index_of:9
#: tvm.tir.data_layout.bijective_layout:12 tvm.tir.data_layout.layout:19
#: tvm.tir.function.IndexMap.from_func:29
#: tvm.tir.function.IndexMap.from_func_with_separators:34
#: tvm.tir.function.IndexMap.inverse:14
#: tvm.tir.function.IndexMap.is_equivalent_to:10
#: tvm.tir.function.IndexMap.map_indices:9
#: tvm.tir.function.IndexMap.map_ndarray:9
#: tvm.tir.function.IndexMap.map_shape:9
#: tvm.tir.function.IndexMap.non_surjective_inverse:13
#: tvm.tir.function.PrimFunc.specialize:49
#: tvm.tir.function.PrimFunc.with_body:12 tvm.tir.function.TensorIntrin.get:13
#: tvm.tir.generic.add:13 tvm.tir.generic.multiply:13
#: tvm.tir.generic.subtract:13 tvm.tir.op.TVMBackendAllocWorkspace:21
#: tvm.tir.op.TVMBackendFreeWorkspace:15 tvm.tir.op.abs:12 tvm.tir.op.acos:9
#: tvm.tir.op.acosh:9 tvm.tir.op.address_of:12 tvm.tir.op.all:13
#: tvm.tir.op.any:12 tvm.tir.op.asin:9 tvm.tir.op.asinh:9 tvm.tir.op.assume:9
#: tvm.tir.op.atan:9 tvm.tir.op.atan2:12 tvm.tir.op.atanh:9
#: tvm.tir.op.bitwise_and:15 tvm.tir.op.bitwise_not:12 tvm.tir.op.bitwise_or:15
#: tvm.tir.op.bitwise_xor:15 tvm.tir.op.call_cpacked:15
#: tvm.tir.op.call_cpacked_lowered:14 tvm.tir.op.call_extern:18
#: tvm.tir.op.call_intrin:21 tvm.tir.op.call_llvm_intrin:18
#: tvm.tir.op.call_llvm_pure_intrin:18 tvm.tir.op.call_packed:19
#: tvm.tir.op.call_packed_lowered:17 tvm.tir.op.call_pure_extern:18
#: tvm.tir.op.call_tir:4 tvm.tir.op.ceil:12 tvm.tir.op.ceildiv:13
#: tvm.tir.op.clz:10 tvm.tir.op.comm_reducer:12
#: tvm.tir.op.comm_reducer.<locals>.reducer:12 tvm.tir.op.copysign:12
#: tvm.tir.op.cos:9 tvm.tir.op.cosh:9 tvm.tir.op.create_barriers:9
#: tvm.tir.op.div:15 tvm.tir.op.end_profile_intrinsic:7 tvm.tir.op.erf:9
#: tvm.tir.op.exp:9 tvm.tir.op.exp10:9 tvm.tir.op.exp2:9 tvm.tir.op.floor:12
#: tvm.tir.op.floordiv:15 tvm.tir.op.floormod:15 tvm.tir.op.fmod:11
#: tvm.tir.op.hypot:12 tvm.tir.op.if_then_else:18 tvm.tir.op.indexdiv:15
#: tvm.tir.op.indexmod:15 tvm.tir.op.infinity:12 tvm.tir.op.isfinite:12
#: tvm.tir.op.isinf:12 tvm.tir.op.isnan:12 tvm.tir.op.isnullptr:12
#: tvm.tir.op.ldexp:12 tvm.tir.op.likely:13 tvm.tir.op.log:9 tvm.tir.op.log10:9
#: tvm.tir.op.log1p:9 tvm.tir.op.log2:9 tvm.tir.op.lookup_param:12
#: tvm.tir.op.max_value:12 tvm.tir.op.min_value:12 tvm.tir.op.mma_fill:18
#: tvm.tir.op.mma_store:27 tvm.tir.op.nearbyint:19 tvm.tir.op.nextafter:12
#: tvm.tir.op.popcount:9 tvm.tir.op.pow:15 tvm.tir.op.power:15
#: tvm.tir.op.ptx_arrive_barrier:10 tvm.tir.op.ptx_arrive_barrier_expect_tx:15
#: tvm.tir.op.ptx_commit_group:5 tvm.tir.op.ptx_cp_async:25
#: tvm.tir.op.ptx_cp_async_barrier:10 tvm.tir.op.ptx_cp_async_bulk:28
#: tvm.tir.op.ptx_init_barrier_thread_count:13 tvm.tir.op.ptx_ldmatrix:31
#: tvm.tir.op.ptx_mma:53 tvm.tir.op.ptx_mma_sp:58
#: tvm.tir.op.ptx_wait_barrier:10 tvm.tir.op.ptx_wait_group:10
#: tvm.tir.op.q_multiply_shift:22 tvm.tir.op.q_multiply_shift_per_axis:21
#: tvm.tir.op.reinterpret:15 tvm.tir.op.ret:9 tvm.tir.op.round:12
#: tvm.tir.op.rsqrt:9 tvm.tir.op.shift_left:12 tvm.tir.op.shift_right:12
#: tvm.tir.op.sigmoid:9 tvm.tir.op.sin:9 tvm.tir.op.sinh:9 tvm.tir.op.sqrt:9
#: tvm.tir.op.start_profile_intrinsic:7 tvm.tir.op.tan:9 tvm.tir.op.tanh:9
#: tvm.tir.op.trace:17 tvm.tir.op.trunc:15 tvm.tir.op.truncdiv:15
#: tvm.tir.op.truncmod:15 tvm.tir.op.tvm_access_ptr:21
#: tvm.tir.op.tvm_bmma_sync:30 tvm.tir.op.tvm_check_return:11
#: tvm.tir.op.tvm_fill_fragment:24 tvm.tir.op.tvm_load_matrix_sync:30
#: tvm.tir.op.tvm_mma_sync:30 tvm.tir.op.tvm_stack_alloca:12
#: tvm.tir.op.tvm_stack_make_array:24 tvm.tir.op.tvm_stack_make_shape:9
#: tvm.tir.op.tvm_store_matrix_sync:30 tvm.tir.op.tvm_struct_get:18
#: tvm.tir.op.tvm_struct_set:18 tvm.tir.op.tvm_thread_allreduce:9
#: tvm.tir.op.tvm_throw_last_error:4 tvm.tir.op.tvm_tuple:9
#: tvm.tir.op.type_annotation:9 tvm.tir.op.undef:4 tvm.tir.op.vectorcombine:12
#: tvm.tir.op.vectorhigh:12 tvm.tir.op.vectorlow:12
#: tvm.tir.schedule.schedule.Schedule.add_unit_loop:9
#: tvm.tir.schedule.schedule.Schedule.blockize:11
#: tvm.tir.schedule.schedule.Schedule.cache_index:18
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:20
#: tvm.tir.schedule.schedule.Schedule.cache_read:25
#: tvm.tir.schedule.schedule.Schedule.cache_write:25
#: tvm.tir.schedule.schedule.Schedule.copy:9
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:25
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:25
#: tvm.tir.schedule.schedule.Schedule.fork_seed:4
#: tvm.tir.schedule.schedule.Schedule.fuse:13
#: tvm.tir.schedule.schedule.Schedule.get:14
#: tvm.tir.schedule.schedule.Schedule.get_block:15
#: tvm.tir.schedule.schedule.Schedule.get_child_blocks:9
#: tvm.tir.schedule.schedule.Schedule.get_consumers:9
#: tvm.tir.schedule.schedule.Schedule.get_loops:9
#: tvm.tir.schedule.schedule.Schedule.get_output_blocks:11
#: tvm.tir.schedule.schedule.Schedule.get_producers:9
#: tvm.tir.schedule.schedule.Schedule.get_sref:13
#: tvm.tir.schedule.schedule.Schedule.merge:13
#: tvm.tir.schedule.schedule.Schedule.reindex:32
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:24
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:27
#: tvm.tir.schedule.schedule.Schedule.rfactor:68
#: tvm.tir.schedule.schedule.Schedule.sample_categorical:13
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:11
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:17
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:15
#: tvm.tir.schedule.schedule.Schedule.split:24
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:9
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:9
#: tvm.tir.schedule.state.ScheduleState.get_sref:9 tvm.tir.stmt.stmt_list:8
#: tvm.tir.stmt.stmt_seq:9 tvm.tir.stmt_functor.ir_transform:21
#: tvm.tir.stmt_functor.renew_defs:11 tvm.tir.stmt_functor.substitute:12
#: tvm.tir.transform.function_pass.prim_func_pass:23
#: tvm.tir.transform.transform.AnnotateDeviceRegions:9
#: tvm.tir.transform.transform.AnnotateEntryFunc:4
#: tvm.tir.transform.transform.Apply:11
#: tvm.tir.transform.transform.ApplyLayoutTransforms:5
#: tvm.tir.transform.transform.BF16ComputeLegalize:4
#: tvm.tir.transform.transform.BF16StorageLegalize:4
#: tvm.tir.transform.transform.BindTarget:8
#: tvm.tir.transform.transform.CoProcSync:4
#: tvm.tir.transform.transform.CombineContextCall:4
#: tvm.tir.transform.transform.CommonSubexprElimTIR:4
#: tvm.tir.transform.transform.CompactBufferAllocation:43
#: tvm.tir.transform.transform.ConvertBlocksToOpaque:6
#: tvm.tir.transform.transform.ConvertForLoopsToSerial:4
#: tvm.tir.transform.transform.ConvertSSA:10
#: tvm.tir.transform.transform.DecorateDeviceScope:4
#: tvm.tir.transform.transform.DefaultGPUSchedule:12
#: tvm.tir.transform.transform.ExtractPrimFuncConstants:4
#: tvm.tir.transform.transform.FP8ComputeLegalize:9
#: tvm.tir.transform.transform.FP8StorageLegalize:4
#: tvm.tir.transform.transform.Filter:5
#: tvm.tir.transform.transform.FlattenBuffer:5
#: tvm.tir.transform.transform.ForceNarrowIndexToInt32:4
#: tvm.tir.transform.transform.HoistExpression:11
#: tvm.tir.transform.transform.HoistIfThenElse:19
#: tvm.tir.transform.transform.InferFragment:4
#: tvm.tir.transform.transform.InjectCopyIntrin:12
#: tvm.tir.transform.transform.InjectDoubleBuffer:4
#: tvm.tir.transform.transform.InjectPTXAsyncCopy:4
#: tvm.tir.transform.transform.InjectPermutedLayout:4
#: tvm.tir.transform.transform.InjectPrefetch:4
#: tvm.tir.transform.transform.InjectRollingBuffer:4
#: tvm.tir.transform.transform.InjectSoftwarePipeline:4
#: tvm.tir.transform.transform.InjectVirtualThread:4
#: tvm.tir.transform.transform.InstallDebugSpans:5
#: tvm.tir.transform.transform.InstrumentBoundCheckers:4
#: tvm.tir.transform.transform.InstrumentProfileIntrinsics:4
#: tvm.tir.transform.transform.LegalizePackedCalls:4
#: tvm.tir.transform.transform.LiftAttrScope:9
#: tvm.tir.transform.transform.LiftThreadBinding:4
#: tvm.tir.transform.transform.LoopPartition:4
#: tvm.tir.transform.transform.LowerAutoCopy:4
#: tvm.tir.transform.transform.LowerCrossThreadReduction:5
#: tvm.tir.transform.transform.LowerCustomDatatypes:6
#: tvm.tir.transform.transform.LowerDeviceKernelLaunch:15
#: tvm.tir.transform.transform.LowerDeviceStorageAccessInfo:4
#: tvm.tir.transform.transform.LowerInitBlock:4
#: tvm.tir.transform.transform.LowerIntrin:4
#: tvm.tir.transform.transform.LowerMatchBuffer:4
#: tvm.tir.transform.transform.LowerOpaqueBlock:4
#: tvm.tir.transform.transform.LowerTVMBuiltin:4
#: tvm.tir.transform.transform.LowerThreadAllreduce:4
#: tvm.tir.transform.transform.LowerWarpMemory:4
#: tvm.tir.transform.transform.MakePackedAPI:24
#: tvm.tir.transform.transform.MakeUnpackedAPI:14
#: tvm.tir.transform.transform.ManifestSharedMemoryLocalStage:4
#: tvm.tir.transform.transform.MergeDynamicSharedMemoryAllocations:5
#: tvm.tir.transform.transform.NarrowDataType:9
#: tvm.tir.transform.transform.PlanAndUpdateBufferAllocationLocation:6
#: tvm.tir.transform.transform.PointerValueTypeRewrite:6
#: tvm.tir.transform.transform.ReduceBranchingThroughOvercompute:4
#: tvm.tir.transform.transform.RemoveAssume:4
#: tvm.tir.transform.transform.RemoveNoOp:4
#: tvm.tir.transform.transform.RemoveStoreUndef:4
#: tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:15
#: tvm.tir.transform.transform.RenormalizeSplitPattern:4
#: tvm.tir.transform.transform.RewriteUnsafeSelect:4
#: tvm.tir.transform.transform.Simplify:4
#: tvm.tir.transform.transform.SkipAssert:4
#: tvm.tir.transform.transform.SplitHostDevice:4
#: tvm.tir.transform.transform.StorageFlatten:14
#: tvm.tir.transform.transform.StorageRewrite:8
#: tvm.tir.transform.transform.TextureFlatten:8
#: tvm.tir.transform.transform.ThreadSync:9
#: tvm.tir.transform.transform.TransformMmaBufferLayout:4
#: tvm.tir.transform.transform.UnifyThreadBinding:9
#: tvm.tir.transform.transform.UnrollLoop:6
#: tvm.tir.transform.transform.VectorizeLoop:10
#: tvm.tir.transform.transform.VerifyMemory:4
#: tvm.tir.transform.transform.VerifyVTCMLimit:4
msgid "Returns"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:10
msgid "src_index: Array of Expr"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_index:11
msgid "The inferred indices in src-layout."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_shape:6
#: tvm.tir.data_layout.BijectiveLayout.forward_shape:6
msgid "shape: Array of Expr"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_shape:6
msgid "The shape in dst-layout."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_shape:10
msgid "src_shape: Array of Expr"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.backward_shape:11
msgid "The inferred shape in src-layout."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.forward_index:6
msgid "The indices in src-layout."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.forward_index:10
msgid "dst_index: Array of Expr"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.forward_index:11
msgid "The inferred indices in dst-layout."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.forward_shape:6
msgid "The shape in src-layout."
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.forward_shape:10
msgid "dst_shape: Array of Expr"
msgstr ""

#: of tvm.tir.data_layout.BijectiveLayout.forward_shape:11
msgid "The inferred shape in dst-layout."
msgstr ""

#: of tvm.tir.stmt.Block:6
msgid "iter_vars"
msgstr ""

#: of tvm.tir.stmt.Block:-1
msgid "List[IterVar]"
msgstr ""

#: of tvm.tir.stmt.Block:6
msgid "The block Variable."
msgstr ""

#: of tvm.tir.stmt.Block:9
msgid "reads"
msgstr ""

#: of tvm.tir.stmt.Block:-1
msgid "List[BufferRegion]"
msgstr ""

#: of tvm.tir.stmt.Block:9
msgid "The read buffer regions of the block."
msgstr ""

#: of tvm.tir.stmt.Block:12
msgid "writes: List[BufferRegion]"
msgstr ""

#: of tvm.tir.stmt.Block:12
msgid "The write buffer regions of the block."
msgstr ""

#: of tvm.tir.stmt.Block:15
msgid "name_hint: str"
msgstr ""

#: of tvm.tir.stmt.Block:15
msgid "the name_hint of the block."
msgstr ""

#: of tvm.tir.stmt.Block:18 tvm.tir.stmt.DeclBuffer:9
msgid "body: Stmt"
msgstr ""

#: of tvm.tir.stmt.Block:18
msgid "The body of the block."
msgstr ""

#: of tvm.tir.stmt.Block:21
msgid "init: Optional[Stmt]"
msgstr ""

#: of tvm.tir.stmt.Block:21
msgid "The init block of the reduction block"
msgstr ""

#: of tvm.tir.stmt.Block:24
msgid "alloc_buffers: Optional[list[Buffer]]"
msgstr ""

#: of tvm.tir.stmt.Block:24
msgid "The buffer allocations"
msgstr ""

#: of tvm.tir.stmt.Block:27
msgid "match_buffers: Optional[List[MatchBufferRegion]]"
msgstr ""

#: of tvm.tir.stmt.Block:27
msgid "The subregion buffer match"
msgstr ""

#: of tvm.tir.stmt.Block:30 tvm.tir.stmt.For:25
msgid "Additional annotation hints."
msgstr ""

#: of tvm.tir.stmt.Block:33
msgid "The location of this block in the source code."
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo:4
msgid ""
"The data structures exposed are: 1) sref2scope: Mapping from the srefs to"
" its corresponding BlockScope 2) stmt2ref: Mapping from blocks to "
"corresponding StmtSRefs"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo:8
msgid ""
"Note that this object does not store SRefs to loops as the purpose is "
"only to expose block level dependences. This provides the advantage that "
"the scope block (parent block) for a given block sref can be directly "
"accessed as sref->parent"
msgstr ""

#: of
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:1:<autosummary>:1
msgid ""
":py:obj:`get_block_scope <tvm.tir.BlockDependenceInfo.get_block_scope>`\\"
" \\(block\\_sref\\)"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:1
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:1:<autosummary>:1
#: tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:1
msgid "Get the BlockScope correpsonding to the block sref"
msgstr ""

#: of
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:1:<autosummary>:1
msgid ":py:obj:`get_sref <tvm.tir.BlockDependenceInfo.get_sref>`\\ \\(block\\)"
msgstr ""

#: of
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:1:<autosummary>:1
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_sref:1
msgid "Return the corresponding sref that points to the block"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:6
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:6
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:6
msgid "block_sref"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:-1
#: tvm.tir.block_dependence_info.BlockDependenceInfo.get_sref:-1
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:-1
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:-1
#: tvm.tir.schedule.state.ScheduleState.get_sref:-1
#: tvm.tir.schedule.state.ScheduleState.replace:-1
msgid "StmtSRef"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:6
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:6
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:6
msgid "The block sref to be retrieved"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:10
msgid "scope"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_block_scope:11
msgid "The corresponding BlockScope"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_sref:6
#: tvm.tir.schedule.state.ScheduleState.get_sref:6 tvm.tir.stmt.stmt_seq:10
#: tvm.tir.stmt_functor.ir_transform:6
msgid "stmt"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_sref:-1
#: tvm.tir.stmt.BlockRealize:-1
msgid "Block"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_sref:6
msgid "The block for which the sref is to be retrived"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_sref:10
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:10
#: tvm.tir.schedule.state.ScheduleState.get_sref:10
msgid "sref"
msgstr ""

#: of tvm.tir.block_dependence_info.BlockDependenceInfo.get_sref:11
#: tvm.tir.schedule.state.ScheduleState.get_block_scope:11
#: tvm.tir.schedule.state.ScheduleState.get_sref:11
msgid "The corresponding sref"
msgstr ""

#: of tvm.tir.stmt.BlockRealize:6
msgid "iter_values"
msgstr ""

#: of tvm.tir.expr.BufferLoad:-1 tvm.tir.expr.CommReducer:-1
#: tvm.tir.expr.ProducerLoad:-1 tvm.tir.function.IndexMap:-1
#: tvm.tir.function.IndexMap.map_indices:-1
#: tvm.tir.function.IndexMap.map_shape:-1 tvm.tir.stmt.BlockRealize:-1
#: tvm.tir.stmt.BufferStore:-1
msgid "List[PrimExpr]"
msgstr ""

#: of tvm.tir.stmt.BlockRealize:6
msgid "The binding values of the block var."
msgstr ""

#: of tvm.tir.stmt.BlockRealize:9
msgid "predicate"
msgstr ""

#: of tvm.tir.stmt.BlockRealize:-1
msgid "Union[PrimExpr, bool]"
msgstr ""

#: of tvm.tir.stmt.BlockRealize:9
msgid "The predicate of the block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:7
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:8
#: tvm.tir.schedule.schedule.Schedule.cache_read:10
#: tvm.tir.schedule.schedule.Schedule.cache_write:10
#: tvm.tir.schedule.schedule.Schedule.compute_at:21
#: tvm.tir.schedule.schedule.Schedule.compute_inline:16
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:19
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:19
#: tvm.tir.schedule.schedule.Schedule.get_block:17
#: tvm.tir.schedule.schedule.Schedule.get_consumers:6
#: tvm.tir.schedule.schedule.Schedule.get_loops:6
#: tvm.tir.schedule.schedule.Schedule.get_producers:6
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:14
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:14
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:14
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:5
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:18
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:19
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:18
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:5
#: tvm.tir.schedule.schedule.Schedule.set_scope:6
#: tvm.tir.schedule.schedule.Schedule.storage_align:8
#: tvm.tir.schedule.schedule.Schedule.transform_block_layout:6
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:5
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:9
#: tvm.tir.stmt.BlockRealize:12
msgid "block"
msgstr ""

#: of tvm.tir.stmt.BlockRealize:12
msgid "The block to realize"
msgstr ""

#: of tvm.tir.stmt.BlockRealize:15
msgid "The location of this block_realize in the source code."
msgstr ""

#: of tvm.tir.block_scope.BlockScope:4
msgid "Glossary:"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:6
msgid ""
"Block scope: A contiguous subtree of the sref tree, rooted at each block "
"sref, whose components are:"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:9
msgid "scope root: a block sref"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:10
msgid "internal srefs: loop srefs"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:11
msgid "scope leaves: block srefs"
msgstr ""

#: of tvm.tir.block_scope.BlockScope:13
msgid ""
"Child block: The scope leaf blocks under the scope root or a specific "
"internal sref"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:1:<autosummary>:1
msgid ""
":py:obj:`get_deps_by_dst <tvm.tir.BlockScope.get_deps_by_dst>`\\ "
"\\(block\\)"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:1
#: tvm.tir.block_scope.BlockScope.get_deps_by_dst:1:<autosummary>:1
msgid "Get all dependencies whose `dst` is the target `block`."
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:1:<autosummary>:1
msgid ""
":py:obj:`get_deps_by_src <tvm.tir.BlockScope.get_deps_by_src>`\\ "
"\\(block\\)"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:1:<autosummary>:1
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:1
msgid "Get all dependencies whose `src` is the target`block`."
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:6
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:6
msgid "block: StmtSRef"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:6
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:6
msgid "The queried block"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:10
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:10
msgid "blocks: List[Dependency]"
msgstr ""

#: of tvm.tir.block_scope.BlockScope.get_deps_by_dst:11
#: tvm.tir.block_scope.BlockScope.get_deps_by_src:11
msgid "The dependencies"
msgstr ""

#: of tvm.tir.expr.Broadcast:6
msgid "The value of the expression."
msgstr ""

#: of tvm.tir.expr.Broadcast:9 tvm.tir.expr.Ramp:12
msgid "lanes"
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_constant_bytes:-1
#: tvm.tir.analysis.analysis.calculate_workspace_bytes:-1
#: tvm.tir.buffer.Buffer.access_ptr:-1 tvm.tir.data_layout.Layout.factor_of:-1
#: tvm.tir.data_layout.Layout.index_of:-1 tvm.tir.expr.Broadcast:-1
#: tvm.tir.expr.IntImm:-1 tvm.tir.expr.IterVar:-1 tvm.tir.expr.Ramp:-1
#: tvm.tir.expr.Reduce:-1 tvm.tir.expr.SizeVar:-1
#: tvm.tir.op.TVMBackendAllocWorkspace:-1 tvm.tir.op.TVMBackendFreeWorkspace:-1
#: tvm.tir.op.create_barriers:-1 tvm.tir.op.ptx_arrive_barrier:-1
#: tvm.tir.op.ptx_arrive_barrier_expect_tx:-1 tvm.tir.op.ptx_cp_async:-1
#: tvm.tir.op.ptx_cp_async_barrier:-1 tvm.tir.op.ptx_cp_async_bulk:-1
#: tvm.tir.op.ptx_init_barrier_thread_count:-1 tvm.tir.op.ptx_wait_barrier:-1
#: tvm.tir.op.ptx_wait_group:-1 tvm.tir.op.tvm_access_ptr:-1
#: tvm.tir.op.tvm_check_return:-1 tvm.tir.op.tvm_stack_alloca:-1
#: tvm.tir.op.tvm_stack_make_shape:-1 tvm.tir.op.tvm_struct_get:-1
#: tvm.tir.op.tvm_struct_set:-1 tvm.tir.schedule.schedule.Schedule.fork_seed:-1
#: tvm.tir.schedule.schedule.Schedule.rfactor:-1
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:-1
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:-1
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:-1
#: tvm.tir.schedule.schedule.Schedule.seed:-1
#: tvm.tir.schedule.schedule.Schedule.set_scope:-1
#: tvm.tir.schedule.schedule.Schedule.storage_align:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:-1
#: tvm.tir.schedule.state.ScheduleState:-1
#: tvm.tir.transform.function_pass.prim_func_pass:-1
#: tvm.tir.transform.transform.NarrowDataType:-1
msgid "int"
msgstr ""

#: of tvm.tir.expr.Broadcast:9 tvm.tir.expr.Ramp:12
msgid "The lanes of the expression."
msgstr ""

#: of tvm.tir.buffer.Buffer:3
msgid ""
"Buffer provide a way to represent data layout specialization of data "
"structure in TVM."
msgstr ""

#: of tvm.tir.buffer.Buffer:6
msgid ""
"Do not construct directly, use :py:func:`~decl_buffer` instead. See the "
"documentation of :py:func:`decl_buffer` for more details."
msgstr ""

#: of tvm.tir.buffer.Buffer:11
msgid "decl_buffer : Declare a buffer"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ""
":py:obj:`access_ptr <tvm.tir.Buffer.access_ptr>`\\ \\(access\\_mask\\[\\,"
" ptr\\_type\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1
#: tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid "Get an access pointer to the head of buffer."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ""
":py:obj:`get_flattened_buffer <tvm.tir.Buffer.get_flattened_buffer>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
#: tvm.tir.buffer.Buffer.get_flattened_buffer:1
msgid "Generate a Buffer that is a flattened version of this buffer."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ":py:obj:`offset_of <tvm.tir.Buffer.offset_of>`\\ \\(indices\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
#: tvm.tir.buffer.Buffer.offset_of:1
msgid "Determine the offset of the provided indices in the flattened buffer."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ":py:obj:`scope <tvm.tir.Buffer.scope>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ""
"Return the storage scope associated with this buffer. Returns ------- "
"scope : str     The storage scope associated with this buffer."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ":py:obj:`vload <tvm.tir.Buffer.vload>`\\ \\(begin\\[\\, dtype\\]\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
#: tvm.tir.buffer.Buffer.vload:1
msgid "Generate an Expr that loads dtype from begin index."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ":py:obj:`vstore <tvm.tir.Buffer.vstore>`\\ \\(begin\\, value\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
#: tvm.tir.buffer.Buffer.vstore:1
msgid "Generate a Stmt that store value into begin index."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:3
msgid ""
"This is the recommended method to get buffer data ptress when interacting"
" with external functions."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:10
msgid "access_mask"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:9
msgid ""
"The access pattern MASK. Indicate whether the access will read or write "
"to the data content."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:14
msgid "ptr_type"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:-1 tvm.tir.buffer.decl_buffer:-1
msgid "str, optional"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:13
msgid ""
"The data type of the result pointer. Do not specify unless we want to "
"cast pointer to specific type."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:18
msgid "content_lanes: int, optional"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:17
msgid ""
"The number of lanes for the data type. This value is greater than one for"
" vector types."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:22
msgid "offset: Expr, optional"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:21
msgid ""
"The offset of pointer. We can use it to offset by the number of elements "
"from the address of ptr."
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:25
msgid "extent: Expr, optional"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:25 tvm.tir.op.tvm_access_ptr:15
msgid "The extent of pointer."
msgstr ""

#: of tvm.runtime.object.Object._move:15 tvm.tir.buffer.Buffer.access_ptr:28
#: tvm.tir.function.IndexMap.non_surjective_inverse:20
#: tvm.tir.function.PrimFunc.specialize:10
#: tvm.tir.schedule.schedule.Schedule.add_unit_loop:14
#: tvm.tir.schedule.schedule.Schedule.annotate:13
#: tvm.tir.schedule.schedule.Schedule.bind:22
#: tvm.tir.schedule.schedule.Schedule.blockize:16
#: tvm.tir.schedule.schedule.Schedule.cache_index:23
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:25
#: tvm.tir.schedule.schedule.Schedule.cache_read:30
#: tvm.tir.schedule.schedule.Schedule.cache_write:30
#: tvm.tir.schedule.schedule.Schedule.compute_at:36
#: tvm.tir.schedule.schedule.Schedule.compute_inline:19
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:30
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:30
#: tvm.tir.schedule.schedule.Schedule.fuse:18
#: tvm.tir.schedule.schedule.Schedule.merge:18
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:20
#: tvm.tir.schedule.schedule.Schedule.parallel:14
#: tvm.tir.schedule.schedule.Schedule.reindex:37
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:29
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:32
#: tvm.tir.schedule.schedule.Schedule.reorder:17
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:11
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:33
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:22
#: tvm.tir.schedule.schedule.Schedule.rfactor:74
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:24
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:33
#: tvm.tir.schedule.schedule.Schedule.set_scope:14
#: tvm.tir.schedule.schedule.Schedule.split:29
#: tvm.tir.schedule.schedule.Schedule.storage_align:20
#: tvm.tir.schedule.schedule.Schedule.tensorize:13
#: tvm.tir.schedule.schedule.Schedule.transform_block_layout:12
#: tvm.tir.schedule.schedule.Schedule.transform_layout:74
#: tvm.tir.schedule.schedule.Schedule.unannotate:11
#: tvm.tir.schedule.schedule.Schedule.unroll:9
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:17
#: tvm.tir.schedule.schedule.Schedule.vectorize:14
#: tvm.tir.transform.function_pass.prim_func_pass:33
msgid "Examples"
msgstr ""

#: of tvm.tir.buffer.Buffer.get_flattened_buffer:5
msgid "flattened"
msgstr ""

#: of tvm.tir.buffer.Buffer.get_flattened_buffer:-1 tvm.tir.expr.BufferLoad:-1
#: tvm.tir.stmt.BufferRealize:-1 tvm.tir.stmt.BufferRegion:-1
#: tvm.tir.stmt.BufferStore:-1 tvm.tir.stmt.MatchBufferRegion:-1
#: tvm.tir.stmt.Prefetch:-1
msgid "Buffer"
msgstr ""

#: of tvm.tir.buffer.Buffer.get_flattened_buffer:6
msgid "The corresponding flat buffer."
msgstr ""

#: of tvm.tir.buffer.Buffer.offset_of:5
msgid "indices : Union[PrimExpr, List[PrimExpr]]"
msgstr ""

#: of tvm.tir.buffer.Buffer.offset_of:7
msgid "The indices of the element in the original buffer."
msgstr ""

#: of tvm.tir.buffer.Buffer.offset_of:11
msgid "flattened_indices: List[PrimExpr]"
msgstr ""

#: of tvm.tir.buffer.Buffer.offset_of:13
msgid "The offset indices of the element in the flattened buffer."
msgstr ""

#: of tvm.tir.buffer.Buffer.scope:1
msgid ""
"Return the storage scope associated with this buffer. Returns ------- "
"scope : str"
msgstr ""

#: of tvm.tir.buffer.Buffer.scope:5
msgid "The storage scope associated with this buffer."
msgstr ""

#: of tvm.tir.buffer.Buffer.vload:6 tvm.tir.buffer.Buffer.vstore:6
msgid "begin"
msgstr ""

#: of tvm.tir.buffer.Buffer.vload:-1 tvm.tir.buffer.Buffer.vstore:-1
#: tvm.tir.expr.Shuffle:-1
msgid "Array of Expr"
msgstr ""

#: of tvm.tir.buffer.Buffer.vload:6 tvm.tir.buffer.Buffer.vstore:6
msgid "The beginning index in unit of Buffer.dtype"
msgstr ""

#: of tvm.tir.buffer.Buffer.vload:9
msgid ""
"The data type to be loaded, can be vector type which have lanes that is "
"multiple of Buffer.dtype"
msgstr ""

#: of tvm.tir.buffer.Buffer.vload:14
msgid "load"
msgstr ""

#: of tvm.tir.buffer.Buffer.vload:-1 tvm.tir.buffer.Buffer.vstore:-1
#: tvm.tir.op.assume:-1 tvm.tir.op.mma_fill:-1 tvm.tir.op.mma_store:-1
#: tvm.tir.op.ptx_cp_async:-1 tvm.tir.op.ptx_cp_async_bulk:-1
#: tvm.tir.op.ptx_ldmatrix:-1 tvm.tir.op.ptx_mma:-1 tvm.tir.op.ptx_mma_sp:-1
#: tvm.tir.op.ret:-1 tvm.tir.op.tvm_access_ptr:-1 tvm.tir.op.tvm_bmma_sync:-1
#: tvm.tir.op.tvm_fill_fragment:-1 tvm.tir.op.tvm_load_matrix_sync:-1
#: tvm.tir.op.tvm_mma_sync:-1 tvm.tir.op.tvm_stack_make_array:-1
#: tvm.tir.op.tvm_store_matrix_sync:-1 tvm.tir.op.tvm_struct_set:-1
#: tvm.tir.op.tvm_thread_allreduce:-1 tvm.tir.op.tvm_tuple:-1
#: tvm.tir.op.type_annotation:-1
msgid "Expr"
msgstr ""

#: of tvm.tir.buffer.Buffer.vload:15
msgid "The corresponding load expression."
msgstr ""

#: of tvm.tir.buffer.Buffer.vstore:9 tvm.tir.stmt.ProducerStore:9
msgid "The value to be stored."
msgstr ""

#: of tvm.tir.buffer.Buffer.vstore:13
msgid "store"
msgstr ""

#: of tvm.tir.buffer.Buffer.vstore:14
msgid "The corresponding store stmt."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:59 tvm.tir.expr.BufferLoad:6
#: tvm.tir.stmt.BufferRealize:6 tvm.tir.stmt.BufferRegion:6
#: tvm.tir.stmt.BufferStore:6 tvm.tir.stmt.MatchBufferRegion:6
#: tvm.tir.stmt.Prefetch:6
msgid "buffer"
msgstr ""

#: of tvm.tir.expr.BufferLoad:6 tvm.tir.expr.ProducerLoad:6
msgid "The buffer to be loaded."
msgstr ""

#: of tvm.tir.expr.BufferLoad:9 tvm.tir.expr.ProducerLoad:9
#: tvm.tir.expr.Shuffle:9 tvm.tir.function.IndexMap.map_indices:6
#: tvm.tir.stmt.BufferStore:12 tvm.tir.stmt.ProducerStore:12
msgid "indices"
msgstr ""

#: of tvm.tir.expr.BufferLoad:9 tvm.tir.expr.ProducerLoad:9
msgid "The buffer indices."
msgstr ""

#: of tvm.tir.stmt.BufferRealize:6 tvm.tir.stmt.BufferStore:6
msgid "The buffer."
msgstr ""

#: of tvm.tir.stmt.BufferRealize:9 tvm.tir.stmt.Prefetch:9
#: tvm.tir.stmt.ProducerRealize:9
msgid "bounds"
msgstr ""

#: of tvm.tir.stmt.BufferRealize:-1 tvm.tir.stmt.BufferRegion:-1
msgid "List[Range]"
msgstr ""

#: of tvm.tir.stmt.BufferRealize:9 tvm.tir.stmt.BufferStore:9
msgid "The value we to be stored."
msgstr ""

#: of tvm.tir.stmt.BufferRealize:12 tvm.tir.stmt.ProducerRealize:12
msgid "The realize condition."
msgstr ""

#: of tvm.tir.stmt.BufferRealize:15
msgid "The body of the statement."
msgstr ""

#: of tvm.tir.stmt.BufferRegion:6
msgid "The buffer of the buffer region"
msgstr ""

#: of tvm.tir.stmt.BufferRegion:8
msgid "region"
msgstr ""

#: of tvm.tir.stmt.BufferRegion:9
msgid "The region array of the buffer region"
msgstr ""

#: of tvm.tir.stmt.BufferStore:12
msgid "The indices location to be stored."
msgstr ""

#: of tvm.tir.expr.Call:6
msgid "The return data type"
msgstr ""

#: of tvm.tir.expr.Call:10 tvm.tir.generic.add:14 tvm.tir.generic.multiply:14
#: tvm.tir.generic.subtract:14 tvm.tir.op.ceildiv:14
msgid "op"
msgstr ""

#: of tvm.tir.expr.Call:-1
msgid "Union[RelayExpr, str]"
msgstr ""

#: of tvm.tir.expr.Call:9
msgid "The function to be called, or the name to the global tvm.Op"
msgstr ""

#: of tvm.tir.expr.Call:13 tvm.tir.op.all:7 tvm.tir.op.any:6
#: tvm.tir.op.call_cpacked:9 tvm.tir.op.call_cpacked_lowered:8
#: tvm.tir.op.call_extern:12 tvm.tir.op.call_intrin:15
#: tvm.tir.op.call_llvm_intrin:12 tvm.tir.op.call_llvm_pure_intrin:12
#: tvm.tir.op.call_packed:13 tvm.tir.op.call_packed_lowered:11
#: tvm.tir.op.call_pure_extern:12 tvm.tir.op.trace:11
#: tvm.tir.op.tvm_stack_make_shape:6 tvm.tir.stmt.stmt_seq:6
msgid "args"
msgstr ""

#: of tvm.tir.expr.Call:13
msgid "The input arguments to the call"
msgstr ""

#: of tvm.tir.expr.Cast:6 tvm.tir.expr.FloatImm:6 tvm.tir.expr.IntImm:6
#: tvm.tir.expr.SizeVar:10 tvm.tir.expr.Var:9
msgid "The data type"
msgstr ""

#: of tvm.tir.expr.Cast:9 tvm.tir.expr.StringImm:6
msgid "The value of the function."
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:6 tvm.tir.expr.CommReducer:6
#: tvm.tir.generic.add:5 tvm.tir.generic.multiply:5 tvm.tir.generic.subtract:5
#: tvm.tir.op.ceildiv:5
msgid "lhs"
msgstr ""

#: of tvm.tir.analysis.analysis.undefined_vars:-1 tvm.tir.expr.CommReducer:-1
#: tvm.tir.function.IndexMap:-1
msgid "List[Var]"
msgstr ""

#: of tvm.tir.expr.CommReducer:6
msgid "The left arguments of the reducer."
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:9 tvm.tir.expr.CommReducer:9
#: tvm.tir.generic.add:7 tvm.tir.generic.multiply:7 tvm.tir.generic.subtract:7
#: tvm.tir.op.ceildiv:7
msgid "rhs"
msgstr ""

#: of tvm.tir.expr.CommReducer:9
msgid "The right arguments of the reducer."
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_allocated_bytes:13
#: tvm.tir.analysis.analysis.calculate_constant_bytes:13
#: tvm.tir.analysis.analysis.calculate_workspace_bytes:13
#: tvm.tir.analysis.analysis.detect_buffer_access_lca:13
#: tvm.tir.analysis.analysis.expr_deep_equal:14
#: tvm.tir.analysis.analysis.get_block_access_region:17
#: tvm.tir.analysis.analysis.get_block_read_write_region:14
#: tvm.tir.analysis.analysis.get_vtcm_compaction_passes:6
#: tvm.tir.analysis.analysis.undefined_vars:13
#: tvm.tir.analysis.analysis.verify_gpu_code:13
#: tvm.tir.analysis.analysis.verify_memory:10
#: tvm.tir.analysis.analysis.verify_ssa:10 tvm.tir.expr.CommReducer:12
#: tvm.tir.function.IndexMap.map_indices:10
#: tvm.tir.function.IndexMap.map_shape:10 tvm.tir.function.TensorIntrin.get:14
#: tvm.tir.op.if_then_else:20 tvm.tir.schedule.schedule.Schedule.blockize:13
#: tvm.tir.schedule.schedule.Schedule.get:15
#: tvm.tir.schedule.schedule.Schedule.get_sref:14
#: tvm.tir.schedule.schedule.Schedule.sample_categorical:14
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:12
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:18
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:16
#: tvm.tir.stmt_functor.ir_transform:22 tvm.tir.stmt_functor.renew_defs:12
#: tvm.tir.stmt_functor.substitute:13
msgid "result"
msgstr ""

#: of tvm.tir.expr.CommReducer:12
msgid "The reduction results."
msgstr ""

#: of tvm.tir.expr.CommReducer:15
msgid "identity_element"
msgstr ""

#: of tvm.tir.expr.CommReducer:15
msgid "The identity elements."
msgstr ""

#: of tvm.tir.stmt.DeclBuffer:6
msgid "buffer: Buffer"
msgstr ""

#: of tvm.tir.stmt.DeclBuffer:6
msgid "The buffer being declared."
msgstr ""

#: of tvm.tir.stmt.DeclBuffer:9
msgid "The body statement to be executed."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:54 tvm.tir.stmt.DeclBuffer:11
msgid "span: Optional[Span]"
msgstr ""

#: of tvm.tir.stmt.DeclBuffer:12
msgid "The location of this DeclBuffer in the source code."
msgstr ""

#: of tvm.tir.stmt.Evaluate:6
msgid "The expression to be evalued."
msgstr ""

#: of tvm.tir.expr.FloatImm:-1
msgid "float"
msgstr ""

#: of tvm.tir.expr.FloatImm:9 tvm.tir.expr.IntImm:9
msgid "The constant value."
msgstr ""

#: of tvm.tir.stmt.For:6
msgid "loop_var"
msgstr ""

#: of tvm.tir.stmt.For:6
msgid "The loop variable."
msgstr ""

#: of tvm.tir.stmt.For:9
msgid "min_val"
msgstr ""

#: of tvm.tir.stmt.For:9
msgid "The beginning value."
msgstr ""

#: of tvm.tir.op.tvm_access_ptr:15 tvm.tir.stmt.For:12
msgid "extent"
msgstr ""

#: of tvm.tir.stmt.For:12
msgid "The length of the loop."
msgstr ""

#: of tvm.tir.stmt.For:15
msgid "kind"
msgstr ""

#: of tvm.tir.stmt.For:-1
msgid "ForKind"
msgstr ""

#: of tvm.tir.stmt.For:15
msgid "The type of the for."
msgstr ""

#: of tvm.tir.stmt.For:22
msgid "thread_binding: Optional[tir.IterVar]"
msgstr ""

#: of tvm.tir.stmt.For:21
msgid "The thread this loop binds to. Only valid if kind is ThreadBinding"
msgstr ""

#: of tvm.tir.stmt.For:25
msgid "annotations: tvm.ir.Map"
msgstr ""

#: of tvm.tir.stmt.ForKind:4
msgid "note"
msgstr ""

#: of tvm.tir.stmt.ForKind:5
msgid ""
"ForKind can change the control flow semantics of the loop and need to be "
"considered in all TIR passes."
msgstr ""

#: of tvm.tir.stmt.IfThenElse:6
msgid "The expression"
msgstr ""

#: of tvm.tir.stmt.IfThenElse:9
msgid "then_case"
msgstr ""

#: of tvm.tir.stmt.IfThenElse:9
msgid "The statement to execute if condition is true."
msgstr ""

#: of tvm.tir.stmt.IfThenElse:12
msgid "else_case"
msgstr ""

#: of tvm.tir.stmt.IfThenElse:12
msgid "The statement to execute if condition is false."
msgstr ""

#: of tvm.tir.function.IndexMap:5
msgid "initial_indices"
msgstr ""

#: of tvm.tir.function.IndexMap:6
msgid "Variables representing the indices prior to remapping."
msgstr ""

#: of tvm.tir.function.IndexMap:7
msgid "final_indices"
msgstr ""

#: of tvm.tir.function.IndexMap:8
msgid "Expressions defining the indices after remapping."
msgstr ""

#: of tvm.tir.function.IndexMap:14 tvm.tir.function.IndexMap.from_func:26
#: tvm.tir.function.IndexMap.from_func_with_separators:28
msgid "inverse_index_map"
msgstr ""

#: of tvm.tir.function.IndexMap:-1 tvm.tir.function.IndexMap.from_func:-1
#: tvm.tir.function.IndexMap.from_func_with_separators:-1
msgid "Union[Callable, Optional[IndexMap]]"
msgstr ""

#: of tvm.tir.function.IndexMap:10 tvm.tir.function.IndexMap.from_func:22
#: tvm.tir.function.IndexMap.from_func_with_separators:24
msgid ""
"The optional pre-defined inverse index map. When this is defined, "
"IndexMap::Inverse will return the pre-defined inverse index map. "
"Otherwise, the inverse index map will be computed on the fly. It is the "
"user's responsibility to ensure the correctness of the pre-defined "
"inverse index map."
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
msgid ""
":py:obj:`from_func <tvm.tir.IndexMap.from_func>`\\ "
"\\(mapping\\_function\\[\\, ndim\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1
#: tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
#: tvm.tir.function.IndexMap.from_func_with_separators:1
msgid "Create an index map from a function"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
msgid ""
":py:obj:`from_func_with_separators "
"<tvm.tir.IndexMap.from_func_with_separators>`\\ \\(mapping\\_function\\)"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
msgid ":py:obj:`inverse <tvm.tir.IndexMap.inverse>`\\ \\(shape\\)"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
#: tvm.tir.function.IndexMap.inverse:1
#: tvm.tir.function.IndexMap.non_surjective_inverse:1
msgid "Return the inverse of the map"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
msgid ""
":py:obj:`is_equivalent_to <tvm.tir.IndexMap.is_equivalent_to>`\\ "
"\\(other\\_map\\)"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
#: tvm.tir.function.IndexMap.is_equivalent_to:1
msgid "Return if the index maps are equivalent."
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
msgid ":py:obj:`map_indices <tvm.tir.IndexMap.map_indices>`\\ \\(indices\\)"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
#: tvm.tir.function.IndexMap.map_indices:1
msgid "Apply the index map to a set of indices"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
msgid ":py:obj:`map_ndarray <tvm.tir.IndexMap.map_ndarray>`\\ \\(arr\\_src\\)"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
#: tvm.tir.function.IndexMap.map_ndarray:1
msgid "Apply thie index map to transform the layout of the input NDArray"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
msgid ":py:obj:`map_shape <tvm.tir.IndexMap.map_shape>`\\ \\(shape\\)"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
#: tvm.tir.function.IndexMap.map_shape:1
msgid "Apply the index map to a buffer shape"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:1:<autosummary>:1
msgid ""
":py:obj:`non_surjective_inverse "
"<tvm.tir.IndexMap.non_surjective_inverse>`\\ \\(shape\\)"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:5
#: tvm.tir.function.IndexMap.from_func_with_separators:5
msgid "mapping_function : Callable"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:7
msgid ""
"The function to map from source indices to target indices. The function "
"should accept `tir.Var` parameters and return a either a `tir.PrimExpr`, "
"or a list of `tir.PrimExpr`. Returning a `tir.PrimExpr` is equivalent to "
"returning a list of length 1 containing that `tir.PrimExpr`."
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:13
#: tvm.tir.function.IndexMap.from_func_with_separators:15
msgid "ndim: Optional[int]"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:15
msgid ""
"The dimensionality of the buffer to which this transformation should be "
"applied.  If mapping_function uses variadic argument `*args`, `ndim` must"
" be specified.  If mapping_function does not use variadic arguments, ndim"
" is optional."
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:30
msgid "index_map: IndexMap"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func:32
msgid "Returns an IndexMap representing the `mapping_function`."
msgstr ""

#: of tvm.tir.function.IndexMap.from_func_with_separators:7
msgid ""
"The function to map from source indices to target indices. The function "
"should accept tir.Var parameters and return either a `tir.PrimExpr` or a "
"list.  Each element of the returned list should be either a "
"`tir.PrimExpr` or the object `IndexMap.AXIS_SEPARATOR`.  Returning a "
"`tir.PrimExpr` is equivalent to returning a list of length 1 containing "
"that `tir.PrimExpr`."
msgstr ""

#: of tvm.tir.function.IndexMap.from_func_with_separators:17
msgid ""
"The dimensionality of the buffer to which this transformation should be "
"applied.  If mapping_function uses variadic argument `*args`, ndim must "
"be specified.  If mapping_function does not use variadic arguments, ndim "
"is optional."
msgstr ""

#: of tvm.tir.function.IndexMap.from_func_with_separators:31
msgid "index_dtype"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func_with_separators:31
msgid "The default index dtype to use for input iters in the mapping function."
msgstr ""

#: of tvm.tir.function.IndexMap.from_func_with_separators:35
msgid "ret: Tuple[IndexMap, List[int]]"
msgstr ""

#: of tvm.tir.function.IndexMap.from_func_with_separators:37
msgid ""
"Returns a tuple whose first element is an IndexMap representing the "
"`mapping_function`, and whose second index is a list of indices at which "
"`IndexMap.AXIS_SEPARATOR` occurred."
msgstr ""

#: of tvm.tir.function.IndexMap.inverse:3
msgid "Throws an error if the function is not bijective."
msgstr ""

#: of tvm.tir.function.IndexMap.inverse:7
#: tvm.tir.function.IndexMap.non_surjective_inverse:7
msgid "shape: List[Union[Range,PrimExpr]]"
msgstr ""

#: of tvm.tir.function.IndexMap.inverse:9
msgid ""
"The region over which the inverse should be determined. Used for "
"validating that the mapping is bijective over this range."
msgstr ""

#: of tvm.tir.function.IndexMap.inverse:15
msgid "inverse : IndexMap"
msgstr ""

#: of tvm.tir.function.IndexMap.inverse:17
msgid "The inverse"
msgstr ""

#: of tvm.tir.function.IndexMap.is_equivalent_to:5
msgid "other_map: IndexMap"
msgstr ""

#: of tvm.tir.function.IndexMap.is_equivalent_to:7
msgid "The IndexMap to which the comparison should be made."
msgstr ""

#: of tvm.tir.function.IndexMap.is_equivalent_to:11
msgid "is_equivalent: bool"
msgstr ""

#: of tvm.tir.function.IndexMap.is_equivalent_to:13
msgid ""
"True if the two mappings represent the same transformation, otherwise "
"False"
msgstr ""

#: of tvm.tir.function.IndexMap.map_indices:6
msgid "The indices to be mapped"
msgstr ""

#: of tvm.tir.function.IndexMap.map_indices:11
msgid "The mapped indices"
msgstr ""

#: of tvm.tir.function.IndexMap.map_ndarray:6
msgid "arr_src"
msgstr ""

#: of tvm.tir.function.IndexMap.map_ndarray:-1
msgid "runtime.NDArray"
msgstr ""

#: of tvm.tir.function.IndexMap.map_ndarray:6
msgid "The NDArray to be transformed"
msgstr ""

#: of tvm.tir.function.IndexMap.map_ndarray:10
msgid "arr_dst"
msgstr ""

#: of tvm.tir.function.IndexMap.map_ndarray:11
msgid "The transformed NDArray"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:11 tvm.tir.function.IndexMap.map_shape:6
#: tvm.tir.op.ptx_mma:10 tvm.tir.op.ptx_mma_sp:10
#: tvm.tir.op.tvm_stack_make_array:9
msgid "shape"
msgstr ""

#: of tvm.tir.function.IndexMap.map_shape:6
msgid "The buffer shape to be mapped"
msgstr ""

#: of tvm.tir.function.IndexMap.map_shape:11
msgid "The mapped shape"
msgstr ""

#: of tvm.tir.function.IndexMap.non_surjective_inverse:3
msgid "Can be applied to transformations that introduce padding."
msgstr ""

#: of tvm.tir.function.IndexMap.non_surjective_inverse:9
msgid ""
"The region over which the inverse should be determined. Used for "
"determining the predicate."
msgstr ""

#: of tvm.tir.function.IndexMap.non_surjective_inverse:14
msgid "result : Tuple[IndexMap, PrimExpr]"
msgstr ""

#: of tvm.tir.function.IndexMap.non_surjective_inverse:16
msgid ""
"The inverse, and a predicate for which the inverse maps to a valid index "
"in the input range."
msgstr ""

#: of tvm.tir.expr.IterVar:3
msgid "IterVar represents axis iterations in the computation."
msgstr ""

#: of tvm.tir.expr.IterVar:8
msgid "dom"
msgstr ""

#: of tvm.tir.expr.IterVar:-1
msgid "Range"
msgstr ""

#: of tvm.tir.expr.IterVar:8
msgid "The domain of the iteration."
msgstr ""

#: of tvm.tir.expr.IterVar:11 tvm.tir.expr.Let:6 tvm.tir.stmt.LetStmt:6
msgid "var"
msgstr ""

#: of tvm.tir.expr.IterVar:-1
msgid "Union[Var, str]"
msgstr ""

#: of tvm.tir.expr.IterVar:11
msgid "The internal variable that is used for iteration."
msgstr ""

#: of tvm.tir.expr.IterVar:14
msgid "iter_type"
msgstr ""

#: of tvm.tir.expr.IterVar:14
msgid "The iteration type."
msgstr ""

#: of tvm.tir.expr.IterVar:17
msgid "thread_tag"
msgstr ""

#: of tvm.tir.expr.IterVar:17
msgid "The thread type tag."
msgstr ""

#: of tvm.tir.expr.IterVar:24
msgid ""
"te.thread_axis: Create thread axis IterVar. te.reduce_axis: Create reduce"
" axis IterVar."
msgstr ""

#: of tvm.tir.data_layout.Layout:1
msgid ""
"Layout is composed of upper cases, lower cases and numbers, where upper "
"case indicates a primal axis and the corresponding lower case with factor"
" size indicates the subordinate axis. For example, NCHW16c can describe a"
" 5-D tensor of [batch_size, channel, height, width, channel_block]. Here "
"subordinate axis channel_block=16 is the factor size of the primal axis C"
" (channel)."
msgstr ""

#: of tvm.tir.data_layout.Layout:10
msgid "layout : Declare a layout"
msgstr ""

#: of tvm.tir.data_layout.Layout.factor_of:1:<autosummary>:1
msgid ":py:obj:`factor_of <tvm.tir.Layout.factor_of>`\\ \\(axis\\)"
msgstr ""

#: of tvm.tir.data_layout.Layout.factor_of:1
#: tvm.tir.data_layout.Layout.factor_of:1:<autosummary>:1
msgid "Get the factor size of the subordinate axis."
msgstr ""

#: of tvm.tir.data_layout.Layout.factor_of:1:<autosummary>:1
msgid ":py:obj:`index_of <tvm.tir.Layout.index_of>`\\ \\(axis\\)"
msgstr ""

#: of tvm.tir.data_layout.Layout.factor_of:1:<autosummary>:1
#: tvm.tir.data_layout.Layout.index_of:1
msgid "Get the index of an axis"
msgstr ""

#: of tvm.tir.data_layout.Layout.factor_of:6
#: tvm.tir.data_layout.Layout.index_of:6
#: tvm.tir.op.comm_reducer.<locals>.reducer:7
#: tvm.tir.schedule.schedule.Schedule.storage_align:12
msgid "axis"
msgstr ""

#: of tvm.tir.data_layout.Layout.factor_of:6
#: tvm.tir.data_layout.Layout.index_of:6
msgid "The axis name, need to be [a-z,A-Z]"
msgstr ""

#: of tvm.tir.data_layout.Layout.factor_of:12
#: tvm.tir.schedule.schedule.Schedule.storage_align:14
msgid "factor"
msgstr ""

#: of tvm.tir.data_layout.Layout.factor_of:11
msgid ""
"the size of the subordinate-axis of axis (if axis is a primal-axis), or "
"the size of axis itself (if axis is a subordinate-axis). Return -1 if "
"axis is not in the layout."
msgstr ""

#: of tvm.tir.data_layout.Layout.index_of:10 tvm.tir.op.tvm_fill_fragment:18
#: tvm.tir.op.tvm_load_matrix_sync:18 tvm.tir.op.tvm_store_matrix_sync:18
#: tvm.tir.op.tvm_struct_get:12 tvm.tir.op.tvm_struct_set:9
msgid "index"
msgstr ""

#: of tvm.tir.data_layout.Layout.index_of:11
msgid "The index of the axis, -1 if not found."
msgstr ""

#: of tvm.tir.expr.Let:6 tvm.tir.stmt.LetStmt:6
msgid "The variable in the binding."
msgstr ""

#: of tvm.tir.expr.Let:9 tvm.tir.stmt.LetStmt:9
msgid "The value in to be binded."
msgstr ""

#: of tvm.tir.expr.Let:12
msgid "The body expression."
msgstr ""

#: of tvm.tir.stmt.MatchBufferRegion:6
msgid "The target buffer"
msgstr ""

#: of tvm.tir.stmt.MatchBufferRegion:8
msgid "source"
msgstr ""

#: of tvm.tir.stmt.MatchBufferRegion:-1
msgid "BufferRegion"
msgstr ""

#: of tvm.tir.stmt.MatchBufferRegion:9
msgid "The region of source buffer"
msgstr ""

#: of tvm.tir.expr.Not:6
msgid "The input value"
msgstr ""

#: of tvm.tir.stmt.Prefetch:6
msgid "The buffer to be prefetched."
msgstr ""

#: of tvm.tir.stmt.Prefetch:-1
msgid "list of Range"
msgstr ""

#: of tvm.tir.stmt.Prefetch:9
msgid "The bounds to be prefetched."
msgstr ""

#: of tvm.tir.function.PrimFunc:6
msgid "params: List[Union[tvm.tir.Var, tvm.tir.Buffer]]"
msgstr ""

#: of tvm.tir.function.PrimFunc:6
msgid "List of input parameters to the function."
msgstr ""

#: of tvm.tir.function.PrimFunc:9
msgid "body: tvm.tir.Stmt"
msgstr ""

#: of tvm.tir.function.PrimFunc:9
msgid "The body of the function."
msgstr ""

#: of tvm.tir.function.PrimFunc:12
msgid "ret_type: tvm.ir.Type"
msgstr ""

#: of tvm.tir.function.PrimFunc:12
msgid "The return type annotation of the function."
msgstr ""

#: of tvm.tir.function.PrimFunc:15
msgid "buffer_map"
msgstr ""

#: of tvm.tir.function.PrimFunc:-1
msgid "Map[tvm.tir.Var, tvm.tir.Buffer]"
msgstr ""

#: of tvm.tir.function.PrimFunc:15
msgid "The buffer binding map."
msgstr ""

#: of tvm.tir.function.PrimFunc:18
msgid "attrs: Optional[tvm.Attrs]"
msgstr ""

#: of tvm.tir.function.PrimFunc:18
msgid "Attributes of the function, can be None"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:1:<autosummary>:1
msgid ":py:obj:`specialize <tvm.tir.PrimFunc.specialize>`\\ \\(param\\_map\\)"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:1
#: tvm.tir.function.PrimFunc.specialize:1:<autosummary>:1
msgid "Specialize parameters of PrimFunc"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:1:<autosummary>:1
msgid ""
":py:obj:`with_body <tvm.tir.PrimFunc.with_body>`\\ \\(new\\_body\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:1:<autosummary>:1
#: tvm.tir.function.PrimFunc.with_body:1
msgid "Create a new PrimFunc with the same set signatures but a new body."
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:7
msgid "param_map"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:-1
msgid "Mapping[Var, Union[PrimExpr, Buffer]]"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:7
msgid "The mapping from function params to the instance"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:11
msgid "We can define a Meta TIR function with symbolic shape:"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:25
msgid "Then we can make it specialized with given shapes or buffers."
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:34
msgid "The specialized function:"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:50
msgid "func"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:-1
#: tvm.tir.function.PrimFunc.with_body:-1 tvm.tir.function.TensorIntrin:-1
#: tvm.tir.function.TensorIntrin.register:-1 tvm.tir.stmt_functor.renew_defs:-1
msgid "PrimFunc"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:51
msgid "The new function with parameter specialized"
msgstr ""

#: of tvm.tir.function.PrimFunc.with_body:6
msgid "new_body"
msgstr ""

#: of tvm.tir.function.PrimFunc.with_body:6
msgid "The new body."
msgstr ""

#: of tvm.tir.function.PrimFunc.with_body:13
msgid "new_func"
msgstr ""

#: of tvm.tir.function.PrimFunc.with_body:14
msgid "The created new function."
msgstr ""

#: of tvm.tir.expr.ProducerLoad:6 tvm.tir.stmt.ProducerRealize:6
#: tvm.tir.stmt.ProducerStore:6
msgid "producer"
msgstr ""

#: of tvm.tir.expr.ProducerLoad:-1 tvm.tir.stmt.ProducerRealize:-1
#: tvm.tir.stmt.ProducerStore:-1
msgid "DataProducer"
msgstr ""

#: of tvm.tir.stmt.ProducerRealize:6 tvm.tir.stmt.ProducerStore:6
msgid "The data producer."
msgstr ""

#: of tvm.tir.stmt.ProducerRealize:-1
msgid "list of range"
msgstr ""

#: of tvm.tir.stmt.ProducerRealize:9
msgid "The bound of realize"
msgstr ""

#: of tvm.tir.stmt.ProducerRealize:15
msgid "The realize body"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:11
#: tvm.tir.stmt.ProducerRealize:18
msgid "storage_scope"
msgstr ""

#: of tvm.tir.stmt.ProducerRealize:18
msgid "The storage scope associated with this realization"
msgstr ""

#: of tvm.tir.stmt.ProducerStore:12
msgid "The index arguments of the store."
msgstr ""

#: of tvm.tir.expr.Ramp:6
msgid "base"
msgstr ""

#: of tvm.tir.expr.Ramp:6
msgid "The base expression."
msgstr ""

#: of tvm.tir.expr.Ramp:9 tvm.tir.op.tvm_load_matrix_sync:24
#: tvm.tir.op.tvm_store_matrix_sync:24
msgid "stride"
msgstr ""

#: of tvm.tir.expr.Ramp:-1
msgid "ramp stride"
msgstr ""

#: of tvm.tir.expr.Ramp:9
msgid "The stride of the ramp."
msgstr ""

#: of tvm.tir.expr.Reduce:6
msgid "combiner"
msgstr ""

#: of tvm.tir.expr.Reduce:-1
msgid "CommReducer"
msgstr ""

#: of tvm.tir.expr.Reduce:6
msgid "The combiner."
msgstr ""

#: of tvm.tir.expr.Reduce:9
msgid "src"
msgstr ""

#: of tvm.tir.expr.Reduce:9 tvm.tir.op.comm_reducer.<locals>.reducer:6
msgid "The source expression."
msgstr ""

#: of tvm.tir.expr.Reduce:12
msgid "rdom"
msgstr ""

#: of tvm.tir.expr.Reduce:-1
msgid "list of IterVar"
msgstr ""

#: of tvm.tir.expr.Reduce:12
msgid "The iteration domain"
msgstr ""

#: of tvm.tir.expr.Reduce:15
msgid "The reduce condition."
msgstr ""

#: of tvm.tir.expr.Reduce:18
msgid "value_index"
msgstr ""

#: of tvm.tir.expr.Reduce:18
msgid "The value index."
msgstr ""

#: of tvm.tir.expr.Reduce:21
msgid "init"
msgstr ""

#: of tvm.tir.expr.Reduce:21
msgid "The initial value for output. This can be an int, float or ProducerLoad"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:3
msgid ""
"A schedule is a set of transformations that change the order of "
"computation but preserve the semantics of computation. Some example of "
"schedules: 1) Split a loop into two; 2) Reorder two loops; 3) Inline the "
"computation of a specific buffer into its consumer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:9
msgid ""
"The schedule class stores auxiliary information to schedule correctly and"
" efficiently."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:11
msgid ""
"Link to tutorial: "
"https://tvm.apache.org/docs/tutorials/language/schedule_primitives.html"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`__init__ <tvm.tir.Schedule.__init__>`\\ \\(mod\\, \\*\\[\\, "
"seed\\, debug\\_mask\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Construct a TensorIR schedule class from an IRModule"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`_create_non_traced <tvm.tir.Schedule._create_non_traced>`\\ "
"\\(mod\\, \\*\\[\\, seed\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule._create_non_traced:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Construct a non-traced TensorIR schedule class from an IRModule."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`add_unit_loop <tvm.tir.Schedule.add_unit_loop>`\\ "
"\\(block\\_or\\_loop\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Create a new unit loop on top of the specific block or loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`annotate <tvm.tir.Schedule.annotate>`\\ \\(block\\_or\\_loop\\, "
"ann\\_key\\, ann\\_val\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Annotate a block/loop with a key value pair"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`bind <tvm.tir.Schedule.bind>`\\ \\(loop\\, thread\\_axis\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Bind the input loop to the given thread axis."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`blockize <tvm.tir.Schedule.blockize>`\\ \\(target\\[\\, "
"preserve\\_unit\\_iters\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Convert multiple blocks or the subtree rooted at a specific loop into a "
"block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`cache_index <tvm.tir.Schedule.cache_index>`\\ \\(block\\, "
"storage\\_scope\\[\\, cse\\_thresh\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Create a block to cache precomputed index for later use."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`cache_inplace <tvm.tir.Schedule.cache_inplace>`\\ \\(block\\, "
"read\\_buffer\\_index\\, ...\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Create blocks that reads & write a buffer region into a cache block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`cache_read <tvm.tir.Schedule.cache_read>`\\ \\(block\\, "
"read\\_buffer\\_index\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Create a block that reads a buffer region into a read cache."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`cache_write <tvm.tir.Schedule.cache_write>`\\ \\(block\\, "
"write\\_buffer\\_index\\, ...\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Create a block that reads a buffer region into a write cache."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`can_decompose_padding "
"<tvm.tir.Schedule.can_decompose_padding>`\\ \\(block\\, loop\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.can_decompose_padding:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Check whether the block match padding pattern and can be decomposed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`compute_at <tvm.tir.Schedule.compute_at>`\\ \\(block\\, "
"loop\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Compute-At."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`compute_inline <tvm.tir.Schedule.compute_inline>`\\ \\(block\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Inline a block into its consumer(s)."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`copy <tvm.tir.Schedule.copy>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.copy:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Returns a copy of the schedule, including both the state and the symbol "
"table, * guaranteeing that * 1) SRef tree is completely reconstructed; * "
"2) The IRModule being scheduled is untouched; * 3) All the random "
"variables are valid in the copy, pointing to the corresponding sref * "
"reconstructed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`decompose_padding <tvm.tir.Schedule.decompose_padding>`\\ "
"\\(block\\, loop\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Decompose a block of padding computation pattern into two separate blocks."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`decompose_reduction <tvm.tir.Schedule.decompose_reduction>`\\ "
"\\(block\\, loop\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Decompose a reduction block into two separate blocks."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`enter_postproc <tvm.tir.Schedule.enter_postproc>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.enter_postproc:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "A no-op that marks the start of postprocessing phase of scheduling"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`fork_seed <tvm.tir.Schedule.fork_seed>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fork_seed:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Returns a forked random state as seed for new schedules"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`fuse <tvm.tir.Schedule.fuse>`\\ \\(\\*loops\\[\\, "
"preserve\\_unit\\_iters\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Fuse a list of consecutive loops into one."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`get <tvm.tir.Schedule.get>`\\ \\(rand\\_var\\_or\\_sref\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Returns: - the corresponding Block that a BlockRV evaluates to; - the "
"corresponding For that a LoopRV evaluates to; - the corresponding integer"
" that a ExprRV evaluates to; - the corresponding Block that a block sref "
"points to; - the corresponding For that a loop sref points to;"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`get_block <tvm.tir.Schedule.get_block>`\\ \\(name\\[\\, "
"func\\_name\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Retrieve a block in a specific function with its name"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`get_child_blocks <tvm.tir.Schedule.get_child_blocks>`\\ "
"\\(block\\_or\\_loop\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Get the leaf blocks of a specific block/loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`get_consumers <tvm.tir.Schedule.get_consumers>`\\ \\(block\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_consumers:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Get the consumers of a specific block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`get_loops <tvm.tir.Schedule.get_loops>`\\ \\(block\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_loops:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Get the parent loops of the block in its scope, from outer to inner"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`get_output_blocks <tvm.tir.Schedule.get_output_blocks>`\\ "
"\\(scope\\_block\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Get the list of output blocks within the given scope An output block is a"
" block which has atleast one buffer being written to, but is not "
"allocated within the PrimFunc"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`get_producers <tvm.tir.Schedule.get_producers>`\\ \\(block\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_producers:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Get the producers of a specific block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`get_sref <tvm.tir.Schedule.get_sref>`\\ "
"\\(rand\\_var\\_or\\_stmt\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_sref:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Returns the corresponding sref to the given 1) LoopRV 2) BlockRV 3) Block"
" 4) For"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`merge <tvm.tir.Schedule.merge>`\\ \\(\\*loops\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Merge a list of loops into one."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`pad_einsum <tvm.tir.Schedule.pad_einsum>`\\ \\(block\\, "
"padding\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Pad the computation of Einsum."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`parallel <tvm.tir.Schedule.parallel>`\\ \\(loop\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Parallelize the input loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`reindex <tvm.tir.Schedule.reindex>`\\ \\(block\\, buffer\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Create a block that read/write a buffer region into a read/write cache "
"with reindexing."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`reindex_cache_read <tvm.tir.Schedule.reindex_cache_read>`\\ "
"\\(block\\, read\\_buffer\\_index\\, ...\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Create a block that reads a buffer region into a read cache using "
"customized indices specified by index map."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`reindex_cache_write <tvm.tir.Schedule.reindex_cache_write>`\\ "
"\\(block\\, ...\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Create a block that reads a buffer region into a write cache using "
"customized indices specified by index map."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`remove_rv <tvm.tir.Schedule.remove_rv>`\\ \\(rand\\_var\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.remove_rv:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Remove a random variable from the symbol table"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`reorder <tvm.tir.Schedule.reorder>`\\ \\(\\*ordered\\_loops\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Reorder a list of loops."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`reorder_block_iter_var "
"<tvm.tir.Schedule.reorder_block_iter_var>`\\ \\(block\\, new\\_order\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Reorder the itervars inside a given block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`reverse_compute_at <tvm.tir.Schedule.reverse_compute_at>`\\ "
"\\(block\\, loop\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Reverse-Compute-At."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`reverse_compute_inline "
"<tvm.tir.Schedule.reverse_compute_inline>`\\ \\(block\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Inline a block into its only producer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`rfactor <tvm.tir.Schedule.rfactor>`\\ \\(loop\\, factor\\_axis\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Factorize an associative reduction block by the specified loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`rolling_buffer <tvm.tir.Schedule.rolling_buffer>`\\ \\(block\\, "
"write\\_buffer\\_index\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Compute the target buffer via rolling buffering, select the outermost "
"rollable axis with a positive bound overlap that appears in the block's "
"ancestor loops as `rolling axis`, fold and circularize the buffer along "
"the rolling dimension, append block predicate to avoid recomputing "
"overlapping elements."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`sample_categorical <tvm.tir.Schedule.sample_categorical>`\\ "
"\\(candidates\\, probs\\[\\, decision\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Sample an integer given the probability distribution"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`sample_compute_location "
"<tvm.tir.Schedule.sample_compute_location>`\\ \\(block\\[\\, "
"decision\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_compute_location:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Sample a compute-at location of the given block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`sample_partitioned_tile "
"<tvm.tir.Schedule.sample_partitioned_tile>`\\ \\(loop\\, n\\[\\, "
"...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Sample the factors to a partitioned tile for a specific loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`sample_perfect_tile <tvm.tir.Schedule.sample_perfect_tile>`\\ "
"\\(loop\\, n\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Sample the factors to perfect tile a specific loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`seed <tvm.tir.Schedule.seed>`\\ \\(seed\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.seed:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Seed the randomness"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`set_axis_separator <tvm.tir.Schedule.set_axis_separator>`\\ "
"\\(block\\, buffer\\, ...\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Set the axis separator of a buffer, where the buffer is specified by a "
"block and a read or write index."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`set_scope <tvm.tir.Schedule.set_scope>`\\ \\(block\\, "
"buffer\\_index\\, storage\\_scope\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Set the storage scope of a buffer, where the buffer is specified by the a"
" block and a write-index."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`show <tvm.tir.Schedule.show>`\\ \\(\\*args\\, \\*\\*kwargs\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.show:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "A sugar for print highlighted TVM script."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`split <tvm.tir.Schedule.split>`\\ \\(loop\\, factors\\[\\, "
"preserve\\_unit\\_iters\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Split a loop into a list of consecutive loops."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`storage_align <tvm.tir.Schedule.storage_align>`\\ \\(block\\, "
"buffer\\_index\\, axis\\, ...\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Set alignment requirement for specific dimension such that stride[axis] "
"== k * factor + offset for some k."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`tensorize <tvm.tir.Schedule.tensorize>`\\ "
"\\(block\\_or\\_loop\\, tensor\\_intrin\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Tensorize the computation enclosed by loop with the tensor intrinsic."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`transform_block_layout "
"<tvm.tir.Schedule.transform_block_layout>`\\ \\(block\\, index\\_map\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Apply a transformation represented by IndexMap to block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`transform_layout <tvm.tir.Schedule.transform_layout>`\\ "
"\\(block\\, buffer\\, index\\_map\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Apply a transformation represented by IndexMap to buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`unannotate <tvm.tir.Schedule.unannotate>`\\ "
"\\(block\\_or\\_loop\\, ann\\_key\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unannotate:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Unannotate a block/loop's annotation with key ann_key"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`unroll <tvm.tir.Schedule.unroll>`\\ \\(loop\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Unroll the input loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`unsafe_hide_buffer_access "
"<tvm.tir.Schedule.unsafe_hide_buffer_access>`\\ \\(block\\, buf\\_type\\,"
" ...\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Hide some buffer access in a given block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
":py:obj:`unsafe_set_dtype <tvm.tir.Schedule.unsafe_set_dtype>`\\ "
"\\(block\\, buffer\\_index\\, dtype\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ""
"Set the data type of a buffer, where the buffer is specified by the a "
"block and write-index."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`vectorize <tvm.tir.Schedule.vectorize>`\\ \\(loop\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Vectorize the input loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid ":py:obj:`work_on <tvm.tir.Schedule.work_on>`\\ \\(func\\_name\\)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.work_on:1
#: tvm.tir.schedule.schedule.Schedule:1:<autosummary>:1
msgid "Instruct the schedule to work on a function in the IRModule."
msgstr ""

#: of tvm.tir.block_scope.StmtSRef:1 tvm.tir.schedule.schedule.Schedule:1
#: tvm.tir.transform.transform.HoistedConditionals:1
#: tvm.tir.transform.transform.HoistedLetBindings:1
msgid "**Attributes:**"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:1:<autosummary>:1
msgid ":py:obj:`func_working_on <tvm.tir.Schedule.func_working_on>`\\"
msgstr ""

#: of tvm.tir.Schedule.func_working_on:1
#: tvm.tir.schedule.schedule.Schedule.__init__:1:<autosummary>:1
msgid ""
"Returns the GlobalVar of the func that the schedule is currently working "
"on"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:1:<autosummary>:1
msgid ":py:obj:`mod <tvm.tir.Schedule.mod>`\\"
msgstr ""

#: of tvm.tir.Schedule.mod:1
#: tvm.tir.schedule.schedule.Schedule.__init__:1:<autosummary>:1
msgid "Returns the AST of the module being scheduled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:1:<autosummary>:1
msgid ":py:obj:`state <tvm.tir.Schedule.state>`\\"
msgstr ""

#: of tvm.tir.Schedule.state:1
#: tvm.tir.schedule.schedule.Schedule.__init__:1:<autosummary>:1
msgid "Returns the ScheduleState in the current schedule class"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:1:<autosummary>:1
msgid ":py:obj:`trace <tvm.tir.Schedule.trace>`\\"
msgstr ""

#: of tvm.tir.Schedule.trace:1
#: tvm.tir.schedule.schedule.Schedule.__init__:1:<autosummary>:1
msgid "Returns the internally maintained trace of scheduling program execution"
msgstr ""

#: of tvm.ir.module.IRModule.with_attr:13 tvm.ir.module.IRModule.with_attrs:8
#: tvm.ir.module.IRModule.without_attr:8
#: tvm.tir.schedule.schedule.Schedule.__init__:5
#: tvm.tir.schedule.state.ScheduleState:14
#: tvm.tir.schedule.state.ScheduleState.__init__:5
msgid "mod"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:-1
#: tvm.tir.schedule.state.ScheduleState.__init__:-1
msgid "Union[PrimFunc, IRModule]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:6
#: tvm.tir.schedule.state.ScheduleState.__init__:6
msgid "The IRModule or PrimFunc to be scheduled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:9
msgid "seed: Optional[int]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:8
msgid ""
"The seed value for schedule's random state Note that None and -1 means "
"use device random, otherwise only integer between 1 and 2147483647 is "
"allowed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:16
#: tvm.tir.schedule.state.ScheduleState:17
#: tvm.tir.schedule.state.ScheduleState.__init__:12
msgid "debug_mask"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:-1
#: tvm.tir.schedule.state.ScheduleState.__init__:-1
msgid "Union[str, int]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:12
#: tvm.tir.schedule.state.ScheduleState.__init__:8
msgid ""
"Do extra correctness checking after the class creation and each time "
"after calling the Replace method. Possible choices of `debug_mask`: 1) "
"\"all\" - Turn on all the checks 2) \"none\" - Turn off all the checks 3)"
" An integer - Turn on checks according to the bitmasks provided in "
"ScheduleDebugMask"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:21
msgid "error_render_level"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:-1
msgid "str = \"detail\""
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:19
msgid ""
"The level of error rendering. Choices: \"detail\", \"fast\", \"none\". - "
"\"detail\": Render a detailed error message, with the TIR and error "
"locations printed - \"fast: Show a simple error message without rendering"
" or string manipulation - \"none\": Do not show any error message."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:31
#: tvm.tir.schedule.state.ScheduleState:21
msgid "enable_check"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:-1
msgid "bool = True"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:24
msgid ""
"The default schedule checks are too strict and might prevent us "
"performing some valid schedules. `enable_check` is an argument to control"
" whether we enable prerequisite checks for some schedule primitives or "
"not: - true: perform prerequisite check before applying some schedules. -"
" false: do not perform some check before applying schedules, but still "
"raise error if schedule fails."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:31
msgid ""
"It's user duty to guarantee schedule correctness if `enable_check` is set"
" to `False`."
msgstr ""

#: of tvm.runtime.object.Object._move:10
#: tvm.tir.analysis.analysis.expr_deep_equal:17 tvm.tir.buffer.decl_buffer:85
#: tvm.tir.expr.Select:4 tvm.tir.op.div:19 tvm.tir.op.if_then_else:23
#: tvm.tir.op.indexdiv:20 tvm.tir.op.indexmod:20 tvm.tir.op.truncdiv:20
#: tvm.tir.op.truncmod:20 tvm.tir.schedule.schedule.Schedule.__init__:34
#: tvm.tir.schedule.schedule.Schedule.blockize:67
#: tvm.tir.schedule.schedule.Schedule.rfactor:124
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:95
#: tvm.tir.schedule.schedule.Schedule.set_scope:63
#: tvm.tir.schedule.schedule.Schedule.storage_align:70
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:13
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:66
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:14
#: tvm.tir.schedule.state.ScheduleState.replace:26
#: tvm.tir.transform.transform.ForceNarrowIndexToInt32:9
#: tvm.tir.transform.transform.LowerDeviceStorageAccessInfo:9
#: tvm.tir.transform.transform.NarrowDataType:14
#: tvm.tir.transform.transform.UnifyThreadBinding:14
msgid "Note"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.__init__:35
msgid "The checks performed includes: 1) VerifySRefTree 2) VerifyCachedFlags"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:6
#: tvm.tir.schedule.schedule.Schedule.get_child_blocks:6
#: tvm.tir.schedule.schedule.Schedule.tensorize:5
msgid "block_or_loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:-1
msgid "Union[LoopRV, BlockRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:6
msgid "The block above which the new loop is created"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:11
msgid "new_loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:-1
#: tvm.tir.schedule.schedule.Schedule.bind:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:-1
#: tvm.tir.schedule.schedule.Schedule.fuse:-1
#: tvm.tir.schedule.schedule.Schedule.merge:-1
#: tvm.tir.schedule.schedule.Schedule.parallel:-1
#: tvm.tir.schedule.schedule.Schedule.rfactor:-1
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:-1
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:-1
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:-1
#: tvm.tir.schedule.schedule.Schedule.split:-1
#: tvm.tir.schedule.schedule.Schedule.unroll:-1
#: tvm.tir.schedule.schedule.Schedule.vectorize:-1
msgid "LoopRV"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:11
msgid "The new unit loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:16
msgid "Before add_unit_loop, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:30
msgid "Create the schedule and do add-unit-loop:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.add_unit_loop:38
msgid "After applying add-unit-loop, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:5
#: tvm.tir.schedule.schedule.Schedule.unannotate:5
msgid "block_or_loop: Union[BlockRV, LoopRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:6
msgid "The block/loop to be annotated"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:7
#: tvm.tir.schedule.schedule.Schedule.unannotate:8
msgid "ann_key"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:8
#: tvm.tir.schedule.schedule.Schedule.unannotate:8
msgid "The annotation key"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:10
msgid "ann_val"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:-1
msgid "AnnotationValueT"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:10
msgid "The annotation value"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:15
msgid "Before annotate, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:28
#: tvm.tir.schedule.schedule.Schedule.unannotate:27
msgid "Create the schedule and do annotate:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.annotate:36
msgid "After applying annotate, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:1
msgid ""
"Bind the input loop to the given thread axis. It requires: 1) The scope "
"block that the loop is in should have stage-pipeline property 2) All the "
"blocks under the loop are complete blocks or reduction blocks, and have "
"affine bindings 3) For each block under the loop, if the thread axis "
"starts with \"threadIdx`, the loop can only be contained in data-parallel"
" block iter and reduction block iters' bindings. Otherwise the loop can "
"only be contained in data-parallel block iters' bindings"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:11
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:22
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:22
#: tvm.tir.schedule.schedule.Schedule.parallel:11
#: tvm.tir.schedule.schedule.Schedule.rfactor:62
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:5
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:5
#: tvm.tir.schedule.schedule.Schedule.split:11
#: tvm.tir.schedule.schedule.Schedule.unroll:6
#: tvm.tir.schedule.schedule.Schedule.vectorize:11
msgid "loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:12
msgid "The loop to be bound to the thread axis"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:19
msgid "thread_axis"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:14
msgid ""
"The thread axis to be bound to the loop. Possible candidates: - "
"blockIdx.x/y/z - threadIdx.x/y/z - vthread.x/y/z - vthread (It is a "
"legacy behavior that will be deprecated. Please use `vthread.x/y/z` "
"instead.)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:24
msgid "Before bind, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:37
msgid "Create the schedule and do bind:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.bind:46
msgid "After applying bind, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:5
#: tvm.tir.transform.transform.BindTarget:5
msgid "target"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:-1
msgid "LoopRV or List[BlockRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:6
msgid "The root of the subtree or the specified blocks."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:8
#: tvm.tir.schedule.schedule.Schedule.split:21
#: tvm.tir.schedule.schedule.Schedule.tensorize:10
msgid "preserve_unit_iters"
msgstr ""

#: of tvm.ir.module.IRModule.astext:-1
#: tvm.tir.analysis.analysis.expr_deep_equal:-1
#: tvm.tir.analysis.analysis.verify_gpu_code:-1
#: tvm.tir.analysis.analysis.verify_memory:-1
#: tvm.tir.analysis.analysis.verify_ssa:-1 tvm.tir.function.TensorIntrin.get:-1
#: tvm.tir.op.ptx_ldmatrix:-1 tvm.tir.op.ptx_mma:-1 tvm.tir.op.ptx_mma_sp:-1
#: tvm.tir.schedule.schedule.Schedule.blockize:-1
#: tvm.tir.schedule.schedule.Schedule.split:-1
#: tvm.tir.schedule.schedule.Schedule.tensorize:-1
#: tvm.tir.schedule.state.ScheduleState:-1
#: tvm.tir.transform.transform.CompactBufferAllocation:-1
#: tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:-1
#: tvm.tir.transform.transform.VectorizeLoop:-1
msgid "bool"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:8
#: tvm.tir.schedule.schedule.Schedule.split:21
#: tvm.tir.schedule.schedule.Schedule.tensorize:10
msgid "Whether or not to preserve unit iterators in block bindings"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:-1
#: tvm.tir.schedule.schedule.Schedule.cache_read:-1
#: tvm.tir.schedule.schedule.Schedule.cache_write:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:-1
#: tvm.tir.schedule.schedule.Schedule.get_block:-1
#: tvm.tir.schedule.schedule.Schedule.reindex:-1
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:-1
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:-1
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:-1
#: tvm.tir.schedule.schedule.Schedule.rfactor:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:-1
msgid "BlockRV"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:13
msgid "The new block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:18
msgid "Before blockize, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:35
#: tvm.tir.schedule.schedule.Schedule.set_scope:35
msgid "Create the schedule and do set_scope:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:45
msgid "After applying blockize, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.blockize:68
msgid ""
"blockize requires there is exactly one block under the given loop and the"
" bindings of the block are divisible by the subspace represented by the "
"loops starting at the given loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:1
msgid ""
"Create a block to cache precomputed index for later use. if there is no "
"index computation, keep unchanged."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:-1
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:-1
#: tvm.tir.schedule.schedule.Schedule.cache_read:-1
#: tvm.tir.schedule.schedule.Schedule.cache_write:-1
#: tvm.tir.schedule.schedule.Schedule.compute_at:-1
#: tvm.tir.schedule.schedule.Schedule.compute_inline:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_padding:-1
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:-1
#: tvm.tir.schedule.schedule.Schedule.get_consumers:-1
#: tvm.tir.schedule.schedule.Schedule.get_loops:-1
#: tvm.tir.schedule.schedule.Schedule.get_producers:-1
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:-1
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:-1
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:-1
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:-1
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:-1
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:-1
#: tvm.tir.schedule.schedule.Schedule.set_scope:-1
#: tvm.tir.schedule.schedule.Schedule.storage_align:-1
#: tvm.tir.schedule.schedule.Schedule.transform_block_layout:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:-1
msgid "Union[BlockRV, str]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:7
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:8
msgid "The target block operates on the target buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:10
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:17
#: tvm.tir.schedule.schedule.Schedule.cache_read:18
#: tvm.tir.schedule.schedule.Schedule.cache_write:18
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:18
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:18
#: tvm.tir.transform.transform.ThreadSync:6
msgid "storage_scope: str"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:10
msgid "The storage scope of cached block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:15
msgid "cse_thresh: int"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:13
msgid ""
"The repeat threshold that determines a common sub expr, default 0 means "
"cache all index computation."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:20
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:22
msgid "cached_blocks"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:-1
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:-1
#: tvm.tir.schedule.schedule.Schedule.get_consumers:-1
#: tvm.tir.schedule.schedule.Schedule.get_output_blocks:-1
#: tvm.tir.schedule.schedule.Schedule.get_producers:-1
msgid "List[BlockRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:20
msgid "The blocks of the stage writing the cache buffers"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:24
#: tvm.tir.schedule.schedule.Schedule.cache_inplace:26
msgid "Before cache_inplace, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:37
msgid "Create the schedule and cache_index:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_index:46
msgid "After applying cache_index, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:1
msgid ""
"Create blocks that reads & write a buffer region into a cache block. It "
"requires the target block both read & write the target buffer. Mainly for"
" inplace operation."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:13
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:16
msgid "read_buffer_index: int"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:11
#: tvm.tir.schedule.schedule.Schedule.cache_read:13
msgid ""
"The index of the buffer in block's read region, the unique name of a read"
" buffer in the block, or a Buffer object that is within the blocks read "
"region."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:16
#: tvm.tir.schedule.schedule.Schedule.cache_read:18
#: tvm.tir.schedule.schedule.Schedule.cache_write:18
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:19
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:19
#: tvm.tir.transform.transform.ThreadSync:6
msgid "The target storage scope."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:22
msgid "The blocks of the cache stage, read cache first, write cache second"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:38
msgid "Create the schedule and cache_inplace:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_inplace:47
msgid "After applying cache_inplace, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:1
msgid "Create a block that reads a buffer region into a read cache. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:3
msgid "There is at most one block who write the buffer in the scope."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:5
#: tvm.tir.schedule.schedule.Schedule.cache_write:5
msgid "The scope block have stage-pipeline property."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:10
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:15
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:15
msgid "The consumer block of the target buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:15
msgid "buffer: Union[int, str, Buffer]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:22
#: tvm.tir.schedule.schedule.Schedule.cache_write:22
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:24
msgid "consumer_blocks: Optional[List[Union[BlockRV, str]]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:21
msgid ""
"An optional list of consumers that should read from the cache. If not "
"specified, all consumers will use the cache."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:27
#: tvm.tir.schedule.schedule.Schedule.cache_write:27
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:26
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:29
msgid "cached_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:27
#: tvm.tir.schedule.schedule.Schedule.cache_write:27
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_read:26
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:29
msgid "The block of the cache stage"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:31
msgid "Before cache_read, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:44
msgid "Create the schedule and cache_read:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_read:53
msgid "After applying cache_read, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:1
msgid "Create a block that reads a buffer region into a write cache. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:3
msgid "There is only one block who write the buffer in the scope."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:10
msgid "The producer block of the target buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:15
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:16
msgid "write_buffer_index: int"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:13
msgid ""
"The index of the buffer in block's write region, the unique name of a "
"write buffer in the block, or a Buffer object that is within the blocks "
"write region."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:21
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:23
msgid ""
"An optional list of consumers that should read directly from the cache. "
"If not specified, all consumers will read from the original buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:31
msgid "Before cache_write, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:44
msgid "Create the schedule and cache_write:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.cache_write:53
msgid "After applying cache_write, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:1
msgid ""
"Compute-At. Move a producer block under the specific loop, and regenerate"
" the loops induced by the block so that the buffer region produced by the"
" producer block could cover those regions consumed by its consumer blocks"
" under the given loop. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:5
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:5
msgid ""
"`block` and `loop` are under the same scope, `loop` is not the ancestor "
"of `block`"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:7
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:7
msgid "The scope block has stage-pipeline property"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:9
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:9
msgid ""
"3) The subtree of the scope block, where the given block is in, satisfies"
" the compact dataflow condition. i.e. all the blocks in the scope block's"
" subtree must be either complete block or reduction block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:13
msgid ""
"4) The block is not an output block with regard to the scope block, i.e. "
"the buffers written by the block are allocated under the scope block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:16
msgid "All the consumers of the block are under the given loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:21
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:18
msgid "The block to be moved"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:24
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:21
msgid "loop: LoopRV"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:24
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:21
msgid "The loop where the block to be moved under"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:27
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:24
msgid "preserve_unit_loops: bool"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:27
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:24
msgid "Whether to keep the trivial loops whose extents are 1"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:33
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:30
msgid "index: int"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:30
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_at:27
msgid ""
"The block index of the loop body subtree blocks: - `index = -1` means "
"inserted into the last possible insertion point; - `index = -2` means "
"inserted into the first possible insertion point; - Otherwise, `index` is"
" a nonnegative number that indicates the insertion point"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:38
msgid "Before compute-at, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:56
msgid "Create the schedule and do compute-at:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_at:66
msgid "After applying compute-at, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:1
msgid "Inline a block into its consumer(s). It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:3
msgid "The block is a complete non-root block, which only produces one buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:5
#: tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:5
msgid "The block must not be the only leaf in the scope."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:7
msgid ""
"The body of the block must be a BufferStore statement in the form of, "
"``A[i, j, k, ...] = ...`` where the indices of the LHS are all distinct "
"atomic variables, and no variables other than those indexing variables "
"are allowed in the statement."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:16
msgid "The block to be inlined to its consumer(s)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:21
msgid "Before compute-inline, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:39
msgid "Create the schedule and do compute-inline:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.compute_inline:47
msgid "After applying compute-inline, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.copy:10
msgid "copy"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.copy:-1
msgid "Schedule"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.copy:11
msgid "A new copy of the schedule"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:3
msgid "The block which fill const pad values into full write region;"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:5
msgid ""
"The block which fill in-bound values into region where pad predicate is "
"true."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:7
msgid "The pad value filling block is inserted right before the given loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:9
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:9
msgid "The schedule primitive requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:11
msgid "The input block is a complete block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:13
#: tvm.tir.schedule.schedule.Schedule.decompose_reduction:13
msgid "The input loop is the ancestor of the block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:15
msgid "The input block is a block which match padding pattern."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:20
msgid "The padding block to be decomposed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:22
msgid "The loop above which the pad value filling block is inserted before."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:27
msgid "pad_value_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:27
msgid "The block filling const pad values."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:31
msgid "Before decompose-padding, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:42
msgid "Create the schedule and do decompose-padding with specified loop:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_padding:51
#: tvm.tir.schedule.schedule.Schedule.pad_einsum:48
msgid "After applying decompose-padding, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:3
msgid ""
"The init block, which is translated from the init statement of the "
"reduction block;"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:5
msgid "The update block, which is the original block without init statement."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:7
msgid "The init block is inserted right before the given loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:11
msgid "The input block is a reduction block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:15
msgid ""
"The input loop is not lower than all the loops related to reduce block "
"var."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:20
msgid "The reduction block to be decomposed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:22
msgid "The loop above which the init block is inserted before."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:27
msgid "init_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:27
msgid "The init block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:31
msgid "Before decompose-reduction, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:46
msgid "Create the schedule and do decompose-reduction with specified loop:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.decompose_reduction:56
msgid "After applying decompose-reduction, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fork_seed:5
#: tvm.tir.schedule.schedule.Schedule.seed:5
msgid "seed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fork_seed:6
msgid "The forked random state, not the same as the current random state"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:1
msgid ""
"Fuse a list of consecutive loops into one. It requires: 1) The loops "
"can't have annotations or thread bindings. 2) The (i+1)-th loop must be "
"the only child of the i-th loop. 3) All loops must start with 0. 4) The "
"domain of a loop to be fused cannot depend on another loop to be fused."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:10
#: tvm.tir.schedule.schedule.Schedule.merge:10
msgid "*loops"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:-1
#: tvm.tir.schedule.schedule.Schedule.get_child_blocks:-1
#: tvm.tir.schedule.schedule.Schedule.get_loops:-1
#: tvm.tir.schedule.schedule.Schedule.merge:-1
#: tvm.tir.schedule.schedule.Schedule.reorder:-1
#: tvm.tir.schedule.schedule.Schedule.split:-1
msgid "List[LoopRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:10
msgid "The loops to be fused"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:15
#: tvm.tir.schedule.schedule.Schedule.merge:15
msgid "fused_loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:15
msgid "The new loop after fusion"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:20
msgid "Before applying fuse, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:33
#: tvm.tir.schedule.schedule.Schedule.merge:38
msgid "Create the schedule and do fuse:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.fuse:42
#: tvm.tir.schedule.schedule.Schedule.merge:48
msgid "After applying fuse, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:11
msgid "rand_var_or_sref"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:-1
msgid "Union[ExprRV, BlockRV, LoopRV, StmtSRef]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:11
#: tvm.tir.schedule.schedule.Schedule.get_sref:10
msgid "The random variable / sref to be evaluated"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:-1
msgid "Optional[Union[int, Block, For]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get:16
#: tvm.tir.schedule.schedule.Schedule.get_sref:15
msgid "The corresponding result"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:3
msgid ""
"By default, if `func_name` is not specified, the schedule will search for"
" the block in the function that is currently being \"worked on\". To "
"switch the function to be worked on, use `work_on` before calling this "
"method."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:17 tvm.tir.expr.SizeVar:7 tvm.tir.expr.Var:6
#: tvm.tir.function.TensorIntrin.get:6 tvm.tir.function.TensorIntrin.register:5
#: tvm.tir.op.call_llvm_intrin:9 tvm.tir.op.call_llvm_pure_intrin:9
#: tvm.tir.schedule.schedule.Schedule.get_block:9
#: tvm.tir.transform.function_pass.prim_func_pass:17
msgid "name"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:10
msgid "The name of the block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:12
#: tvm.tir.schedule.schedule.Schedule.work_on:12
msgid "func_name"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:-1
msgid "Optional[str] = None"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:12
msgid "The name of the function"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_block:17
msgid ""
"The block retrieved IndexError is raised if 0 or multiple blocks exist "
"with the specific name."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:-1
#: tvm.tir.schedule.schedule.Schedule.tensorize:-1
msgid "Union[BlockRV, LoopRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:6
msgid "The query block/loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:10
msgid "blocks"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_child_blocks:11
msgid "A list of leaf blocks inside a specific block/loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_consumers:6
#: tvm.tir.schedule.schedule.Schedule.get_producers:6
msgid "The block in the query"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_consumers:10
msgid "consumers"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_consumers:11
msgid "A list of consumers of the given block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_loops:6
msgid "The query block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_loops:10
msgid "loops"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_loops:11
msgid "A list of loops above the given block in its scope, from outer to inner"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:8
msgid "scope_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:-1
msgid "Union[BlockRV, str],"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:8
msgid "The scope block from which output blocks are collected"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:12
msgid "output_blocks"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_output_blocks:13
msgid "A list of all blocks that write to some output buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_producers:10
msgid "producers"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_producers:11
msgid "A list of producers of the given block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_sref:10
msgid "rand_var_or_stmt"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_sref:-1
msgid "Union[BlockRV, LoopRV, Block, For]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.get_sref:-1
msgid "Optional[StmtSRef]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.merge:1
msgid ""
"Merge a list of loops into one. The loops under their LCA requires: 1) "
"Under the same scope. 2) Can't have annotations or thread bindings. 3) "
"Start with 0 and have same extent and same nesting depth. 4) From target "
"loop to their LCA, The inner loop must be the only child of the outer "
"loop."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.merge:10
msgid "The loops to be merged"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.merge:15
msgid "The new loop after merge"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.merge:20
msgid "Before applying merge, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:3
msgid ""
"On a block with trivial binding, this primitive pads the iteration domain"
" of the block by the given padding factors, for example, 127 -> 128, 132 "
"-> 144 when padding factor is 16. Extra producer and consumer padding "
"blocks will be generated to avoid out-of-bound buffer access."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:8
msgid ""
"Einsum pattern means all the indices on the buffer access are either by "
"constants (e.g. B[0]) or by variables (e.g. B[i]), but not by composite "
"expressions (e.g. B[i + 1])."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:14
msgid "The block that matches the Einsum pattern."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:17
msgid "padding"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:-1
#: tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:-1
#: tvm.tir.schedule.schedule.Schedule.sample_categorical:-1
#: tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:-1
msgid "List[int]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:17
msgid "The padding for each block iter."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:22
msgid "Before applying pad-einsum, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.pad_einsum:39
msgid "Create the schedule and do pad-einsum with specified block:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:1
msgid ""
"Parallelize the input loop. It requires: 1) The scope block that the loop"
" is in should have stage-pipeline property 2) All the blocks under the "
"loop are complete blocks or reduction blocks, and have affine bindings 3)"
" For each block under the loop, the loop can only be contained in data-"
"parallel block iters' bindings"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:11
msgid "The loop to be parallelized"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:16
msgid "Before parallel, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:29
msgid "Create the schedule and do parallel:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.parallel:37
msgid "After applying parallel, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:1
msgid ""
"Create a block that read/write a buffer region into a read/write cache "
"with reindexing. The layout of the cache will be the same as by the "
"iterators of the block that reads/writes the buffer. It requires: 1) "
"There is only one block who reads/writes the target buffer 2) There is "
"only one buffer load/store of this buffer in the block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:9
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:6
#: tvm.tir.schedule.schedule.Schedule.transform_layout:5
msgid "block : Union[BlockRV, str]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:11
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:8
#: tvm.tir.schedule.schedule.Schedule.transform_layout:7
msgid ""
"The block that accesses the target buffer.  If a string, this must "
"uniquely identify a block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:14
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:11
#: tvm.tir.schedule.schedule.Schedule.transform_layout:10
msgid "buffer: Union[Tuple[str,int], Buffer, str]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:16
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:13
#: tvm.tir.schedule.schedule.Schedule.transform_layout:12
msgid ""
"The buffer to be transformed, or a specification of how to identify the "
"buffer to be transformed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:19
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:16
#: tvm.tir.schedule.schedule.Schedule.transform_layout:15
msgid ""
"If `buffer` if a tuple of ``(str,int)``, the first item should be either "
"\"read\" or \"write\", and the second item is an index into the block's "
"read or write regions."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:23
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:20
#: tvm.tir.schedule.schedule.Schedule.transform_layout:19
msgid ""
"If `buffer` is a string, it is the name of the buffer, which must exist "
"within the reads/writes of the block.  In addition, the reads/writes of "
"the block may not contain more than one buffer with this name."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:28
#: tvm.tir.schedule.schedule.Schedule.set_axis_separator:25
#: tvm.tir.schedule.schedule.Schedule.transform_layout:24
msgid ""
"If `buffer` is a Buffer object, it must exist within the reads/writes of "
"the block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:34
msgid "reindex_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:34
msgid "The block of the reindex stage"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:39
msgid "Before reindex, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:53
msgid "Create the schedule and do reindex:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex:61
msgid "After applying reindex, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:1
msgid ""
"Create a block that reads a buffer region into a read cache using "
"customized indices specified by index map. The read region of the buffer "
"must be a single point."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:4
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:4
msgid ""
"The cache stage block follows the original order of loops and block "
"itervars in the block. If a block itervar does not appear in the buffer "
"access region, it and its corresponding loop variables will be omitted. "
"User can then use `transform_block_layout` primitive to reorder the block"
" itervars and surrounding loops of the cache read/write block."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:9
msgid ""
"Unlike `cache_read`, `reindex_cache_read` only supports single consumer, "
"please use `cache_read` when there are multiple consumers."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:17
msgid "The index of the buffer in block's read region."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:21
msgid "index_map: Union[IndexMap, Callable]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:21
#: tvm.tir.schedule.schedule.Schedule.reindex_cache_write:21
msgid ""
"User defined indices to access allocated cache buffer, maps from block "
"iter vars."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:30
msgid "Before reindex_cache_read, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:43
msgid "Create the schedule and reindex_cache_read:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:52
msgid "After applying reindex_cache_read, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_read:72
msgid ""
"reindex_cache_write transform_block_layout transform_layout cache_read "
"reindex"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:1
msgid ""
"Create a block that reads a buffer region into a write cache using "
"customized indices specified by index map. The write region of the buffer"
" must be a single point."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:9
msgid ""
"Unlike `cache_write`, `reindex_cache_write` only supports single "
"consumer, please use `cache_write` when there are multiple consumers."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:17
#: tvm.tir.schedule.schedule.Schedule.rolling_buffer:21
#: tvm.tir.schedule.schedule.Schedule.storage_align:11
msgid "The index of the buffer in block's write region."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:20
msgid "index_map: Union[Callable, IndexMap]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:33
msgid "Before reindex_cache_write, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:46
msgid "Create the schedule and reindex_cache_write:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:55
msgid "After applying reindex_cache_write, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reindex_cache_write:75
msgid ""
"reindex_cache_read transform_block_layout transform_layout cache_write "
"reindex"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.remove_rv:5
msgid "rand_var"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.remove_rv:-1
msgid "Union[BlockRV, LoopRV, ExprRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.remove_rv:6
msgid "The random variable to be removed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:1
msgid ""
"Reorder a list of loops. It doesn't require the loops to be consecutive. "
"It requires: 1) The loops are in the same chain. That means: the loops "
"can be ordered to [l_1, l_2, ... , l_n] where l_i is an ancestor of "
"l_{i+1} and there are only single-branch loops between l_1 and l_n (which"
" also indicates they are under the same scope). 2) After reordering, the "
"domain of an outer loop cannot depend on any of the inner loops. 3) For "
"every block under the loop nests, its block binding must be affine, and "
"the block variables must be either data parallel or reduction. 4) No "
"duplicated loops are allowed in the arguments."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:14
msgid "*ordered_loops"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:14
msgid "The loops in the new order"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:19
msgid "Before reorder, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:32
msgid "Create the schedule and do reorder:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder:41
msgid "After applying reorder, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:6
msgid "The block to be transformed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:8
msgid "new_order"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:8
msgid "The new block itervar order."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:13
msgid "Before reorder_block_iter_var, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:30
msgid "Create the schedule and do reorder_block_iter_var:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:38
msgid "After applying reorder_block_iter_var, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reorder_block_iter_var:59
msgid "reorder"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:1
msgid ""
"Reverse-Compute-At. Move a consumer block under the specific loop, and "
"regenerate the loops induced by the block so that the buffer region "
"consumed by the consumer block could cover those regions produced by its "
"producer blocks under the given loop. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:13
msgid "All the producers of the block are under the given loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:35
msgid "Before reverse-compute-at, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:53
msgid "Create the schedule and do reverse-compute-at:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_at:63
msgid "After applying reverse-compute-at, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:1
msgid "Inline a block into its only producer. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:3
msgid ""
"The block is a complete non-root block, which only produces and consumes "
"one buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:7
msgid ""
"The only producer of the block is a read-after-write producer and a "
"complete non-root block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:10
msgid ""
"The body of the block must be a BufferStore statement in the form of, "
"``B[f(i, j, k, ...)] = g(i, j, k, A[i, j, k, ...] ...)`` where the "
"indices of each `BufferLoad` on the RHS are all distinct atomic "
"variables, and no variables other than those indexing variables are "
"allowed in the statement."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:19
msgid "The block to be inlined to its producer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:24
msgid "Before reverse-compute-inline, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:42
msgid "Create the schedule and do reverse-compute-inline:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.reverse_compute_inline:50
msgid "After applying reverse-compute-inline, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:3
msgid ""
"An associative reduction cannot be parallelized directly, because it "
"leads to potential race condition during accumulation. Alternatively, the"
" reduction could be factorized on a loop with the following steps: - Step"
" 1: evenly slice the reduction into `n` separate chunks, where `n` is the"
" loop extent - Step 2: compute the chunks separately and write the result"
" into `n` intermediate buffers; - Step 3: accumulate the `n` separate "
"buffer into the result buffer. Note that the Step 2 above introduces "
"opportunities for parallelization."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:11
msgid ""
"RFactor is a schedule primitive that implements the transformation "
"described above: Given a block that writes to buffer `B`, it factorizes a"
" loop of extent `n`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:14
msgid "For example, the pseudocode below accumulates `B[i] = sum(A[i, : , : ])`:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:23
msgid ""
"Suppose RFactor is applied on the innermost loop `k` and `factor_axis = "
"1`. RFactor then creates an intermediate buffer and two blocks."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:26
msgid ""
"1. The intermediate buffer, or \"rf-buffer\" is a buffer of rank `ndim(B)"
" + 1` and size `size(B) * n`, whose shape expands from `shape(B)` by "
"adding an axis of `n` at the position specified by `factor_axis`. For "
"example,"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:30
msgid "shape(B) = [1, 2, 3], factor_axis = 0  => shape(B_rf) = [n, 1, 2, 3]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:31
msgid "shape(B) = [1, 2, 3], factor_axis = 1  => shape(B_rf) = [1, n, 2, 3]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:32
msgid "shape(B) = [1, 2, 3], factor_axis = 2  => shape(B_rf) = [1, 2, n, 3]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:33
msgid "shape(B) = [1, 2, 3], factor_axis = 3  => shape(B_rf) = [1, 2, 3, n]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:35
msgid ""
"2. The rfactor block, or \"rf-block\", is a block that writes to the `rf-"
"buffer` without accumulating over the loop `k`, i.e. the loop `k` is "
"converted from a reduction loop to a data parallel loop. In our example, "
"the rf-block is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:48
msgid ""
"3. The write-back block, or `wb-block`, is a block that accumulates the "
"rf-buffer into the result buffer. All the reduction loops are removed "
"except the loop `k` for accumulation. In our example, the wb-block is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:63
msgid "The loop outside block for which we want to do rfactor"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:65
msgid "factor_axis"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:65
msgid ""
"The position where the new dimension is placed in the new introduced "
"rfactor buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:71
msgid "rf_block"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:70
msgid ""
"The block which computes partial results over each slices (i.e., the "
"first block as described in the above illustration)"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:76
msgid "Before rfactor, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:91
msgid "Create the schedule and do rfactor:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:100
msgid "After applying rfactor, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rfactor:126
msgid ""
"Rfactor requires: 1) `loop` has only one child block, and it is a "
"reduction block; 2) `loop` is a reduction loop, i.e. the loop variable is"
" bound to only reduction variables in the block binding; 3) `loop` is not"
" parallelized, vectorized, unrolled or bound to any thread axis; 4) The "
"block scope that `loop` is in is a staged-pipeline; 5) The outermost loop"
" outside the reduction block should has the reduction block as its first "
"child block; 6) The outermost reduction loop should have only one child "
"block; 7) An unary extent loop that is not bound to any reduction or data"
" parallel variables in the block binding should not appear under some "
"reduction loop; 8) The reduction block should write to only one buffer, "
"and its init and body are both simple `BufferStore`s, and the pattern is "
"registered as an associative reducer. The pre-defined patterns include: "
"plus, multiplication, min and max; 9) Each of the loops on top of the "
"block cannot be bound to a data parallel and a reduction block binding at"
" the same time; 10) `factor_axis` should be in range `[-ndim(B) - 1, "
"ndim(B)]`, where `B` is the buffer that the reduction block writes to. "
"Negative indexing is normalized according to numpy convention."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:1
msgid ""
"Compute the target buffer via rolling buffering, select the outermost "
"rollable axis with a positive bound overlap that appears in the block's "
"ancestor loops as `rolling axis`, fold and circularize the buffer along "
"the rolling dimension, append block predicate to avoid recomputing "
"overlapping elements. It requires:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:6
msgid "The block is not an output block and has only RAW dependencies."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:8
msgid "The buffer to be an intermediate buffer defined via `alloc_buffer`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:10
msgid ""
"3) The LCA of the producer and consumer of the buffer is a for loop, "
"typically, the producer and consumer of the buffer are cascaded through "
"compute_at."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:13
msgid ""
"4) The access region of the buffer has at least one dimension that "
"contains a positive bound overlap."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:19
#: tvm.tir.schedule.schedule.Schedule.storage_align:9
msgid "The producer block of the buffer."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:21
msgid "write_buffer_index"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:26
msgid "Before rolling_buffer, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:55
msgid "Create the schedule and do rolling_buffer:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:63
msgid "After applying rolling_buffer, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.rolling_buffer:96
msgid ""
"The region_cover property of the consumer block of the target buffer will"
" become false."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:5
msgid "candidates"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:6
msgid "The candidates to be sampled from"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:7
msgid "probs"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:-1
msgid "List[float]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:8
msgid "The probability of each candidate"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:10
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:8
msgid "decision"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:-1
#: tvm.tir.schedule.schedule.Schedule.sample_compute_location:-1
msgid "Optional[int]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:10
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:14
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:12
msgid "The sampling decision, if any"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:-1
msgid "ExprRV"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_categorical:15
msgid "The random variable sampled from candidates"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_compute_location:6
msgid "The block whose compute-at location is to be sampled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_compute_location:8
msgid "The sampling decision"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_compute_location:13
msgid "The sampled loop where the input block is to be computed at"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:6
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:6
msgid "The loop to be tiled"
msgstr ""

#: of tvm.tir.op.mma_store:12 tvm.tir.op.tvm_fill_fragment:12
#: tvm.tir.op.tvm_load_matrix_sync:12 tvm.tir.op.tvm_store_matrix_sync:12
#: tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:7
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:7
msgid "n"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:8
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:8
msgid "The number of tiles to be sampled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:9
msgid "partition_pos"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:10
msgid "The position to partition tiles to two parts"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:11
msgid "innerpart_factor"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:12
msgid "The factor of the second part"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:14
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:12
msgid "decision: Optional[List[int]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:-1
#: tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:-1
msgid "List[ExprRV]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_partitioned_tile:19
msgid "A list of length `n`, the random partitioned tile sizes sampled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:9
msgid "max_innermost_factor"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:10
msgid "The maximum tile size allowed to be sampled in the innermost loop"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.sample_perfect_tile:17
msgid "A list of length `n`, the random perfect tile sizes sampled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.seed:6
msgid "The new random seed, -1 if use device random, otherwise non-negative"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:28
msgid "axis_separators : Optional[List[int]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:30
msgid "The axis separators."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:35
msgid "Before set_axis_separator, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:54
msgid "Create the schedule and do set_axis_separator:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_axis_separator:63
msgid "After applying set_axis_separator, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:7
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:10
msgid "The producer block of the buffer"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:8
#: tvm.tir.schedule.schedule.Schedule.storage_align:10
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:11
msgid "buffer_index"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:9
#: tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:12
msgid "The index of the buffer in block's write region"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:11
msgid "The storage scope to be set"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:16
msgid "Before set_scope, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:43
msgid "After applying set_scope, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.set_scope:64
msgid ""
"`set_scope` requires the buffer to be an intermediate buffer defined via "
"`alloc_buffer`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.show:3
msgid ""
"All parameters are forwarded to the underlying `Module.show` and "
"`Trace.show` methods."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:1
msgid ""
"Split a loop into a list of consecutive loops. It requires: 1) The loop "
"can't have annotation or thread binding. 2) The loop must start with 0. "
"Predicates may be added to ensure the total loop numbers keeps unchanged."
" In `factors`, at most one of the factors can be None, which will be "
"automatically inferred."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:11
msgid "The loop to be split"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:18
msgid "factors: List[Union[int, ExprRV, None]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:14
msgid ""
"The splitting factors Potential inputs are: - None - ExprRV - Positive "
"constant integers"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:26
msgid "split_loops"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:26
msgid "The new loops after split"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:31
msgid "Before split, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:44
msgid "Create the schedule and do split:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.split:53
msgid "After applying split, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:1
msgid ""
"Set alignment requirement for specific dimension such that stride[axis] "
"== k * factor + offset for some k. This is useful to set memory layout "
"for more friendly memory access pattern. For example, we can set "
"alignment to be factor=2, offset=1 to avoid bank conflict for thread "
"access on higher dimension in GPU shared memory."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:13
msgid "The dimension to be specified for alignment."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:15
msgid "The factor multiple of alignment."
msgstr ""

#: of tvm.tir.op.mma_fill:15 tvm.tir.op.tvm_access_ptr:12
#: tvm.tir.schedule.schedule.Schedule.storage_align:17
msgid "offset"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:17
msgid "The required offset factor."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:22
msgid "Before storage_align, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:40
msgid "Create the schedule and do storage_align:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:48
msgid "After applying storage_align, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:67
msgid "After lowering passes, buffer B will have strides as [129, 1]."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.storage_align:71
msgid ""
"Storage_align requires the buffer to be an intermediate buffer defined "
"via `alloc_buffer`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:6
msgid "The loop to be tensorized."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:7
msgid "tensor_intrin"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:8
msgid "The tensor intrin or the name of the tensor intrin."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:15
msgid "Before tensorize, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:36
msgid "Declare and register the tensor intrinsic:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:80
msgid "Create the schedule and do tensorize:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.tensorize:90
msgid "After applying tensorize, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:6
msgid "The block to be transformed"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:9
msgid "index_map"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:-1
msgid "Union[IndexMap, Callable]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:9
#: tvm.tir.schedule.schedule.Schedule.transform_layout:29
msgid "The transformation to apply."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:14
msgid "Before transform_block_layout, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:28
msgid "Create the schedule and do transform_block_layout:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_block_layout:36
msgid "After applying transform_block_layout, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:27
msgid "index_map : Union[IndexMap, Callable]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:31
msgid ""
"If `index_map` is a callable, and the returned list contains "
"IndexMap.AXIS_SEPARATOR, the SetAxisSeparators primitive will be called "
"in addition to the TransformLayout primitive."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:36
msgid "pad_value: Optional[Union[int, float, PrimExpr, IndexMap, Callable]]"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:38
msgid ""
"The value to be used for any padding introduced by the transformation.  "
"If the schedule contains a producer block for the specified buffer, the "
"pad value will be written as part of the producer block if possible, or "
"after the producer block otherwise.  Otherwise, if the buffer is an "
"input, will insert an annotation block to state that the padding contains"
" the known value."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:46
msgid ""
"The pad value may not contain instances of BufferLoad, except where it "
"loads a value from the buffer being transformed (e.g. to create a "
"circular buffer with padding that consists of repeated elements)."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:51
msgid ""
"Note: If applied to an input buffer, the calling scope is responsible for"
" ensuring that the pad_value is present. Algebraic symplifications, "
"branch elimination, and other optimizations may assume that this "
"precondition is met, and may result in incorrect results being returned."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:57
msgid "If None, the transformation may not introduce padding."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:59
msgid ""
"If an int, float or PrimExpr, the transformation is the specific value to"
" be present in the padding."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:62
msgid ""
"If an IndexMap or Callable, the transformation is the value to be present"
" in the padding in terms of the transformed index."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:66
msgid "assume_injective_transform : bool"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:68
msgid ""
"If set to true, the schedule  primitive will assume the index_map is "
"injective and skip checking overlapping of the mapped indices. This can "
"be useful for complicated index_map that the analysis does not cover. It "
"is the callers' responsibility to ensure the index map is injective, "
"otherwise, the correctness of the schedule is not guaranteed."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:75
msgid "Before transform_layout, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:93
msgid "Create the schedule and do transform_layout:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.transform_layout:102
msgid "After applying transform_layout, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unannotate:6
msgid "The block/loop to be unannotated"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unannotate:13
msgid "Before unannotate, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unannotate:35
msgid "After applying unannotate, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:1
msgid "Unroll the input loop. It requires nothing"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:6
msgid "The loop to be unrolled"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:11
msgid "Before unroll, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:24
msgid "Create the schedule and do unroll:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unroll:32
msgid "After applying unroll, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:1
msgid ""
"Hide some buffer access in a given block. This is an unsafe schedule "
"primitive."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:6
msgid "The block where we hide read access."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:7
msgid "buf_type"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:8
msgid "The buffer type: \"read\"/\"write\"."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:10
msgid "buf_index_array"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:10
msgid "The array of buffer indices we hide access."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_hide_buffer_access:14
msgid ""
"This schedule primitive is unsafe, and may fail dependency analysis. One "
"use case of `unsafe_hide_buffer_access` is to hide the buffer access to "
"indices buffers (e.g. in sparse computation) so that we can further "
"tensorize the block (the indices buffers appeared in read/write regions "
"may fail the pattern matching in `tensorize` primitive, and hide the "
"access to these buffers could address the issue)."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:4
msgid ""
"This schedule primitive is unsafe and may change the correctness of "
"program because of type conversion, please use with caution."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:14
msgid "The data type to be set"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:19
msgid "Before unsafe_set_dtype, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:38
msgid "Create the schedule and do unsafe_set_dtype:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:46
msgid "After applying set_dtype, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.unsafe_set_dtype:67
msgid ""
"`unsafe_set_dtype` requires the buffer to be an intermediate buffer "
"defined via `alloc_buffer`."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:1
msgid ""
"Vectorize the input loop. It requires: 1) The scope block that the loop "
"is in should have stage-pipeline property 2) All the blocks under the "
"loop are complete blocks or reduction blocks, and have affine bindings 3)"
" For each block under the loop, the loop can only be contained in data-"
"parallel block iters' bindings"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:11
msgid "The loop to be vectorized"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:16
msgid "Before vectorize, in TensorIR, the IR is:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:29
msgid "Create the schedule and do vectorize:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.vectorize:37
msgid "After applying vectorize, the IR becomes:"
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.work_on:3
msgid ""
"By default, the schedule works on the function with the name \"main\", or"
" the only function in the IRModule if there is only one. If there is "
"multiple functions in the IRModule, and none of their names are \"main\","
" users will have to call this method to explicitly specify which function"
" to work on."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.work_on:8
msgid ""
"This sugar function will guide the `GetBlock` method if its `func_name` "
"is not specified."
msgstr ""

#: of tvm.tir.schedule.schedule.Schedule.work_on:13
msgid "The name of the function to work on."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:4
msgid ""
"The data structure contains the following information 1) The AST being "
"scheduled (mod) 2) The sref tree of schedulable statements (indicated by "
"the srefs) 3) The dependency information of each block scope (block_info)"
" 4) A reverse mapping from the AST nodes to that in the sref tree "
"(get_sref) 5) A debug flag, if set, extra checking is enabled "
"(debug_mask) 6) A enable check flag, if False, some prerequisite checks "
"are disabled."
msgstr ""

#: of tvm.ir.module.IRModule.with_attr:-1 tvm.ir.module.IRModule.with_attrs:-1
#: tvm.ir.module.IRModule.without_attr:-1
#: tvm.tir.schedule.state.ScheduleState:-1
msgid "IRModule"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:15
msgid "The AST of the module being scheduled"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:17
msgid ""
"Do extra correctness checking after the object construction and each time"
" after calling the Replace method."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState:20
msgid ""
"Indicates whether we enable prerequisite checks for some schedule "
"primitives or not, defaults to `True`."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
msgid ""
":py:obj:`__init__ <tvm.tir.ScheduleState.__init__>`\\ \\(mod\\, \\*\\[\\,"
" debug\\_mask\\, enable\\_check\\]\\)"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1
#: tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
msgid "Construct a schedule state from an IRModule or a PrimFunc"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
msgid ""
":py:obj:`_get_cached_flags <tvm.tir.ScheduleState._get_cached_flags>`\\ "
"\\(block\\_sref\\)"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
#: tvm.tir.schedule.state.ScheduleState._get_cached_flags:1
msgid "Get the cached flags of the corresponding block"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
msgid ""
":py:obj:`get_block_scope <tvm.tir.ScheduleState.get_block_scope>`\\ "
"\\(block\\_sref\\)"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
msgid ":py:obj:`get_sref <tvm.tir.ScheduleState.get_sref>`\\ \\(stmt\\)"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
#: tvm.tir.schedule.state.ScheduleState.get_sref:1
msgid "Return the corresponding sref that points to the stmt"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
msgid ""
":py:obj:`replace <tvm.tir.ScheduleState.replace>`\\ \\(src\\_sref\\, "
"tgt\\_stmt\\[\\, block\\_sref\\_reuse\\]\\)"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.__init__:1:<autosummary>:1
msgid ""
"Replace the part of the AST, as being pointed to by `src_sref`, with a "
"specific statement `tgt_stmt`, and maintain the sref tree accordingly."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:11
msgid "flags"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:-1
msgid "CachedFlags"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:11
msgid "Three flags: affine_binding, region_cover, stage_pipeline"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState._get_cached_flags:15
msgid "It is an API intended for internal testing use."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.get_sref:-1
msgid "Union[Block, For]"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.get_sref:6
msgid "The schedulable statement in the TensorIR to be retrieved for its sref"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:1
msgid ""
"Replace the part of the AST, as being pointed to by `src_sref`, with a "
"specific statement `tgt_stmt`, and maintain the sref tree accordingly. "
"Replace will try to perform copy on write as much as possible when the "
"ScheduleState holds the only copy to the IRModule and IR nodes."
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:6
msgid ""
"Only 3 types of replacements are allowed: from `src_sref->stmt` to "
"`tgt_stmt`. 1) Block -> Block 2) Loop -> Loop 3) Loop -> BlockRealize"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:14
msgid "src_sref"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:14
msgid "The sref to the statement to be replaced in the TensorIR AST"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:17
msgid "tgt_stmt"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:-1
msgid "Union[Block, For, BlockRealize]"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:17
msgid "The statement to be replaced to"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:23
msgid "block_sref_reuse"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:-1
msgid "Optional[Dict[Block, Block]] = None"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:20
msgid ""
"Maps an old block (to be replaced in the subtree under `src_sref->stmt`) "
"to a new block (replaced to, in the subtree under `tgt_stmt`), and "
"enforces reuse of srefs between them (rather than create new srefs) i.e. "
"after being replaced, the sref that points to the old block will point to"
" the new one"
msgstr ""

#: of tvm.tir.schedule.state.ScheduleState.replace:27
msgid ""
"The reuse of loop srefs are detected automatically according to the reuse"
" of loop vars."
msgstr ""

#: of tvm.tir.expr.Select:5
msgid ""
"Select may compute both true_value and false_value. Use "
":py:class:`tvm.tir.if_then_else` instead if you want to get a conditional"
" expression that only evaluates the correct branch."
msgstr ""

#: of tvm.tir.expr.Select:13
msgid "The condition expression."
msgstr ""

#: of tvm.tir.expr.Select:16
msgid "true_value"
msgstr ""

#: of tvm.tir.expr.Select:16
msgid "The value to take when condition is true."
msgstr ""

#: of tvm.tir.expr.Select:19
msgid "false_value"
msgstr ""

#: of tvm.tir.expr.Select:19
msgid "The value to take when condition is false."
msgstr ""

#: of tvm.tir.stmt.SeqStmt:6
msgid "seq"
msgstr ""

#: of tvm.tir.stmt.SeqStmt:-1
msgid "List[Stmt]"
msgstr ""

#: of tvm.tir.stmt.SeqStmt:6
msgid "The statements"
msgstr ""

#: of tvm.tir.expr.Shuffle:6
msgid "vectors"
msgstr ""

#: of tvm.tir.expr.Shuffle:6
msgid "The vectors"
msgstr ""

#: of tvm.tir.expr.Shuffle:-1
msgid "Array of indices"
msgstr ""

#: of tvm.tir.expr.Shuffle:9
msgid "The indices"
msgstr ""

#: of tvm.tir.expr.SizeVar:2
msgid "which is greater or equal to zero."
msgstr ""

#: of tvm.tir.expr.SizeVar:7 tvm.tir.expr.Var:6
msgid "The name"
msgstr ""

#: of tvm.tir.block_scope.StmtSRef:3
msgid ""
"Glossary - Block sref: An StmtSref that points to a TensorIR block. - "
"Loop sref: An StmtSRef that points to a TensorIR for loop. - Parent sref:"
" The parent sref of an sref is the block/loop sref that points to its "
"closest schedulable statement of its ancestors on the TensorIR AST. - "
"Root sref: Sref to the root block. Every sref has exactly one parent sref"
" except for root sref. - Sref tree: The parent-children-relationship of "
"srefs that forms a tree, uniquely determined by the TensorIR AST."
msgstr ""

#: of tvm.tir.block_scope.StmtSRef:1:<autosummary>:1
msgid ":py:obj:`inline_mark <tvm.tir.StmtSRef.inline_mark>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.block_scope.StmtSRef.inline_mark:1
#: tvm.tir.block_scope.StmtSRef:1:<autosummary>:1
msgid ""
"A special StmtSRef, which doesn't point to any stmt in the AST, only "
"serving as a \"mark\" to hint compute-at to do the work of compute-inline"
msgstr ""

#: of tvm.tir.block_scope.StmtSRef:1:<autosummary>:1
msgid ":py:obj:`root_mark <tvm.tir.StmtSRef.root_mark>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.block_scope.StmtSRef.root_mark:1
#: tvm.tir.block_scope.StmtSRef:1:<autosummary>:1
msgid ""
"A special StmtSRef, which doesn't point to any stmt in the AST, only "
"serving as a \"mark\" to hint compute-at to do nothing"
msgstr ""

#: of tvm.tir.block_scope.StmtSRef.inline_mark:1:<autosummary>:1
msgid ":py:obj:`parent <tvm.tir.StmtSRef.parent>`\\"
msgstr ""

#: of tvm.tir.StmtSRef.parent:1
#: tvm.tir.block_scope.StmtSRef.inline_mark:1:<autosummary>:1
msgid "The parent sref"
msgstr ""

#: of tvm.tir.block_scope.StmtSRef.inline_mark:1:<autosummary>:1
msgid ":py:obj:`stmt <tvm.tir.StmtSRef.stmt>`\\"
msgstr ""

#: of tvm.tir.StmtSRef.stmt:1
#: tvm.tir.block_scope.StmtSRef.inline_mark:1:<autosummary>:1
msgid "The block/for stmt the object refers to"
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:6
#: tvm.tir.op.TVMBackendFreeWorkspace:6
msgid "device_type"
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:6
#: tvm.tir.op.TVMBackendFreeWorkspace:6
msgid "The device type which the space will be allocated."
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:9
#: tvm.tir.op.TVMBackendFreeWorkspace:9
msgid "device_id"
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:9
#: tvm.tir.op.TVMBackendFreeWorkspace:9
msgid "The device id which the space will be allocated."
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:12
msgid "nbytes"
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:12
msgid "The size of the space requested."
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:15
msgid "dtype_code_hint"
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:15
msgid ""
"The type code of the array elements. Only used in certain backends such "
"as OpenGL."
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:18
msgid "dtype_bits_hint"
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:18
msgid ""
"The type bits of the array elements. Only used in certain backends such "
"as OpenGL."
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:22
#: tvm.tir.op.TVMBackendFreeWorkspace:16 tvm.tir.op.address_of:13
#: tvm.tir.op.assume:10 tvm.tir.op.call_cpacked:17
#: tvm.tir.op.call_cpacked_lowered:16 tvm.tir.op.call_extern:19
#: tvm.tir.op.call_intrin:22 tvm.tir.op.call_llvm_intrin:19
#: tvm.tir.op.call_llvm_pure_intrin:19 tvm.tir.op.call_packed:21
#: tvm.tir.op.call_packed_lowered:19 tvm.tir.op.call_pure_extern:19
#: tvm.tir.op.call_tir:5 tvm.tir.op.create_barriers:10
#: tvm.tir.op.end_profile_intrinsic:8 tvm.tir.op.lookup_param:13
#: tvm.tir.op.mma_fill:19 tvm.tir.op.mma_store:28
#: tvm.tir.op.ptx_arrive_barrier:11 tvm.tir.op.ptx_arrive_barrier_expect_tx:16
#: tvm.tir.op.ptx_commit_group:6 tvm.tir.op.ptx_cp_async:26
#: tvm.tir.op.ptx_cp_async_barrier:11 tvm.tir.op.ptx_cp_async_bulk:29
#: tvm.tir.op.ptx_init_barrier_thread_count:14 tvm.tir.op.ptx_ldmatrix:32
#: tvm.tir.op.ptx_mma:54 tvm.tir.op.ptx_mma_sp:59
#: tvm.tir.op.ptx_wait_barrier:11 tvm.tir.op.ptx_wait_group:11
#: tvm.tir.op.start_profile_intrinsic:8 tvm.tir.op.trace:19
#: tvm.tir.op.tvm_access_ptr:22 tvm.tir.op.tvm_bmma_sync:31
#: tvm.tir.op.tvm_check_return:12 tvm.tir.op.tvm_fill_fragment:25
#: tvm.tir.op.tvm_load_matrix_sync:31 tvm.tir.op.tvm_mma_sync:31
#: tvm.tir.op.tvm_stack_alloca:13 tvm.tir.op.tvm_stack_make_array:25
#: tvm.tir.op.tvm_stack_make_shape:10 tvm.tir.op.tvm_store_matrix_sync:31
#: tvm.tir.op.tvm_struct_get:19 tvm.tir.op.tvm_struct_set:19
#: tvm.tir.op.tvm_thread_allreduce:10 tvm.tir.op.tvm_tuple:10
#: tvm.tir.op.type_annotation:10 tvm.tir.op.undef:5 tvm.tir.op.vectorcombine:13
#: tvm.tir.op.vectorhigh:13 tvm.tir.op.vectorlow:13
msgid "call"
msgstr ""

#: of tvm.tir.op.TVMBackendAllocWorkspace:23
#: tvm.tir.op.TVMBackendFreeWorkspace:17 tvm.tir.op.address_of:14
#: tvm.tir.op.assume:11 tvm.tir.op.call_cpacked:17
#: tvm.tir.op.call_cpacked_lowered:16 tvm.tir.op.call_extern:20
#: tvm.tir.op.call_intrin:23 tvm.tir.op.call_llvm_intrin:20
#: tvm.tir.op.call_llvm_pure_intrin:20 tvm.tir.op.call_packed:21
#: tvm.tir.op.call_packed_lowered:19 tvm.tir.op.call_pure_extern:20
#: tvm.tir.op.call_tir:6 tvm.tir.op.create_barriers:11
#: tvm.tir.op.end_profile_intrinsic:9 tvm.tir.op.lookup_param:14
#: tvm.tir.op.mma_fill:20 tvm.tir.op.mma_store:29
#: tvm.tir.op.ptx_arrive_barrier:12 tvm.tir.op.ptx_arrive_barrier_expect_tx:17
#: tvm.tir.op.ptx_commit_group:7 tvm.tir.op.ptx_cp_async:27
#: tvm.tir.op.ptx_cp_async_barrier:12 tvm.tir.op.ptx_cp_async_bulk:30
#: tvm.tir.op.ptx_init_barrier_thread_count:15 tvm.tir.op.ptx_ldmatrix:33
#: tvm.tir.op.ptx_mma:55 tvm.tir.op.ptx_mma_sp:60
#: tvm.tir.op.ptx_wait_barrier:12 tvm.tir.op.ptx_wait_group:12
#: tvm.tir.op.start_profile_intrinsic:9 tvm.tir.op.trace:19
#: tvm.tir.op.tvm_access_ptr:23 tvm.tir.op.tvm_bmma_sync:32
#: tvm.tir.op.tvm_check_return:13 tvm.tir.op.tvm_fill_fragment:26
#: tvm.tir.op.tvm_load_matrix_sync:32 tvm.tir.op.tvm_mma_sync:32
#: tvm.tir.op.tvm_stack_alloca:14 tvm.tir.op.tvm_stack_make_array:26
#: tvm.tir.op.tvm_stack_make_shape:11 tvm.tir.op.tvm_store_matrix_sync:32
#: tvm.tir.op.tvm_struct_get:20 tvm.tir.op.tvm_struct_set:20
#: tvm.tir.op.tvm_thread_allreduce:11 tvm.tir.op.tvm_tuple:11
#: tvm.tir.op.type_annotation:11 tvm.tir.op.undef:6 tvm.tir.op.vectorcombine:14
#: tvm.tir.op.vectorhigh:14 tvm.tir.op.vectorlow:14
msgid "The call expression."
msgstr ""

#: of tvm.tir.op.TVMBackendFreeWorkspace:12
msgid "ptr"
msgstr ""

#: of tvm.tir.op.TVMBackendFreeWorkspace:12
msgid "The result allocated space pointer."
msgstr ""

#: of tvm.tir.function.TensorIntrin:6 tvm.tir.function.TensorIntrin.register:7
msgid "desc"
msgstr ""

#: of tvm.tir.function.TensorIntrin:6 tvm.tir.function.TensorIntrin.register:8
msgid "The function to describe the computation."
msgstr ""

#: of tvm.tir.function.TensorIntrin:9 tvm.tir.function.TensorIntrin.register:9
msgid "impl"
msgstr ""

#: of tvm.tir.function.TensorIntrin:9 tvm.tir.function.TensorIntrin.register:10
msgid "The function of the implementation for the execution."
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:1:<autosummary>:1
msgid ""
":py:obj:`get <tvm.tir.TensorIntrin.get>`\\ \\(name\\[\\, "
"allow\\_missing\\]\\)"
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:1
#: tvm.tir.function.TensorIntrin.get:1:<autosummary>:1
msgid "Look up a tensor intrinsic by its name."
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:1:<autosummary>:1
msgid ""
":py:obj:`register <tvm.tir.TensorIntrin.register>`\\ \\(name\\, desc\\, "
"impl\\[\\, override\\]\\)"
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:1:<autosummary>:1
#: tvm.tir.function.TensorIntrin.register:1
msgid "Register a tensor intrinsic with its name."
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:6
msgid "The name of the TensorIntrin to look up."
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:8
msgid "allow_missing"
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:9
msgid ""
"Whether to allow missing tensor intrin. If False, raise an error if the "
"tensor intrin"
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:10
msgid "doesn't exist."
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:-1
msgid "Optional[TensorIntrin]"
msgstr ""

#: of tvm.tir.function.TensorIntrin.get:15
msgid "The TensorIntrin with the specified name, or None if not found."
msgstr ""

#: of tvm.tir.function.TensorIntrin.register:6
msgid "The name of the TensorIntrin to register."
msgstr ""

#: of tvm.tir.function.TensorIntrin.register:11
msgid "override: bool"
msgstr ""

#: of tvm.tir.function.TensorIntrin.register:12
msgid "Whether override existing intrinsic."
msgstr ""

#: of tvm.tir.expr.Var:-1
msgid "Union[str, tvm.irType]"
msgstr ""

#: of tvm.tir.stmt.While:6
msgid "The termination condition."
msgstr ""

#: of tvm.tir.op.abs:6 tvm.tir.op.acos:6 tvm.tir.op.acosh:6 tvm.tir.op.asin:6
#: tvm.tir.op.asinh:6 tvm.tir.op.atan:6 tvm.tir.op.atanh:6
#: tvm.tir.op.bitwise_and:6 tvm.tir.op.bitwise_not:6 tvm.tir.op.bitwise_or:6
#: tvm.tir.op.bitwise_xor:6 tvm.tir.op.ceil:6 tvm.tir.op.clz:7 tvm.tir.op.cos:6
#: tvm.tir.op.cosh:6 tvm.tir.op.erf:6 tvm.tir.op.exp:6 tvm.tir.op.exp10:6
#: tvm.tir.op.exp2:6 tvm.tir.op.floor:6 tvm.tir.op.fmod:5 tvm.tir.op.isfinite:6
#: tvm.tir.op.isinf:6 tvm.tir.op.isnan:6 tvm.tir.op.isnullptr:6
#: tvm.tir.op.log:6 tvm.tir.op.log10:6 tvm.tir.op.log1p:6 tvm.tir.op.log2:6
#: tvm.tir.op.nearbyint:13 tvm.tir.op.popcount:6 tvm.tir.op.pow:6
#: tvm.tir.op.power:6 tvm.tir.op.q_multiply_shift:12
#: tvm.tir.op.q_multiply_shift_per_axis:5 tvm.tir.op.round:6 tvm.tir.op.rsqrt:6
#: tvm.tir.op.shift_left:6 tvm.tir.op.shift_right:6 tvm.tir.op.sigmoid:6
#: tvm.tir.op.sin:6 tvm.tir.op.sinh:6 tvm.tir.op.sqrt:6 tvm.tir.op.tan:6
#: tvm.tir.op.tanh:6 tvm.tir.op.trunc:9
msgid "x"
msgstr ""

#: of tvm.tir.op.abs:6 tvm.tir.op.acos:6 tvm.tir.op.acosh:6 tvm.tir.op.asin:6
#: tvm.tir.op.asinh:6 tvm.tir.op.atan:6 tvm.tir.op.atan2:6 tvm.tir.op.atan2:9
#: tvm.tir.op.atanh:6 tvm.tir.op.ceil:6 tvm.tir.op.copysign:6
#: tvm.tir.op.copysign:9 tvm.tir.op.cos:6 tvm.tir.op.cosh:6 tvm.tir.op.erf:6
#: tvm.tir.op.exp:6 tvm.tir.op.exp10:6 tvm.tir.op.exp2:6 tvm.tir.op.floor:6
#: tvm.tir.op.fmod:6 tvm.tir.op.fmod:8 tvm.tir.op.hypot:6 tvm.tir.op.hypot:9
#: tvm.tir.op.isfinite:6 tvm.tir.op.isinf:6 tvm.tir.op.isnan:6
#: tvm.tir.op.isnullptr:6 tvm.tir.op.ldexp:6 tvm.tir.op.ldexp:9
#: tvm.tir.op.likely:7 tvm.tir.op.log:6 tvm.tir.op.log10:6 tvm.tir.op.log1p:6
#: tvm.tir.op.log2:6 tvm.tir.op.nearbyint:13 tvm.tir.op.nextafter:6
#: tvm.tir.op.nextafter:9 tvm.tir.op.popcount:6 tvm.tir.op.pow:6
#: tvm.tir.op.power:6 tvm.tir.op.round:6 tvm.tir.op.rsqrt:6
#: tvm.tir.op.shift_left:6 tvm.tir.op.shift_left:9 tvm.tir.op.shift_right:6
#: tvm.tir.op.shift_right:9 tvm.tir.op.sigmoid:6 tvm.tir.op.sin:6
#: tvm.tir.op.sinh:6 tvm.tir.op.sqrt:6 tvm.tir.op.tan:6 tvm.tir.op.tanh:6
#: tvm.tir.op.trunc:9
msgid "Input argument."
msgstr ""

#: of tvm.tir.op.abs:9 tvm.tir.op.address_of:9 tvm.tir.op.all:10
#: tvm.tir.op.any:9 tvm.tir.op.bitwise_and:12 tvm.tir.op.bitwise_not:9
#: tvm.tir.op.bitwise_or:12 tvm.tir.op.bitwise_xor:12
#: tvm.tir.op.call_cpacked:12 tvm.tir.op.call_cpacked_lowered:11
#: tvm.tir.op.call_extern:15 tvm.tir.op.call_intrin:18
#: tvm.tir.op.call_llvm_intrin:15 tvm.tir.op.call_llvm_pure_intrin:15
#: tvm.tir.op.call_packed:16 tvm.tir.op.call_packed_lowered:14
#: tvm.tir.op.call_pure_extern:15 tvm.tir.op.ceil:9 tvm.tir.op.floor:9
#: tvm.tir.op.infinity:9 tvm.tir.op.isfinite:9 tvm.tir.op.isinf:9
#: tvm.tir.op.isnan:9 tvm.tir.op.isnullptr:9 tvm.tir.op.likely:10
#: tvm.tir.op.lookup_param:9 tvm.tir.op.max_value:9 tvm.tir.op.min_value:9
#: tvm.tir.op.nearbyint:16 tvm.tir.op.pow:12 tvm.tir.op.power:12
#: tvm.tir.op.reinterpret:12 tvm.tir.op.round:9 tvm.tir.op.trunc:12
msgid "The location of this operator in the source code."
msgstr ""

#: of tvm.tir.op.abs:13 tvm.tir.op.acos:10 tvm.tir.op.acosh:10
#: tvm.tir.op.asin:10 tvm.tir.op.asinh:10 tvm.tir.op.atan:10
#: tvm.tir.op.atan2:13 tvm.tir.op.atanh:10 tvm.tir.op.bitwise_and:9
#: tvm.tir.op.bitwise_or:9 tvm.tir.op.bitwise_xor:9 tvm.tir.op.ceil:13
#: tvm.tir.op.clz:11 tvm.tir.op.copysign:13 tvm.tir.op.cos:10
#: tvm.tir.op.cosh:10 tvm.tir.op.erf:10 tvm.tir.op.exp:10 tvm.tir.op.exp10:10
#: tvm.tir.op.exp2:10 tvm.tir.op.floor:13 tvm.tir.op.fmod:8 tvm.tir.op.hypot:13
#: tvm.tir.op.isfinite:13 tvm.tir.op.isinf:13 tvm.tir.op.isnan:13
#: tvm.tir.op.isnullptr:13 tvm.tir.op.ldexp:13 tvm.tir.op.likely:14
#: tvm.tir.op.log:10 tvm.tir.op.log10:10 tvm.tir.op.log1p:10 tvm.tir.op.log2:10
#: tvm.tir.op.nearbyint:20 tvm.tir.op.nextafter:13 tvm.tir.op.popcount:10
#: tvm.tir.op.pow:9 tvm.tir.op.power:9 tvm.tir.op.q_multiply_shift:14
#: tvm.tir.op.q_multiply_shift:23 tvm.tir.op.q_multiply_shift_per_axis:7
#: tvm.tir.op.round:13 tvm.tir.op.rsqrt:10 tvm.tir.op.shift_left:9
#: tvm.tir.op.shift_right:9 tvm.tir.op.sigmoid:10 tvm.tir.op.sin:10
#: tvm.tir.op.sinh:10 tvm.tir.op.sqrt:10 tvm.tir.op.tan:10 tvm.tir.op.tanh:10
#: tvm.tir.op.trunc:16
msgid "y"
msgstr ""

#: of tvm.tir.op.abs:14 tvm.tir.op.acos:11 tvm.tir.op.acosh:11
#: tvm.tir.op.asin:11 tvm.tir.op.asinh:11 tvm.tir.op.atan:11
#: tvm.tir.op.atan2:14 tvm.tir.op.atanh:11 tvm.tir.op.bitwise_and:17
#: tvm.tir.op.bitwise_not:14 tvm.tir.op.bitwise_or:17 tvm.tir.op.bitwise_xor:17
#: tvm.tir.op.ceil:14 tvm.tir.op.clz:12 tvm.tir.op.copysign:14
#: tvm.tir.op.cos:11 tvm.tir.op.cosh:11 tvm.tir.op.erf:11 tvm.tir.op.exp:11
#: tvm.tir.op.exp10:11 tvm.tir.op.exp2:11 tvm.tir.op.floor:14
#: tvm.tir.op.fmod:13 tvm.tir.op.hypot:14 tvm.tir.op.isfinite:14
#: tvm.tir.op.isinf:14 tvm.tir.op.isnan:14 tvm.tir.op.isnullptr:14
#: tvm.tir.op.ldexp:14 tvm.tir.op.log:11 tvm.tir.op.log10:11
#: tvm.tir.op.log1p:11 tvm.tir.op.log2:11 tvm.tir.op.nearbyint:21
#: tvm.tir.op.nextafter:14 tvm.tir.op.popcount:11 tvm.tir.op.pow:17
#: tvm.tir.op.power:17 tvm.tir.op.q_multiply_shift:24
#: tvm.tir.op.q_multiply_shift_per_axis:23 tvm.tir.op.round:14
#: tvm.tir.op.rsqrt:11 tvm.tir.op.shift_left:14 tvm.tir.op.shift_right:14
#: tvm.tir.op.sigmoid:11 tvm.tir.op.sin:11 tvm.tir.op.sinh:11
#: tvm.tir.op.sqrt:11 tvm.tir.op.tan:11 tvm.tir.op.tanh:11 tvm.tir.op.trunc:17
#: tvm.tir.stmt_functor.ir_transform:23 tvm.tir.stmt_functor.substitute:14
msgid "The result."
msgstr ""

#: of tvm.tir.generic.add:-1 tvm.tir.generic.multiply:-1
#: tvm.tir.generic.subtract:-1 tvm.tir.op.ceildiv:-1
msgid "object"
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:6 tvm.tir.generic.add:6
#: tvm.tir.generic.multiply:6 tvm.tir.generic.subtract:6 tvm.tir.op.ceildiv:6
msgid "The left operand."
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:9 tvm.tir.generic.add:8
#: tvm.tir.generic.multiply:8 tvm.tir.generic.subtract:8 tvm.tir.op.ceildiv:8
msgid "The right operand."
msgstr ""

#: of tvm.tir.generic.add:10 tvm.tir.generic.multiply:10
#: tvm.tir.generic.subtract:10 tvm.tir.op.ceildiv:10 tvm.tir.op.div:12
#: tvm.tir.op.floordiv:12 tvm.tir.op.floormod:12 tvm.tir.op.if_then_else:15
#: tvm.tir.op.indexdiv:12 tvm.tir.op.indexmod:12 tvm.tir.op.truncdiv:12
#: tvm.tir.op.truncmod:12
msgid "The location of this operator in the source."
msgstr ""

#: of tvm.tir.generic.add:-1 tvm.tir.generic.multiply:-1
#: tvm.tir.generic.subtract:-1 tvm.tir.op.ceildiv:-1 tvm.tir.op.infinity:-1
#: tvm.tir.op.max_value:-1 tvm.tir.op.min_value:-1 tvm.tir.op.reinterpret:-1
msgid "tvm.Expr"
msgstr ""

#: of tvm.tir.generic.add:15
msgid "The result Expr of add operaton."
msgstr ""

#: of tvm.tir.op.address_of:6
msgid "buffer_load: BufferLoad"
msgstr ""

#: of tvm.tir.op.address_of:6
msgid "The buffer load."
msgstr ""

#: of tvm.tir.op.all:2
msgid "arguments"
msgstr ""

#: of tvm.tir.op.all:-1 tvm.tir.op.any:-1 tvm.tir.op.call_extern:-1
#: tvm.tir.op.call_intrin:-1 tvm.tir.op.call_llvm_intrin:-1
#: tvm.tir.op.call_llvm_pure_intrin:-1 tvm.tir.op.call_pure_extern:-1
#: tvm.tir.op.vectorcombine:-1 tvm.tir.op.vectorhigh:-1 tvm.tir.op.vectorlow:-1
msgid "list"
msgstr ""

#: of tvm.tir.op.all:7 tvm.tir.op.any:6
msgid "List of symbolic boolean expressions"
msgstr ""

#: of tvm.tir.op.all:14 tvm.tir.op.any:13
msgid "expr: Expr"
msgstr ""

#: of tvm.tir.op.all:15 tvm.tir.op.any:14
msgid "Expression"
msgstr ""

#: of tvm.tir.op.assume:6 tvm.tir.op.if_then_else:6 tvm.tir.op.likely:7
msgid "cond"
msgstr ""

#: of tvm.tir.op.assume:6
msgid "The constraint condition."
msgstr ""

#: of tvm.tir.op.atan2:6 tvm.tir.op.copysign:6 tvm.tir.op.hypot:6
#: tvm.tir.op.ldexp:6 tvm.tir.op.nextafter:6
msgid "x1"
msgstr ""

#: of tvm.tir.op.atan2:9 tvm.tir.op.copysign:9 tvm.tir.op.hypot:9
#: tvm.tir.op.ldexp:9 tvm.tir.op.nextafter:9
msgid "x2"
msgstr ""

#: of tvm.tir.data_layout.bijective_layout:13
msgid "bijective_layout"
msgstr ""

#: of tvm.tir.data_layout.bijective_layout:-1
msgid "BijectiveLayout"
msgstr ""

#: of tvm.tir.data_layout.bijective_layout:14
msgid "The created bijective layout"
msgstr ""

#: of tvm.tir.op.bitwise_and:6 tvm.tir.op.bitwise_or:6 tvm.tir.op.bitwise_xor:6
msgid "Left operand"
msgstr ""

#: of tvm.tir.op.bitwise_and:9 tvm.tir.op.bitwise_or:9 tvm.tir.op.bitwise_xor:9
msgid "Right operand"
msgstr ""

#: of tvm.tir.op.bitwise_and:16 tvm.tir.op.bitwise_not:13
#: tvm.tir.op.bitwise_or:16 tvm.tir.op.bitwise_xor:16 tvm.tir.op.div:16
#: tvm.tir.op.floordiv:16 tvm.tir.op.floormod:16 tvm.tir.op.indexdiv:17
#: tvm.tir.op.indexmod:17 tvm.tir.op.truncdiv:17 tvm.tir.op.truncmod:17
msgid "res"
msgstr ""

#: of tvm.tir.op.bitwise_not:6
msgid "Input operand"
msgstr ""

#: of tvm.tir.op.call_cpacked:3
msgid ""
"Same as call_packed, except that the first argument is the function name "
"(as in call_extern), and the last argument is the resource handle."
msgstr ""

#: of tvm.tir.op.call_cpacked:-1 tvm.tir.op.call_cpacked_lowered:-1
#: tvm.tir.op.call_packed:-1 tvm.tir.op.call_packed_lowered:-1
msgid "list of Expr or Buffer."
msgstr ""

#: of tvm.tir.op.call_cpacked:9 tvm.tir.op.call_cpacked_lowered:8
#: tvm.tir.op.call_extern:12 tvm.tir.op.call_intrin:15
#: tvm.tir.op.call_packed:13 tvm.tir.op.call_packed_lowered:11
#: tvm.tir.op.call_pure_extern:12 tvm.tir.op.trace:11
msgid "Positional arguments."
msgstr ""

#: of tvm.tir.op.call_cpacked:21 tvm.tir.op.call_cpacked_lowered:20
#: tvm.tir.op.call_packed:25 tvm.tir.op.call_packed_lowered:23
msgid "te.extern : Create tensor with extern function call."
msgstr ""

#: of tvm.tir.op.call_cpacked_lowered:1
msgid ""
"Lowered version of call c-packed. Same as call_packed, except that the "
"first argument is the function name (as in call_extern), and the last "
"argument is the resource handle."
msgstr ""

#: of tvm.tir.op.call_extern:6 tvm.tir.op.call_intrin:9
#: tvm.tir.op.call_llvm_intrin:6 tvm.tir.op.call_llvm_pure_intrin:6
#: tvm.tir.op.call_pure_extern:6 tvm.tir.op.mma_fill:6 tvm.tir.op.mma_store:6
#: tvm.tir.op.ptx_cp_async:7 tvm.tir.op.ptx_cp_async_bulk:7
#: tvm.tir.op.ptx_ldmatrix:7 tvm.tir.op.ptx_mma:7 tvm.tir.op.ptx_mma_sp:7
#: tvm.tir.op.vectorhigh:6 tvm.tir.op.vectorlow:6
msgid "The data type of the result."
msgstr ""

#: of tvm.tir.op.call_extern:9 tvm.tir.op.call_intrin:12
#: tvm.tir.op.call_pure_extern:9
msgid "func_name: str"
msgstr ""

#: of tvm.tir.op.call_extern:9 tvm.tir.op.call_pure_extern:9
msgid "The extern function name."
msgstr ""

#: of tvm.tir.op.call_intrin:3
msgid ""
"Intrinsics can be overloaded with multiple data types via the intrinsic "
"translation rule."
msgstr ""

#: of tvm.tir.op.call_intrin:12
msgid "The intrinsic function name."
msgstr ""

#: of tvm.tir.op.call_llvm_intrin:9 tvm.tir.op.call_llvm_pure_intrin:9
msgid "The name of the llvm intrinsic function."
msgstr ""

#: of tvm.tir.op.call_llvm_intrin:12 tvm.tir.op.call_llvm_pure_intrin:12
msgid "Poistional arguments."
msgstr ""

#: of tvm.tir.op.call_packed:3
msgid ""
"The argument to packed function can be Expr or Buffer. The argument is "
"the corresponding POD type when Expr is presented."
msgstr ""

#: of tvm.tir.op.call_packed:6
msgid ""
"When the argument is Buffer, the corresponding PackedFunc will receive an"
" TVMArrayHandle whose content is valid during the callback period. If the"
" PackedFunc is a python callback, then the corresponding argument is "
"NDArray."
msgstr ""

#: of tvm.tir.op.call_packed_lowered:1
msgid ""
"Lowered version of call packed. The argument to packed function can be "
"Expr or Buffer. The argument is the corresponding POD type when Expr is "
"presented. When the argument is Buffer, the corresponding PackedFunc will"
" recieve an TVMArrayHandle whose content is valid during the callback "
"period. If the PackedFunc is a python callback, then the corresponding "
"argument is NDArray."
msgstr ""

#: of tvm.tir.op.ceildiv:15
msgid "The result Expr of ceildiv operaton."
msgstr ""

#: of tvm.tir.op.clz:6
msgid "Input 32 or 64 bit integer. The result is undefined if the input is 0."
msgstr ""

#: of tvm.tir.op.comm_reducer:6
msgid "fcombine"
msgstr ""

#: of tvm.tir.op.comm_reducer:-1
msgid "function(Expr -> Expr -> Expr)"
msgstr ""

#: of tvm.tir.op.comm_reducer:6
msgid "A binary function which takes two Expr as input to return a Expr."
msgstr ""

#: of tvm.tir.op.comm_reducer:9
msgid "fidentity"
msgstr ""

#: of tvm.tir.op.comm_reducer:-1
msgid "function(str -> Expr)"
msgstr ""

#: of tvm.tir.op.comm_reducer:9
msgid "A function which takes a type string as input to return a const Expr."
msgstr ""

#: of tvm.tir.op.comm_reducer:19
msgid "reducer"
msgstr ""

#: of tvm.tir.op.comm_reducer:-1 tvm.tir.stmt_functor.ir_transform:-1
#: tvm.tir.transform.transform.InjectCopyIntrin:-1
msgid "function"
msgstr ""

#: of tvm.tir.op.comm_reducer:14
msgid ""
"A function which creates a reduce expression over axis. There are two "
"ways to use it:"
msgstr ""

#: of tvm.tir.op.comm_reducer:17
msgid "accept (expr, axis, where) to produce an Reduce Expr on specified axis;"
msgstr ""

#: of tvm.tir.op.comm_reducer:19
msgid "simply use it with multiple Exprs."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:62 tvm.tir.op.comm_reducer:22
#: tvm.tir.op.comm_reducer.<locals>.reducer:17
#: tvm.tir.transform.transform.CompactBufferAllocation:6
msgid "Example"
msgstr ""

#: of tvm.tir.op.create_barriers:6
msgid "barrier_count"
msgstr ""

#: of tvm.tir.op.create_barriers:6
msgid "The number of barriers to create."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:3
msgid ""
"Normally buffer is created automatically during lower and build. This is "
"only needed if user want to specify their own buffer layout."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:6
msgid "See the note below for detailed discussion on usage of buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:-1
msgid "tuple of Expr"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:11
msgid "The shape of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:17
msgid "The name of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:20 tvm.tir.op.tvm_access_ptr:9
#: tvm.tir.op.tvm_stack_make_array:6
msgid "data"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:-1
msgid "Var, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:20
msgid "The data pointer in the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:23
msgid "strides: array of Expr"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:23
msgid "The stride of the buffer."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:27
msgid "elem_offset: Expr, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:26
msgid ""
"The beginning offset of the array to data. In terms of number of elements"
" of dtype."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:31
msgid "scope: str, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:30
msgid ""
"The storage scope of the buffer, if not global. If scope equals empty "
"string, it means it is global memory."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:35
msgid "data_alignment: int, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:34
msgid ""
"The alignment of data pointer in bytes. If -1 is passed, the alignment "
"will be set to TVM's internal default."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:41
msgid "offset_factor: int, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:38
msgid ""
"The factor of elem_offset field, when set, elem_offset is required to be "
"multiple of offset_factor. If 0 is pssed, the alignment will be set to 1."
" if non-zero is passed, we will created a Var for elem_offset if "
"elem_offset is not None."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:46
msgid "buffer_type: str, optional, {\"\", \"auto_broadcast\"}"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:44
msgid ""
"auto_broadcast buffer allows one to implement broadcast computation "
"without considering whether dimension size equals to one. TVM maps "
"buffer[i][j][k] -> buffer[i][0][k] if dimension j's shape equals 1."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:51
msgid "axis_separators"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:-1
msgid "list of int, optional"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:49
msgid ""
"If passed, a list of separators between groups of axes, each of which is "
"flattened to an output axis.  For flat memory spaces, should either be "
"None, or an empty list."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:54
msgid "The location of the decl_buffer creation in the source."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:-1
msgid "tvm.tir.Buffer"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:59
msgid "The created buffer"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:63
msgid ""
"Here's an example of how broadcast buffer can be used to define a "
"symbolic broadcast operation,"
msgstr ""

#: of tvm.tir.buffer.decl_buffer:86
msgid ""
"Buffer data structure reflects the DLTensor structure in dlpack. While "
"DLTensor data structure is very general, it is usually helpful to create "
"function that only handles specific case of data structure and make "
"compiled function benefit from it."
msgstr ""

#: of tvm.tir.buffer.decl_buffer:91
msgid ""
"If user pass strides and elem_offset is passed as None when constructing "
"the function, then the function will be specialized for the DLTensor that"
" is compact and aligned. If user pass a fully generic symbolic array to "
"the strides, then the resulting function becomes fully generic."
msgstr ""

#: of tvm.tir.op.div:6 tvm.tir.op.indexdiv:6 tvm.tir.op.indexmod:6
msgid "The left hand operand, known to be non-negative."
msgstr ""

#: of tvm.tir.op.div:9 tvm.tir.op.indexdiv:9 tvm.tir.op.indexmod:9
msgid "The right hand operand, known to be non-negative."
msgstr ""

#: of tvm.tir.op.div:17 tvm.tir.op.floordiv:17 tvm.tir.op.floormod:17
#: tvm.tir.op.indexdiv:17 tvm.tir.op.indexmod:17 tvm.tir.op.truncdiv:17
#: tvm.tir.op.truncmod:17
msgid "The result expression."
msgstr ""

#: of tvm.tir.op.div:20
msgid "When operands are integers, returns truncdiv(a, b, span)."
msgstr ""

#: of tvm.tir.op.end_profile_intrinsic:1
msgid "End profile intrinsic. Parameters ---------- id : int"
msgstr ""

#: of tvm.tir.op.end_profile_intrinsic:5 tvm.tir.op.start_profile_intrinsic:5
msgid "The intrinsic id."
msgstr ""

#: of tvm.tir.op.floordiv:6 tvm.tir.op.floormod:6 tvm.tir.op.truncdiv:6
#: tvm.tir.op.truncmod:6
msgid "The left hand operand"
msgstr ""

#: of tvm.tir.op.floordiv:9 tvm.tir.op.floormod:9 tvm.tir.op.truncdiv:9
#: tvm.tir.op.truncmod:9
msgid "The right hand operand"
msgstr ""

#: of tvm.tir.op.fmod:12 tvm.tir.op.pow:16 tvm.tir.op.power:16
#: tvm.tir.op.q_multiply_shift_per_axis:22 tvm.tir.op.shift_left:13
#: tvm.tir.op.shift_right:13
msgid "z"
msgstr ""

#: of tvm.tir.op.if_then_else:6
msgid "The condition"
msgstr ""

#: of tvm.tir.op.if_then_else:9
msgid "t"
msgstr ""

#: of tvm.tir.op.if_then_else:9
msgid "The result expression if cond is true."
msgstr ""

#: of tvm.tir.op.if_then_else:12
msgid "f"
msgstr ""

#: of tvm.tir.op.if_then_else:12
msgid "The result expression if cond is false."
msgstr ""

#: of tvm.tir.op.if_then_else:20
msgid "The result of conditional expression."
msgstr ""

#: of tvm.tir.op.if_then_else:24
msgid ""
"Unlike Select, if_then_else will not execute the branch that does not "
"satisfy the condition. You can use it to guard against out of bound "
"access. Unlike Select, if_then_else cannot be vectorized if some lanes in"
" the vector have different conditions."
msgstr ""

#: of tvm.tir.op.indexdiv:21 tvm.tir.op.indexmod:21
msgid ""
"Use this function to split non-negative indices. This function may take "
"advantage of operands' non-negativeness."
msgstr ""

#: of tvm.tir.op.indexmod:1
msgid "Compute the remainder of indexdiv. a and b are non-negative."
msgstr ""

#: of tvm.tir.op.infinity:6 tvm.tir.op.max_value:6 tvm.tir.op.min_value:6
#: tvm.tir.op.reinterpret:6 tvm.tir.op.type_annotation:6
msgid "The data type."
msgstr ""

#: of tvm.tir.op.infinity:14
msgid "The infinity value of dtype."
msgstr ""

#: of tvm.tir.data_layout.layout:12
msgid "layout_str"
msgstr ""

#: of tvm.tir.data_layout.layout:6
msgid ""
"A layout representation is composed of upper cases, lower cases and "
"numbers, where upper case indicates a primal axis and the corresponding "
"lower case with factor size indicates the subordinate axis. For example, "
"NCHW16c can describe a 5-D tensor of [batch_size, channel, height, width,"
" channel_block]. Here subordinate axis channel_block=16 is the factor "
"size of the primal axis C (channel)."
msgstr ""

#: of tvm.tir.data_layout.layout:15
msgid ""
"The dtype of generated axes vars in the returned layout. It is required "
"to be integer type."
msgstr ""

#: of tvm.tir.data_layout.layout:20 tvm.tir.op.tvm_load_matrix_sync:27
#: tvm.tir.op.tvm_store_matrix_sync:27
msgid "layout"
msgstr ""

#: of tvm.tir.data_layout.layout:-1
msgid "Layout"
msgstr ""

#: of tvm.tir.data_layout.layout:21
msgid "The created layout"
msgstr ""

#: of tvm.tir.op.likely:15
msgid "The marked expression."
msgstr ""

#: of tvm.tir.op.lookup_param:6
msgid "param_name"
msgstr ""

#: of tvm.tir.op.lookup_param:6
msgid "The name of param."
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:5
msgid "expr"
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:-1
msgid "IterVar"
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:8
msgid "The reduction IterVar axis"
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:9
msgid "where"
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:-1
msgid "optional, Expr"
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:10
msgid "Filtering predicate of the reduction."
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:14
msgid "The result value."
msgstr ""

#: of tvm.tir.op.max_value:14
msgid "The maximum value of dtype."
msgstr ""

#: of tvm.tir.op.min_value:14
msgid "The minimum value of dtype."
msgstr ""

#: of tvm.tir.op.mma_fill:9
msgid "local_size"
msgstr ""

#: of tvm.tir.op.mma_fill:-1 tvm.tir.op.mma_store:-1 tvm.tir.op.ptx_ldmatrix:-1
#: tvm.tir.op.q_multiply_shift_per_axis:-1
msgid "IntImm"
msgstr ""

#: of tvm.tir.op.mma_fill:9
msgid "The number of elements."
msgstr ""

#: of tvm.tir.op.mma_fill:12 tvm.tir.op.ptx_ldmatrix:19
msgid "local_ptr"
msgstr ""

#: of tvm.tir.op.mma_fill:12 tvm.tir.op.mma_store:15
msgid "The destination pointer variable."
msgstr ""

#: of tvm.tir.op.mma_fill:15
msgid "The destination offset."
msgstr ""

#: of tvm.tir.op.mma_store:9 tvm.tir.op.tvm_fill_fragment:9
#: tvm.tir.op.tvm_load_matrix_sync:9 tvm.tir.op.tvm_store_matrix_sync:9
msgid "m"
msgstr ""

#: of tvm.tir.op.mma_store:9 tvm.tir.op.mma_store:12 tvm.tir.op.ptx_mma:10
#: tvm.tir.op.ptx_mma_sp:10
msgid "The shape of mma fragment."
msgstr ""

#: of tvm.tir.op.mma_store:15
msgid "dst_ptr"
msgstr ""

#: of tvm.tir.op.mma_store:18
msgid "src_ptr"
msgstr ""

#: of tvm.tir.op.mma_store:18
msgid "The source pointer variable."
msgstr ""

#: of tvm.tir.op.mma_store:21
msgid "src_offset"
msgstr ""

#: of tvm.tir.op.mma_store:21
msgid "The source offset."
msgstr ""

#: of tvm.tir.op.mma_store:24
msgid "dst_stride"
msgstr ""

#: of tvm.tir.op.mma_store:24
msgid "The destination stride."
msgstr ""

#: of tvm.tir.generic.multiply:15
msgid "The result Expr of multiply operaton."
msgstr ""

#: of tvm.tir.op.nearbyint:1
msgid ""
"Round elements of the array to the nearest integer. This intrinsic uses "
"llvm.nearbyint instead of llvm.round which is faster but will results "
"different from te.round. Notably nearbyint rounds according to the "
"rounding mode, whereas te.round (llvm.round) ignores that. For "
"differences between the two see: "
"https://en.cppreference.com/w/cpp/numeric/math/round "
"https://en.cppreference.com/w/cpp/numeric/math/nearbyint"
msgstr ""

#: of tvm.tir.op.pow:9 tvm.tir.op.power:9
msgid "The exponent"
msgstr ""

#: of tvm.tir.op.ptx_arrive_barrier:7 tvm.tir.op.ptx_arrive_barrier_expect_tx:8
#: tvm.tir.op.ptx_cp_async_barrier:7 tvm.tir.op.ptx_cp_async_bulk:25
#: tvm.tir.op.ptx_init_barrier_thread_count:7 tvm.tir.op.ptx_wait_barrier:7
msgid "barrier_id"
msgstr ""

#: of tvm.tir.op.ptx_arrive_barrier:7 tvm.tir.op.ptx_arrive_barrier_expect_tx:8
#: tvm.tir.op.ptx_cp_async_barrier:7 tvm.tir.op.ptx_cp_async_bulk:25
#: tvm.tir.op.ptx_init_barrier_thread_count:7 tvm.tir.op.ptx_wait_barrier:7
msgid "The ID of the barrier shared memory pointer."
msgstr ""

#: of tvm.tir.op.ptx_arrive_barrier_expect_tx:12
msgid "byte_count"
msgstr ""

#: of tvm.tir.op.ptx_arrive_barrier_expect_tx:11
msgid ""
"Increases the tx count of the mbarrier object to track completion of "
"addtional async transactions."
msgstr ""

#: of tvm.tir.op.ptx_cp_async:10 tvm.tir.op.ptx_cp_async_bulk:10
msgid "shared_ptr"
msgstr ""

#: of tvm.tir.op.ptx_cp_async:10 tvm.tir.op.ptx_cp_async_bulk:10
#: tvm.tir.op.ptx_ldmatrix:25
msgid "The shared memory pointer variable."
msgstr ""

#: of tvm.tir.op.ptx_cp_async:13 tvm.tir.op.ptx_cp_async_bulk:13
msgid "shared_offset"
msgstr ""

#: of tvm.tir.op.ptx_cp_async:13 tvm.tir.op.ptx_cp_async_bulk:13
msgid "The offset of shared memory pointer."
msgstr ""

#: of tvm.tir.op.ptx_cp_async:16 tvm.tir.op.ptx_cp_async_bulk:16
msgid "global_ptr"
msgstr ""

#: of tvm.tir.op.ptx_cp_async:16 tvm.tir.op.ptx_cp_async_bulk:16
msgid "The global memory pointer variable."
msgstr ""

#: of tvm.tir.op.ptx_cp_async:19 tvm.tir.op.ptx_cp_async_bulk:19
msgid "global_offset"
msgstr ""

#: of tvm.tir.op.ptx_cp_async:19 tvm.tir.op.ptx_cp_async_bulk:19
msgid "The offset of global memory pointer."
msgstr ""

#: of tvm.tir.op.ptx_cp_async:22 tvm.tir.op.ptx_cp_async_bulk:22
msgid "bytes"
msgstr ""

#: of tvm.tir.op.ptx_cp_async:22 tvm.tir.op.ptx_cp_async_bulk:22
msgid "The data size to copy."
msgstr ""

#: of tvm.tir.op.ptx_init_barrier_thread_count:10
msgid "thread_count"
msgstr ""

#: of tvm.tir.op.ptx_init_barrier_thread_count:10
msgid "Number of threads expected to arrive at the barrier."
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:10
msgid "trans"
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:10
msgid "The matrix is loaded in column-major format."
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:13 tvm.tir.op.ptx_wait_group:7
#: tvm.tir.op.tvm_stack_alloca:9
msgid "num"
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:13
msgid "The number of matrices."
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:16
msgid "type"
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:-1
msgid "Literal[\".b16\"]"
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:16
msgid "The data type of the matrices."
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:19
msgid "The local pointer variable."
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:22
msgid "local_offset"
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:22
msgid "The offset of local pointer."
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:25
msgid "smem_ptr"
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:28
msgid "smem_offset"
msgstr ""

#: of tvm.tir.op.ptx_ldmatrix:28
msgid "The offset of shared memort pointer."
msgstr ""

#: of tvm.tir.op.ptx_mma:13 tvm.tir.op.ptx_mma_sp:13
msgid "A_layout"
msgstr ""

#: of tvm.tir.op.ptx_mma:-1 tvm.tir.op.ptx_mma_sp:-1
msgid "Literal[\"row\", \"col\"]"
msgstr ""

#: of tvm.tir.op.ptx_mma:13 tvm.tir.op.ptx_mma_sp:13
msgid "The layout of multiplicand fragment A."
msgstr ""

#: of tvm.tir.op.ptx_mma:16 tvm.tir.op.ptx_mma_sp:16
msgid "B_layout"
msgstr ""

#: of tvm.tir.op.ptx_mma:16 tvm.tir.op.ptx_mma_sp:16
msgid "The layout of multiplicand fragment B."
msgstr ""

#: of tvm.tir.op.ptx_mma:19 tvm.tir.op.ptx_mma_sp:19
msgid "A_dtype"
msgstr ""

#: of tvm.tir.op.ptx_mma:19 tvm.tir.op.ptx_mma_sp:19
msgid "The data type of multiplicand fragment A."
msgstr ""

#: of tvm.tir.op.ptx_mma:22 tvm.tir.op.ptx_mma_sp:22
msgid "B_dtype"
msgstr ""

#: of tvm.tir.op.ptx_mma:22 tvm.tir.op.ptx_mma_sp:22
msgid "The data type of multiplicand fragment B."
msgstr ""

#: of tvm.tir.op.ptx_mma:25 tvm.tir.op.ptx_mma_sp:25
msgid "C_dtype"
msgstr ""

#: of tvm.tir.op.ptx_mma:25
msgid "The data type of accumulator fragment C."
msgstr ""

#: of tvm.tir.op.ptx_mma:28 tvm.tir.op.ptx_mma_sp:28
msgid "multiplicand_a"
msgstr ""

#: of tvm.tir.op.ptx_mma:28 tvm.tir.op.ptx_mma_sp:28
msgid "The multiplicand fragment A variable."
msgstr ""

#: of tvm.tir.op.ptx_mma:31 tvm.tir.op.ptx_mma_sp:31
msgid "a_index"
msgstr ""

#: of tvm.tir.op.ptx_mma:31 tvm.tir.op.ptx_mma:37 tvm.tir.op.ptx_mma_sp:31
msgid "The index of multiplicand fragment A."
msgstr ""

#: of tvm.tir.op.ptx_mma:34 tvm.tir.op.ptx_mma_sp:34
msgid "multiplicand_b"
msgstr ""

#: of tvm.tir.op.ptx_mma:34 tvm.tir.op.ptx_mma_sp:34
msgid "The multiplicand fragment B variable."
msgstr ""

#: of tvm.tir.op.ptx_mma:37 tvm.tir.op.ptx_mma_sp:37
msgid "b_index"
msgstr ""

#: of tvm.tir.op.ptx_mma:40 tvm.tir.op.ptx_mma_sp:40
msgid "accumulator"
msgstr ""

#: of tvm.tir.op.ptx_mma:40 tvm.tir.op.ptx_mma_sp:40
msgid "The accumulator fragment C variable."
msgstr ""

#: of tvm.tir.op.ptx_mma:43 tvm.tir.op.ptx_mma_sp:43
msgid "c_index"
msgstr ""

#: of tvm.tir.op.ptx_mma:43 tvm.tir.op.ptx_mma_sp:43
msgid "The index of accumulator fragment C."
msgstr ""

#: of tvm.tir.op.ptx_mma:47 tvm.tir.op.ptx_mma_sp:55
msgid "saturate"
msgstr ""

#: of tvm.tir.op.ptx_mma:46 tvm.tir.op.ptx_mma_sp:55
msgid "The optional saturation at the output."
msgstr ""

#: of tvm.tir.op.ptx_mma:50
msgid "operator"
msgstr ""

#: of tvm.tir.op.ptx_mma:-1
msgid "Optional[Literal[\"xor\", \"and\"]]"
msgstr ""

#: of tvm.tir.op.ptx_mma:50
msgid "The 1-bit operator."
msgstr ""

#: of tvm.tir.op.ptx_mma_sp:25
msgid "The data type of multiplicand fragment C."
msgstr ""

#: of tvm.tir.op.ptx_mma_sp:37
msgid "The index of multiplicand fragment B."
msgstr ""

#: of tvm.tir.op.ptx_mma_sp:46
msgid "metadata"
msgstr ""

#: of tvm.tir.op.ptx_mma_sp:46
msgid "The metadata of operand."
msgstr ""

#: of tvm.tir.op.ptx_mma_sp:49
msgid "meta_index"
msgstr ""

#: of tvm.tir.op.ptx_mma_sp:49
msgid "The metadata index of operand."
msgstr ""

#: of tvm.tir.op.ptx_mma_sp:52
msgid "sparse_selector"
msgstr ""

#: of tvm.tir.op.ptx_mma_sp:52
msgid "The sparse selector indicating the thread that stores the metadata."
msgstr ""

#: of tvm.tir.op.ptx_wait_group:7
msgid "The number of the most recent uncommitted pending cp.async groups to wait."
msgstr ""

#: of tvm.tir.op.q_multiply_shift:1
msgid ""
"Execute a multiplication between two Q-numbers x and y followed by a "
"right shift s. The mathematical expression is:"
msgstr ""
"执行两个 Q-numbers x 和 y 之间的乘法运算，然后进行右移 s 位。数学表达式为："

#: of tvm.tir.op.q_multiply_shift:4
msgid "out = round(x*y*2^-s)"
msgstr ":math:`out = round(x*y*2^{-s})`"

#: of tvm.tir.op.q_multiply_shift:6
msgid ""
"More about Q-numbers here: "
"https://en.wikipedia.org/wiki/Q_(number_format) The rounding rule is to "
"the nearest value, rounding half up (i.e., round(x.1) = x and round (x.5)"
" = x+1)"
msgstr ""
"关于 Q-numbers 的更多信息可以在这里找到：https://en.wikipedia.org/wiki/Q_(number_format)。"
"舍入规则是向最接近的值舍入，半途而上的舍入（也称为银行家舍入法，即 ``round(x.1) = x`` 和 ``round (x.5) = x+1``"

#: of tvm.tir.op.q_multiply_shift:13
msgid "First Q-number"
msgstr "第一个 Q-number"

#: of tvm.tir.op.q_multiply_shift:15
msgid "Second Q-number"
msgstr "第二个 Q-number"

#: of tvm.tir.op.q_multiply_shift:16 tvm.tir.op.q_multiply_shift_per_axis:13
msgid "q"
msgstr ""

#: of tvm.tir.op.q_multiply_shift:17
msgid "Number of fractional bits in x and y. Needs to be > 0"
msgstr "x 和 y 的分数位数。需要大于 0。"

#: of tvm.tir.op.q_multiply_shift:19
msgid "s"
msgstr ""

#: of tvm.tir.op.q_multiply_shift:19
msgid "Integer shift"
msgstr "整数移位，也称为循环移位或循环左移/右移，是一种将二进制数中的位向左或向右移动一定数量的运算。"
"在整数移位中，最左边的位被丢弃，最右边的位移到最左边，而其他位则按照一定的规则进行移动。"
"例如，对于 8 位二进制数，向左移位 1 位等于将其乘以 2，向右移位 1 位等于将其除以2。"

#: of tvm.tir.op.q_multiply_shift_per_axis:6
msgid "First Q-number."
msgstr ""

#: of tvm.tir.op.q_multiply_shift_per_axis:8
msgid "Second Q-number."
msgstr ""

#: of tvm.tir.op.q_multiply_shift_per_axis:9
msgid "ls"
msgstr ""

#: of tvm.tir.op.q_multiply_shift_per_axis:10
msgid "Integer left shift."
msgstr ""

#: of tvm.tir.op.q_multiply_shift_per_axis:11
msgid "rs"
msgstr ""

#: of tvm.tir.op.q_multiply_shift_per_axis:12
msgid "Integer right shift."
msgstr ""

#: of tvm.tir.op.q_multiply_shift_per_axis:14
msgid "Number of fractional bits in x and y. Needs to be > 0."
msgstr ""

#: of tvm.tir.op.q_multiply_shift_per_axis:15
msgid "is_lshift_required"
msgstr ""

#: of tvm.tir.op.q_multiply_shift_per_axis:16
msgid "Whether we need to do left shift or not."
msgstr "是否需要进行左移运算。"

#: of tvm.tir.op.q_multiply_shift_per_axis:18
msgid "is_rshift_required"
msgstr ""

#: of tvm.tir.op.q_multiply_shift_per_axis:18
msgid "Whether we need to do right shift or not."
msgstr "是否需要进行右移运算。"

#: of tvm.tir.op.reinterpret:9
msgid "The input value."
msgstr "输入的值"

#: of tvm.tir.op.reinterpret:17
msgid "The reinterpret cast value of dtype."
msgstr "reinterpret 转换数据类型为 dtype 的值。"

#: of tvm.tir.op.ret:6
msgid "val"
msgstr ""

#: of tvm.tir.op.ret:6
msgid ""
"The returned tir expression, whose data type is int, float or void "
"pointer."
msgstr ""
"返回的 tir 表达式的数据类型为 int、float 或 void 指针。"

#: of tvm.tir.op.ret:10 tvm.tir.op.tvm_throw_last_error:5
msgid "ret"
msgstr ""

#: of tvm.tir.op.ret:11 tvm.tir.op.tvm_throw_last_error:6
msgid "The return expression"
msgstr ""

#: of tvm.tir.op.start_profile_intrinsic:1
msgid "Start profile intrinsic. Parameters ---------- id : int"
msgstr ""

#: of tvm.tir.stmt.stmt_list:5
msgid "stmt : A block statement"
msgstr ""

#: of tvm.tir.stmt.stmt_list:9
msgid "stmt_list"
msgstr ""

#: of tvm.tir.stmt.stmt_list:-1
msgid "list of Stmt"
msgstr ""

#: of tvm.tir.stmt.stmt_list:10
msgid "The unpacked list of statements"
msgstr ""

#: of tvm.tir.stmt.stmt_seq:-1
msgid "list of Expr or Var"
msgstr ""

#: of tvm.tir.stmt.stmt_seq:6
msgid "List of statements to be combined as sequence."
msgstr ""

#: of tvm.tir.stmt.stmt_seq:11
msgid "The combined statement."
msgstr ""

#: of tvm.tir.generic.subtract:15
msgid "The result Expr of subtract operaton."
msgstr ""

#: of tvm.tir.op.trace:3
msgid ""
"The trace function allows to trace specific tensor at the runtime. The "
"tracing value should come as last argument. The trace action should be "
"specified, by default tvm.default_trace_action is used."
msgstr ""

#: of tvm.tir.op.trace:-1
msgid "list of Expr or Buffers."
msgstr ""

#: of tvm.tir.op.trace:14
msgid "trace_action"
msgstr ""

#: of tvm.tir.op.trace:-1
msgid "str."
msgstr ""

#: of tvm.tir.op.trace:14
msgid "The name of the trace action."
msgstr ""

#: of tvm.tir.op.trace:23
msgid "tvm.tir.call_packed : Creates packed function."
msgstr ""

#: of tvm.tir.op.trunc:3
msgid ""
"The truncated value of the scalar x is the nearest integer i which is "
"closer to zero than x is."
msgstr ""

#: of tvm.tir.op.truncdiv:21 tvm.tir.op.truncmod:21
msgid "This is the default integer division behavior in C."
msgstr ""

#: of tvm.tir.op.tvm_access_ptr:6
msgid "ptype"
msgstr ""

#: of tvm.tir.op.tvm_access_ptr:6
msgid "The data type of pointer."
msgstr ""

#: of tvm.tir.op.tvm_access_ptr:-1
msgid "DType*"
msgstr ""

#: of tvm.tir.op.tvm_access_ptr:9
msgid "The data of pointer."
msgstr ""

#: of tvm.tir.op.tvm_access_ptr:12
msgid "The offset of pointer."
msgstr ""

#: of tvm.tir.op.tvm_access_ptr:18
msgid "rw_mask"
msgstr ""

#: of tvm.tir.op.tvm_access_ptr:18
msgid "The read write mask."
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:6 tvm.tir.op.tvm_mma_sync:6
msgid "fragment_d"
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:6
msgid "The bwmma fragment_d."
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:9 tvm.tir.op.tvm_mma_sync:9
msgid "index_d"
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:9 tvm.tir.op.tvm_mma_sync:9
msgid "The fragment_d index."
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:12 tvm.tir.op.tvm_mma_sync:12
msgid "fragment_a"
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:12
msgid "The bwmma fragment_a."
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:15 tvm.tir.op.tvm_mma_sync:15
msgid "index_a"
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:15 tvm.tir.op.tvm_mma_sync:15
msgid "The fragment_a index."
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:18 tvm.tir.op.tvm_mma_sync:18
msgid "fragment_b"
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:18
msgid "The bwmma fragment_b."
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:21 tvm.tir.op.tvm_mma_sync:21
msgid "index_b"
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:21 tvm.tir.op.tvm_mma_sync:21
msgid "The fragment_b index."
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:24 tvm.tir.op.tvm_mma_sync:24
msgid "fragment_c"
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:24
msgid "The bwmma fragment_c."
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:27 tvm.tir.op.tvm_mma_sync:27
msgid "index_c"
msgstr ""

#: of tvm.tir.op.tvm_bmma_sync:27 tvm.tir.op.tvm_mma_sync:27
msgid "The fragment_c index."
msgstr ""

#: of tvm.tir.op.tvm_check_return:1
msgid "Return new on stack dtype[num] Parameters ---------- expected : int"
msgstr ""

#: of tvm.tir.op.tvm_check_return:5
msgid "The expected return code."
msgstr ""

#: of tvm.tir.op.tvm_check_return:6
msgid "return_unexpected"
msgstr ""

#: of tvm.tir.op.tvm_check_return:7
msgid "The unexpected return code."
msgstr ""

#: of tvm.tir.op.tvm_check_return:8
msgid "nested_call"
msgstr ""

#: of tvm.tir.op.tvm_check_return:9
msgid "The call expression to check return."
msgstr ""

#: of tvm.tir.op.tvm_fill_fragment:6 tvm.tir.op.tvm_load_matrix_sync:6
#: tvm.tir.op.tvm_store_matrix_sync:6
msgid "fragment"
msgstr ""

#: of tvm.tir.op.tvm_fill_fragment:6
msgid "The wmma fragment"
msgstr ""

#: of tvm.tir.op.tvm_fill_fragment:-1 tvm.tir.op.tvm_load_matrix_sync:-1
#: tvm.tir.op.tvm_store_matrix_sync:-1
msgid "UIntImm"
msgstr ""

#: of tvm.tir.op.tvm_fill_fragment:9 tvm.tir.op.tvm_fill_fragment:12
#: tvm.tir.op.tvm_fill_fragment:15 tvm.tir.op.tvm_load_matrix_sync:9
#: tvm.tir.op.tvm_load_matrix_sync:12 tvm.tir.op.tvm_load_matrix_sync:15
#: tvm.tir.op.tvm_store_matrix_sync:9 tvm.tir.op.tvm_store_matrix_sync:12
#: tvm.tir.op.tvm_store_matrix_sync:15
msgid "The shape of wmma fragment."
msgstr ""

#: of tvm.tir.op.tvm_fill_fragment:15 tvm.tir.op.tvm_load_matrix_sync:15
#: tvm.tir.op.tvm_store_matrix_sync:15
msgid "k"
msgstr ""

#: of tvm.tir.op.tvm_fill_fragment:18 tvm.tir.op.tvm_load_matrix_sync:18
#: tvm.tir.op.tvm_store_matrix_sync:18
msgid "The fragment index."
msgstr ""

#: of tvm.tir.op.tvm_fill_fragment:21
msgid "The value to be filled in fragment."
msgstr ""

#: of tvm.tir.op.tvm_load_matrix_sync:6 tvm.tir.op.tvm_store_matrix_sync:6
msgid "The wmma fragment."
msgstr ""

#: of tvm.tir.op.tvm_load_matrix_sync:21 tvm.tir.op.tvm_store_matrix_sync:21
msgid "buffer_ptr"
msgstr ""

#: of tvm.tir.op.tvm_load_matrix_sync:21 tvm.tir.op.tvm_store_matrix_sync:21
msgid "The fragment buffer pointer."
msgstr ""

#: of tvm.tir.op.tvm_load_matrix_sync:24 tvm.tir.op.tvm_store_matrix_sync:24
msgid "The fragment stride."
msgstr ""

#: of tvm.tir.op.tvm_load_matrix_sync:-1 tvm.tir.op.tvm_store_matrix_sync:-1
msgid "Literal[\"row_major\", \"column_major\"]"
msgstr ""

#: of tvm.tir.op.tvm_load_matrix_sync:27 tvm.tir.op.tvm_store_matrix_sync:27
msgid "The fragment layout."
msgstr ""

#: of tvm.tir.op.tvm_mma_sync:6
msgid "The wmma fragment_d."
msgstr ""

#: of tvm.tir.op.tvm_mma_sync:12
msgid "The wmma fragment_a."
msgstr ""

#: of tvm.tir.op.tvm_mma_sync:18
msgid "The wmma fragment_b."
msgstr ""

#: of tvm.tir.op.tvm_mma_sync:24
msgid "The wmma fragment_c."
msgstr ""

#: of tvm.tir.op.tvm_stack_alloca:6
msgid "dtype_str"
msgstr ""

#: of tvm.tir.op.tvm_stack_alloca:6 tvm.tir.op.tvm_stack_make_array:18
msgid "The data type of array."
msgstr ""

#: of tvm.tir.op.tvm_stack_alloca:9
msgid "The size of array."
msgstr ""

#: of tvm.tir.op.tvm_stack_make_array:6
msgid "The data of array."
msgstr ""

#: of tvm.tir.op.tvm_stack_make_array:9
msgid "The shape of array."
msgstr ""

#: of tvm.tir.op.tvm_stack_make_array:12
msgid "strides"
msgstr ""

#: of tvm.tir.op.tvm_stack_make_array:12
msgid "The strides of array."
msgstr ""

#: of tvm.tir.op.tvm_stack_make_array:15
msgid "ndim"
msgstr ""

#: of tvm.tir.op.tvm_stack_make_array:15
msgid "The dimensions of array."
msgstr ""

#: of tvm.tir.op.tvm_stack_make_array:18
msgid "arr_dtype"
msgstr ""

#: of tvm.tir.op.tvm_stack_make_array:21
msgid "elem_offse"
msgstr ""

#: of tvm.tir.op.tvm_stack_make_array:21
msgid "The element offset of array."
msgstr ""

#: of tvm.tir.op.tvm_stack_make_shape:6
msgid "The tuple shape."
msgstr ""

#: of tvm.tir.op.tvm_struct_get:6
msgid "The date type of the result."
msgstr ""

#: of tvm.tir.op.tvm_struct_get:9 tvm.tir.op.tvm_struct_set:6
msgid "arr"
msgstr ""

#: of tvm.tir.op.tvm_struct_get:-1 tvm.tir.op.tvm_struct_set:-1
msgid "StructType*"
msgstr ""

#: of tvm.tir.op.tvm_struct_get:9 tvm.tir.op.tvm_struct_set:6
msgid "The array of struct."
msgstr ""

#: of tvm.tir.op.tvm_struct_get:12 tvm.tir.op.tvm_struct_set:9
msgid "The index of struct."
msgstr ""

#: of tvm.tir.op.tvm_struct_get:15 tvm.tir.op.tvm_struct_set:12
msgid "field"
msgstr ""

#: of tvm.tir.op.tvm_struct_get:15 tvm.tir.op.tvm_struct_set:12
msgid "The field of struct."
msgstr ""

#: of tvm.tir.op.tvm_struct_set:15
msgid "The value to be set in field."
msgstr ""

#: of tvm.tir.op.tvm_thread_allreduce:6
msgid "freduce_args"
msgstr ""

#: of tvm.tir.op.tvm_thread_allreduce:6
msgid "The args."
msgstr ""

#: of tvm.tir.op.tvm_tuple:6
msgid "The value in tuple."
msgstr ""

#: of tvm.tir.op.vectorcombine:6
msgid "vec1"
msgstr ""

#: of tvm.tir.op.vectorcombine:6 tvm.tir.op.vectorcombine:9
#: tvm.tir.op.vectorhigh:9 tvm.tir.op.vectorlow:9
msgid "The input vector."
msgstr ""

#: of tvm.tir.op.vectorcombine:9
msgid "vec2"
msgstr ""

#: of tvm.tir.op.vectorhigh:9 tvm.tir.op.vectorlow:9
msgid "vec"
msgstr ""

#: ../../notebook/docs/reference/api/python/tir.rst:30
msgid "tvm.tir.transform"
msgstr ""

#: of tvm.tir.transform:1
msgid "Namespace of all TIR transformations"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`AnnotateDeviceRegions "
"<tvm.tir.transform.AnnotateDeviceRegions>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.AnnotateDeviceRegions:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Annotate locations that should be run on the device"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`AnnotateEntryFunc <tvm.tir.transform.AnnotateEntryFunc>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.AnnotateEntryFunc:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Set a PrimFunc as the entry point if it is only function in IRModule."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`Apply <tvm.tir.transform.Apply>`\\ \\(ftransform\\)"
msgstr ""

#: of tvm.tir.transform.transform.Apply:1 tvm.tir.transform:1:<autosummary>:1
msgid "Apply ftransform to each function in the Module."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`ApplyLayoutTransforms "
"<tvm.tir.transform.ApplyLayoutTransforms>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.ApplyLayoutTransforms:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Reshape buffers that appear in the \"layout_transform_map\" fucntion "
"attribute."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`BF16ComputeLegalize <tvm.tir.transform.BF16ComputeLegalize>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.BF16ComputeLegalize:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Legalize bf16 compute Ops."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`BF16StorageLegalize <tvm.tir.transform.BF16StorageLegalize>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.BF16StorageLegalize:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Legalize bf16 storage types to u16."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`BindTarget <tvm.tir.transform.BindTarget>`\\ \\(target\\)"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
"Annotate a PrimFunc with a given target. Parameters ------- target : "
"tvm.target.Target     target."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`CoProcSync <tvm.tir.transform.CoProcSync>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.CoProcSync:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Detect and insert sync points to co-processor."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`CombineContextCall <tvm.tir.transform.CombineContextCall>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.CombineContextCall:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Combine context calls in the host function."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`CommonSubexprElimTIR <tvm.tir.transform.CommonSubexprElimTIR>`\\"
" \\(\\[enable\\_cse\\_tir\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.transform.transform.CommonSubexprElimTIR:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Replace redundant computations by new variables."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`CompactBufferAllocation "
"<tvm.tir.transform.CompactBufferAllocation>`\\ \\(\\[is\\_strict\\]\\)"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid "Compact the buffer access region."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`ConvertBlocksToOpaque "
"<tvm.tir.transform.ConvertBlocksToOpaque>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.ConvertBlocksToOpaque:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Substitute all the block vars with the PrimExprs they are bound to, "
"indicated by the corresponding iter_values in BlockRealize, and then "
"convert the blocks into opaque ones by removing all the iter_values in "
"BlockRealize and iter_vars in Block."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`ConvertForLoopsToSerial "
"<tvm.tir.transform.ConvertForLoopsToSerial>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.ConvertForLoopsToSerial:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Convert Parallel For Loops to Serial For Loops."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`ConvertSSA <tvm.tir.transform.ConvertSSA>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.ConvertSSA:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Convert an IRModule to be SSA form."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`DecorateDeviceScope <tvm.tir.transform.DecorateDeviceScope>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.DecorateDeviceScope:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Decorate all the function's body as device function."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`DefaultGPUSchedule <tvm.tir.transform.DefaultGPUSchedule>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
"The pass sets default thread bindings for PrimFuncs, including symbolic "
"shape functions, allowing their build and execution on GPU devices."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`ExtractPrimFuncConstants "
"<tvm.tir.transform.ExtractPrimFuncConstants>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.ExtractPrimFuncConstants:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Collects and unificates tir non-scalar constants to module's attr "
"'Constants' array."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`FP8ComputeLegalize <tvm.tir.transform.FP8ComputeLegalize>`\\ "
"\\(\\[promote\\_dtype\\_str\\]\\)"
msgstr ""

#: of tvm.tir.transform.transform.FP8ComputeLegalize:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Legalize fp8 compute Ops."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`FP8StorageLegalize <tvm.tir.transform.FP8StorageLegalize>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.FP8StorageLegalize:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Legalize fp8 storage types to u8."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`Filter <tvm.tir.transform.Filter>`\\ \\(fcond\\)"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid "Filter out PrimFuncs that does not satisfy the given condition."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`FlattenBuffer <tvm.tir.transform.FlattenBuffer>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.FlattenBuffer:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Flatten the multi-dimensional BufferLoad and BufferStore to single "
"dimensional BufferLoad/BufferStore for the TIR not contains opaque block."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`ForceNarrowIndexToInt32 "
"<tvm.tir.transform.ForceNarrowIndexToInt32>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.ForceNarrowIndexToInt32:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Force narrow down indexing expressions and integer buffers to int32 dtype."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`HoistExpression <tvm.tir.transform.HoistExpression>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.HoistExpression:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Generalized verison of HoistIfThenElse."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`HoistIfThenElse <tvm.tir.transform.HoistIfThenElse>`\\ "
"\\(\\[variant\\]\\)"
msgstr ""

#: of tvm.tir.transform.transform.HoistIfThenElse:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Hoist loop-invariant IfThenElse nodes to outside the eligible loops."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`InferFragment <tvm.tir.transform.InferFragment>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InferFragment:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Infer the TensorCore fragment infomation using tensor intrinsics."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`InjectCopyIntrin <tvm.tir.transform.InjectCopyIntrin>`\\ "
"\\(pragma\\_key\\, fintrin\\)"
msgstr ""

#: of tvm.tir.transform.transform.InjectCopyIntrin:1
#: tvm.tir.transform.transform.InjectVirtualThread:1
#: tvm.tir.transform.transform.LoopPartition:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Inject virtual thread loops."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`InjectDoubleBuffer <tvm.tir.transform.InjectDoubleBuffer>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InjectDoubleBuffer:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Inject double buffer statements."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`InjectPTXAsyncCopy <tvm.tir.transform.InjectPTXAsyncCopy>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InjectPTXAsyncCopy:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Rewrite global to shared memory copy on CUDA with asyncronous copy."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`InjectPermutedLayout <tvm.tir.transform.InjectPermutedLayout>`\\"
" \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InjectPermutedLayout:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Inject permuted layout in mma"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`InjectPrefetch <tvm.tir.transform.InjectPrefetch>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InjectPrefetch:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Inject prefetch instructions into stmt."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`InjectRollingBuffer <tvm.tir.transform.InjectRollingBuffer>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InjectRollingBuffer:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Inject rolling buffer statements."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`InjectSoftwarePipeline "
"<tvm.tir.transform.InjectSoftwarePipeline>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InjectSoftwarePipeline:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Transform annotated loops into pipelined one that parallelize producers "
"and consumers"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`InjectVirtualThread <tvm.tir.transform.InjectVirtualThread>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`InstallDebugSpans <tvm.tir.transform.InstallDebugSpans>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InstallDebugSpans:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Add line information from the TIR printer as spans on each statement and "
"expression."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`InstrumentBoundCheckers "
"<tvm.tir.transform.InstrumentBoundCheckers>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InstrumentBoundCheckers:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Instruments bound checkers."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`InstrumentProfileIntrinsics "
"<tvm.tir.transform.InstrumentProfileIntrinsics>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.InstrumentProfileIntrinsics:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Insert intrinsic calls to instrument function and loop level profiling."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`LegalizePackedCalls <tvm.tir.transform.LegalizePackedCalls>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LegalizePackedCalls:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Legalize packed calls to have its arguments wrapped in TVMValues"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`LiftAttrScope <tvm.tir.transform.LiftAttrScope>`\\ "
"\\(attr\\_key\\)"
msgstr ""

#: of tvm.tir.transform.transform.LiftAttrScope:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lift common attrs with attr_key to outer scope."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`LiftThreadBinding <tvm.tir.transform.LiftThreadBinding>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LiftThreadBinding:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lift the same thread bindings to their LCA loops."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`LoopPartition <tvm.tir.transform.LoopPartition>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`LowerAutoCopy <tvm.tir.transform.LowerAutoCopy>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerAutoCopy:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Automatically do memory optimizations for auto copy blocks"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`LowerCrossThreadReduction "
"<tvm.tir.transform.LowerCrossThreadReduction>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerCrossThreadReduction:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Lower cross-thread reduction from thread bindings to intrinsic function "
"calls."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`LowerCustomDatatypes <tvm.tir.transform.LowerCustomDatatypes>`\\"
" \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerCustomDatatypes:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lower custom datatypes."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`LowerDeviceKernelLaunch "
"<tvm.tir.transform.LowerDeviceKernelLaunch>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerDeviceKernelLaunch:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lower cross-device function calls."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`LowerDeviceStorageAccessInfo "
"<tvm.tir.transform.LowerDeviceStorageAccessInfo>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerDeviceStorageAccessInfo:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lower attached storage access information on device."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`LowerInitBlock <tvm.tir.transform.LowerInitBlock>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerInitBlock:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lower block init stmt into IfThenElse statements."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`LowerIntrin <tvm.tir.transform.LowerIntrin>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerIntrin:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lower target specific intrinsic calls."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`LowerMatchBuffer <tvm.tir.transform.LowerMatchBuffer>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid "Remove match buffers inside the block."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`LowerOpaqueBlock <tvm.tir.transform.LowerOpaqueBlock>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerOpaqueBlock:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Remove the block to ensure that the TIR can not be scheduled again."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`LowerTVMBuiltin <tvm.tir.transform.LowerTVMBuiltin>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerTVMBuiltin:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lower tvm builtin intrinsics."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`LowerThreadAllreduce <tvm.tir.transform.LowerThreadAllreduce>`\\"
" \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerThreadAllreduce:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lower cross thread alleduce."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`LowerWarpMemory <tvm.tir.transform.LowerWarpMemory>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.LowerWarpMemory:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lower warp memory access to low-level device related function calls."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`MakePackedAPI <tvm.tir.transform.MakePackedAPI>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.MakePackedAPI:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Transform the PrimFuncs in the module to a packed func API."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`MakeUnpackedAPI <tvm.tir.transform.MakeUnpackedAPI>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.MakeUnpackedAPI:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Transform the PrimFuncs in the module to a C API compatible with internal"
" calls."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`ManifestSharedMemoryLocalStage "
"<tvm.tir.transform.ManifestSharedMemoryLocalStage>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.ManifestSharedMemoryLocalStage:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Add the explicit local stage for the shared memory access on GPU."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`MergeDynamicSharedMemoryAllocations "
"<tvm.tir.transform.MergeDynamicSharedMemoryAllocations>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.MergeDynamicSharedMemoryAllocations:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"This pass merges multiple TIR-level dynamic shared memory allocations "
"into one allocation."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`NarrowDataType <tvm.tir.transform.NarrowDataType>`\\ "
"\\(target\\_bits\\)"
msgstr ""

#: of tvm.tir.transform.transform.NarrowDataType:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Narrow down PrimExpr datatype in stmt to target_bits."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`PlanAndUpdateBufferAllocationLocation "
"<tvm.tir.transform.PlanAndUpdateBufferAllocationLocation>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
"Locate the buffer allocation to the exact position (usually is the lca of"
" buffer access)."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`PointerValueTypeRewrite "
"<tvm.tir.transform.PointerValueTypeRewrite>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.PointerValueTypeRewrite:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Rewrite the pointer content type of arguments, as well as Alloc internal "
"to the function to use the most frequently accessed type for load/store "
"to avoid pointer casting in backend when possible."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`ReduceBranchingThroughOvercompute "
"<tvm.tir.transform.ReduceBranchingThroughOvercompute>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.ReduceBranchingThroughOvercompute:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Reduce branching by introducing overcompute"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`RemoveAssume <tvm.tir.transform.RemoveAssume>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.RemoveAssume:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Remove all instances of builtin::assume"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`RemoveNoOp <tvm.tir.transform.RemoveNoOp>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.RemoveNoOp:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Remove No Op from the Stmt."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`RemoveStoreUndef <tvm.tir.transform.RemoveStoreUndef>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.RemoveStoreUndef:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Remove stores of undefined values from the Stmt."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`RemoveWeightLayoutRewriteBlock "
"<tvm.tir.transform.RemoveWeightLayoutRewriteBlock>`\\ \\(\\[...\\]\\)"
msgstr ""

#: of tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Remove weight layout rewrite block before benchmarking during tuning "
"stage."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`RenormalizeSplitPattern "
"<tvm.tir.transform.RenormalizeSplitPattern>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.RenormalizeSplitPattern:1
#: tvm.tir.transform:1:<autosummary>:1
msgid ""
"Renormalize the split pattern from floordiv(floormod()) to "
"floormod(floordiv())"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`RewriteUnsafeSelect <tvm.tir.transform.RewriteUnsafeSelect>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.RewriteUnsafeSelect:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Detect and rewrite unsafe select that contains memory access."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`Simplify <tvm.tir.transform.Simplify>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.Simplify:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Run arithmetic simplifications on the statements and expressions."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`SkipAssert <tvm.tir.transform.SkipAssert>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.SkipAssert:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Skip assert stmt."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`SplitHostDevice <tvm.tir.transform.SplitHostDevice>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.SplitHostDevice:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Split the function into a host function and device functions."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`StorageFlatten <tvm.tir.transform.StorageFlatten>`\\ "
"\\(cache\\_line\\_size\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.transform.transform.StorageFlatten:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Flatten the multi-dimensional read/write to 1D."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`StorageRewrite <tvm.tir.transform.StorageRewrite>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.StorageRewrite:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Rewrite storage allocation pattern."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`TextureFlatten <tvm.tir.transform.TextureFlatten>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.TextureFlatten:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Flatten the multi-dimensional read/write to 2D."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`ThreadSync <tvm.tir.transform.ThreadSync>`\\ "
"\\(storage\\_scope\\)"
msgstr ""

#: of tvm.tir.transform.transform.ThreadSync:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Insert sync between parallel read/write of shared buffers."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`TransformMmaBufferLayout "
"<tvm.tir.transform.TransformMmaBufferLayout>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.TransformMmaBufferLayout:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Transform mma buffer layout"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`UnifyThreadBinding <tvm.tir.transform.UnifyThreadBinding>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
"Unify all the thread bindings for \"blockIdx.x/y/z\", "
"\"threadIdx.x/y/z\", and \"vthread.x/y/z\"."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`UnrollLoop <tvm.tir.transform.UnrollLoop>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.UnrollLoop:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Unroll the constant loop marked by unroll."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`VectorizeLoop <tvm.tir.transform.VectorizeLoop>`\\ "
"\\(\\[enable\\_vectorize\\]\\)"
msgstr ""

#: of tvm.tir.transform.transform.VectorizeLoop:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Lower vectorization loops."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ":py:obj:`VerifyMemory <tvm.tir.transform.VerifyMemory>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.verify_memory:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
#: tvm.tir.transform.transform.VerifyMemory:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Verify if func contains illegal host side direct memory access."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`VerifyVTCMLimit <tvm.tir.transform.VerifyVTCMLimit>`\\ "
"\\(limit\\)"
msgstr ""

#: of tvm.tir.transform.transform.VerifyVTCMLimit:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Verify if the size of the allocated vtcm memory satisfies the limit."
msgstr ""

#: of tvm.tir.transform:1:<autosummary>:1
msgid ""
":py:obj:`prim_func_pass <tvm.tir.transform.prim_func_pass>`\\ "
"\\(\\[pass\\_func\\, opt\\_level\\, name\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:1
#: tvm.tir.transform:1:<autosummary>:1
msgid "Decorate a function pass."
msgstr ""

#: of tvm.tir.transform.transform.AnnotateDeviceRegions:1:<autosummary>:1
msgid ""
":py:obj:`HoistedConditionals <tvm.tir.transform.HoistedConditionals>`\\ "
"\\(value\\)"
msgstr ""

#: of tvm.tir.transform.transform.AnnotateDeviceRegions:1:<autosummary>:1
#: tvm.tir.transform.transform.HoistedConditionals:1
msgid "Flags for use in HoistExpressionConfig.conditional_types"
msgstr ""

#: of tvm.tir.transform.transform.AnnotateDeviceRegions:1:<autosummary>:1
msgid ""
":py:obj:`HoistedLetBindings <tvm.tir.transform.HoistedLetBindings>`\\ "
"\\(value\\)"
msgstr ""

#: of tvm.tir.transform.transform.AnnotateDeviceRegions:1:<autosummary>:1
#: tvm.tir.transform.transform.HoistedLetBindings:1
msgid "Flags for use in HoistExpressionConfig.let_binding_types"
msgstr ""

#: of tvm.tir.transform.transform.AnnotateDeviceRegions:1:<autosummary>:1
msgid ":py:obj:`PrimFuncPass <tvm.tir.transform.PrimFuncPass>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.transform.transform.AnnotateDeviceRegions:1:<autosummary>:1
msgid "A pass that works on each :py:func:`tvm.tir.PrimFunc` in a module."
msgstr ""

#: of tvm.tir.transform.transform.AnnotateDeviceRegions:3
msgid ""
"Insert `AttrStmt` nodes specifying a target on which regions within the "
"PrimFunc should be executed.  Only modifies functions that have a "
"`tvm::attr::kTarget` attribute, and where that target defines a host."
msgstr ""

#: of tvm.tir.analysis.analysis.OOBChecker:5
#: tvm.tir.transform.transform.AnnotateDeviceRegions:10
#: tvm.tir.transform.transform.AnnotateEntryFunc:5
#: tvm.tir.transform.transform.Apply:12
#: tvm.tir.transform.transform.ApplyLayoutTransforms:6
#: tvm.tir.transform.transform.BF16ComputeLegalize:5
#: tvm.tir.transform.transform.BF16StorageLegalize:5
#: tvm.tir.transform.transform.BindTarget:9
#: tvm.tir.transform.transform.CoProcSync:5
#: tvm.tir.transform.transform.CombineContextCall:5
#: tvm.tir.transform.transform.CommonSubexprElimTIR:5
#: tvm.tir.transform.transform.CompactBufferAllocation:44
#: tvm.tir.transform.transform.ConvertBlocksToOpaque:7
#: tvm.tir.transform.transform.ConvertForLoopsToSerial:5
#: tvm.tir.transform.transform.ConvertSSA:11
#: tvm.tir.transform.transform.DecorateDeviceScope:5
#: tvm.tir.transform.transform.ExtractPrimFuncConstants:5
#: tvm.tir.transform.transform.FP8ComputeLegalize:10
#: tvm.tir.transform.transform.FP8StorageLegalize:5
#: tvm.tir.transform.transform.Filter:6
#: tvm.tir.transform.transform.FlattenBuffer:6
#: tvm.tir.transform.transform.ForceNarrowIndexToInt32:6
#: tvm.tir.transform.transform.HoistExpression:12
#: tvm.tir.transform.transform.HoistIfThenElse:20
#: tvm.tir.transform.transform.InferFragment:5
#: tvm.tir.transform.transform.InjectCopyIntrin:13
#: tvm.tir.transform.transform.InjectDoubleBuffer:5
#: tvm.tir.transform.transform.InjectPTXAsyncCopy:5
#: tvm.tir.transform.transform.InjectPermutedLayout:5
#: tvm.tir.transform.transform.InjectPrefetch:5
#: tvm.tir.transform.transform.InjectRollingBuffer:5
#: tvm.tir.transform.transform.InjectSoftwarePipeline:5
#: tvm.tir.transform.transform.InjectVirtualThread:5
#: tvm.tir.transform.transform.InstallDebugSpans:6
#: tvm.tir.transform.transform.InstrumentBoundCheckers:5
#: tvm.tir.transform.transform.InstrumentProfileIntrinsics:5
#: tvm.tir.transform.transform.LegalizePackedCalls:5
#: tvm.tir.transform.transform.LiftAttrScope:10
#: tvm.tir.transform.transform.LiftThreadBinding:5
#: tvm.tir.transform.transform.LoopPartition:5
#: tvm.tir.transform.transform.LowerAutoCopy:5
#: tvm.tir.transform.transform.LowerCrossThreadReduction:6
#: tvm.tir.transform.transform.LowerCustomDatatypes:7
#: tvm.tir.transform.transform.LowerDeviceKernelLaunch:16
#: tvm.tir.transform.transform.LowerDeviceStorageAccessInfo:6
#: tvm.tir.transform.transform.LowerInitBlock:5
#: tvm.tir.transform.transform.LowerIntrin:5
#: tvm.tir.transform.transform.LowerMatchBuffer:5
#: tvm.tir.transform.transform.LowerOpaqueBlock:5
#: tvm.tir.transform.transform.LowerTVMBuiltin:5
#: tvm.tir.transform.transform.LowerThreadAllreduce:5
#: tvm.tir.transform.transform.LowerWarpMemory:5
#: tvm.tir.transform.transform.MakePackedAPI:25
#: tvm.tir.transform.transform.MakeUnpackedAPI:15
#: tvm.tir.transform.transform.ManifestSharedMemoryLocalStage:5
#: tvm.tir.transform.transform.MergeDynamicSharedMemoryAllocations:6
#: tvm.tir.transform.transform.NarrowDataType:11
#: tvm.tir.transform.transform.PlanAndUpdateBufferAllocationLocation:7
#: tvm.tir.transform.transform.PointerValueTypeRewrite:7
#: tvm.tir.transform.transform.ReduceBranchingThroughOvercompute:5
#: tvm.tir.transform.transform.RemoveAssume:5
#: tvm.tir.transform.transform.RemoveNoOp:5
#: tvm.tir.transform.transform.RemoveStoreUndef:5
#: tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:16
#: tvm.tir.transform.transform.RenormalizeSplitPattern:5
#: tvm.tir.transform.transform.RewriteUnsafeSelect:5
#: tvm.tir.transform.transform.Simplify:5
#: tvm.tir.transform.transform.SkipAssert:5
#: tvm.tir.transform.transform.SplitHostDevice:5
#: tvm.tir.transform.transform.StorageFlatten:15
#: tvm.tir.transform.transform.StorageRewrite:9
#: tvm.tir.transform.transform.TextureFlatten:9
#: tvm.tir.transform.transform.ThreadSync:10
#: tvm.tir.transform.transform.TransformMmaBufferLayout:5
#: tvm.tir.transform.transform.UnifyThreadBinding:11
#: tvm.tir.transform.transform.UnrollLoop:7
#: tvm.tir.transform.transform.VectorizeLoop:11
#: tvm.tir.transform.transform.VerifyMemory:5
#: tvm.tir.transform.transform.VerifyVTCMLimit:5
msgid "fpass"
msgstr ""

#: of tvm.tir.analysis.analysis.OOBChecker:-1
#: tvm.tir.transform.transform.AnnotateDeviceRegions:-1
#: tvm.tir.transform.transform.AnnotateEntryFunc:-1
#: tvm.tir.transform.transform.Apply:-1
#: tvm.tir.transform.transform.ApplyLayoutTransforms:-1
#: tvm.tir.transform.transform.BF16ComputeLegalize:-1
#: tvm.tir.transform.transform.BF16StorageLegalize:-1
#: tvm.tir.transform.transform.BindTarget:-1
#: tvm.tir.transform.transform.CoProcSync:-1
#: tvm.tir.transform.transform.CombineContextCall:-1
#: tvm.tir.transform.transform.CommonSubexprElimTIR:-1
#: tvm.tir.transform.transform.CompactBufferAllocation:-1
#: tvm.tir.transform.transform.ConvertBlocksToOpaque:-1
#: tvm.tir.transform.transform.ConvertForLoopsToSerial:-1
#: tvm.tir.transform.transform.ConvertSSA:-1
#: tvm.tir.transform.transform.DecorateDeviceScope:-1
#: tvm.tir.transform.transform.ExtractPrimFuncConstants:-1
#: tvm.tir.transform.transform.FP8ComputeLegalize:-1
#: tvm.tir.transform.transform.FP8StorageLegalize:-1
#: tvm.tir.transform.transform.Filter:-1
#: tvm.tir.transform.transform.FlattenBuffer:-1
#: tvm.tir.transform.transform.ForceNarrowIndexToInt32:-1
#: tvm.tir.transform.transform.HoistExpression:-1
#: tvm.tir.transform.transform.HoistIfThenElse:-1
#: tvm.tir.transform.transform.InferFragment:-1
#: tvm.tir.transform.transform.InjectCopyIntrin:-1
#: tvm.tir.transform.transform.InjectDoubleBuffer:-1
#: tvm.tir.transform.transform.InjectPTXAsyncCopy:-1
#: tvm.tir.transform.transform.InjectPermutedLayout:-1
#: tvm.tir.transform.transform.InjectPrefetch:-1
#: tvm.tir.transform.transform.InjectRollingBuffer:-1
#: tvm.tir.transform.transform.InjectSoftwarePipeline:-1
#: tvm.tir.transform.transform.InjectVirtualThread:-1
#: tvm.tir.transform.transform.InstallDebugSpans:-1
#: tvm.tir.transform.transform.InstrumentBoundCheckers:-1
#: tvm.tir.transform.transform.InstrumentProfileIntrinsics:-1
#: tvm.tir.transform.transform.LegalizePackedCalls:-1
#: tvm.tir.transform.transform.LiftAttrScope:-1
#: tvm.tir.transform.transform.LiftThreadBinding:-1
#: tvm.tir.transform.transform.LoopPartition:-1
#: tvm.tir.transform.transform.LowerAutoCopy:-1
#: tvm.tir.transform.transform.LowerCrossThreadReduction:-1
#: tvm.tir.transform.transform.LowerCustomDatatypes:-1
#: tvm.tir.transform.transform.LowerDeviceKernelLaunch:-1
#: tvm.tir.transform.transform.LowerDeviceStorageAccessInfo:-1
#: tvm.tir.transform.transform.LowerInitBlock:-1
#: tvm.tir.transform.transform.LowerIntrin:-1
#: tvm.tir.transform.transform.LowerMatchBuffer:-1
#: tvm.tir.transform.transform.LowerOpaqueBlock:-1
#: tvm.tir.transform.transform.LowerTVMBuiltin:-1
#: tvm.tir.transform.transform.LowerThreadAllreduce:-1
#: tvm.tir.transform.transform.LowerWarpMemory:-1
#: tvm.tir.transform.transform.MakePackedAPI:-1
#: tvm.tir.transform.transform.MakeUnpackedAPI:-1
#: tvm.tir.transform.transform.ManifestSharedMemoryLocalStage:-1
#: tvm.tir.transform.transform.MergeDynamicSharedMemoryAllocations:-1
#: tvm.tir.transform.transform.NarrowDataType:-1
#: tvm.tir.transform.transform.PlanAndUpdateBufferAllocationLocation:-1
#: tvm.tir.transform.transform.PointerValueTypeRewrite:-1
#: tvm.tir.transform.transform.ReduceBranchingThroughOvercompute:-1
#: tvm.tir.transform.transform.RemoveAssume:-1
#: tvm.tir.transform.transform.RemoveNoOp:-1
#: tvm.tir.transform.transform.RemoveStoreUndef:-1
#: tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:-1
#: tvm.tir.transform.transform.RenormalizeSplitPattern:-1
#: tvm.tir.transform.transform.RewriteUnsafeSelect:-1
#: tvm.tir.transform.transform.Simplify:-1
#: tvm.tir.transform.transform.SkipAssert:-1
#: tvm.tir.transform.transform.SplitHostDevice:-1
#: tvm.tir.transform.transform.StorageFlatten:-1
#: tvm.tir.transform.transform.StorageRewrite:-1
#: tvm.tir.transform.transform.TextureFlatten:-1
#: tvm.tir.transform.transform.ThreadSync:-1
#: tvm.tir.transform.transform.TransformMmaBufferLayout:-1
#: tvm.tir.transform.transform.UnifyThreadBinding:-1
#: tvm.tir.transform.transform.UnrollLoop:-1
#: tvm.tir.transform.transform.VectorizeLoop:-1
#: tvm.tir.transform.transform.VerifyMemory:-1
#: tvm.tir.transform.transform.VerifyVTCMLimit:-1
msgid "tvm.transform.Pass"
msgstr ""

#: of tvm.tir.analysis.analysis.OOBChecker:6
#: tvm.tir.transform.transform.AnnotateDeviceRegions:11
#: tvm.tir.transform.transform.AnnotateEntryFunc:6
#: tvm.tir.transform.transform.Apply:13
#: tvm.tir.transform.transform.ApplyLayoutTransforms:7
#: tvm.tir.transform.transform.BF16ComputeLegalize:6
#: tvm.tir.transform.transform.BF16StorageLegalize:6
#: tvm.tir.transform.transform.BindTarget:10
#: tvm.tir.transform.transform.CoProcSync:6
#: tvm.tir.transform.transform.CombineContextCall:6
#: tvm.tir.transform.transform.CommonSubexprElimTIR:6
#: tvm.tir.transform.transform.CompactBufferAllocation:45
#: tvm.tir.transform.transform.ConvertBlocksToOpaque:8
#: tvm.tir.transform.transform.ConvertForLoopsToSerial:6
#: tvm.tir.transform.transform.ConvertSSA:12
#: tvm.tir.transform.transform.DecorateDeviceScope:6
#: tvm.tir.transform.transform.ExtractPrimFuncConstants:6
#: tvm.tir.transform.transform.FP8ComputeLegalize:11
#: tvm.tir.transform.transform.FP8StorageLegalize:6
#: tvm.tir.transform.transform.Filter:7
#: tvm.tir.transform.transform.FlattenBuffer:7
#: tvm.tir.transform.transform.ForceNarrowIndexToInt32:6
#: tvm.tir.transform.transform.HoistExpression:13
#: tvm.tir.transform.transform.HoistIfThenElse:21
#: tvm.tir.transform.transform.InferFragment:6
#: tvm.tir.transform.transform.InjectCopyIntrin:14
#: tvm.tir.transform.transform.InjectDoubleBuffer:6
#: tvm.tir.transform.transform.InjectPTXAsyncCopy:6
#: tvm.tir.transform.transform.InjectPermutedLayout:6
#: tvm.tir.transform.transform.InjectPrefetch:6
#: tvm.tir.transform.transform.InjectRollingBuffer:6
#: tvm.tir.transform.transform.InjectSoftwarePipeline:6
#: tvm.tir.transform.transform.InjectVirtualThread:6
#: tvm.tir.transform.transform.InstallDebugSpans:7
#: tvm.tir.transform.transform.InstrumentBoundCheckers:6
#: tvm.tir.transform.transform.InstrumentProfileIntrinsics:6
#: tvm.tir.transform.transform.LegalizePackedCalls:6
#: tvm.tir.transform.transform.LiftAttrScope:11
#: tvm.tir.transform.transform.LiftThreadBinding:6
#: tvm.tir.transform.transform.LoopPartition:6
#: tvm.tir.transform.transform.LowerAutoCopy:6
#: tvm.tir.transform.transform.LowerCrossThreadReduction:7
#: tvm.tir.transform.transform.LowerCustomDatatypes:8
#: tvm.tir.transform.transform.LowerDeviceKernelLaunch:17
#: tvm.tir.transform.transform.LowerDeviceStorageAccessInfo:6
#: tvm.tir.transform.transform.LowerInitBlock:6
#: tvm.tir.transform.transform.LowerIntrin:6
#: tvm.tir.transform.transform.LowerMatchBuffer:6
#: tvm.tir.transform.transform.LowerOpaqueBlock:6
#: tvm.tir.transform.transform.LowerTVMBuiltin:6
#: tvm.tir.transform.transform.LowerThreadAllreduce:6
#: tvm.tir.transform.transform.LowerWarpMemory:6
#: tvm.tir.transform.transform.MakePackedAPI:26
#: tvm.tir.transform.transform.MakeUnpackedAPI:16
#: tvm.tir.transform.transform.ManifestSharedMemoryLocalStage:6
#: tvm.tir.transform.transform.MergeDynamicSharedMemoryAllocations:7
#: tvm.tir.transform.transform.NarrowDataType:11
#: tvm.tir.transform.transform.PlanAndUpdateBufferAllocationLocation:8
#: tvm.tir.transform.transform.PointerValueTypeRewrite:8
#: tvm.tir.transform.transform.ReduceBranchingThroughOvercompute:6
#: tvm.tir.transform.transform.RemoveAssume:6
#: tvm.tir.transform.transform.RemoveNoOp:6
#: tvm.tir.transform.transform.RemoveStoreUndef:6
#: tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:17
#: tvm.tir.transform.transform.RenormalizeSplitPattern:6
#: tvm.tir.transform.transform.RewriteUnsafeSelect:6
#: tvm.tir.transform.transform.Simplify:6
#: tvm.tir.transform.transform.SkipAssert:6
#: tvm.tir.transform.transform.SplitHostDevice:6
#: tvm.tir.transform.transform.StorageFlatten:16
#: tvm.tir.transform.transform.StorageRewrite:10
#: tvm.tir.transform.transform.TextureFlatten:10
#: tvm.tir.transform.transform.ThreadSync:11
#: tvm.tir.transform.transform.TransformMmaBufferLayout:6
#: tvm.tir.transform.transform.UnifyThreadBinding:11
#: tvm.tir.transform.transform.UnrollLoop:8
#: tvm.tir.transform.transform.VectorizeLoop:12
#: tvm.tir.transform.transform.VerifyMemory:6
#: tvm.tir.transform.transform.VerifyVTCMLimit:6
msgid "The result pass"
msgstr ""

#: of tvm.tir.transform.transform.Apply:3
msgid "This function is a thin wrapper around tvm.tir.transform.prim_func_pass"
msgstr ""

#: of tvm.tir.transform.transform.Apply:8
msgid "ftransform: tvm.tir.PrimFunc -> tvm.tir.PrimFunc"
msgstr ""

#: of tvm.tir.transform.transform.Apply:8
msgid "The transformation pass."
msgstr ""

#: of tvm.tir.transform.transform.BindTarget:1
msgid ""
"Annotate a PrimFunc with a given target. Parameters ------- target : "
"tvm.target.Target"
msgstr ""

#: of tvm.tir.transform.transform.CompactBufferAllocation:1
msgid ""
"Compact the buffer access region. by removing the buffer regions that are"
" not accessed, i.e. narrowing the buffer shape and adjust the access "
"region if necessary."
msgstr ""

#: of tvm.tir.transform.transform.CompactBufferAllocation:8
msgid ""
"Before narrowing, ``B`` is a ``[16, 16]`` buffer, but only a skinny "
"vector ``B[i, 0:16]`` is accessed."
msgstr ""

#: of tvm.tir.transform.transform.CompactBufferAllocation:21
msgid ""
"This pass narrows the buffer shape and adjust its accessed region "
"accordingly.  In this particular case, because only a ``1 * 16`` vector "
"of ``B`` is accessed, the pass narrows ``B`` to shape ``[1, 16]``, and "
"changes the access to ``B[i, j]`` to ``B[0, j]``."
msgstr ""

#: of tvm.tir.transform.transform.CompactBufferAllocation:40
msgid "is_strict"
msgstr ""

#: of tvm.tir.transform.transform.CompactBufferAllocation:39
msgid ""
"Ensure the compacted shape to be always smaller than the original shape. "
"Otherwise it allows to grow the shape to match actual accessed buffer "
"regions."
msgstr ""

#: of tvm.tir.transform.transform.ConvertSSA:3
msgid ""
"This pass handles cases where the same `tir.Var` appears in multiple "
"functions within the same module.  For example, after extracting a "
"fragment from one function into another, where the same `tir.Var` may be "
"defined both as within the body of the original function, and as a "
"parameter within the hoisted function."
msgstr ""

#: of tvm.tir.transform.transform.DefaultGPUSchedule:1
msgid ""
"The pass sets default thread bindings for PrimFuncs, including symbolic "
"shape functions, allowing their build and execution on GPU devices. It "
"examines all the blocks within the PrimFunc and conducts loop fusion, "
"splitting, and reordering operation based on the loop extent and target "
"information, such as the maximum thread block number and maximum thread "
"per block."
msgstr ""

#: of tvm.tir.transform.transform.DefaultGPUSchedule:7
msgid ""
"The primary objective of this pass is not to optimize performance, but "
"rather to generate a valid GPU kernel for unscheduled or symbolic shape "
"PrimFuncs. The pass is currently only working for CUDA targets."
msgstr ""

#: of tvm.tir.transform.transform.DefaultGPUSchedule:13
msgid "ret: tvm.transform.Pass"
msgstr ""

#: of tvm.tir.transform.transform.FP8ComputeLegalize:6
msgid "promote_dtype"
msgstr ""

#: of tvm.tir.transform.transform.FP8ComputeLegalize:6
msgid "The data type we promote fp8 to, options: float16/float32."
msgstr ""

#: of tvm.tir.transform.transform.Filter:1
msgid ""
"Filter out PrimFuncs that does not satisfy the given condition. `fcond` "
"should be a function that takes a primfunc and returns boolean."
msgstr ""

#: of tvm.tir.transform.transform.ForceNarrowIndexToInt32:10
msgid "This pass should not be used in default cases."
msgstr ""

#: of tvm.tir.transform.transform.HoistExpression:3
msgid ""
"Hoist loop-invariant expressions to outside the eligible loops. Searches "
"for expressions in:"
msgstr ""

#: of tvm.tir.transform.transform.HoistExpression:6
msgid "LetStmt bindings"
msgstr ""

#: of tvm.tir.transform.transform.HoistExpression:7
msgid "IfThenElse conditions"
msgstr ""

#: of tvm.tir.transform.transform.HoistExpression:8
msgid "Boolean operators"
msgstr ""

#: of tvm.tir.transform.transform.HoistIfThenElse:16
msgid "variant"
msgstr ""

#: of tvm.tir.transform.transform.HoistIfThenElse:-1
msgid "Optional[String]"
msgstr ""

#: of tvm.tir.transform.transform.HoistIfThenElse:6
msgid ""
"The variant of the pass. variant can have any one of following values "
"[\"basic\", None(Default)]."
msgstr ""

#: of tvm.tir.transform.transform.HoistIfThenElse:9
msgid ""
"The basic variant supports basic hoisting scenarios where it expects the "
"For & If Nodes are in place consecutively and does not involve global "
"scope variables or more advanced scenarios."
msgstr ""

#: of tvm.tir.transform.transform.HoistIfThenElse:13
msgid ""
"Default variant supports all hoisting scenarios,i.e., {\"Basic\" + "
"\"Advanced\"} supported with control with PassContext configs like below:"
msgstr ""

#: of tvm.tir.transform.transform.HoistIfThenElse:16
msgid "config={\"tir.HoistIfThenElse\": {\"support_block_scope_hosting\": True}}"
msgstr ""

#: of tvm.tir.transform.transform.HoistedConditionals:3
msgid ""
"Each bitflag represents a type of expression that should be hoisted to "
"the outermost loop possible."
msgstr ""

#: of tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
msgid ":py:obj:`All <tvm.tir.transform.HoistedConditionals.All>`\\"
msgstr ""

#: ../../docstring of tvm.tir.transform.HoistedConditionals.All:1
#: tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
msgid "Enable all hoisting of conditionals"
msgstr ""

#: of tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
msgid ""
":py:obj:`BooleanExpression "
"<tvm.tir.transform.HoistedConditionals.BooleanExpression>`\\"
msgstr ""

#: ../../docstring of
#: tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
#: tvm.tir.transform.HoistedConditionals.BooleanExpression:1
msgid "If set, look for hoist candidates in all boolean expressions"
msgstr ""

#: of tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
msgid ":py:obj:`IfElseExpr <tvm.tir.transform.HoistedConditionals.IfElseExpr>`\\"
msgstr ""

#: ../../docstring of
#: tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
#: tvm.tir.transform.HoistedConditionals.IfElseExpr:1
msgid "If set, look for hoist candidates in tir.if_then_else"
msgstr ""

#: of tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
msgid ":py:obj:`IfElseStmt <tvm.tir.transform.HoistedConditionals.IfElseStmt>`\\"
msgstr ""

#: ../../docstring of
#: tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
#: tvm.tir.transform.HoistedConditionals.IfElseStmt:1
msgid "If set, look for hoist candidates in IfElseStmt"
msgstr ""

#: of tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
msgid ":py:obj:`Never <tvm.tir.transform.HoistedConditionals.Never>`\\"
msgstr ""

#: ../../docstring of
#: tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
#: tvm.tir.transform.HoistedConditionals.Never:1
msgid "No hoisting of conditionals"
msgstr ""

#: of tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
msgid ""
":py:obj:`UsingBlockVar "
"<tvm.tir.transform.HoistedConditionals.UsingBlockVar>`\\"
msgstr ""

#: of tvm.tir.transform.HoistedConditionals.All:1:<autosummary>:1
msgid "If set, allow hoisting of conditionals that use a block variable (e.g."
msgstr ""

#: ../../docstring of tvm.tir.transform.HoistedConditionals.UsingBlockVar:1
msgid ""
"If set, allow hoisting of conditionals that use a block variable (e.g. "
"threadIdx.x)"
msgstr ""

#: of tvm.tir.transform.transform.HoistedLetBindings:3
msgid ""
"Each bitflag represents a type of let binding expression that should be "
"hoisted to the outermost loop possible."
msgstr ""

#: of tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
msgid ":py:obj:`All <tvm.tir.transform.HoistedLetBindings.All>`\\"
msgstr ""

#: ../../docstring of tvm.tir.transform.HoistedLetBindings.All:1
#: tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
msgid "Enable all hoisting of let bindings"
msgstr ""

#: of tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
msgid ":py:obj:`LetExpr <tvm.tir.transform.HoistedLetBindings.LetExpr>`\\"
msgstr ""

#: ../../docstring of
#: tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
#: tvm.tir.transform.HoistedLetBindings.LetExpr:1
msgid "Bindings occuring in Let expressions"
msgstr ""

#: of tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
msgid ":py:obj:`LetStmt <tvm.tir.transform.HoistedLetBindings.LetStmt>`\\"
msgstr ""

#: ../../docstring of
#: tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
#: tvm.tir.transform.HoistedLetBindings.LetStmt:1
msgid "Bindings occuring in LetStmt"
msgstr ""

#: of tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
msgid ":py:obj:`Never <tvm.tir.transform.HoistedLetBindings.Never>`\\"
msgstr ""

#: ../../docstring of
#: tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
#: tvm.tir.transform.HoistedLetBindings.Never:1
msgid "No hoisting of let bindings"
msgstr ""

#: of tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
msgid ""
":py:obj:`RequiredByConditional "
"<tvm.tir.transform.HoistedLetBindings.RequiredByConditional>`\\"
msgstr ""

#: ../../docstring of
#: tvm.tir.transform.HoistedLetBindings.All:1:<autosummary>:1
#: tvm.tir.transform.HoistedLetBindings.RequiredByConditional:1
msgid "Bindings that are used by a hoisted conditional"
msgstr ""

#: of tvm.tir.transform.transform.InjectCopyIntrin:6
msgid "pragma_key"
msgstr ""

#: of tvm.tir.transform.transform.InjectCopyIntrin:6
msgid "The pragma key for hint of copy."
msgstr ""

#: of tvm.tir.transform.transform.InjectCopyIntrin:9
msgid "fintrin"
msgstr ""

#: of tvm.tir.transform.transform.InjectCopyIntrin:9
msgid ""
"The function with signature copyintrin(src, dst, pad_before, pad_after, "
"pad_value)"
msgstr ""

#: of tvm.tir.transform.transform.LiftAttrScope:6
msgid "The attribute key to be checked."
msgstr ""

#: of tvm.tir.transform.transform.LowerCustomDatatypes:3
msgid ""
"See tvm::datatypes::Registry for more information on adding custom "
"datatypes."
msgstr ""

#: of tvm.tir.transform.transform.LowerDeviceKernelLaunch:3
msgid ""
"Prior to this pass, host to device calls are represented as subroutine "
"calls, with environment parameters (e.g. env_thread) specified "
"internally.  The device function is an internal function, without a "
"`tvm::attr::kGlobalSymbol` attribute."
msgstr ""

#: of tvm.tir.transform.transform.LowerDeviceKernelLaunch:8
msgid ""
"After this pass, host to device calls are represented as tvm_call_packed "
"built-in.  The device function is an externally-exposed function, with a "
"non-empty `tvm::attr::kGlobalSymbol` attribute."
msgstr ""

#: of tvm.tir.transform.transform.LowerDeviceStorageAccessInfo:10
msgid "Run this pass after all storage access analysis finish."
msgstr ""

#: of tvm.tir.transform.transform.LowerMatchBuffer:1
msgid "Remove match buffers inside the block. Also, it will validate the binding."
msgstr ""

#: of tvm.tir.transform.transform.MakePackedAPI:3
msgid ""
"Prior to this pass, the PrimFunc may have Buffer arguments defined in the"
" `PrimFuncNode::buffer_map`.  This pass consumes the `buffer_map`, using "
"it to generate `TVMArgs` and `TVMRetValue*` arguments that implement the "
"`PackedFunc` API."
msgstr ""

#: of tvm.tir.transform.transform.MakePackedAPI:8
msgid ""
"For static shapes, the `BufferNode::shape`, `BufferNode::strides`, and "
"`BufferNode::elem_offset` member variables are used to generate runtime "
"checks on the corresponding member variables in the user-provided "
"`DLTensor*` or `tvm.nd.array` argument.  (e.g. A PrimFunc that accepts a "
"buffer of shape `[16,32]` validates that the `DLTensor::shape` array is "
"`[16,32]`.)"
msgstr ""

#: of tvm.tir.transform.transform.MakePackedAPI:15
msgid ""
"For dynamic Buffers, in which one or more of these `BufferNode` member "
"variables use `tir.Var` that are not defined by other PrimFunc "
"parameters, these are instead used to define the variables based on the "
"corresponding `DLTensor` members.  (e.g. A PrimFunc that accepts a buffer"
" of shape `[tir.Var(\"n\"), tir.Var(\"m\")]`, when passed a `DLTensor` of"
" shape `[16,32]`, will define `n = 16` and `n=32`, based on the "
"argument's shape."
msgstr ""

#: of tvm.tir.transform.transform.MakeUnpackedAPI:3
msgid ""
"Prior to this pass, the PrimFunc may have Buffer arguments defined in the"
" `PrimFuncNode::buffer_map`.  This pass consumes the `buffer_map`, using "
"it to generate `T*` arguments (e.g. `float32*`) that can be directly "
"called by a C API."
msgstr ""

#: of tvm.tir.transform.transform.MakeUnpackedAPI:8
msgid ""
"For static shapes, no runtime validation is performed to confirm that the"
" argument buffer's shape matches the expected shape.  For dynamic shapes,"
" `MakeUnpackedAPI` requires that the dynamic parameters be passed as "
"separate `tir.Var` parameters."
msgstr ""

#: of tvm.tir.transform.transform.NarrowDataType:6
msgid "target_bits"
msgstr ""

#: of tvm.tir.transform.transform.NarrowDataType:6
msgid "The target bit configuration."
msgstr ""

#: of tvm.tir.transform.transform.NarrowDataType:15
msgid "Run this pass after StorageFlatten."
msgstr ""

#: of tvm.tir.transform.transform.PlanAndUpdateBufferAllocationLocation:1
msgid ""
"Locate the buffer allocation to the exact position (usually is the lca of"
" buffer access). This pass will inject opaque block with alloc_buffers at"
" the allocation site."
msgstr ""

#: of tvm.tir.transform.function_pass.PrimFuncPass:1
msgid ""
"A pass that works on each :py:func:`tvm.tir.PrimFunc` in a module. A "
"function pass class should be created through "
"py:func:`tvm.tir.transform.function_pass`."
msgstr ""

#: of tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:12
msgid "skip_ndarray_rewrite"
msgstr ""

#: of tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:6
msgid ""
"If True, exact rewrite of NDArray, according to the given index map, will"
" be skipped. Only the shape of the NDArray is transformed correctly, and "
"the content of the destination array will be filled with random values."
msgstr ""

#: of tvm.tir.transform.transform.RemoveWeightLayoutRewriteBlock:10
msgid ""
"When this pass is called many times during MetaSchedule tuning, the raw "
"data of NDArray, before and after rewrite, does not matter. Since NDArray"
" layout rewrite, using IndexMap's MapNDArray, is currently slow, skipping"
" the exact rewrite is sometimes necessary."
msgstr ""

#: of tvm.tir.transform.transform.StorageFlatten:7
msgid "cache_line_size: int"
msgstr ""

#: of tvm.tir.transform.transform.StorageFlatten:7
msgid "The size of CPU cache line."
msgstr ""

#: of tvm.tir.transform.transform.StorageFlatten:11
msgid "create_bound_attribute:"
msgstr ""

#: of tvm.tir.transform.transform.StorageFlatten:10
msgid "Whether to create bound attributes."
msgstr ""

#: of tvm.tir.transform.transform.StorageRewrite:3
msgid ""
"Moves the allocation to outer most possible scope. Trying to share space "
"between allocations to make a static allocation plan when possible."
msgstr ""

#: of tvm.tir.transform.transform.UnifyThreadBinding:1
msgid ""
"Unify all the thread bindings for \"blockIdx.x/y/z\", "
"\"threadIdx.x/y/z\", and \"vthread.x/y/z\". Before the unification, two "
"vars that are bound to a thread axis (e.g., \"threadIdx.x\") use "
"different IterVars and variables in their AttrStmts. After the "
"unification, we use a consolidated IterVar and a variable for them."
msgstr ""

#: of tvm.tir.transform.transform.UnifyThreadBinding:15
msgid ""
"`vthread` is a legacy behavior that will be deprecated, though thread "
"bindings of `vthread` are still also unified in this pass. Please use "
"`vthread.x`, `vthread.y` and `vthread.z` instead."
msgstr ""

#: of tvm.tir.transform.transform.UnrollLoop:3
msgid ""
"This pass also automatically attach pragma unroll tag to loops which "
"meets the standard."
msgstr ""

#: of tvm.tir.transform.transform.VectorizeLoop:7
msgid "enable_vectorize"
msgstr ""

#: of tvm.tir.transform.transform.VectorizeLoop:6
msgid ""
"Whether vectorization is enabled. Will lower to scalar loop when it is "
"turned off."
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:3
msgid ""
"This function returns a callback when pass_func is provided. Otherwise, "
"it returns the created function pass using the given optimization "
"function."
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:10
msgid "pass_func"
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:-1
msgid ""
"Optional[Callable[(tvm.tir.PrimFunc, IRModule, PassContext) -> "
"tvm.tir.PrimFunc]]"
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:10
msgid "The transformation function or class."
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:13
msgid "opt_level"
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:13
msgid "The optimization level of this module pass."
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:-1
msgid "Optional[str]"
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:16
msgid ""
"The name of the function pass. The name could be empty. In this case, the"
" name of the optimization function will be used as the pass name."
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:20
msgid "required"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:-1
#: tvm.tir.transform.function_pass.prim_func_pass:-1
msgid "Optional[List[str]]"
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:20
msgid "The list of passes that the function pass is dependent on."
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:24
msgid "create_function_pass : Union[Callable, FunctionPass]"
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:26
msgid ""
"A decorator will be returned if pass_func is not provided, otherwise "
"return the decorated result. The returned decorator has two behaviors "
"depending on the input: A new FunctionPass will be returned when we "
"decorate a pass function. A new FunctionPass class will be returned when "
"we decorate a class type."
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:34
msgid "The following code block decorates a function pass class."
msgstr ""

#: of tvm.tir.transform.function_pass.prim_func_pass:48
msgid ""
"The following code creates a function pass by decorating a user defined "
"transform function."
msgstr ""

#: ../../notebook/docs/reference/api/python/tir.rst:38
msgid "tvm.tir.analysis"
msgstr ""

#: of tvm.tir.analysis:1
msgid "Namespace of all TIR analysis utils."
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1
msgid ""
":py:obj:`Block <tvm.tir.analysis.Block>`\\ \\(iter\\_vars\\, reads\\, "
"writes\\, name\\_hint\\, body\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1
msgid ":py:obj:`Buffer <tvm.tir.analysis.Buffer>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1
msgid ""
":py:obj:`BufferRegion <tvm.tir.analysis.BufferRegion>`\\ \\(buffer\\, "
"region\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1
msgid ""
":py:obj:`IRModule <tvm.tir.analysis.IRModule>`\\ \\(\\[functions\\, "
"type\\_definitions\\, ...\\]\\)"
msgstr ""

#: of tvm.ir.module.IRModule:1 tvm.tir.analysis:1:<autosummary>:1
msgid "IRModule that holds functions and type definitions."
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1
msgid ":py:obj:`Object <tvm.tir.analysis.Object>`\\ \\(\\)"
msgstr ""

#: of tvm.runtime.object.Object:1 tvm.tir.analysis:1:<autosummary>:1
msgid "Base class for all tvm's runtime objects."
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1
msgid ":py:obj:`PrimExpr <tvm.tir.analysis.PrimExpr>`\\ \\(\\)"
msgstr ""

#: of tvm.ir.expr.PrimExpr:1 tvm.tir.analysis:1:<autosummary>:1
msgid "Base class of all primitive expressions."
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1
msgid ""
":py:obj:`PrimFunc <tvm.tir.analysis.PrimFunc>`\\ \\(params\\, body\\[\\, "
"ret\\_type\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1
msgid ":py:obj:`Stmt <tvm.tir.analysis.Stmt>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.analysis:1:<autosummary>:1
msgid ":py:obj:`Var <tvm.tir.analysis.Var>`\\ \\(name\\, dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ":py:obj:`OOBChecker <tvm.tir.analysis.OOBChecker>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.OOBChecker:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Detect out of bounds memory access in arrays."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`apply_prim_func_arg_and_result_memory_constraints "
"<tvm.tir.analysis.apply_prim_func_arg_and_result_memory_constraints>`\\ "
"\\(...\\)"
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
"Returns func written to capture the memory (aka storage) scope "
"constraints for each of the func's parameters given by "
"arg_and_result_memory_scopes."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`calculate_allocated_bytes "
"<tvm.tir.analysis.calculate_allocated_bytes>`\\ \\(func\\_or\\_mod\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_allocated_bytes:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Calculate allocated memory per memory scope required by TIR PrimFuncs."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`calculate_constant_bytes "
"<tvm.tir.analysis.calculate_constant_bytes>`\\ \\(func\\, ...\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_constant_bytes:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
"Calculate the constant size in bytes needed by the TIR allocates inside "
"the TIR PrimFunc."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`calculate_workspace_bytes "
"<tvm.tir.analysis.calculate_workspace_bytes>`\\ \\(func\\, ...\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_workspace_bytes:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
"Calculate the workspace size in bytes needed by the TIR allocates inside "
"the TIR PrimFunc."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`detect_buffer_access_lca "
"<tvm.tir.analysis.detect_buffer_access_lca>`\\ \\(func\\)"
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
"Detect the lowest common ancestor(LCA) of buffer access, including both "
"high-level access (BufferLoad, BufferStore) and low-level access "
"(BufferLoad, BufferStore and opaque access)."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`estimate_tir_flops <tvm.tir.analysis.estimate_tir_flops>`\\ "
"\\(stmt\\_or\\_mod\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.estimate_tir_flops:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Estimate the FLOPs of a TIR fragment."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`expr_deep_equal <tvm.tir.analysis.expr_deep_equal>`\\ \\(lhs\\, "
"rhs\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Deeply compare two nested expressions."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`find_anchor_block <tvm.tir.analysis.find_anchor_block>`\\ "
"\\(mod\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.find_anchor_block:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Find the \"anchor block\" of the given module."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`get_block_access_region "
"<tvm.tir.analysis.get_block_access_region>`\\ \\(block\\, "
"buffer\\_var\\_map\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:2
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Detect which regions of tensors in this block are read or written to."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`get_block_read_write_region "
"<tvm.tir.analysis.get_block_read_write_region>`\\ \\(block\\, ...\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_read_write_region:2
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Auto detect the block read/write region according to its body stmt."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`get_prim_func_arg_and_result_memory_constraints "
"<tvm.tir.analysis.get_prim_func_arg_and_result_memory_constraints>`\\ "
"\\(...\\)"
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
"Returns the memory (aka storage) scope constraints for all the arguments "
"and result of func."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`get_vtcm_compaction_passes "
"<tvm.tir.analysis.get_vtcm_compaction_passes>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.get_vtcm_compaction_passes:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
"Utility function to get the list of lowering passes to be applied to "
"calculate the compacted VTCM allocation size"
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`undefined_vars <tvm.tir.analysis.undefined_vars>`\\ "
"\\(node\\[\\, defs\\]\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.undefined_vars:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Find undefined vars in a TIR statement or expression."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`verify_gpu_code <tvm.tir.analysis.verify_gpu_code>`\\ \\(func\\,"
" constraints\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.verify_gpu_code:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Verify if module contains illegal host side direct memory access."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ":py:obj:`verify_memory <tvm.tir.analysis.verify_memory>`\\ \\(func\\)"
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ":py:obj:`verify_ssa <tvm.tir.analysis.verify_ssa>`\\ \\(func\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.verify_ssa:1
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Verify if the func is in SSA form."
msgstr ""

#: of tvm.tir.stmt.Block:1:<autosummary>:1
msgid ""
":py:obj:`verify_well_formed <tvm.tir.analysis.verify_well_formed>`\\ "
"\\(obj\\[\\, assert\\_mode\\]\\)"
msgstr ""

#: of tvm.tir.analysis.analysis.verify_well_formed:2
#: tvm.tir.stmt.Block:1:<autosummary>:1
msgid "Verify if the given TIR is well-formed. The verification includes:"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ""
":py:obj:`access_ptr <tvm.tir.analysis.Buffer.access_ptr>`\\ "
"\\(access\\_mask\\[\\, ptr\\_type\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ""
":py:obj:`get_flattened_buffer "
"<tvm.tir.analysis.Buffer.get_flattened_buffer>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ":py:obj:`offset_of <tvm.tir.analysis.Buffer.offset_of>`\\ \\(indices\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ":py:obj:`scope <tvm.tir.analysis.Buffer.scope>`\\ \\(\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ""
":py:obj:`vload <tvm.tir.analysis.Buffer.vload>`\\ \\(begin\\[\\, "
"dtype\\]\\)"
msgstr ""

#: of tvm.tir.buffer.Buffer.access_ptr:1:<autosummary>:1
msgid ":py:obj:`vstore <tvm.tir.analysis.Buffer.vstore>`\\ \\(begin\\, value\\)"
msgstr ""

#: of tvm.ir.module.IRModule:3
msgid "IRModule is the basic unit for all IR transformations across the stack."
msgstr ""

#: of tvm.ir.module.IRModule:8
msgid "functions: Optional[dict]."
msgstr ""

#: of tvm.ir.module.IRModule:8
msgid "Map of global var to BaseFunc"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ":py:obj:`__getitem__ <tvm.tir.analysis.IRModule.__getitem__>`\\ \\(var\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1
#: tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid "Lookup a global definition by name or by variable."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`__setitem__ <tvm.tir.analysis.IRModule.__setitem__>`\\ \\(var\\,"
" val\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.__setitem__:1
msgid "Add a mapping to the module."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`astext <tvm.tir.analysis.IRModule.astext>`\\ "
"\\(\\[show\\_meta\\_data\\, annotate\\]\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.astext:1
msgid "Get the text format of the expression."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`from_expr <tvm.tir.analysis.IRModule.from_expr>`\\ \\(expr\\[\\,"
" functions\\, type\\_defs\\]\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.from_expr:1
msgid "Construct a module from a standalone expression."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ":py:obj:`get_attr <tvm.tir.analysis.IRModule.get_attr>`\\ \\(attr\\_key\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.get_attr:1
msgid "Get the IRModule attribute."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`get_constructor <tvm.tir.analysis.IRModule.get_constructor>`\\ "
"\\(tag\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.get_constructor:1
msgid "Look up an ADT constructor by tag."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`get_global_type_var "
"<tvm.tir.analysis.IRModule.get_global_type_var>`\\ \\(name\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.get_global_type_var:1
msgid "Get a global type variable in the function by name."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`get_global_type_vars "
"<tvm.tir.analysis.IRModule.get_global_type_vars>`\\ \\(\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.get_global_type_vars:1
msgid "Collect all global type vars defined in this module."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`get_global_var <tvm.tir.analysis.IRModule.get_global_var>`\\ "
"\\(name\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.get_global_var:1
msgid "Get a global variable in the function by name."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`get_global_vars <tvm.tir.analysis.IRModule.get_global_vars>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.get_global_vars:1
msgid "Collect all global vars defined in this module."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ":py:obj:`update <tvm.tir.analysis.IRModule.update>`\\ \\(other\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.update:1
msgid "Insert functions in another Module to current one."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`update_func <tvm.tir.analysis.IRModule.update_func>`\\ \\(var\\,"
" func\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.update_func:1
msgid "Update the function corresponding to a global variable in the module."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`update_global_info "
"<tvm.tir.analysis.IRModule.update_global_info>`\\ \\(name\\, "
"global\\_info\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.update_global_info:1
msgid "Update global info in the module"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`with_attr <tvm.tir.analysis.IRModule.with_attr>`\\ "
"\\(attr\\_key\\, attr\\_value\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
#: tvm.ir.module.IRModule.with_attr:1
msgid "Copy the IRModule and add an attribute to it."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`with_attrs <tvm.tir.analysis.IRModule.with_attrs>`\\ "
"\\(attr\\_map\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
"Copy the IRModule and add the given attribute map to it. Parameters "
"---------- attr_map: Union[DictAttrs, Dict[str, Object]]     The "
"attribute map Returns ------- mod : IRModule     A new copy of the "
"IRModule with the attribute."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
":py:obj:`without_attr <tvm.tir.analysis.IRModule.without_attr>`\\ "
"\\(attr\\_key\\)"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:1:<autosummary>:1
msgid ""
"Copy the IRModule and remove an attribute key and its associated value. "
"Parameters ---------- attr_key : str     The attribute key. Returns "
"------- mod : IRModule     A new copy of the IRModule without the "
"attribute."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:6
msgid "var: Union[String, GlobalVar, GlobalTypeVar]"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:6
msgid "The name or global variable."
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:10
#: tvm.ir.module.IRModule.__setitem__:8
msgid "val: Union[Function, Type]"
msgstr ""

#: of tvm.ir.module.IRModule.__getitem__:11
msgid "The definition referenced by :code:`var` (either a function or type)."
msgstr ""

#: of tvm.ir.module.IRModule.__setitem__:6 tvm.ir.module.IRModule.update_func:7
msgid "var: GlobalVar"
msgstr ""

#: of tvm.ir.module.IRModule.__setitem__:6 tvm.ir.module.IRModule.update_func:7
msgid "The global variable."
msgstr ""

#: of tvm.ir.module.IRModule.__setitem__:9
msgid "The value."
msgstr ""

#: of tvm.ir.module.IRModule.astext:7
msgid "show_meta_data"
msgstr ""

#: of tvm.ir.module.IRModule.astext:6
msgid "Whether to include meta data section in the text if there is meta data."
msgstr ""

#: of tvm.ir.module.IRModule.astext:11
msgid "annotate: Optional[Object->str]"
msgstr ""

#: of tvm.ir.module.IRModule.astext:10
msgid ""
"Optionally annotate function to provide additional information in the "
"comment block."
msgstr ""

#: of tvm.ir.module.IRModule.astext:16
msgid "text"
msgstr ""

#: of tvm.ir.module.IRModule.astext:16
msgid "The text format of the expression."
msgstr ""

#: of tvm.ir.module.IRModule.astext:19
msgid "Notes"
msgstr ""

#: of tvm.ir.module.IRModule.astext:20
msgid ""
"The meta data section is necessary to fully parse the text format. "
"However, it can contain dumps that are big (e.g constant weights), so it "
"can be helpful to skip printing the meta data section."
msgstr ""

#: of tvm.ir.module.IRModule.from_expr:6
msgid "expr: RelayExpr"
msgstr ""

#: of tvm.ir.module.IRModule.from_expr:6
msgid "The starting expression"
msgstr ""

#: of tvm.ir.module.IRModule.from_expr:9
msgid "global_funcs: Optional[dict]"
msgstr ""

#: of tvm.ir.module.IRModule.from_expr:9
msgid "Map of global vars to function definitions"
msgstr ""

#: of tvm.ir.module.IRModule.from_expr:12
msgid "type_defs: Optional[dict]"
msgstr ""

#: of tvm.ir.module.IRModule.from_expr:12
msgid "Map of global type vars to type definitions"
msgstr ""

#: of tvm.ir.module.IRModule.from_expr:18
msgid "mod: Module"
msgstr ""

#: of tvm.ir.module.IRModule.from_expr:17
msgid ""
"A module containing the passed definitions, where expr is set as the "
"entry point (wrapped in a function if necessary)"
msgstr ""

#: of tvm.ir.module.IRModule.get_attr:6 tvm.ir.module.IRModule.with_attr:6
#: tvm.ir.module.IRModule.without_attr:5
msgid "The attribute key."
msgstr ""

#: of tvm.ir.module.IRModule.get_attr:10 tvm.ir.module.IRModule.with_attr:9
msgid "attr_value"
msgstr ""

#: of tvm.ir.module.IRModule.get_attr:-1
msgid "Any"
msgstr ""

#: of tvm.ir.module.IRModule.get_attr:11
msgid "Attribute value"
msgstr ""

#: of tvm.ir.module.IRModule.get_constructor:6
msgid "tag: int"
msgstr ""

#: of tvm.ir.module.IRModule.get_constructor:6
msgid "The tag for a constructor."
msgstr ""

#: of tvm.ir.module.IRModule.get_constructor:11
msgid "constructor: Constructor"
msgstr ""

#: of tvm.ir.module.IRModule.get_constructor:11
msgid "The constructor associated with the given tag,"
msgstr ""

#: of tvm.ir.module.IRModule.get_constructor:14
#: tvm.ir.module.IRModule.get_global_type_var:14
#: tvm.ir.module.IRModule.get_global_var:14
msgid "Raises"
msgstr ""

#: of tvm.ir.module.IRModule.get_constructor:15
msgid "tvm.error.TVMError if the corresponding constructor cannot be found."
msgstr ""

#: of tvm.ir.module.IRModule.get_global_type_var:6
#: tvm.ir.module.IRModule.get_global_var:6
#: tvm.ir.module.IRModule.update_global_info:6
msgid "name: str"
msgstr ""

#: of tvm.ir.module.IRModule.get_global_type_var:6
msgid "The name of the global type variable."
msgstr ""

#: of tvm.ir.module.IRModule.get_global_type_var:11
msgid "global_type_var: GlobalTypeVar"
msgstr ""

#: of tvm.ir.module.IRModule.get_global_type_var:11
#: tvm.ir.module.IRModule.get_global_var:11
msgid "The global variable mapped to :code:`name`."
msgstr ""

#: of tvm.ir.module.IRModule.get_global_type_var:15
msgid "tvm.error.TVMError if we cannot find corresponding global type var."
msgstr ""

#: of tvm.ir.module.IRModule.get_global_type_vars:5
msgid "global_type_vars: Array[GlobalTypeVar]"
msgstr ""

#: of tvm.ir.module.IRModule.get_global_type_vars:6
msgid "An array of global type vars."
msgstr ""

#: of tvm.ir.module.IRModule.get_global_var:6
msgid "The name of the global variable."
msgstr ""

#: of tvm.ir.module.IRModule.get_global_var:11
msgid "global_var: GlobalVar"
msgstr ""

#: of tvm.ir.module.IRModule.get_global_var:15
msgid "tvm.error.TVMError if we cannot find corresponding global var."
msgstr ""

#: of tvm.ir.module.IRModule.get_global_vars:5
msgid "global_vars: Array[GlobalVar]"
msgstr ""

#: of tvm.ir.module.IRModule.get_global_vars:6
msgid "An array of global vars."
msgstr ""

#: of tvm.ir.module.IRModule.update:5
msgid "other: IRModule"
msgstr ""

#: of tvm.ir.module.IRModule.update:6
msgid "The module to merge into the current Module."
msgstr ""

#: of tvm.ir.module.IRModule.update_func:9
msgid "func: tvm.relay.Function"
msgstr ""

#: of tvm.ir.module.IRModule.update_func:10
msgid "The function to be inserted."
msgstr ""

#: of tvm.ir.module.IRModule.update_global_info:6
msgid "The name for the global info."
msgstr ""

#: of tvm.ir.module.IRModule.update_global_info:8
msgid "global_info: List[GlobalInfo]"
msgstr ""

#: of tvm.ir.module.IRModule.update_global_info:9
msgid "The global info to be updated."
msgstr ""

#: of tvm.ir.module.IRModule.with_attr:-1
msgid "Object"
msgstr ""

#: of tvm.ir.module.IRModule.with_attr:9
msgid "The new attribute value."
msgstr ""

#: of tvm.ir.module.IRModule.with_attr:14 tvm.ir.module.IRModule.with_attrs:9
msgid "A new copy of the IRModule with the attribute"
msgstr ""

#: of tvm.ir.module.IRModule.with_attrs:1
msgid ""
"Copy the IRModule and add the given attribute map to it. Parameters "
"---------- attr_map: Union[DictAttrs, Dict[str, Object]]"
msgstr ""

#: of tvm.ir.module.IRModule.with_attrs:5
msgid "The attribute map"
msgstr ""

#: of tvm.ir.module.IRModule.without_attr:1
msgid ""
"Copy the IRModule and remove an attribute key and its associated value. "
"Parameters ---------- attr_key : str"
msgstr ""

#: of tvm.ir.module.IRModule.without_attr:9
msgid "A new copy of the IRModule without the attribute"
msgstr ""

#: of tvm.runtime.object.Object._move:1:<autosummary>:1
msgid ":py:obj:`_move <tvm.tir.analysis.Object._move>`\\ \\(\\)"
msgstr ""

#: of tvm.runtime.object.Object._move:1
#: tvm.runtime.object.Object._move:1:<autosummary>:1
msgid "Create an RValue reference to the object and mark the object as moved."
msgstr ""

#: of tvm.runtime.object.Object._move:3
msgid ""
"This is a advanced developer API that can be useful when passing an "
"unique reference to an Object that you no longer needed to a function."
msgstr ""

#: of tvm.runtime.object.Object._move:6
msgid ""
"A unique reference can trigger copy on write optimization that avoids "
"copy when we transform an object."
msgstr ""

#: of tvm.runtime.object.Object._move:11
msgid ""
"All the reference of the object becomes invalid after it is moved. Be "
"very careful when using this feature."
msgstr ""

#: of tvm.runtime.object.Object._move:26
msgid "rvalue : The rvalue reference."
msgstr ""

#: of tvm.ir.expr.PrimExpr:3
msgid "PrimExpr is used in the low-level code optimizations and integer analysis."
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:1:<autosummary>:1
msgid ""
":py:obj:`specialize <tvm.tir.analysis.PrimFunc.specialize>`\\ "
"\\(param\\_map\\)"
msgstr ""

#: of tvm.tir.function.PrimFunc.specialize:1:<autosummary>:1
msgid ""
":py:obj:`with_body <tvm.tir.analysis.PrimFunc.with_body>`\\ "
"\\(new\\_body\\[\\, span\\]\\)"
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:1
msgid ""
"Returns func written to capture the memory (aka storage) scope "
"constraints for each of the func's parameters given by "
"arg_and_result_memory_scopes. However, arg_and_result_memory_scopes "
"should be w.r.t. the func's representation as a Relay Function of "
"relay_func_type before lowering and conversion to DPS."
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:6
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:5
msgid "Visible for testing."
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:8
msgid ""
"CAUTION: This is experimental. The resulting PrimFunc may not have fully "
"accounted for all new memory scopes."
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:14
#: tvm.tir.analysis.analysis.calculate_constant_bytes:6
#: tvm.tir.analysis.analysis.calculate_workspace_bytes:6
#: tvm.tir.analysis.analysis.detect_buffer_access_lca:9
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:10
#: tvm.tir.analysis.analysis.verify_gpu_code:6
#: tvm.tir.analysis.analysis.verify_memory:6
#: tvm.tir.analysis.analysis.verify_ssa:6
msgid "func: tvm.tir.PrimFunc"
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:14
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:10
msgid "The function to retrieve constraints from."
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:17
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:13
msgid "relay_func_type: tvm.relay.FuncType"
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:17
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:13
msgid "The type of the Relay Function from which the func was derived."
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:21
msgid "arg_and_result_memory_scopes: Array[AnyStr]"
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:20
msgid ""
"Memory constraints for funcs args and result in Relay form. The empty "
"string denotes 'no constraint'."
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:25
msgid "result: tvm.tir.PrimFunc"
msgstr ""

#: of
#: tvm.tir.analysis.analysis.apply_prim_func_arg_and_result_memory_constraints:26
msgid "The rewritten func."
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_allocated_bytes:7
msgid "func_or_mod: Union[PrimFunc, IRModule]"
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_allocated_bytes:6
msgid ""
"The function or module to be detected. If a module is passed, allocated "
"memory is calculated for all PrimFuncs inside the module"
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_allocated_bytes:-1
msgid "Union[Dict[str, int], Dict[str, Dict[str, int]]]"
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_allocated_bytes:12
msgid ""
"Allocated memory size per scope in bytes for each function in the "
"IRModule returned as a dict with function names as keys and a dict of "
"allocated sizes as values. If a single PrimFunc is passed, the function "
"name is returned as \"main\""
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_constant_bytes:7
#: tvm.tir.analysis.analysis.calculate_workspace_bytes:7
#: tvm.tir.analysis.analysis.detect_buffer_access_lca:9
msgid "The function to be detected."
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_constant_bytes:9
msgid "constant_byte_alignment"
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_constant_bytes:9
#: tvm.tir.analysis.analysis.calculate_workspace_bytes:9
msgid "The byte alignment required for each tensor"
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_constant_bytes:14
#: tvm.tir.analysis.analysis.calculate_workspace_bytes:14
msgid "Workspace size in bytes."
msgstr ""

#: of tvm.tir.analysis.analysis.calculate_workspace_bytes:9
msgid "workspace_byte_alignment"
msgstr ""

#: of tvm.tir.analysis.analysis.detect_buffer_access_lca:1
msgid ""
"Detect the lowest common ancestor(LCA) of buffer access, including both "
"high-level access (BufferLoad, BufferStore) and low-level access "
"(BufferLoad, BufferStore and opaque access). The LCA may be a For loop or"
" a Block."
msgstr ""

#: of tvm.tir.analysis.analysis.detect_buffer_access_lca:-1
msgid "Dict[Buffer, Stmt]"
msgstr ""

#: of tvm.tir.analysis.analysis.detect_buffer_access_lca:14
msgid "Map from buffer to the LCA of all access to it."
msgstr ""

#: of tvm.tir.analysis.analysis.estimate_tir_flops:6
msgid "stmt_or_mod: Union[Stmt, IRModule]"
msgstr ""

#: of tvm.tir.analysis.analysis.estimate_tir_flops:6
msgid "The TIR fragment or IRModule to be estimated."
msgstr ""

#: of tvm.tir.analysis.analysis.estimate_tir_flops:10
msgid "flops: float"
msgstr ""

#: of tvm.tir.analysis.analysis.estimate_tir_flops:11
msgid "The estimated FLOPs."
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:14
msgid "The comparison result"
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:19
msgid ""
"This function does not remap variable bindings, it will not return true "
"for (let x = 1 in x + 1) vs (let y = 1 in y + 1), unless x.same_as(y). "
"Use py:func:`tvm.ir.structural_equal` to handle structural variable "
"remapping."
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:23
msgid ""
"Due to the restriction of not remapping variables, this function can run "
"faster than StructuralEqual and can be used as a utility function during "
"arithmetic simplifications."
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:27
msgid ""
"Always consider py:func:`tvm.ir.structural_equal` first, which handles "
"the structural remapping."
msgstr ""

#: of tvm.tir.analysis.analysis.expr_deep_equal:32
msgid "tvm.ir.structural_equal"
msgstr ""

#: of tvm.tir.analysis.analysis.find_anchor_block:3
msgid ""
"We define the anchor block to be the block with (1) an init statement and"
" (2) having the biggest flops count. The latter condition is only used "
"when there are multiple blocks with an init statement."
msgstr ""

#: of tvm.tir.analysis.analysis.find_anchor_block:7
msgid ""
"For example, if the input module is conv2d + fused spatial blocks, conv2d"
" is the anchor block. The input module may not contain more than one such"
" block. For example, a module having two conv2d is not allowed as an "
"input."
msgstr ""

#: of tvm.tir.analysis.analysis.find_anchor_block:11
msgid ""
"However, a module created from winograd convolution has multiple blocks "
"with an init statement (input transform, batched GEMM, and output "
"transform). We use the second condition, the flops count, to determine "
"that the batched GEMM block is the anchor block."
msgstr ""

#: of tvm.tir.analysis.analysis.find_anchor_block:17
msgid "mod: tvm.ir.IRModule"
msgstr ""

#: of tvm.tir.analysis.analysis.find_anchor_block:18
msgid "The input TIR module."
msgstr ""

#: of tvm.tir.analysis.analysis.find_anchor_block:21
msgid "anchor_block: Block"
msgstr ""

#: of tvm.tir.analysis.analysis.find_anchor_block:22
msgid "The anchor block if found, None otherwise."
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:2
msgid "Regions are sorted by order of appearance in the AST."
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:7
#: tvm.tir.analysis.analysis.get_block_read_write_region:7
msgid "block: tvm.tir.Block"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:7
#: tvm.tir.analysis.analysis.get_block_read_write_region:7
msgid "The block in which we are detecting read/write regions."
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:10
#: tvm.tir.analysis.analysis.get_block_read_write_region:10
msgid "buffer_var_map"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:-1
#: tvm.tir.analysis.analysis.get_block_read_write_region:-1
msgid "Dict[Var, Buffer]"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:10
#: tvm.tir.analysis.analysis.get_block_read_write_region:10
msgid ""
"The outside buffers which may access the block. Mapping from buffer var "
"to the buffer"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:-1
#: tvm.tir.analysis.analysis.get_block_read_write_region:-1
msgid "List[List[BufferRegion]]"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:17
msgid "Array of access regions. There are three arrays of BufferRegion:"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:16
msgid "first: read regions"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:17
msgid "second: write regions"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_access_region:18
msgid "third: opaque regions"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_read_write_region:2
msgid "An opaque access will be counted as both a read and a write access"
msgstr ""

#: of tvm.tir.analysis.analysis.get_block_read_write_region:15
msgid ""
"An array only consisting of the read regions and write regions of the "
"input block"
msgstr ""

#: of
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:1
msgid ""
"Returns the memory (aka storage) scope constraints for all the arguments "
"and result of func. However the result will be w.r.t. the func's "
"representation as a Relay Function of relay_func_type before lowering and"
" conversion to DPS."
msgstr ""

#: of
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:18
msgid "result: List[AnyStr]"
msgstr ""

#: of
#: tvm.tir.analysis.analysis.get_prim_func_arg_and_result_memory_constraints:18
msgid ""
"Memory scope constraints for funcs args and result in Relay form. The "
"empty string denotes 'no constraint'."
msgstr ""

#: of tvm.tir.analysis.analysis.get_vtcm_compaction_passes:-1
msgid "List[tvm.transform.Pass]"
msgstr ""

#: of tvm.tir.analysis.analysis.get_vtcm_compaction_passes:7
msgid "returns list of passes"
msgstr ""

#: of tvm.tir.analysis.analysis.undefined_vars:6
msgid "node: Union[Stmt, PrimExpr]"
msgstr ""

#: of tvm.tir.analysis.analysis.undefined_vars:6
msgid "The TIR statement or expression to be checked."
msgstr ""

#: of tvm.tir.analysis.analysis.undefined_vars:9
msgid "defs: Optional[List[Var]]"
msgstr ""

#: of tvm.tir.analysis.analysis.undefined_vars:9
msgid "The vars that is defined"
msgstr ""

#: of tvm.tir.analysis.analysis.undefined_vars:14
msgid "The undefined vars."
msgstr ""

#: of tvm.tir.analysis.analysis.verify_gpu_code:6
#: tvm.tir.analysis.analysis.verify_memory:6
#: tvm.tir.analysis.analysis.verify_ssa:6
msgid "The module to be verified."
msgstr ""

#: of tvm.tir.analysis.analysis.verify_gpu_code:9
msgid "constraints"
msgstr ""

#: of tvm.tir.analysis.analysis.verify_gpu_code:-1
msgid "Dict[str, int]"
msgstr ""

#: of tvm.tir.analysis.analysis.verify_gpu_code:9
msgid "The attribute constraints."
msgstr ""

#: of tvm.tir.analysis.analysis.verify_gpu_code:14
#: tvm.tir.analysis.analysis.verify_memory:11
#: tvm.tir.analysis.analysis.verify_ssa:11
msgid "The result of verification."
msgstr ""

#: of tvm.tir.analysis.analysis.verify_well_formed:2
msgid "Check if expressions not contain vars that is defined outside the block."
msgstr ""

#: of tvm.tir.analysis.analysis.verify_well_formed:7
msgid "obj: Union[tvm.tir.PrimFunc, tvm.ir.IRModule]"
msgstr ""

#: of tvm.tir.analysis.analysis.verify_well_formed:7
msgid "The function or module to be verified."
msgstr ""

#: of tvm.tir.analysis.analysis.verify_well_formed:10
msgid "assert_mode: bool"
msgstr ""

#: of tvm.tir.analysis.analysis.verify_well_formed:10
msgid "The indicator if it raises an error when the function is not well-formed."
msgstr ""

#: of tvm.tir.analysis.analysis.verify_well_formed:14
msgid "result: bool"
msgstr ""

#: of tvm.tir.analysis.analysis.verify_well_formed:15
msgid "Whether it is a well-formed TIR function."
msgstr ""

#: ../../notebook/docs/reference/api/python/tir.rst:47
msgid "tvm.tir.stmt_functor"
msgstr ""

#: of tvm.tir.stmt_functor:1
msgid "Statement functor utilities for IR transformations"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
msgid ""
":py:obj:`ir_transform <tvm.tir.stmt_functor.ir_transform>`\\ \\(stmt\\, "
"preorder\\, postorder\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1
#: tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
msgid "Recursively visit and transform ir nodes in post DFS order."
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
msgid ""
":py:obj:`post_order_visit <tvm.tir.stmt_functor.post_order_visit>`\\ "
"\\(stmt\\, fvisit\\)"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
#: tvm.tir.stmt_functor.post_order_visit:2
msgid "Recursively visit the ir in post DFS order node, apply fvisit"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
msgid ""
":py:obj:`pre_order_visit <tvm.tir.stmt_functor.pre_order_visit>`\\ "
"\\(stmt\\, fvisit\\)"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
#: tvm.tir.stmt_functor.pre_order_visit:2
msgid "Recursive pre-order visit on stmt AST, applying fvisit on each node."
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
msgid ":py:obj:`renew_defs <tvm.tir.stmt_functor.renew_defs>`\\ \\(func\\)"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
msgid "Re-generate the definition nodes for a TIR, including VarDef, BufferDef."
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
msgid ""
":py:obj:`substitute <tvm.tir.stmt_functor.substitute>`\\ \\(node\\, "
"vmap\\)"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:1:<autosummary>:1
#: tvm.tir.stmt_functor.substitute:1
msgid "Substitute the var specified by vmap."
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:6
msgid "The input to be transformed."
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:12
msgid "preorder: function"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:9
msgid ""
"The function called in before recursive mutation If preorder returns "
"None, then the transform will proceed to recursive call. If preorder "
"returns a not None tvm.tir.Stmt/Expr, the transformer will simply return "
"it and won't do further recursion."
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:15
msgid "postorder"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:15
msgid "The function called after recursive mutation."
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:18
msgid "only_enable"
msgstr ""

#: of tvm.tir.stmt_functor.ir_transform:18
msgid "List of types that we only enable."
msgstr ""

#: of tvm.tir.stmt_functor.post_order_visit:2
msgid "Each node is guaranteed to be visited only once."
msgstr ""

#: of tvm.tir.stmt_functor.post_order_visit:6
msgid "fvisit: function"
msgstr ""

#: of tvm.tir.stmt_functor.post_order_visit:7
#: tvm.tir.stmt_functor.pre_order_visit:7
msgid "The visitor function."
msgstr ""

#: of tvm.tir.stmt_functor.pre_order_visit:2
msgid "If fvisit returns False, it won't visit the children of the node."
msgstr ""

#: of tvm.tir.stmt_functor.pre_order_visit:6
msgid "fvisit: function of the signature Object -> bool"
msgstr ""

#: of tvm.tir.stmt_functor.renew_defs:1
msgid ""
"Re-generate the definition nodes for a TIR, including VarDef, BufferDef. "
"This pass works as a simple DeepCopy to duplicate a function with "
"different Vars and Buffers but the same behavior"
msgstr ""

#: of tvm.tir.stmt_functor.renew_defs:8
msgid "func: PrimFunc"
msgstr ""

#: of tvm.tir.stmt_functor.renew_defs:8
msgid "The input function"
msgstr ""

#: of tvm.tir.stmt_functor.renew_defs:13
msgid "The new generated func."
msgstr ""

#: of tvm.tir.stmt_functor.substitute:6
msgid "node: ObjectRef"
msgstr ""

#: of tvm.tir.stmt_functor.substitute:6
msgid "The input."
msgstr ""

#: of tvm.tir.stmt_functor.substitute:9
msgid "vmap"
msgstr ""

#: of tvm.tir.stmt_functor.substitute:-1
msgid "Dict[Var, PrimExpr]"
msgstr ""

#: of tvm.tir.stmt_functor.substitute:9
msgid "The variable mapping."
msgstr ""

#~ msgid ":py:obj:`BijectiveLayout <tvm.tir.BijectiveLayout>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`BlockScope <tvm.tir.BlockScope>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Buffer <tvm.tir.Buffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`DataProducer <tvm.tir.DataProducer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Layout <tvm.tir.Layout>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Stmt <tvm.tir.Stmt>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`StmtSRef <tvm.tir.StmtSRef>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`PrimFuncPass <tvm.tir.transform.PrimFuncPass>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Buffer <tvm.tir.analysis.Buffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Object <tvm.tir.analysis.Object>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`PrimExpr <tvm.tir.analysis.PrimExpr>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Stmt <tvm.tir.analysis.Stmt>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`BijectiveLayout <tvm.tir.BijectiveLayout>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`BlockScope <tvm.tir.BlockScope>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Buffer <tvm.tir.Buffer>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`DataProducer <tvm.tir.DataProducer>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Layout <tvm.tir.Layout>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Stmt <tvm.tir.Stmt>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`StmtSRef <tvm.tir.StmtSRef>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`PrimFuncPass <tvm.tir.transform.PrimFuncPass>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Buffer <tvm.tir.analysis.Buffer>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Object <tvm.tir.analysis.Object>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`PrimExpr <tvm.tir.analysis.PrimExpr>`\\"
#~ msgstr ""

#~ msgid ":py:obj:`Stmt <tvm.tir.analysis.Stmt>`\\"
#~ msgstr ""

#~ msgid "Namespace for Tensor-level IR"
#~ msgstr ""

#~ msgid "**Classes:**"
#~ msgstr ""

#~ msgid ":py:obj:`Add <tvm.tir.Add>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Add node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Allocate <tvm.tir.Allocate>`\\ \\(buffer\\_var\\,"
#~ " dtype\\, extents\\, ...\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Allocate node."
#~ msgstr ""

#~ msgid ":py:obj:`And <tvm.tir.And>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "And node."
#~ msgstr ""

#~ msgid ":py:obj:`Any <tvm.tir.Any>`\\ \\(\\[span\\]\\)"
#~ msgstr ""

#~ msgid "Any node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AssertStmt <tvm.tir.AssertStmt>`\\ "
#~ "\\(condition\\, message\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "AssertStmt node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AttrStmt <tvm.tir.AttrStmt>`\\ \\(node\\, "
#~ "attr\\_key\\, value\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "AttrStmt node."
#~ msgstr ""

#~ msgid "Bijective mapping for two layouts (src-layout and dst-layout)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Block <tvm.tir.Block>`\\ \\(iter\\_vars\\, "
#~ "reads\\, writes\\, name\\_hint\\, body\\)"
#~ msgstr ""

#~ msgid "Block node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BlockRealize <tvm.tir.BlockRealize>`\\ "
#~ "\\(iter\\_values\\, predicate\\, block\\)"
#~ msgstr ""

#~ msgid "BlockRealize node."
#~ msgstr ""

#~ msgid ""
#~ "An object corresponds to each block "
#~ "sref in the sref tree, which "
#~ "tracks the producer-consumer dependency "
#~ "between blocks."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Broadcast <tvm.tir.Broadcast>`\\ \\(value\\, "
#~ "lanes\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Broadcast node."
#~ msgstr ""

#~ msgid "Symbolic data buffer in TVM."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferLoad <tvm.tir.BufferLoad>`\\ \\(buffer\\,"
#~ " indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Buffer load node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferRealize <tvm.tir.BufferRealize>`\\ "
#~ "\\(buffer\\, bounds\\, condition\\, body\\)"
#~ msgstr ""

#~ msgid "Buffer realize node."
#~ msgstr ""

#~ msgid ":py:obj:`BufferRegion <tvm.tir.BufferRegion>`\\ \\(buffer\\, region\\)"
#~ msgstr ""

#~ msgid "BufferRegion node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferStore <tvm.tir.BufferStore>`\\ \\(buffer\\,"
#~ " value\\, indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Buffer store node."
#~ msgstr ""

#~ msgid ":py:obj:`Call <tvm.tir.Call>`\\ \\(dtype\\, op\\, args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Call node."
#~ msgstr ""

#~ msgid ":py:obj:`CallEffectKind <tvm.tir.CallEffectKind>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Possible kinds of Call effects."
#~ msgstr ""

#~ msgid ":py:obj:`Cast <tvm.tir.Cast>`\\ \\(dtype\\, value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Cast expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CommReducer <tvm.tir.CommReducer>`\\ \\(lhs\\,"
#~ " rhs\\, result\\, identity\\_element\\)"
#~ msgstr ""

#~ msgid "Commutative reduce operator"
#~ msgstr ""

#~ msgid ":py:obj:`Div <tvm.tir.Div>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Div node."
#~ msgstr ""

#~ msgid ":py:obj:`EQ <tvm.tir.EQ>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "EQ node."
#~ msgstr ""

#~ msgid ":py:obj:`Evaluate <tvm.tir.Evaluate>`\\ \\(value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Evaluate node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FloatImm <tvm.tir.FloatImm>`\\ \\(dtype\\, "
#~ "value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Float constant."
#~ msgstr ""

#~ msgid ":py:obj:`FloorDiv <tvm.tir.FloorDiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "FloorDiv node."
#~ msgstr ""

#~ msgid ":py:obj:`FloorMod <tvm.tir.FloorMod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "FloorMod node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`For <tvm.tir.For>`\\ \\(loop\\_var\\, "
#~ "min\\_val\\, extent\\, kind\\, body\\[\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "For node."
#~ msgstr ""

#~ msgid ":py:obj:`ForKind <tvm.tir.ForKind>`\\ \\(value\\)"
#~ msgstr ""

#~ msgid "The kind of the for loop."
#~ msgstr ""

#~ msgid ":py:obj:`GE <tvm.tir.GE>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "GE node."
#~ msgstr ""

#~ msgid ":py:obj:`GT <tvm.tir.GT>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "GT node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`IfThenElse <tvm.tir.IfThenElse>`\\ "
#~ "\\(condition\\, then\\_case\\, else\\_case\\)"
#~ msgstr ""

#~ msgid "IfThenElse node."
#~ msgstr ""

#~ msgid ":py:obj:`IntImm <tvm.tir.IntImm>`\\ \\(dtype\\, value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Int constant."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`IterVar <tvm.tir.IterVar>`\\ \\(dom\\, "
#~ "var\\, iter\\_type\\[\\, thread\\_tag\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Represent iteration variable."
#~ msgstr ""

#~ msgid ":py:obj:`LE <tvm.tir.LE>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "LE node."
#~ msgstr ""

#~ msgid ":py:obj:`LT <tvm.tir.LT>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "LT node."
#~ msgstr ""

#~ msgid ""
#~ "Layout is composed of upper cases, "
#~ "lower cases and numbers, where upper "
#~ "case indicates a primal axis and "
#~ "the corresponding lower case with factor"
#~ " size indicates the subordinate axis."
#~ msgstr ""

#~ msgid ":py:obj:`Let <tvm.tir.Let>`\\ \\(var\\, value\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Let node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LetStmt <tvm.tir.LetStmt>`\\ \\(var\\, "
#~ "value\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "LetStmt node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Load <tvm.tir.Load>`\\ \\(dtype\\, "
#~ "buffer\\_var\\, index\\[\\, predicate\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Load node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MatchBufferRegion <tvm.tir.MatchBufferRegion>`\\ "
#~ "\\(buffer\\, source\\)"
#~ msgstr ""

#~ msgid "MatchBufferRegion node."
#~ msgstr ""

#~ msgid ":py:obj:`Max <tvm.tir.Max>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Max node."
#~ msgstr ""

#~ msgid ":py:obj:`Min <tvm.tir.Min>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Min node."
#~ msgstr ""

#~ msgid ":py:obj:`Mod <tvm.tir.Mod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Mod node."
#~ msgstr ""

#~ msgid ":py:obj:`Mul <tvm.tir.Mul>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Mul node."
#~ msgstr ""

#~ msgid ":py:obj:`NE <tvm.tir.NE>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "NE node."
#~ msgstr ""

#~ msgid ":py:obj:`Not <tvm.tir.Not>`\\ \\(a\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Not node."
#~ msgstr ""

#~ msgid ":py:obj:`Or <tvm.tir.Or>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Or node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Prefetch <tvm.tir.Prefetch>`\\ \\(buffer\\, "
#~ "bounds\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Prefetch node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PrimFunc <tvm.tir.PrimFunc>`\\ \\(params\\, "
#~ "body\\[\\, ret\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "A function declaration expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ProducerLoad <tvm.tir.ProducerLoad>`\\ "
#~ "\\(producer\\, indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Producer load node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ProducerRealize <tvm.tir.ProducerRealize>`\\ "
#~ "\\(producer\\, bounds\\, condition\\, ...\\)"
#~ msgstr ""

#~ msgid "ProducerRealize node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ProducerStore <tvm.tir.ProducerStore>`\\ "
#~ "\\(producer\\, value\\, indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "ProducerStore node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Ramp <tvm.tir.Ramp>`\\ \\(base\\, stride\\,"
#~ " lanes\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Ramp node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Reduce <tvm.tir.Reduce>`\\ \\(combiner\\, "
#~ "src\\, rdom\\, condition\\, ...\\)"
#~ msgstr ""

#~ msgid "Reduce node."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Schedule <tvm.tir.Schedule>`\\ \\(mod\\, "
#~ "\\*\\[\\, seed\\, debug\\_mask\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "The user-facing schedule class"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ScheduleState <tvm.tir.ScheduleState>`\\ "
#~ "\\(mod\\, \\*\\[\\, debug\\_mask\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "The state of scheduling, which exposes"
#~ " a `Replace` method as the primary"
#~ " resort for all the scheduling "
#~ "primitives to manipulate the TensorIR."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Select <tvm.tir.Select>`\\ \\(condition\\, "
#~ "true\\_value\\, false\\_value\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Select node."
#~ msgstr ""

#~ msgid ":py:obj:`SeqStmt <tvm.tir.SeqStmt>`\\ \\(seq\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Sequence of statements."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Shuffle <tvm.tir.Shuffle>`\\ \\(vectors\\, "
#~ "indices\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Shuffle node."
#~ msgstr ""

#~ msgid ":py:obj:`SizeVar <tvm.tir.SizeVar>`\\ \\(name\\, dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Symbolic variable to represent a tensor index size"
#~ msgstr ""

#~ msgid "Base class of all the statements."
#~ msgstr ""

#~ msgid ""
#~ "An object that refers to schedulable "
#~ "elements in the TensorIR, aka \"sref\"."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Store <tvm.tir.Store>`\\ \\(buffer\\_var\\, "
#~ "value\\, index\\[\\, predicate\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Store node."
#~ msgstr ""

#~ msgid ":py:obj:`StringImm <tvm.tir.StringImm>`\\ \\(value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "String constant."
#~ msgstr ""

#~ msgid ":py:obj:`Sub <tvm.tir.Sub>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Sub node."
#~ msgstr ""

#~ msgid ":py:obj:`TensorIntrin <tvm.tir.TensorIntrin>`\\ \\(desc\\, impl\\)"
#~ msgstr ""

#~ msgid "A tensor intrinsic."
#~ msgstr ""

#~ msgid ":py:obj:`Var <tvm.tir.Var>`\\ \\(name\\, dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Symbolic variable."
#~ msgstr ""

#~ msgid ":py:obj:`While <tvm.tir.While>`\\ \\(condition\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "While node."
#~ msgstr ""

#~ msgid "**Exceptions:**"
#~ msgstr ""

#~ msgid ":py:obj:`ScheduleError <tvm.tir.ScheduleError>`\\"
#~ msgstr ""

#~ msgid "Error that happens during TensorIR scheduling."
#~ msgstr ""

#~ msgid "**Functions:**"
#~ msgstr ""

#~ msgid ":py:obj:`abs <tvm.tir.abs>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Get absolute value of the input element-wise."
#~ msgstr ""

#~ msgid ":py:obj:`acos <tvm.tir.acos>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take acos of input x."
#~ msgstr ""

#~ msgid ":py:obj:`acosh <tvm.tir.acosh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid ":py:obj:`all <tvm.tir.all>`\\ \\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new expression of the intersection of all conditions in the"
#~ msgstr ""

#~ msgid ":py:obj:`any <tvm.tir.any>`\\ \\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new experssion of the union of all conditions in the arguments"
#~ msgstr ""

#~ msgid ":py:obj:`asin <tvm.tir.asin>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take asin of input x."
#~ msgstr ""

#~ msgid ":py:obj:`asinh <tvm.tir.asinh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take asinh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`atan <tvm.tir.atan>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take atan of input x."
#~ msgstr ""

#~ msgid ":py:obj:`atan2 <tvm.tir.atan2>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Take arctan2(x1, x2)."
#~ msgstr ""

#~ msgid ":py:obj:`atanh <tvm.tir.atanh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take atanh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`bijective_layout <tvm.tir.bijective_layout>`\\ "
#~ "\\(src\\_layout\\, dst\\_layout\\)"
#~ msgstr ""

#~ msgid "Create a bijective layout mapping."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_extern <tvm.tir.call_extern>`\\ \\(dtype\\,"
#~ " func\\_name\\, \\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling a extern function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_intrin <tvm.tir.call_intrin>`\\ \\(dtype\\,"
#~ " func\\_name\\, \\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling an intrinsic function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_llvm_intrin <tvm.tir.call_llvm_intrin>`\\ "
#~ "\\(dtype\\, name\\, \\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling a llvm intrinsic function"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_llvm_pure_intrin "
#~ "<tvm.tir.call_llvm_pure_intrin>`\\ \\(dtype\\, name\\, "
#~ "\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling a pure llvm intrinsic function"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_packed <tvm.tir.call_packed>`\\ "
#~ "\\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by call an external packed function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`call_pure_extern <tvm.tir.call_pure_extern>`\\ "
#~ "\\(dtype\\, func\\_name\\, \\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Build expression by calling a pure extern function."
#~ msgstr ""

#~ msgid ":py:obj:`ceil <tvm.tir.ceil>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take ceil of float input x."
#~ msgstr ""

#~ msgid ":py:obj:`clz <tvm.tir.clz>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Count leading zero bits of an integer x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`comm_reducer <tvm.tir.comm_reducer>`\\ "
#~ "\\(fcombine\\, fidentity\\[\\, name\\]\\)"
#~ msgstr ""

#~ msgid "Create a commutative reducer for reduction."
#~ msgstr ""

#~ msgid ":py:obj:`copysign <tvm.tir.copysign>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Change the sign of x1 to that of x2, element-wise."
#~ msgstr ""

#~ msgid ":py:obj:`cos <tvm.tir.cos>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take cos of input x."
#~ msgstr ""

#~ msgid ":py:obj:`cosh <tvm.tir.cosh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take cosh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`decl_buffer <tvm.tir.decl_buffer>`\\ "
#~ "\\(shape\\[\\, dtype\\, name\\, data\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Declare a new symbolic buffer."
#~ msgstr ""

#~ msgid ":py:obj:`div <tvm.tir.div>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute a / b as in C/C++ semantics."
#~ msgstr ""

#~ msgid ":py:obj:`erf <tvm.tir.erf>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take gauss error function of the input x."
#~ msgstr ""

#~ msgid ":py:obj:`exp <tvm.tir.exp>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take exponential of input x."
#~ msgstr ""

#~ msgid ":py:obj:`exp10 <tvm.tir.exp10>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Calculate 10**x"
#~ msgstr ""

#~ msgid ":py:obj:`exp2 <tvm.tir.exp2>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Calculate 2**x"
#~ msgstr ""

#~ msgid ":py:obj:`floor <tvm.tir.floor>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take floor of float input x."
#~ msgstr ""

#~ msgid ":py:obj:`floordiv <tvm.tir.floordiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the floordiv of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`floormod <tvm.tir.floormod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the floormod of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`fmod <tvm.tir.fmod>`\\ \\(x\\, y\\)"
#~ msgstr ""

#~ msgid "Return the remainder of x divided by y with the same sign as x."
#~ msgstr ""

#~ msgid ":py:obj:`hypot <tvm.tir.hypot>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Equivalent to sqrt(x1**2 + x2**2), element-wise."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`if_then_else <tvm.tir.if_then_else>`\\ \\(cond\\,"
#~ " t\\, f\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Conditional selection expression."
#~ msgstr ""

#~ msgid ":py:obj:`indexdiv <tvm.tir.indexdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute floor(a / b) where a and b are non-negative."
#~ msgstr ""

#~ msgid ":py:obj:`indexmod <tvm.tir.indexmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the remainder of indexdiv."
#~ msgstr ""

#~ msgid ":py:obj:`isfinite <tvm.tir.isfinite>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is finite."
#~ msgstr ""

#~ msgid ":py:obj:`isinf <tvm.tir.isinf>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is infinite."
#~ msgstr ""

#~ msgid ":py:obj:`isnan <tvm.tir.isnan>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is Nan."
#~ msgstr ""

#~ msgid ":py:obj:`layout <tvm.tir.layout>`\\ \\(layout\\_str\\)"
#~ msgstr ""

#~ msgid "Create a layout node from a string."
#~ msgstr ""

#~ msgid ":py:obj:`ldexp <tvm.tir.ldexp>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Returns x1 * (2 ** x2)."
#~ msgstr ""

#~ msgid ":py:obj:`log <tvm.tir.log>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log of input x."
#~ msgstr ""

#~ msgid ":py:obj:`log10 <tvm.tir.log10>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log10 of input x."
#~ msgstr ""

#~ msgid ":py:obj:`log1p <tvm.tir.log1p>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log(x + 1) with respect to input x."
#~ msgstr ""

#~ msgid ":py:obj:`log2 <tvm.tir.log2>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log2 of input x."
#~ msgstr ""

#~ msgid ":py:obj:`max <tvm.tir.max>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
#~ msgstr ""

#~ msgid "Create a max expression over axis."
#~ msgstr ""

#~ msgid ":py:obj:`max_value <tvm.tir.max_value>`\\ \\(dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "maximum value of dtype"
#~ msgstr ""

#~ msgid ":py:obj:`min <tvm.tir.min>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
#~ msgstr ""

#~ msgid "Create a min expression over axis."
#~ msgstr ""

#~ msgid ":py:obj:`min_value <tvm.tir.min_value>`\\ \\(dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "minimum value of dtype"
#~ msgstr ""

#~ msgid ":py:obj:`nearbyint <tvm.tir.nearbyint>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Round elements of the array to the nearest integer."
#~ msgstr ""

#~ msgid ":py:obj:`nextafter <tvm.tir.nextafter>`\\ \\(x1\\, x2\\)"
#~ msgstr ""

#~ msgid "Return the next floating-point value after x1 towards x2."
#~ msgstr ""

#~ msgid ":py:obj:`popcount <tvm.tir.popcount>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Count the number of set bits in input x."
#~ msgstr ""

#~ msgid ":py:obj:`power <tvm.tir.power>`\\ \\(x\\, y\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "x power y"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`q_multiply_shift <tvm.tir.q_multiply_shift>`\\ "
#~ "\\(x\\, y\\, q\\, s\\)"
#~ msgstr ""

#~ msgid ""
#~ "Execute a multiplication between two "
#~ "Q-numbers x and y followed by a"
#~ " right shift s."
#~ msgstr ""

#~ msgid ":py:obj:`ret <tvm.tir.ret>`\\ \\(val\\)"
#~ msgstr ""

#~ msgid "Create a tir return expression"
#~ msgstr ""

#~ msgid ":py:obj:`round <tvm.tir.round>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`rsqrt <tvm.tir.rsqrt>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take reciprocal of square root of input x."
#~ msgstr ""

#~ msgid ":py:obj:`sigmoid <tvm.tir.sigmoid>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Quick function to get sigmoid"
#~ msgstr ""

#~ msgid ":py:obj:`sin <tvm.tir.sin>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take sin of input x."
#~ msgstr ""

#~ msgid ":py:obj:`sinh <tvm.tir.sinh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take sinh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`sqrt <tvm.tir.sqrt>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take square root of input x."
#~ msgstr ""

#~ msgid ":py:obj:`stmt_list <tvm.tir.stmt_list>`\\ \\(stmt\\)"
#~ msgstr ""

#~ msgid "Make list of stmt from blocks."
#~ msgstr ""

#~ msgid ":py:obj:`stmt_seq <tvm.tir.stmt_seq>`\\ \\(\\*args\\)"
#~ msgstr ""

#~ msgid "Make sequence of statements"
#~ msgstr ""

#~ msgid ":py:obj:`sum <tvm.tir.sum>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
#~ msgstr ""

#~ msgid "Create a sum expression over axis."
#~ msgstr ""

#~ msgid ":py:obj:`tan <tvm.tir.tan>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take tan of input x."
#~ msgstr ""

#~ msgid ":py:obj:`tanh <tvm.tir.tanh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take hyperbolic tanh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`trace <tvm.tir.trace>`\\ \\(args\\[\\, trace\\_action\\]\\)"
#~ msgstr ""

#~ msgid "Trace tensor data at the runtime."
#~ msgstr ""

#~ msgid ":py:obj:`trunc <tvm.tir.trunc>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Get truncated value of the input."
#~ msgstr ""

#~ msgid ":py:obj:`truncdiv <tvm.tir.truncdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the truncdiv of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`truncmod <tvm.tir.truncmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the truncmod of two expressions."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "The left hand operand."
#~ msgstr ""

#~ msgid "The right hand operand."
#~ msgstr ""

#~ msgid "The location of this itervar in the source code."
#~ msgstr ""

#~ msgid "The buffer variable."
#~ msgstr ""

#~ msgid "The data type of the buffer."
#~ msgstr ""

#~ msgid "The extents of the allocate"
#~ msgstr ""

#~ msgid "The condition."
#~ msgstr ""

#~ msgid "The body statement."
#~ msgstr ""

#~ msgid "Additional annotation hints"
#~ msgstr ""

#~ msgid "span"
#~ msgstr ""

#~ msgid "Optional[Span]"
#~ msgstr ""

#~ msgid "The assert condition."
#~ msgstr ""

#~ msgid "The error message."
#~ msgstr ""

#~ msgid "The node to annotate the attribute"
#~ msgstr ""

#~ msgid "Attribute type key."
#~ msgstr ""

#~ msgid "The value of the attribute"
#~ msgstr ""

#~ msgid ""
#~ "Bijective mapping for two layouts "
#~ "(src-layout and dst-layout). It "
#~ "provides shape and index conversion "
#~ "between each other."
#~ msgstr ""

#~ msgid ""
#~ "Do not construct directly, use "
#~ ":any:`bijective_layout` instead. See the "
#~ "documentation of :any:`bijective_layout` for "
#~ "more details."
#~ msgstr ""

#~ msgid "source layout."
#~ msgstr ""

#~ msgid "destination layout."
#~ msgstr ""

#~ msgid ":obj:`bijective_layout`"
#~ msgstr ""

#~ msgid "Declare a layout"
#~ msgstr ""

#~ msgid "**Methods:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`backward_index "
#~ "<tvm.tir.BijectiveLayout.backward_index>`\\ \\(index\\)"
#~ msgstr ""

#~ msgid "Given the indices of the dst-layout, infer the src index."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`backward_shape "
#~ "<tvm.tir.BijectiveLayout.backward_shape>`\\ \\(shape\\)"
#~ msgstr ""

#~ msgid "Given the shape of the dst-layout, infer the src shape."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`forward_index "
#~ "<tvm.tir.BijectiveLayout.forward_index>`\\ \\(index\\)"
#~ msgstr ""

#~ msgid "Given the indices of the src-layout, infer the dst index."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`forward_shape "
#~ "<tvm.tir.BijectiveLayout.forward_shape>`\\ \\(shape\\)"
#~ msgstr ""

#~ msgid "Given the shape of the src-layout, infer the dst shape."
#~ msgstr ""

#~ msgid "The indices in dst-layout."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "**src_index** -- The inferred indices in src-layout."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "The shape in dst-layout."
#~ msgstr ""

#~ msgid "**src_shape** -- The inferred shape in src-layout."
#~ msgstr ""

#~ msgid "The indices in src-layout."
#~ msgstr ""

#~ msgid "**dst_index** -- The inferred indices in dst-layout."
#~ msgstr ""

#~ msgid "The shape in src-layout."
#~ msgstr ""

#~ msgid "**dst_shape** -- The inferred shape in dst-layout."
#~ msgstr ""

#~ msgid "The block Variable."
#~ msgstr ""

#~ msgid "The read buffer regions of the block."
#~ msgstr ""

#~ msgid "The write buffer regions of the block."
#~ msgstr ""

#~ msgid "the name_hint of the block."
#~ msgstr ""

#~ msgid "The body of the block."
#~ msgstr ""

#~ msgid "The init block of the reduction block"
#~ msgstr ""

#~ msgid "The buffer allocations"
#~ msgstr ""

#~ msgid "The subregion buffer match"
#~ msgstr ""

#~ msgid "Additional annotation hints."
#~ msgstr ""

#~ msgid "The location of this block in the source code."
#~ msgstr ""

#~ msgid "The binding values of the block var."
#~ msgstr ""

#~ msgid "The predicate of the block."
#~ msgstr ""

#~ msgid "The block to realize"
#~ msgstr ""

#~ msgid "The location of this block_realize in the source code."
#~ msgstr ""

#~ msgid "Glossary:"
#~ msgstr ""

#~ msgid ""
#~ "Block scope: A contiguous subtree of "
#~ "the sref tree, rooted at each "
#~ "block sref, whose components are:"
#~ msgstr ""

#~ msgid "scope root: a block sref"
#~ msgstr ""

#~ msgid "internal srefs: loop srefs"
#~ msgstr ""

#~ msgid "scope leaves: block srefs"
#~ msgstr ""

#~ msgid ""
#~ "Child block: The scope leaf blocks "
#~ "under the scope root or a specific"
#~ " internal sref"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_deps_by_dst <tvm.tir.BlockScope.get_deps_by_dst>`\\"
#~ " \\(block\\)"
#~ msgstr ""

#~ msgid "Get all dependencies whose `dst` is the target `block`."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_deps_by_src <tvm.tir.BlockScope.get_deps_by_src>`\\"
#~ " \\(block\\)"
#~ msgstr ""

#~ msgid "Get all dependencies whose `src` is the target`block`."
#~ msgstr ""

#~ msgid "The queried block"
#~ msgstr ""

#~ msgid "**blocks** -- The dependencies"
#~ msgstr ""

#~ msgid "The value of the expression."
#~ msgstr ""

#~ msgid "The lanes of the expression."
#~ msgstr ""

#~ msgid ""
#~ "Buffer provide a way to represent "
#~ "data layout specialization of data "
#~ "structure in TVM."
#~ msgstr ""

#~ msgid ""
#~ "Do not construct directly, use "
#~ ":py:func:`~decl_buffer` instead. See the "
#~ "documentation of :py:func:`decl_buffer` for "
#~ "more details."
#~ msgstr ""

#~ msgid ":obj:`decl_buffer`"
#~ msgstr ""

#~ msgid "Declare a buffer"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`access_ptr <tvm.tir.Buffer.access_ptr>`\\ "
#~ "\\(access\\_mask\\[\\, ptr\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Get an access pointer to the head of buffer."
#~ msgstr ""

#~ msgid ":py:obj:`scope <tvm.tir.Buffer.scope>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Return the storage scope associated with this buffer."
#~ msgstr ""

#~ msgid ":py:obj:`vload <tvm.tir.Buffer.vload>`\\ \\(begin\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Generate an Expr that loads dtype from begin index."
#~ msgstr ""

#~ msgid ":py:obj:`vstore <tvm.tir.Buffer.vstore>`\\ \\(begin\\, value\\)"
#~ msgstr ""

#~ msgid "Generate a Stmt that store value into begin index."
#~ msgstr ""

#~ msgid ""
#~ "This is the recommended method to "
#~ "get buffer data ptress when interacting"
#~ " with external functions."
#~ msgstr ""

#~ msgid ""
#~ "The access pattern MASK. Indicate "
#~ "whether the access will read or "
#~ "write to the data content."
#~ msgstr ""

#~ msgid ""
#~ "The data type of the result "
#~ "pointer. Do not specify unless we "
#~ "want to cast pointer to specific "
#~ "type."
#~ msgstr ""

#~ msgid ""
#~ "The number of lanes for the data"
#~ " type. This value is greater than "
#~ "one for vector types."
#~ msgstr ""

#~ msgid ""
#~ "The offset of pointer. We can use"
#~ " it to offset by the number of"
#~ " elements from the address of ptr."
#~ msgstr ""

#~ msgid "实际案例"
#~ msgstr ""

#~ msgid ""
#~ "Return the storage scope associated with"
#~ " this buffer. :returns: **scope** -- "
#~ "The storage scope associated with this"
#~ " buffer. :rtype: str"
#~ msgstr ""

#~ msgid "The beginning index in unit of Buffer.dtype"
#~ msgstr ""

#~ msgid ""
#~ "The data type to be loaded, can"
#~ " be vector type which have lanes "
#~ "that is multiple of Buffer.dtype"
#~ msgstr ""

#~ msgid "**load** -- The corresponding load expression."
#~ msgstr ""

#~ msgid "The value to be stored."
#~ msgstr ""

#~ msgid "**store** -- The corresponding store stmt."
#~ msgstr ""

#~ msgid "The buffer to be loaded."
#~ msgstr ""

#~ msgid "The buffer indices."
#~ msgstr ""

#~ msgid "The buffer."
#~ msgstr ""

#~ msgid "The value we to be stored."
#~ msgstr ""

#~ msgid "The realize condition."
#~ msgstr ""

#~ msgid "The body of the statement."
#~ msgstr ""

#~ msgid "The buffer of the buffer region"
#~ msgstr ""

#~ msgid "The region array of the buffer region"
#~ msgstr ""

#~ msgid "The indices location to be stored."
#~ msgstr ""

#~ msgid "The return data type"
#~ msgstr ""

#~ msgid "The function to be called, or the name to the global tvm.Op"
#~ msgstr ""

#~ msgid "The input arguments to the call"
#~ msgstr ""

#~ msgid "The data type"
#~ msgstr ""

#~ msgid "The value of the function."
#~ msgstr ""

#~ msgid "The left arguments of the reducer."
#~ msgstr ""

#~ msgid "The right arguments of the reducer."
#~ msgstr ""

#~ msgid "The reduction results."
#~ msgstr ""

#~ msgid "The identity elements."
#~ msgstr ""

#~ msgid "The expression to be evalued."
#~ msgstr ""

#~ msgid "The constant value."
#~ msgstr ""

#~ msgid "The loop variable."
#~ msgstr ""

#~ msgid "The beginning value."
#~ msgstr ""

#~ msgid "The length of the loop."
#~ msgstr ""

#~ msgid "The type of the for."
#~ msgstr ""

#~ msgid "The thread this loop binds to. Only valid if kind is ThreadBinding"
#~ msgstr ""

#~ msgid ""
#~ "ForKind can change the control flow "
#~ "semantics of the loop and need to"
#~ " be considered in all TIR passes."
#~ msgstr ""

#~ msgid "The expression"
#~ msgstr ""

#~ msgid "The statement to execute if condition is true."
#~ msgstr ""

#~ msgid "The statement to execute if condition is false."
#~ msgstr ""

#~ msgid "IterVar represents axis iterations in the computation."
#~ msgstr ""

#~ msgid "The domain of the iteration."
#~ msgstr ""

#~ msgid "The internal variable that is used for iteration."
#~ msgstr ""

#~ msgid "The iteration type."
#~ msgstr ""

#~ msgid "The thread type tag."
#~ msgstr ""

#~ msgid ":obj:`te.thread_axis`"
#~ msgstr ""

#~ msgid "Create thread axis IterVar."
#~ msgstr ""

#~ msgid ":obj:`te.reduce_axis`"
#~ msgstr ""

#~ msgid "Create reduce axis IterVar."
#~ msgstr ""

#~ msgid ""
#~ "Layout is composed of upper cases, "
#~ "lower cases and numbers, where upper "
#~ "case indicates a primal axis and "
#~ "the corresponding lower case with factor"
#~ " size indicates the subordinate axis. "
#~ "For example, NCHW16c can describe a "
#~ "5-D tensor of [batch_size, channel, "
#~ "height, width, channel_block]. Here "
#~ "subordinate axis channel_block=16 is the "
#~ "factor size of the primal axis C"
#~ " (channel)."
#~ msgstr ""

#~ msgid ":obj:`layout`"
#~ msgstr ""

#~ msgid ":py:obj:`factor_of <tvm.tir.Layout.factor_of>`\\ \\(axis\\)"
#~ msgstr ""

#~ msgid "Get the factor size of the subordinate axis."
#~ msgstr ""

#~ msgid ":py:obj:`index_of <tvm.tir.Layout.index_of>`\\ \\(axis\\)"
#~ msgstr ""

#~ msgid "Get the index of an axis"
#~ msgstr ""

#~ msgid "The axis name, need to be [a-z,A-Z]"
#~ msgstr ""

#~ msgid ""
#~ "**factor** -- the size of the "
#~ "subordinate-axis of axis (if axis is"
#~ " a primal-axis), or the size of"
#~ " axis itself (if axis is a "
#~ "subordinate-axis). Return -1 if axis "
#~ "is not in the layout."
#~ msgstr ""

#~ msgid "**index** -- The index of the axis, -1 if not found."
#~ msgstr ""

#~ msgid "The variable in the binding."
#~ msgstr ""

#~ msgid "The value in to be binded."
#~ msgstr ""

#~ msgid "The body expression."
#~ msgstr ""

#~ msgid "The data type."
#~ msgstr ""

#~ msgid "The buffer variable in the load expression."
#~ msgstr ""

#~ msgid "The index in the load."
#~ msgstr ""

#~ msgid "The load predicate."
#~ msgstr ""

#~ msgid "The target buffer"
#~ msgstr ""

#~ msgid "The region of source buffer"
#~ msgstr ""

#~ msgid "The input value"
#~ msgstr ""

#~ msgid "The buffer to be prefetched."
#~ msgstr ""

#~ msgid "The bounds to be prefetched."
#~ msgstr ""

#~ msgid "List of input parameters to the function."
#~ msgstr ""

#~ msgid "The body of the function."
#~ msgstr ""

#~ msgid "The return type annotation of the function."
#~ msgstr ""

#~ msgid "The buffer binding map."
#~ msgstr ""

#~ msgid "Attributes of the function, can be None"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`script <tvm.tir.PrimFunc.script>`\\ "
#~ "\\(\\[tir\\_prefix\\, show\\_meta\\]\\)"
#~ msgstr ""

#~ msgid "Print IRModule into TVMScript"
#~ msgstr ""

#~ msgid ":py:obj:`specialize <tvm.tir.PrimFunc.specialize>`\\ \\(param\\_map\\)"
#~ msgstr ""

#~ msgid "Specialize parameters of PrimFunc"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`with_body <tvm.tir.PrimFunc.with_body>`\\ "
#~ "\\(new\\_body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new PrimFunc with the same set signatures but a new body."
#~ msgstr ""

#~ msgid "The tir namespace prefix"
#~ msgstr ""

#~ msgid "Whether to show meta information"
#~ msgstr ""

#~ msgid "**script** -- The TVM Script of the PrimFunc"
#~ msgstr ""

#~ msgid "The mapping from function params to the instance"
#~ msgstr ""

#~ msgid "We can define a Meta TIR function with symbolic shape:"
#~ msgstr ""

#~ msgid "Then we can make it specialized with given shapes or buffers."
#~ msgstr ""

#~ msgid "The specialized function:"
#~ msgstr ""

#~ msgid "**func** -- The new function with parameter specialized"
#~ msgstr ""

#~ msgid "The new body."
#~ msgstr ""

#~ msgid "**new_func** -- The created new function."
#~ msgstr ""

#~ msgid "The data producer."
#~ msgstr ""

#~ msgid "The bound of realize"
#~ msgstr ""

#~ msgid "The realize body"
#~ msgstr ""

#~ msgid "The storage scope associated with this realization"
#~ msgstr ""

#~ msgid "The index arguments of the store."
#~ msgstr ""

#~ msgid "The base expression."
#~ msgstr ""

#~ msgid "The stride of the ramp."
#~ msgstr ""

#~ msgid "The combiner."
#~ msgstr ""

#~ msgid "The source expression."
#~ msgstr ""

#~ msgid "The iteration domain"
#~ msgstr ""

#~ msgid "The reduce condition."
#~ msgstr ""

#~ msgid "The value index."
#~ msgstr ""

#~ msgid "The initial value for output. This can be an int, float or ProducerLoad"
#~ msgstr ""

#~ msgid ""
#~ "A schedule is a set of "
#~ "transformations that change the order of"
#~ " computation but preserve the semantics "
#~ "of computation. Some example of "
#~ "schedules: 1) Split a loop into "
#~ "two; 2) Reorder two loops; 3) "
#~ "Inline the computation of a specific "
#~ "buffer into its consumer"
#~ msgstr ""

#~ msgid ""
#~ "The schedule class stores auxiliary "
#~ "information to schedule correctly and "
#~ "efficiently."
#~ msgstr ""

#~ msgid ""
#~ "Link to tutorial: "
#~ "https://tvm.apache.org/docs/tutorials/language/schedule_primitives.html"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`annotate <tvm.tir.Schedule.annotate>`\\ "
#~ "\\(block\\_or\\_loop\\, ann\\_key\\, ann\\_val\\)"
#~ msgstr ""

#~ msgid "Annotate a block/loop with a key value pair"
#~ msgstr ""

#~ msgid ":py:obj:`bind <tvm.tir.Schedule.bind>`\\ \\(loop\\, thread\\_axis\\)"
#~ msgstr ""

#~ msgid "Bind the input loop to the given thread axis."
#~ msgstr ""

#~ msgid ":py:obj:`blockize <tvm.tir.Schedule.blockize>`\\ \\(loop\\)"
#~ msgstr ""

#~ msgid "Convert the subtree rooted at a specific loop into a block."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cache_read <tvm.tir.Schedule.cache_read>`\\ "
#~ "\\(block\\, read\\_buffer\\_index\\, ...\\)"
#~ msgstr ""

#~ msgid "Create a block that reads a buffer region into a read cache."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cache_write <tvm.tir.Schedule.cache_write>`\\ "
#~ "\\(block\\, write\\_buffer\\_index\\, ...\\)"
#~ msgstr ""

#~ msgid "Create a block that reads a buffer region into a write cache."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`compute_at <tvm.tir.Schedule.compute_at>`\\ "
#~ "\\(block\\, loop\\[\\, preserve\\_unit\\_loops\\]\\)"
#~ msgstr ""

#~ msgid "Compute-At."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`compute_inline <tvm.tir.Schedule.compute_inline>`\\ "
#~ "\\(block\\)"
#~ msgstr ""

#~ msgid "Inline a block into its consumer(s)."
#~ msgstr ""

#~ msgid ":py:obj:`copy <tvm.tir.Schedule.copy>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns a copy of the schedule, "
#~ "including both the state and the "
#~ "symbol table, * guaranteeing that * "
#~ "1) SRef tree is completely "
#~ "reconstructed; * 2) The IRModule being"
#~ " scheduled is untouched; * 3) All "
#~ "the random variables are valid in "
#~ "the copy, pointing to the corresponding"
#~ " sref * reconstructed"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`decompose_reduction "
#~ "<tvm.tir.Schedule.decompose_reduction>`\\ \\(block\\, "
#~ "loop\\)"
#~ msgstr ""

#~ msgid "Decompose a reduction block into two separate blocks."
#~ msgstr ""

#~ msgid ":py:obj:`enter_postproc <tvm.tir.Schedule.enter_postproc>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "A no-op that marks the start of postprocessing phase of scheduling"
#~ msgstr ""

#~ msgid ":py:obj:`fork_seed <tvm.tir.Schedule.fork_seed>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Returns a forked random state as seed for new schedules"
#~ msgstr ""

#~ msgid ":py:obj:`fuse <tvm.tir.Schedule.fuse>`\\ \\(\\*loops\\)"
#~ msgstr ""

#~ msgid "Fuse a list of consecutive loops into one."
#~ msgstr ""

#~ msgid ":py:obj:`get <tvm.tir.Schedule.get>`\\ \\(rand\\_var\\_or\\_sref\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns: - the corresponding Block that"
#~ " a BlockRV evaluates to; - the "
#~ "corresponding For that a LoopRV "
#~ "evaluates to; - the corresponding "
#~ "integer that a ExprRV evaluates to; "
#~ "- the corresponding Block that a "
#~ "block sref points to; - the "
#~ "corresponding For that a loop sref "
#~ "points to;"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_block <tvm.tir.Schedule.get_block>`\\ "
#~ "\\(name\\[\\, func\\_name\\]\\)"
#~ msgstr ""

#~ msgid "Retrieve a block in a specific function with its name"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_child_blocks <tvm.tir.Schedule.get_child_blocks>`\\"
#~ " \\(block\\_or\\_loop\\)"
#~ msgstr ""

#~ msgid "Get the leaf blocks of a specific block/loop"
#~ msgstr ""

#~ msgid ":py:obj:`get_consumers <tvm.tir.Schedule.get_consumers>`\\ \\(block\\)"
#~ msgstr ""

#~ msgid "Get the consumers of a specific block"
#~ msgstr ""

#~ msgid ":py:obj:`get_loops <tvm.tir.Schedule.get_loops>`\\ \\(block\\)"
#~ msgstr ""

#~ msgid "Get the parent loops of the block in its scope, from outer to inner"
#~ msgstr ""

#~ msgid ":py:obj:`get_producers <tvm.tir.Schedule.get_producers>`\\ \\(block\\)"
#~ msgstr ""

#~ msgid "Get the producers of a specific block"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_sref <tvm.tir.Schedule.get_sref>`\\ "
#~ "\\(rand\\_var\\_or\\_stmt\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns the corresponding sref to the"
#~ " given 1) LoopRV 2) BlockRV 3) "
#~ "Block 4) For"
#~ msgstr ""

#~ msgid ":py:obj:`parallel <tvm.tir.Schedule.parallel>`\\ \\(loop\\)"
#~ msgstr ""

#~ msgid "Parallelize the input loop."
#~ msgstr ""

#~ msgid ":py:obj:`remove_rv <tvm.tir.Schedule.remove_rv>`\\ \\(rand\\_var\\)"
#~ msgstr ""

#~ msgid "Remove a random variable from the symbol table"
#~ msgstr ""

#~ msgid ":py:obj:`reorder <tvm.tir.Schedule.reorder>`\\ \\(\\*ordered\\_loops\\)"
#~ msgstr ""

#~ msgid "Reorder a list of loops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reverse_compute_at "
#~ "<tvm.tir.Schedule.reverse_compute_at>`\\ \\(block\\, "
#~ "loop\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Reverse-Compute-At."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reverse_compute_inline "
#~ "<tvm.tir.Schedule.reverse_compute_inline>`\\ \\(block\\)"
#~ msgstr ""

#~ msgid "Inline a block into its only producer."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`rfactor <tvm.tir.Schedule.rfactor>`\\ \\(loop\\,"
#~ " factor\\_axis\\)"
#~ msgstr ""

#~ msgid "Factorize an associative reduction block by the specified loop."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sample_categorical "
#~ "<tvm.tir.Schedule.sample_categorical>`\\ \\(candidates\\, "
#~ "probs\\[\\, decision\\]\\)"
#~ msgstr ""

#~ msgid "Sample an integer given the probability distribution"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sample_compute_location "
#~ "<tvm.tir.Schedule.sample_compute_location>`\\ \\(block\\[\\, "
#~ "decision\\]\\)"
#~ msgstr ""

#~ msgid "Sample a compute-at location of the given block"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sample_perfect_tile "
#~ "<tvm.tir.Schedule.sample_perfect_tile>`\\ \\(loop\\, "
#~ "n\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Sample the factors to perfect tile a specific loop"
#~ msgstr ""

#~ msgid ":py:obj:`seed <tvm.tir.Schedule.seed>`\\ \\(seed\\)"
#~ msgstr ""

#~ msgid "Seed the randomness"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`set_scope <tvm.tir.Schedule.set_scope>`\\ "
#~ "\\(block\\, buffer\\_index\\, storage\\_scope\\)"
#~ msgstr ""

#~ msgid ""
#~ "Set the storage scope of a buffer,"
#~ " where the buffer is specified by "
#~ "the a block and a write-index"
#~ msgstr ""

#~ msgid ":py:obj:`show <tvm.tir.Schedule.show>`\\ \\(rand\\_var\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns a string representation of the"
#~ " value that the random variable "
#~ "evaluates to"
#~ msgstr ""

#~ msgid ":py:obj:`split <tvm.tir.Schedule.split>`\\ \\(loop\\, factors\\)"
#~ msgstr ""

#~ msgid "Split a loop into a list of consecutive loops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`storage_align <tvm.tir.Schedule.storage_align>`\\ "
#~ "\\(block\\, buffer\\_index\\, axis\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Set alignment requirement for specific "
#~ "dimension such that stride[axis] == k"
#~ " * factor + offset for some k."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tensorize <tvm.tir.Schedule.tensorize>`\\ "
#~ "\\(block\\_or\\_loop\\, tensor\\_intrin\\)"
#~ msgstr ""

#~ msgid "Tensorize the computation enclosed by loop with the tensor intrinsic."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`unannotate <tvm.tir.Schedule.unannotate>`\\ "
#~ "\\(block\\_or\\_loop\\, ann\\_key\\)"
#~ msgstr ""

#~ msgid "Unannotate a block/loop's annotation with key ann_key"
#~ msgstr ""

#~ msgid ":py:obj:`unroll <tvm.tir.Schedule.unroll>`\\ \\(loop\\)"
#~ msgstr ""

#~ msgid "Unroll the input loop."
#~ msgstr ""

#~ msgid ":py:obj:`vectorize <tvm.tir.Schedule.vectorize>`\\ \\(loop\\)"
#~ msgstr ""

#~ msgid "Vectorize the input loop."
#~ msgstr ""

#~ msgid "**Attributes:**"
#~ msgstr ""

#~ msgid ":py:obj:`mod <tvm.tir.Schedule.mod>`\\"
#~ msgstr ""

#~ msgid "Returns the AST of the module being scheduled"
#~ msgstr ""

#~ msgid ":py:obj:`state <tvm.tir.Schedule.state>`\\"
#~ msgstr ""

#~ msgid "Returns the ScheduleState in the current schedule class"
#~ msgstr ""

#~ msgid ":py:obj:`trace <tvm.tir.Schedule.trace>`\\"
#~ msgstr ""

#~ msgid "Returns the internally maintained trace of scheduling program execution"
#~ msgstr ""

#~ msgid "The block/loop to be annotated"
#~ msgstr ""

#~ msgid "The annotation key"
#~ msgstr ""

#~ msgid "The annotation value"
#~ msgstr ""

#~ msgid "Before annotate, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do annotate:"
#~ msgstr ""

#~ msgid "After applying annotate, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Bind the input loop to the given"
#~ " thread axis. It requires: 1) The "
#~ "scope block that the loop is in"
#~ " should have stage-pipeline property "
#~ "2) All the blocks under the loop"
#~ " are complete blocks or reduction "
#~ "blocks, and have affine bindings 3) "
#~ "For each block under the loop, if"
#~ " the thread axis starts with "
#~ "\"threadIdx`, the loop can only be "
#~ "contained in data-parallel block iter"
#~ " and reduction block iters' bindings. "
#~ "Otherwise the loop can only be "
#~ "contained in data-parallel block iters'"
#~ " bindings"
#~ msgstr ""

#~ msgid "The loop to be bound to the thread axis"
#~ msgstr ""

#~ msgid ""
#~ "The thread axis to be bound to "
#~ "the loop. Possible candidates: - "
#~ "blockIdx.x/y/z - threadIdx.x/y/z - "
#~ "vthread.x/y/z - vthread (It is a "
#~ "legacy behavior that will be deprecated."
#~ " Please use `vthread.x/y/z` instead.)"
#~ msgstr ""

#~ msgid "Before bind, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do bind:"
#~ msgstr ""

#~ msgid "After applying bind, the IR becomes:"
#~ msgstr ""

#~ msgid "The root of the subtree."
#~ msgstr ""

#~ msgid "**result** -- The new block."
#~ msgstr ""

#~ msgid "Before blockize, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do set_scope:"
#~ msgstr ""

#~ msgid "After applying blockize, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "blockize requires there is exactly one"
#~ " block under the given loop and "
#~ "the bindings of the block are "
#~ "divisible by the subspace represented by"
#~ " the loops starting at the given "
#~ "loop."
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a read cache. It "
#~ "requires:"
#~ msgstr ""

#~ msgid "There is at most one block who write the buffer in the scope."
#~ msgstr ""

#~ msgid "The scope block have stage-pipeline property."
#~ msgstr ""

#~ msgid "The consumer block of the target buffer."
#~ msgstr ""

#~ msgid "The index of the buffer in block's read region."
#~ msgstr ""

#~ msgid "The target storage scope."
#~ msgstr ""

#~ msgid "**cached_block** -- The block of the cache stage"
#~ msgstr ""

#~ msgid "Before cache_read, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and cache_read:"
#~ msgstr ""

#~ msgid "After applying cache_read, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Create a block that reads a buffer"
#~ " region into a write cache. It "
#~ "requires:"
#~ msgstr ""

#~ msgid "There is only one block who write the buffer in the scope."
#~ msgstr ""

#~ msgid "The producer block of the target buffer."
#~ msgstr ""

#~ msgid "The index of the buffer in block's write region."
#~ msgstr ""

#~ msgid "Before cache_write, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and cache_write:"
#~ msgstr ""

#~ msgid "After applying cache_write, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Compute-At. Move a producer block "
#~ "under the specific loop, and regenerate"
#~ " the loops induced by the block "
#~ "so that the buffer region produced "
#~ "by the producer block could cover "
#~ "those regions consumed by its consumer"
#~ " blocks under the given loop. It "
#~ "requires:"
#~ msgstr ""

#~ msgid ""
#~ "`block` and `loop` are under the "
#~ "same scope, `loop` is not the "
#~ "ancestor of `block`"
#~ msgstr ""

#~ msgid "The scope block has stage-pipeline property"
#~ msgstr ""

#~ msgid ""
#~ "3) The subtree of the scope block,"
#~ " where the given block is in, "
#~ "satisfies the compact dataflow condition. "
#~ "i.e. all the blocks in the scope"
#~ " block's subtree must be either "
#~ "complete block or reduction block"
#~ msgstr ""

#~ msgid ""
#~ "4) The block is not an output "
#~ "block with regard to the scope "
#~ "block, i.e. the buffers written by "
#~ "the block are allocated under the "
#~ "scope block"
#~ msgstr ""

#~ msgid "All the consumers of the block are under the given loop"
#~ msgstr ""

#~ msgid "The block to be moved"
#~ msgstr ""

#~ msgid "The loop where the block to be moved under"
#~ msgstr ""

#~ msgid "Whether to keep the trivial loops whose extents are 1"
#~ msgstr ""

#~ msgid "Before compute-at, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do compute-at:"
#~ msgstr ""

#~ msgid "After applying compute-at, the IR becomes:"
#~ msgstr ""

#~ msgid "Inline a block into its consumer(s). It requires:"
#~ msgstr ""

#~ msgid "The block is a complete non-root block, which only produces one buffer"
#~ msgstr ""

#~ msgid "The block must not be the only leaf in the scope."
#~ msgstr ""

#~ msgid ""
#~ "The body of the block must be "
#~ "a BufferStore statement in the form "
#~ "of, ``A[i, j, k, ...] = ...`` "
#~ "where the indices of the LHS are"
#~ " all distinct atomic variables, and "
#~ "no variables other than those indexing"
#~ " variables are allowed in the "
#~ "statement."
#~ msgstr ""

#~ msgid "The block to be inlined to its consumer(s)"
#~ msgstr ""

#~ msgid "Before compute-inline, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do compute-inline:"
#~ msgstr ""

#~ msgid "After applying compute-inline, the IR becomes:"
#~ msgstr ""

#~ msgid "**copy** -- A new copy of the schedule"
#~ msgstr ""

#~ msgid ""
#~ "The init block, which is translated "
#~ "from the init statement of the "
#~ "reduction block;"
#~ msgstr ""

#~ msgid "The update block, which is the original block without init statement."
#~ msgstr ""

#~ msgid "The init block is inserted right before the given loop."
#~ msgstr ""

#~ msgid "The schedule primitive requires:"
#~ msgstr ""

#~ msgid "The input block is a reduction block."
#~ msgstr ""

#~ msgid "The input loop is the ancestor of the block."
#~ msgstr ""

#~ msgid ""
#~ "The input loop is not lower than"
#~ " all the loops related to reduce "
#~ "block var."
#~ msgstr ""

#~ msgid "The reduction block to be decomposed"
#~ msgstr ""

#~ msgid "The loop above which the init block is inserted before."
#~ msgstr ""

#~ msgid "**init_block** -- The init block"
#~ msgstr ""

#~ msgid "Before decompose-reduction, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do decompose-reduction with specified loop:"
#~ msgstr ""

#~ msgid "After applying decompose-reduction, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "**seed** -- The forked random state, "
#~ "not the same as the current random"
#~ " state"
#~ msgstr ""

#~ msgid ""
#~ "Fuse a list of consecutive loops "
#~ "into one. It requires: 1) The "
#~ "loops can't have annotations or thread"
#~ " bindings. 2) The (i+1)-th loop must"
#~ " be the only child of the i-th"
#~ " loop. 3) All loops must start "
#~ "with 0. 4) The domain of a "
#~ "loop to be fused cannot depend on"
#~ " another loop to be fused."
#~ msgstr ""

#~ msgid "The loops to be fused"
#~ msgstr ""

#~ msgid "**fused_loop** -- The new loop after fusion"
#~ msgstr ""

#~ msgid "Before applying fuse, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do fuse:"
#~ msgstr ""

#~ msgid "After applying fuse, the IR becomes:"
#~ msgstr ""

#~ msgid "The random variable / sref to be evaluated"
#~ msgstr ""

#~ msgid "**result** -- The corresponding result"
#~ msgstr ""

#~ msgid "The name of the block"
#~ msgstr ""

#~ msgid "The name of the function"
#~ msgstr ""

#~ msgid ""
#~ "**block** -- The block retrieved "
#~ "IndexError is raised if 0 or "
#~ "multiple blocks exist with the specific"
#~ " name."
#~ msgstr ""

#~ msgid "The query block/loop"
#~ msgstr ""

#~ msgid "**blocks** -- A list of leaf blocks inside a specific block/loop"
#~ msgstr ""

#~ msgid "The block in the query"
#~ msgstr ""

#~ msgid "**consumers** -- A list of consumers of the given block"
#~ msgstr ""

#~ msgid "The query block"
#~ msgstr ""

#~ msgid ""
#~ "**loops** -- A list of loops above"
#~ " the given block in its scope, "
#~ "from outer to inner"
#~ msgstr ""

#~ msgid "**producers** -- A list of producers of the given block"
#~ msgstr ""

#~ msgid ""
#~ "Parallelize the input loop. It requires:"
#~ " 1) The scope block that the "
#~ "loop is in should have stage-"
#~ "pipeline property 2) All the blocks "
#~ "under the loop are complete blocks "
#~ "or reduction blocks, and have affine "
#~ "bindings 3) For each block under "
#~ "the loop, the loop can only be "
#~ "contained in data-parallel block iters'"
#~ " bindings"
#~ msgstr ""

#~ msgid "The loop to be parallelized"
#~ msgstr ""

#~ msgid "Before parallel, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do parallel:"
#~ msgstr ""

#~ msgid "After applying parallel, the IR becomes:"
#~ msgstr ""

#~ msgid "The random variable to be removed"
#~ msgstr ""

#~ msgid ""
#~ "Reorder a list of loops. It "
#~ "doesn't require the loops to be "
#~ "consecutive. It requires: 1) The loops"
#~ " are in the same chain. That "
#~ "means: the loops can be ordered to"
#~ " [l_1, l_2, ... , l_n] where "
#~ "l_i is an ancestor of l_{i+1} and"
#~ " there are only single-branch loops"
#~ " between l_1 and l_n (which also "
#~ "indicates they are under the same "
#~ "scope). 2) After reordering, the domain"
#~ " of an outer loop cannot depend "
#~ "on any of the inner loops. 3) "
#~ "For every block under the loop "
#~ "nests, its block binding must be "
#~ "affine, and the block variables must "
#~ "be either data parallel or reduction."
#~ " 4) No duplicated loops are allowed"
#~ " in the arguments."
#~ msgstr ""

#~ msgid "The loops in the new order"
#~ msgstr ""

#~ msgid "Before reorder, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reorder:"
#~ msgstr ""

#~ msgid "After applying reorder, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Reverse-Compute-At. Move a consumer "
#~ "block under the specific loop, and "
#~ "regenerate the loops induced by the "
#~ "block so that the buffer region "
#~ "consumed by the consumer block could "
#~ "cover those regions produced by its "
#~ "producer blocks under the given loop."
#~ " It requires:"
#~ msgstr ""

#~ msgid "All the producers of the block are under the given loop"
#~ msgstr ""

#~ msgid "Before reverse-compute-at, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reverse-compute-at:"
#~ msgstr ""

#~ msgid "After applying reverse-compute-at, the IR becomes:"
#~ msgstr ""

#~ msgid "Inline a block into its only producer. It requires:"
#~ msgstr ""

#~ msgid ""
#~ "The block is a complete non-root"
#~ " block, which only produces and "
#~ "consumes one buffer"
#~ msgstr ""

#~ msgid ""
#~ "The only producer of the block is"
#~ " a read-after-write producer and "
#~ "a complete non-root block"
#~ msgstr ""

#~ msgid ""
#~ "The body of the block must be "
#~ "a BufferStore statement in the form "
#~ "of, ``B[f(i, j, k, ...)] = g(i,"
#~ " j, k, A[i, j, k, ...] ...)``"
#~ " where the indices of each "
#~ "`BufferLoad` on the RHS are all "
#~ "distinct atomic variables, and no "
#~ "variables other than those indexing "
#~ "variables are allowed in the statement."
#~ msgstr ""

#~ msgid "The block to be inlined to its producer"
#~ msgstr ""

#~ msgid "Before reverse-compute-inline, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do reverse-compute-inline:"
#~ msgstr ""

#~ msgid "After applying reverse-compute-inline, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "An associative reduction cannot be "
#~ "parallelized directly, because it leads "
#~ "to potential race condition during "
#~ "accumulation. Alternatively, the reduction "
#~ "could be factorized on a loop with"
#~ " the following steps: - Step 1: "
#~ "evenly slice the reduction into `n` "
#~ "separate chunks, where `n` is the "
#~ "loop extent - Step 2: compute the"
#~ " chunks separately and write the "
#~ "result into `n` intermediate buffers; -"
#~ " Step 3: accumulate the `n` separate"
#~ " buffer into the result buffer. Note"
#~ " that the Step 2 above introduces "
#~ "opportunities for parallelization."
#~ msgstr ""

#~ msgid ""
#~ "RFactor is a schedule primitive that "
#~ "implements the transformation described above:"
#~ " Given a block that writes to "
#~ "buffer `B`, it factorizes a loop "
#~ "of extent `n`."
#~ msgstr ""

#~ msgid ""
#~ "For example, the pseudocode below "
#~ "accumulates `B[i] = sum(A[i, : , :"
#~ " ])`:"
#~ msgstr ""

#~ msgid ""
#~ "Suppose RFactor is applied on the "
#~ "innermost loop `k` and `factor_axis ="
#~ " 1`. RFactor then creates an "
#~ "intermediate buffer and two blocks."
#~ msgstr ""

#~ msgid ""
#~ "1. The intermediate buffer, or \"rf-"
#~ "buffer\" is a buffer of rank "
#~ "`ndim(B) + 1` and size `size(B) *"
#~ " n`, whose shape expands from "
#~ "`shape(B)` by adding an axis of "
#~ "`n` at the position specified by "
#~ "`factor_axis`. For example,"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 0  => shape(B_rf) = [n, 1, 2, 3]"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 1  => shape(B_rf) = [1, n, 2, 3]"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 2  => shape(B_rf) = [1, 2, n, 3]"
#~ msgstr ""

#~ msgid "shape(B) = [1, 2, 3], factor_axis = 3  => shape(B_rf) = [1, 2, 3, n]"
#~ msgstr ""

#~ msgid ""
#~ "2. The rfactor block, or \"rf-"
#~ "block\", is a block that writes to"
#~ " the `rf-buffer` without accumulating "
#~ "over the loop `k`, i.e. the loop"
#~ " `k` is converted from a reduction"
#~ " loop to a data parallel loop. "
#~ "In our example, the rf-block is:"
#~ msgstr ""

#~ msgid ""
#~ "3. The write-back block, or "
#~ "`wb-block`, is a block that "
#~ "accumulates the rf-buffer into the "
#~ "result buffer. All the reduction loops"
#~ " are removed except the loop `k` "
#~ "for accumulation. In our example, the"
#~ " wb-block is:"
#~ msgstr ""

#~ msgid "The loop outside block for which we want to do rfactor"
#~ msgstr ""

#~ msgid ""
#~ "The position where the new dimension "
#~ "is placed in the new introduced "
#~ "rfactor buffer"
#~ msgstr ""

#~ msgid ""
#~ "**rf_block** -- The block which computes"
#~ " partial results over each slices "
#~ "(i.e., the first block as described "
#~ "in the above illustration)"
#~ msgstr ""

#~ msgid "Before rfactor, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do rfactor:"
#~ msgstr ""

#~ msgid "After applying rfactor, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Rfactor requires: 1) `loop` has only "
#~ "one child block, and it is a "
#~ "reduction block; 2) `loop` is a "
#~ "reduction loop, i.e. the loop variable"
#~ " is bound to only reduction variables"
#~ " in the block binding; 3) `loop` "
#~ "is not parallelized, vectorized, unrolled "
#~ "or bound to any thread axis; 4)"
#~ " The block scope that `loop` is "
#~ "in is a staged-pipeline; 5) The"
#~ " outermost loop outside the reduction "
#~ "block should has the reduction block "
#~ "as its first child block; 6) The"
#~ " outermost reduction loop should have "
#~ "only one child block; 7) An unary"
#~ " extent loop that is not bound "
#~ "to any reduction or data parallel "
#~ "variables in the block binding should"
#~ " not appear under some reduction "
#~ "loop; 8) The reduction block should "
#~ "write to only one buffer, and its"
#~ " init and body are both simple "
#~ "`BufferStore`s, and the pattern is "
#~ "registered as an associative reducer. "
#~ "The pre-defined patterns include: plus,"
#~ " multiplication, min and max; 9) Each"
#~ " of the loops on top of the "
#~ "block cannot be bound to a data"
#~ " parallel and a reduction block "
#~ "binding at the same time; 10) "
#~ "`factor_axis` should be in range "
#~ "`[-ndim(B) - 1, ndim(B)]`, where `B` "
#~ "is the buffer that the reduction "
#~ "block writes to. Negative indexing is"
#~ " normalized according to numpy convention."
#~ msgstr ""

#~ msgid "The candidates to be sampled from"
#~ msgstr ""

#~ msgid "The probability of each candidate"
#~ msgstr ""

#~ msgid "The sampling decision, if any"
#~ msgstr ""

#~ msgid "**result** -- The random variable sampled from candidates"
#~ msgstr ""

#~ msgid "The block whose compute-at location is to be sampled"
#~ msgstr ""

#~ msgid "The sampling decision"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The sampled loop where "
#~ "the input block is to be computed"
#~ " at"
#~ msgstr ""

#~ msgid "The loop to be tiled"
#~ msgstr ""

#~ msgid "The number of tiles to be sampled"
#~ msgstr ""

#~ msgid "The maximum tile size allowed to be sampled in the innermost loop"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- A list of length "
#~ "`n`, the random perfect tile sizes "
#~ "sampled"
#~ msgstr ""

#~ msgid "The new random seed, -1 if use device random, otherwise non-negative"
#~ msgstr ""

#~ msgid "The producer block of the buffer"
#~ msgstr ""

#~ msgid "The index of the buffer in block's write region"
#~ msgstr ""

#~ msgid "The storage scope to be set"
#~ msgstr ""

#~ msgid "Before set_scope, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "After applying set_scope, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Set_scope requires the buffer to be "
#~ "an intermediate buffer defined via "
#~ "`alloc_buffer`."
#~ msgstr ""

#~ msgid "The random variable to be evaluated"
#~ msgstr ""

#~ msgid "**str_repr** -- The string representation"
#~ msgstr ""

#~ msgid ""
#~ "Split a loop into a list of "
#~ "consecutive loops. It requires: 1) The"
#~ " loop can't have annotation or thread"
#~ " binding. 2) The loop must start "
#~ "with 0. Predicates may be added to"
#~ " ensure the total loop numbers keeps"
#~ " unchanged. In `factors`, at most one"
#~ " of the factors can be None, "
#~ "which will be automatically inferred."
#~ msgstr ""

#~ msgid "The loop to be split"
#~ msgstr ""

#~ msgid ""
#~ "The splitting factors Potential inputs "
#~ "are: - None - ExprRV - Positive"
#~ " constant integers"
#~ msgstr ""

#~ msgid "**split_loops** -- The new loops after split"
#~ msgstr ""

#~ msgid "Before split, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do split:"
#~ msgstr ""

#~ msgid "After applying split, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Set alignment requirement for specific "
#~ "dimension such that stride[axis] == k"
#~ " * factor + offset for some k."
#~ " This is useful to set memory "
#~ "layout for more friendly memory access"
#~ " pattern. For example, we can set "
#~ "alignment to be factor=2, offset=1 to"
#~ " avoid bank conflict for thread "
#~ "access on higher dimension in GPU "
#~ "shared memory."
#~ msgstr ""

#~ msgid "The producer block of the buffer."
#~ msgstr ""

#~ msgid "The dimension to be specified for alignment."
#~ msgstr ""

#~ msgid "The factor multiple of alignment."
#~ msgstr ""

#~ msgid "The required offset factor."
#~ msgstr ""

#~ msgid "Before storage_align, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do storage_align:"
#~ msgstr ""

#~ msgid "After applying storage_align, the IR becomes:"
#~ msgstr ""

#~ msgid "After lowering passes, buffer B will have strides as [129, 1]."
#~ msgstr ""

#~ msgid ""
#~ "Storage_align requires the buffer to be"
#~ " an intermediate buffer defined via "
#~ "`alloc_buffer`."
#~ msgstr ""

#~ msgid "The loop to be tensorized."
#~ msgstr ""

#~ msgid "The tensor intrin or the name of the tensor intrin."
#~ msgstr ""

#~ msgid "Before tensorize, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Declare and register the tensor intrinsic:"
#~ msgstr ""

#~ msgid "Create the schedule and do tensorize:"
#~ msgstr ""

#~ msgid "After applying tensorize, the IR becomes:"
#~ msgstr ""

#~ msgid "The block/loop to be unannotated"
#~ msgstr ""

#~ msgid "Before unannotate, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "After applying unannotate, the IR becomes:"
#~ msgstr ""

#~ msgid "Unroll the input loop. It requires nothing"
#~ msgstr ""

#~ msgid "The loop to be unrolled"
#~ msgstr ""

#~ msgid "Before unroll, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do unroll:"
#~ msgstr ""

#~ msgid "After applying unroll, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "Vectorize the input loop. It requires:"
#~ " 1) The scope block that the "
#~ "loop is in should have stage-"
#~ "pipeline property 2) All the blocks "
#~ "under the loop are complete blocks "
#~ "or reduction blocks, and have affine "
#~ "bindings 3) For each block under "
#~ "the loop, the loop can only be "
#~ "contained in data-parallel block iters'"
#~ " bindings"
#~ msgstr ""

#~ msgid "The loop to be vectorized"
#~ msgstr ""

#~ msgid "Before vectorize, in TensorIR, the IR is:"
#~ msgstr ""

#~ msgid "Create the schedule and do vectorize:"
#~ msgstr ""

#~ msgid "After applying vectorize, the IR becomes:"
#~ msgstr ""

#~ msgid ""
#~ "The data structure contains the "
#~ "following information 1) The AST being"
#~ " scheduled (mod) 2) The sref tree "
#~ "of schedulable statements (indicated by "
#~ "the srefs) 3) The dependency information"
#~ " of each block scope (block_info) 4)"
#~ " A reverse mapping from the AST "
#~ "nodes to that in the sref tree "
#~ "(get_sref) 5) A debug flag, if "
#~ "set, extra checking is enabled "
#~ "(debug_mask)"
#~ msgstr ""

#~ msgid "The AST of the module being scheduled"
#~ msgstr ""

#~ msgid ""
#~ "Do extra correctness checking after the"
#~ " object construction and each time "
#~ "after calling the Replace method."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_block_scope "
#~ "<tvm.tir.ScheduleState.get_block_scope>`\\ \\(block\\_sref\\)"
#~ msgstr ""

#~ msgid "Get the BlockScope correpsonding to the block sref"
#~ msgstr ""

#~ msgid ":py:obj:`get_sref <tvm.tir.ScheduleState.get_sref>`\\ \\(stmt\\)"
#~ msgstr ""

#~ msgid "Return the corresponding sref that points to the stmt"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`replace <tvm.tir.ScheduleState.replace>`\\ "
#~ "\\(src\\_sref\\, tgt\\_stmt\\[\\, "
#~ "block\\_sref\\_reuse\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Replace the part of the AST, as"
#~ " being pointed to by `src_sref`, with"
#~ " a specific statement `tgt_stmt`, and "
#~ "maintain the sref tree accordingly."
#~ msgstr ""

#~ msgid "The block sref to be retrieved"
#~ msgstr ""

#~ msgid "**sref** -- The corresponding sref"
#~ msgstr ""

#~ msgid "The schedulable statement in the TensorIR to be retrieved for its sref"
#~ msgstr ""

#~ msgid ""
#~ "Replace the part of the AST, as"
#~ " being pointed to by `src_sref`, with"
#~ " a specific statement `tgt_stmt`, and "
#~ "maintain the sref tree accordingly. "
#~ "Replace will try to perform copy "
#~ "on write as much as possible when"
#~ " the ScheduleState holds the only "
#~ "copy to the IRModule and IR nodes."
#~ msgstr ""

#~ msgid ""
#~ "Only 3 types of replacements are "
#~ "allowed: from `src_sref->stmt` to `tgt_stmt`."
#~ " 1) Block -> Block 2) Loop ->"
#~ " Loop 3) Loop -> BlockRealize"
#~ msgstr ""

#~ msgid "The sref to the statement to be replaced in the TensorIR AST"
#~ msgstr ""

#~ msgid "The statement to be replaced to"
#~ msgstr ""

#~ msgid ""
#~ "Maps an old block (to be replaced"
#~ " in the subtree under `src_sref->stmt`) "
#~ "to a new block (replaced to, in"
#~ " the subtree under `tgt_stmt`), and "
#~ "enforces reuse of srefs between them "
#~ "(rather than create new srefs) i.e. "
#~ "after being replaced, the sref that "
#~ "points to the old block will point"
#~ " to the new one"
#~ msgstr ""

#~ msgid ""
#~ "The reuse of loop srefs are "
#~ "detected automatically according to the "
#~ "reuse of loop vars."
#~ msgstr ""

#~ msgid ""
#~ "Select may compute both true_value and"
#~ " false_value. Use :py:class:`tvm.tir.if_then_else` "
#~ "instead if you want to get a "
#~ "conditional expression that only evaluates "
#~ "the correct branch."
#~ msgstr ""

#~ msgid "The condition expression."
#~ msgstr ""

#~ msgid "The value to take when condition is true."
#~ msgstr ""

#~ msgid "The value to take when condition is false."
#~ msgstr ""

#~ msgid "The statements"
#~ msgstr ""

#~ msgid "The vectors"
#~ msgstr ""

#~ msgid "The indices"
#~ msgstr ""

#~ msgid "which is greater or equal to zero."
#~ msgstr ""

#~ msgid "The name"
#~ msgstr ""

#~ msgid ""
#~ "Glossary - Block sref: An StmtSref "
#~ "that points to a TensorIR block. -"
#~ " Loop sref: An StmtSRef that points"
#~ " to a TensorIR for loop. - "
#~ "Parent sref: The parent sref of an"
#~ " sref is the block/loop sref that "
#~ "points to its closest schedulable "
#~ "statement of its ancestors on the "
#~ "TensorIR AST. - Root sref: Sref to"
#~ " the root block. Every sref has "
#~ "exactly one parent sref except for "
#~ "root sref. - Sref tree: The "
#~ "parent-children-relationship of srefs that"
#~ " forms a tree, uniquely determined by"
#~ " the TensorIR AST."
#~ msgstr ""

#~ msgid ":py:obj:`inline_mark <tvm.tir.StmtSRef.inline_mark>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "A special StmtSRef, which doesn't point"
#~ " to any stmt in the AST, only"
#~ " serving as a \"mark\" to hint "
#~ "compute-at to do the work of "
#~ "compute-inline"
#~ msgstr ""

#~ msgid ":py:obj:`root_mark <tvm.tir.StmtSRef.root_mark>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "A special StmtSRef, which doesn't point"
#~ " to any stmt in the AST, only"
#~ " serving as a \"mark\" to hint "
#~ "compute-at to do nothing"
#~ msgstr ""

#~ msgid ":py:obj:`parent <tvm.tir.StmtSRef.parent>`\\"
#~ msgstr ""

#~ msgid "The parent sref"
#~ msgstr ""

#~ msgid ":py:obj:`stmt <tvm.tir.StmtSRef.stmt>`\\"
#~ msgstr ""

#~ msgid "The block/for stmt the object refers to"
#~ msgstr ""

#~ msgid "The buffer Variable."
#~ msgstr ""

#~ msgid "The value we want to store."
#~ msgstr ""

#~ msgid "The index in the store expression."
#~ msgstr ""

#~ msgid "The store predicate."
#~ msgstr ""

#~ msgid "The function to describe the computation."
#~ msgstr ""

#~ msgid "The function of the implementation for the execution."
#~ msgstr ""

#~ msgid ":py:obj:`get <tvm.tir.TensorIntrin.get>`\\ \\(name\\)"
#~ msgstr ""

#~ msgid "Look up a tensor intrinsic by its name."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`register <tvm.tir.TensorIntrin.register>`\\ "
#~ "\\(name\\, desc\\, impl\\)"
#~ msgstr ""

#~ msgid "Register a tensor intrinsic with its name."
#~ msgstr ""

#~ msgid "The name of the TensorIntrin to look up."
#~ msgstr ""

#~ msgid "**result** -- The TensorIntrin with the specified name."
#~ msgstr ""

#~ msgid "The name of the TensorIntrin to register."
#~ msgstr ""

#~ msgid "The termination condition."
#~ msgstr ""

#~ msgid "Input argument."
#~ msgstr ""

#~ msgid "The location of this operator in the source code."
#~ msgstr ""

#~ msgid "**y** -- The result."
#~ msgstr ""

#~ msgid "arguments"
#~ msgstr ""

#~ msgid "List of symbolic boolean expressions"
#~ msgstr ""

#~ msgid "**expr** -- Expression"
#~ msgstr ""

#~ msgid "**bijective_layout** -- The created bijective layout"
#~ msgstr ""

#~ msgid "The data type of the result."
#~ msgstr ""

#~ msgid "The extern function name."
#~ msgstr ""

#~ msgid "Positional arguments."
#~ msgstr ""

#~ msgid "**call** -- The call expression."
#~ msgstr ""

#~ msgid ""
#~ "Intrinsics can be overloaded with "
#~ "multiple data types via the intrinsic"
#~ " translation rule."
#~ msgstr ""

#~ msgid "The intrinsic function name."
#~ msgstr ""

#~ msgid "The name of the llvm intrinsic function."
#~ msgstr ""

#~ msgid "Poistional arguments."
#~ msgstr ""

#~ msgid ""
#~ "The argument to packed function can "
#~ "be Expr or Buffer. The argument is"
#~ " the corresponding POD type when Expr"
#~ " is presented."
#~ msgstr ""

#~ msgid ""
#~ "When the argument is Buffer, the "
#~ "corresponding PackedFunc will recieve an "
#~ "TVMArrayHandle whose content is valid "
#~ "during the callback period. If the "
#~ "PackedFunc is a python callback, then"
#~ " the corresponding argument is NDArray."
#~ msgstr ""

#~ msgid ":obj:`te.extern`"
#~ msgstr ""

#~ msgid "Create tensor with extern function call."
#~ msgstr ""

#~ msgid "Input 32 or 64 bit integer. The result is undefined if the input is 0."
#~ msgstr ""

#~ msgid "A binary function which takes two Expr as input to return a Expr."
#~ msgstr ""

#~ msgid "A function which takes a type string as input to return a const Expr."
#~ msgstr ""

#~ msgid ""
#~ "**reducer** -- A function which creates"
#~ " a reduce expression over axis. There"
#~ " are two ways to use it:  1."
#~ " accept (expr, axis, where) to "
#~ "produce an Reduce Expr on    specified"
#~ " axis; 2. simply use it with "
#~ "multiple Exprs."
#~ msgstr ""

#~ msgid ""
#~ "**reducer** -- A function which creates"
#~ " a reduce expression over axis. There"
#~ " are two ways to use it:"
#~ msgstr ""

#~ msgid "accept (expr, axis, where) to produce an Reduce Expr on specified axis;"
#~ msgstr ""

#~ msgid "simply use it with multiple Exprs."
#~ msgstr ""

#~ msgid "示例"
#~ msgstr ""

#~ msgid ""
#~ "Normally buffer is created automatically "
#~ "during lower and build. This is "
#~ "only needed if user want to "
#~ "specify their own buffer layout."
#~ msgstr ""

#~ msgid "See the note below for detailed discussion on usage of buffer."
#~ msgstr ""

#~ msgid "The shape of the buffer."
#~ msgstr ""

#~ msgid "The name of the buffer."
#~ msgstr ""

#~ msgid "The data pointer in the buffer."
#~ msgstr ""

#~ msgid "The stride of the buffer."
#~ msgstr ""

#~ msgid ""
#~ "The beginning offset of the array "
#~ "to data. In terms of number of "
#~ "elements of dtype."
#~ msgstr ""

#~ msgid ""
#~ "The storage scope of the buffer, "
#~ "if not global. If scope equals "
#~ "empty string, it means it is "
#~ "global memory."
#~ msgstr ""

#~ msgid ""
#~ "The alignment of data pointer in "
#~ "bytes. If -1 is passed, the "
#~ "alignment will be set to TVM's "
#~ "internal default."
#~ msgstr ""

#~ msgid ""
#~ "The factor of elem_offset field, when"
#~ " set, elem_offset is required to be"
#~ " multiple of offset_factor. If 0 is"
#~ " pssed, the alignment will be set "
#~ "to 1. if non-zero is passed, "
#~ "we will created a Var for "
#~ "elem_offset if elem_offset is not None."
#~ msgstr ""

#~ msgid ""
#~ "auto_broadcast buffer allows one to "
#~ "implement broadcast computation without "
#~ "considering whether dimension size equals "
#~ "to one. TVM maps buffer[i][j][k] -> "
#~ "buffer[i][0][k] if dimension j's shape "
#~ "equals 1."
#~ msgstr ""

#~ msgid "The location of the decl_buffer creation in the source."
#~ msgstr ""

#~ msgid "**buffer** -- The created buffer"
#~ msgstr ""

#~ msgid ""
#~ "Here's an example of how broadcast "
#~ "buffer can be used to define a "
#~ "symbolic broadcast operation,"
#~ msgstr ""

#~ msgid ""
#~ "Buffer data structure reflects the "
#~ "DLTensor structure in dlpack. While "
#~ "DLTensor data structure is very general,"
#~ " it is usually helpful to create "
#~ "function that only handles specific case"
#~ " of data structure and make compiled"
#~ " function benefit from it."
#~ msgstr ""

#~ msgid ""
#~ "If user pass strides and elem_offset "
#~ "is passed as None when constructing "
#~ "the function, then the function will "
#~ "be specialized for the DLTensor that "
#~ "is compact and aligned. If user "
#~ "pass a fully generic symbolic array "
#~ "to the strides, then the resulting "
#~ "function becomes fully generic."
#~ msgstr ""

#~ msgid "The left hand operand, known to be non-negative."
#~ msgstr ""

#~ msgid "The right hand operand, known to be non-negative."
#~ msgstr ""

#~ msgid "The location of this operator in the source."
#~ msgstr ""

#~ msgid "**res** -- The result expression."
#~ msgstr ""

#~ msgid "When operands are integers, returns truncdiv(a, b, span)."
#~ msgstr ""

#~ msgid "The left hand operand"
#~ msgstr ""

#~ msgid "The right hand operand"
#~ msgstr ""

#~ msgid "**z** -- The result."
#~ msgstr ""

#~ msgid "The condition"
#~ msgstr ""

#~ msgid "The result expression if cond is true."
#~ msgstr ""

#~ msgid "The result expression if cond is false."
#~ msgstr ""

#~ msgid "**result** -- The result of conditional expression."
#~ msgstr ""

#~ msgid ""
#~ "Unlike Select, if_then_else will not "
#~ "execute the branch that does not "
#~ "satisfy the condition. You can use "
#~ "it to guard against out of bound"
#~ " access. Unlike Select, if_then_else cannot"
#~ " be vectorized if some lanes in "
#~ "the vector have different conditions."
#~ msgstr ""

#~ msgid ""
#~ "Use this function to split non-"
#~ "negative indices. This function may take"
#~ " advantage of operands' non-negativeness."
#~ msgstr ""

#~ msgid "Compute the remainder of indexdiv. a and b are non-negative."
#~ msgstr ""

#~ msgid ""
#~ "A layout representation is composed of"
#~ " upper cases, lower cases and "
#~ "numbers, where upper case indicates a"
#~ " primal axis and the corresponding "
#~ "lower case with factor size indicates"
#~ " the subordinate axis. For example, "
#~ "NCHW16c can describe a 5-D tensor "
#~ "of [batch_size, channel, height, width, "
#~ "channel_block]. Here subordinate axis "
#~ "channel_block=16 is the factor size of"
#~ " the primal axis C (channel)."
#~ msgstr ""

#~ msgid "**layout** -- The created layout"
#~ msgstr ""

#~ msgid "The reduction IterVar axis"
#~ msgstr ""

#~ msgid "Filtering predicate of the reduction."
#~ msgstr ""

#~ msgid "**value** -- The result value."
#~ msgstr ""

#~ msgid "**value** -- The maximum value of dtype."
#~ msgstr ""

#~ msgid "**value** -- The minimum value of dtype."
#~ msgstr ""

#~ msgid ""
#~ "Round elements of the array to the"
#~ " nearest integer. This intrinsic uses "
#~ "llvm.nearbyint instead of llvm.round which "
#~ "is faster but will results different "
#~ "from te.round. Notably nearbyint rounds "
#~ "according to the rounding mode, whereas"
#~ " te.round (llvm.round) ignores that. For"
#~ " differences between the two see: "
#~ "https://en.cppreference.com/w/cpp/numeric/math/round "
#~ "https://en.cppreference.com/w/cpp/numeric/math/nearbyint"
#~ msgstr ""

#~ msgid "The exponent"
#~ msgstr ""

#~ msgid ""
#~ "Execute a multiplication between two "
#~ "Q-numbers x and y followed by a"
#~ " right shift s. The mathematical "
#~ "expression is:"
#~ msgstr ""

#~ msgid "out = round(x*y*2^-s)"
#~ msgstr ""

#~ msgid ""
#~ "More about Q-numbers here: "
#~ "https://en.wikipedia.org/wiki/Q_(number_format) The "
#~ "rounding rule is to the nearest "
#~ "value, rounding half up (i.e., "
#~ "round(x.1) = x and round (x.5) ="
#~ " x+1)"
#~ msgstr ""

#~ msgid "First Q-number"
#~ msgstr ""

#~ msgid "Second Q-number"
#~ msgstr ""

#~ msgid "Number of fractional bits in x and y. Needs to be > 0"
#~ msgstr ""

#~ msgid "Integer shift"
#~ msgstr ""

#~ msgid ""
#~ "The returned tir expression, whose data"
#~ " type is int, float or void "
#~ "pointer."
#~ msgstr ""

#~ msgid "**ret** -- The return expression"
#~ msgstr ""

#~ msgid "**stmt_list** -- The unpacked list of statements"
#~ msgstr ""

#~ msgid "List of statements to be combined as sequence."
#~ msgstr ""

#~ msgid "**stmt** -- The combined statement."
#~ msgstr ""

#~ msgid ""
#~ "The trace function allows to trace "
#~ "specific tensor at the runtime. The "
#~ "tracing value should come as last "
#~ "argument. The trace action should be "
#~ "specified, by default tvm.default_trace_action "
#~ "is used."
#~ msgstr ""

#~ msgid "The name of the trace action."
#~ msgstr ""

#~ msgid ":obj:`tvm.tir.call_packed`"
#~ msgstr ""

#~ msgid "Creates packed function."
#~ msgstr ""

#~ msgid ""
#~ "The truncated value of the scalar "
#~ "x is the nearest integer i which"
#~ " is closer to zero than x is."
#~ msgstr ""

#~ msgid "This is the default integer division behavior in C."
#~ msgstr ""

#~ msgid "Namespace of all TIR transformations"
#~ msgstr ""

#~ msgid ":py:obj:`Apply <tvm.tir.transform.Apply>`\\ \\(ftransform\\)"
#~ msgstr ""

#~ msgid "Apply ftransform to each function in the Module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BF16CastElimination "
#~ "<tvm.tir.transform.BF16CastElimination>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Eliminate verbose casting between fp32 "
#~ "and bf16 Checks if the AST has "
#~ "the pattern: castto32(castto16(some_fp32_op(...))) "
#~ "The verbose casting is generated by "
#~ "BF16Promote for multiple bf16 Ops in "
#~ "a row."
#~ msgstr ""

#~ msgid ":py:obj:`BF16Legalize <tvm.tir.transform.BF16Legalize>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Legalize bf16 typed Ops."
#~ msgstr ""

#~ msgid ":py:obj:`BF16Promote <tvm.tir.transform.BF16Promote>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Promote bf16 to fp32."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BF16TypeLowering "
#~ "<tvm.tir.transform.BF16TypeLowering>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Replace all bf16 type with uint16."
#~ msgstr ""

#~ msgid ":py:obj:`CoProcSync <tvm.tir.transform.CoProcSync>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Detect and insert sync points to co-processor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CombineContextCall "
#~ "<tvm.tir.transform.CombineContextCall>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Combine context calls in the host function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CommonSubexprElimTIR "
#~ "<tvm.tir.transform.CommonSubexprElimTIR>`\\ "
#~ "\\(\\[enable\\_cse\\_tir\\]\\)"
#~ msgstr ""

#~ msgid "Replace redundant computations by new variables."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CompactBufferAllocation "
#~ "<tvm.tir.transform.CompactBufferAllocation>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Compact the buffer access region."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ConvertBlocksToOpaque "
#~ "<tvm.tir.transform.ConvertBlocksToOpaque>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Substitute all the block vars with "
#~ "the PrimExprs they are bound to, "
#~ "indicated by the corresponding iter_values "
#~ "in BlockRealize, and then convert the"
#~ " blocks into opaque ones by removing"
#~ " all the iter_values in BlockRealize "
#~ "and iter_vars in Block."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ConvertForLoopsToSerial "
#~ "<tvm.tir.transform.ConvertForLoopsToSerial>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Convert Parallel For Loops to Serial For Loops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DecorateDeviceScope "
#~ "<tvm.tir.transform.DecorateDeviceScope>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Decorate all the function's body as device function."
#~ msgstr ""

#~ msgid ":py:obj:`Filter <tvm.tir.transform.Filter>`\\ \\(fcond\\)"
#~ msgstr ""

#~ msgid "Filter functions by the calling convention attribute."
#~ msgstr ""

#~ msgid ":py:obj:`FlattenBuffer <tvm.tir.transform.FlattenBuffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Flatten the multi-dimensional BufferLoad "
#~ "and BufferStore to single dimensional "
#~ "Load/Store."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`HoistIfThenElse <tvm.tir.transform.HoistIfThenElse>`\\"
#~ " \\(\\[variant\\]\\)"
#~ msgstr ""

#~ msgid "Hoist loop-invariant IfThenElse nodes to outside the eligible loops."
#~ msgstr ""

#~ msgid ":py:obj:`InferFragment <tvm.tir.transform.InferFragment>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Infer the TensorCore fragment infomation using tensor intrinsics."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectCopyIntrin "
#~ "<tvm.tir.transform.InjectCopyIntrin>`\\ \\(pragma\\_key\\, "
#~ "fintrin\\)"
#~ msgstr ""

#~ msgid "Inject virtual thread loops."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectDoubleBuffer "
#~ "<tvm.tir.transform.InjectDoubleBuffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Inject double buffer statements."
#~ msgstr ""

#~ msgid ":py:obj:`InjectPrefetch <tvm.tir.transform.InjectPrefetch>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Inject prefetch instructions into stmt."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectRollingBuffer "
#~ "<tvm.tir.transform.InjectRollingBuffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Inject rolling buffer statements."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InjectVirtualThread "
#~ "<tvm.tir.transform.InjectVirtualThread>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InstrumentBoundCheckers "
#~ "<tvm.tir.transform.InstrumentBoundCheckers>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Instruments bound checkers."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LegalizePackedCalls "
#~ "<tvm.tir.transform.LegalizePackedCalls>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Legalize packed calls to have its arguments wrapped in TVMValues"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LiftAttrScope <tvm.tir.transform.LiftAttrScope>`\\ "
#~ "\\(attr\\_key\\)"
#~ msgstr ""

#~ msgid "Lift common attrs with attr_key to outer scope."
#~ msgstr ""

#~ msgid ":py:obj:`LoopPartition <tvm.tir.transform.LoopPartition>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerCrossThreadReduction "
#~ "<tvm.tir.transform.LowerCrossThreadReduction>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Lower cross-thread reduction from thread"
#~ " bindings to intrinsic function calls."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerCustomDatatypes "
#~ "<tvm.tir.transform.LowerCustomDatatypes>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower custom datatypes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerDeviceStorageAccessInfo "
#~ "<tvm.tir.transform.LowerDeviceStorageAccessInfo>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower attached storage access information on device."
#~ msgstr ""

#~ msgid ":py:obj:`LowerInitBlock <tvm.tir.transform.LowerInitBlock>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower block init stmt into IfThenElse statements."
#~ msgstr ""

#~ msgid ":py:obj:`LowerIntrin <tvm.tir.transform.LowerIntrin>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower target specific intrinsic calls."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerMatchBuffer "
#~ "<tvm.tir.transform.LowerMatchBuffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Remove match buffers inside the block."
#~ msgstr ""

#~ msgid ":py:obj:`LowerTVMBuiltin <tvm.tir.transform.LowerTVMBuiltin>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower tvm builtin intrinsics."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LowerThreadAllreduce "
#~ "<tvm.tir.transform.LowerThreadAllreduce>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower cross thread alleduce."
#~ msgstr ""

#~ msgid ":py:obj:`LowerWarpMemory <tvm.tir.transform.LowerWarpMemory>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lower warp memory access to low-level device related function calls."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MakePackedAPI <tvm.tir.transform.MakePackedAPI>`\\ "
#~ "\\(\\[num\\_unpacked\\_params\\]\\)"
#~ msgstr ""

#~ msgid "Transform the PrimFuncs in the module to a packed func API."
#~ msgstr ""

#~ msgid ":py:obj:`MakeUnpackedAPI <tvm.tir.transform.MakeUnpackedAPI>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Transform the PrimFuncs in the module"
#~ " to a C API compatible with "
#~ "internal calls."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MergeDynamicSharedMemoryAllocations "
#~ "<tvm.tir.transform.MergeDynamicSharedMemoryAllocations>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "This pass merges multiple TIR-level "
#~ "dynamic shared memory allocations into "
#~ "one allocation."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`NarrowDataType <tvm.tir.transform.NarrowDataType>`\\"
#~ " \\(target\\_bits\\)"
#~ msgstr ""

#~ msgid "Narrow down PrimExpr datatype in stmt to target_bits."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PlanAndUpdateBufferAllocationLocation "
#~ "<tvm.tir.transform.PlanAndUpdateBufferAllocationLocation>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Locate the buffer allocation to the "
#~ "exact position (usually is the lca "
#~ "of buffer access)."
#~ msgstr ""

#~ msgid ":py:obj:`RemoveNoOp <tvm.tir.transform.RemoveNoOp>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Remove No Op from the Stmt."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RewriteUnsafeSelect "
#~ "<tvm.tir.transform.RewriteUnsafeSelect>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Detect and rewrite unsafe select that contains memory access."
#~ msgstr ""

#~ msgid ":py:obj:`Simplify <tvm.tir.transform.Simplify>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Run arithmetic simplifications on the statements and expressions."
#~ msgstr ""

#~ msgid ":py:obj:`SkipAssert <tvm.tir.transform.SkipAssert>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Skip assert stmt."
#~ msgstr ""

#~ msgid ":py:obj:`SplitHostDevice <tvm.tir.transform.SplitHostDevice>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Split the function into a host function and device functions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`StorageFlatten <tvm.tir.transform.StorageFlatten>`\\"
#~ " \\(cache\\_line\\_size\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Flatten the multi-dimensional read/write to 1D."
#~ msgstr ""

#~ msgid ":py:obj:`StorageRewrite <tvm.tir.transform.StorageRewrite>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Rewrite storage allocation pattern."
#~ msgstr ""

#~ msgid ":py:obj:`TextureFlatten <tvm.tir.transform.TextureFlatten>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Flatten the multi-dimensional read/write to 2D."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ThreadSync <tvm.tir.transform.ThreadSync>`\\ "
#~ "\\(storage\\_scope\\)"
#~ msgstr ""

#~ msgid "Insert sync between parallel read/write of shared buffers."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`UnifyThreadBinding "
#~ "<tvm.tir.transform.UnifyThreadBinding>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Unify all the thread bindings for "
#~ "\"blockIdx.x/y/z\", \"threadIdx.x/y/z\", and "
#~ "\"vthread.x/y/z\"."
#~ msgstr ""

#~ msgid ":py:obj:`UnrollLoop <tvm.tir.transform.UnrollLoop>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Unroll the constant loop marked by unroll."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`VectorizeLoop <tvm.tir.transform.VectorizeLoop>`\\ "
#~ "\\(\\[enable\\_vectorize\\]\\)"
#~ msgstr ""

#~ msgid "Lower vectorization loops."
#~ msgstr ""

#~ msgid ":py:obj:`VerifyMemory <tvm.tir.transform.VerifyMemory>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Verify if func contains illegal host side direct memory access."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`prim_func_pass <tvm.tir.transform.prim_func_pass>`\\"
#~ " \\(\\[pass\\_func\\, opt\\_level\\, name\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Decorate a function pass."
#~ msgstr ""

#~ msgid "A pass that works on each :py:func:`tvm.tir.PrimFunc` in a module."
#~ msgstr ""

#~ msgid "This function is a thin wrapper around tvm.tir.transform.prim_func_pass"
#~ msgstr ""

#~ msgid "The transformation pass."
#~ msgstr ""

#~ msgid "**fpass** -- The result pass"
#~ msgstr ""

#~ msgid ""
#~ "Eliminate verbose casting between fp32 "
#~ "and bf16 Checks if the AST has "
#~ "the pattern: castto32(castto16(some_fp32_op(...))) "
#~ "The verbose casting is generated by "
#~ "BF16Promote for multiple bf16 Ops in "
#~ "a row. e.g.: X[i] + Y[i] + "
#~ "T[i] => bf16((float32(bf16((float32(X[i]) + "
#~ "float32(Y[i])))) + float32(T[i]))) After this"
#~ " pass: bf16(float32(X[i]) + float32(Y[i]) +"
#~ " float32(T[i]))"
#~ msgstr ""

#~ msgid ""
#~ "Legalize bf16 typed Ops. Runs "
#~ "BF16Promote, BF16CastElimination and "
#~ "BF16TypeLowering"
#~ msgstr ""

#~ msgid ""
#~ "Promote bf16 to fp32. Add a cast"
#~ " to fp32 before Ops, then add a"
#~ " cast back to bf16."
#~ msgstr ""

#~ msgid ""
#~ "Replace all bf16 type with uint16. "
#~ "Also lower the casting between fp32 "
#~ "and bf16"
#~ msgstr ""

#~ msgid ""
#~ "Compact the buffer access region. by "
#~ "removing the buffer regions that are "
#~ "not accessed, i.e. narrowing the buffer"
#~ " shape and adjust the access region"
#~ " if necessary."
#~ msgstr ""

#~ msgid ""
#~ "Before narrowing, ``B`` is a ``[16, "
#~ "16]`` buffer, but only a skinny "
#~ "vector ``B[i, 0:16]`` is accessed."
#~ msgstr ""

#~ msgid ""
#~ "This pass narrows the buffer shape "
#~ "and adjust its accessed region "
#~ "accordingly.  In this particular case, "
#~ "because only a ``1 * 16`` vector"
#~ " of ``B`` is accessed, the pass "
#~ "narrows ``B`` to shape ``[1, 16]``, "
#~ "and changes the access to ``B[i, "
#~ "j]`` to ``B[0, j]``."
#~ msgstr ""

#~ msgid "The condition of the filtering."
#~ msgstr ""

#~ msgid ""
#~ "Flatten the multi-dimensional BufferLoad "
#~ "and BufferStore to single dimensional "
#~ "Load/Store. Also remove Block to ensure"
#~ " that the flattened TIR can not "
#~ "be scheduled again."
#~ msgstr ""

#~ msgid ""
#~ "The variant of the pass. variant "
#~ "can have any one of following "
#~ "values [\"basic\", None(Default)].  The basic"
#~ " variant supports basic hoisting scenarios"
#~ " where it expects the For & If"
#~ " Nodes are in place consecutively and"
#~ " does not involve global scope "
#~ "variables or more advanced scenarios.  "
#~ "Default variant supports all hoisting "
#~ "scenarios,i.e., {\"Basic\" + \"Advanced\"} "
#~ "supported with control with PassContext "
#~ "configs like below:      "
#~ "config={\"tir.HoistIfThenElse\": "
#~ "{\"support_block_scope_hosting\": True}}"
#~ msgstr ""

#~ msgid ""
#~ "The variant of the pass. variant "
#~ "can have any one of following "
#~ "values [\"basic\", None(Default)]."
#~ msgstr ""

#~ msgid ""
#~ "The basic variant supports basic "
#~ "hoisting scenarios where it expects the"
#~ " For & If Nodes are in place"
#~ " consecutively and does not involve "
#~ "global scope variables or more advanced"
#~ " scenarios."
#~ msgstr ""

#~ msgid ""
#~ "Default variant supports all hoisting "
#~ "scenarios,i.e., {\"Basic\" + \"Advanced\"} "
#~ "supported with control with PassContext "
#~ "configs like below:"
#~ msgstr ""

#~ msgid ""
#~ "config={\"tir.HoistIfThenElse\": "
#~ "{\"support_block_scope_hosting\": True}}"
#~ msgstr ""

#~ msgid "The pragma key for hint of copy."
#~ msgstr ""

#~ msgid ""
#~ "The function with signature copyintrin(src,"
#~ " dst, pad_before, pad_after, pad_value)"
#~ msgstr ""

#~ msgid "The attribute key to be checked."
#~ msgstr ""

#~ msgid ""
#~ "See tvm::datatypes::Registry for more "
#~ "information on adding custom datatypes."
#~ msgstr ""

#~ msgid "Run this pass after all storage access analysis finish."
#~ msgstr ""

#~ msgid ""
#~ "Remove match buffers inside the block."
#~ " Also, it will validate the binding."
#~ msgstr ""

#~ msgid ""
#~ "Number of parameters that we hope "
#~ "to directly pass via normal arguments"
#~ " following the PackedFunc input signature."
#~ " If it is specified as -1 or"
#~ " it is less than the number of"
#~ " arguments, the pass will packed "
#~ "arguments still."
#~ msgstr ""

#~ msgid "The target bit configuration."
#~ msgstr ""

#~ msgid "Run this pass after StorageFlatten."
#~ msgstr ""

#~ msgid ""
#~ "Locate the buffer allocation to the "
#~ "exact position (usually is the lca "
#~ "of buffer access). This pass will "
#~ "inject opaque block with alloc_buffers "
#~ "at the allocation site."
#~ msgstr ""

#~ msgid ""
#~ "A pass that works on each "
#~ ":py:func:`tvm.tir.PrimFunc` in a module. A "
#~ "function pass class should be created"
#~ " through py:func:`tvm.tir.transform.function_pass`."
#~ msgstr ""

#~ msgid "The size of CPU cache line."
#~ msgstr ""

#~ msgid "Whether to create bound attributes."
#~ msgstr ""

#~ msgid ""
#~ "Moves the allocation to outer most "
#~ "possible scope. Trying to share space"
#~ " between allocations to make a static"
#~ " allocation plan when possible."
#~ msgstr ""

#~ msgid ""
#~ "Unify all the thread bindings for "
#~ "\"blockIdx.x/y/z\", \"threadIdx.x/y/z\", and "
#~ "\"vthread.x/y/z\". Before the unification, two"
#~ " vars that are bound to a "
#~ "thread axis (e.g., \"threadIdx.x\") use "
#~ "different IterVars and variables in "
#~ "their AttrStmts. After the unification, "
#~ "we use a consolidated IterVar and "
#~ "a variable for them."
#~ msgstr ""

#~ msgid ""
#~ "`vthread` is a legacy behavior that "
#~ "will be deprecated, though thread "
#~ "bindings of `vthread` are still also "
#~ "unified in this pass. Please use "
#~ "`vthread.x`, `vthread.y` and `vthread.z` "
#~ "instead."
#~ msgstr ""

#~ msgid ""
#~ "This pass also automatically attach "
#~ "pragma unroll tag to loops which "
#~ "meets the standard."
#~ msgstr ""

#~ msgid ""
#~ "Whether vectorization is enabled. Will "
#~ "lower to scalar loop when it is"
#~ " turned off."
#~ msgstr ""

#~ msgid ""
#~ "This function returns a callback when"
#~ " pass_func is provided. Otherwise, it "
#~ "returns the created function pass using"
#~ " the given optimization function."
#~ msgstr ""

#~ msgid "The transformation function or class."
#~ msgstr ""

#~ msgid "The optimization level of this module pass."
#~ msgstr ""

#~ msgid ""
#~ "The name of the function pass. The"
#~ " name could be empty. In this "
#~ "case, the name of the optimization "
#~ "function will be used as the pass"
#~ " name."
#~ msgstr ""

#~ msgid "The list of passes that the function pass is dependent on."
#~ msgstr ""

#~ msgid ""
#~ "**create_function_pass** -- A decorator will"
#~ " be returned if pass_func is not "
#~ "provided, otherwise return the decorated "
#~ "result. The returned decorator has two"
#~ " behaviors depending on the input: A"
#~ " new FunctionPass will be returned "
#~ "when we decorate a pass function. "
#~ "A new FunctionPass class will be "
#~ "returned when we decorate a class "
#~ "type."
#~ msgstr ""

#~ msgid "The following code block decorates a function pass class."
#~ msgstr ""

#~ msgid ""
#~ "The following code creates a function"
#~ " pass by decorating a user defined"
#~ " transform function."
#~ msgstr ""

#~ msgid "Namespace of all TIR analysis utils."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Block <tvm.tir.analysis.Block>`\\ "
#~ "\\(iter\\_vars\\, reads\\, writes\\, name\\_hint\\,"
#~ " body\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BufferRegion <tvm.tir.analysis.BufferRegion>`\\ "
#~ "\\(buffer\\, region\\)"
#~ msgstr ""

#~ msgid "Base class for all tvm's runtime objects."
#~ msgstr ""

#~ msgid "Base class of all primitive expressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PrimFunc <tvm.tir.analysis.PrimFunc>`\\ "
#~ "\\(params\\, body\\[\\, ret\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Var <tvm.tir.analysis.Var>`\\ \\(name\\, "
#~ "dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`apply_prim_func_arg_and_result_memory_constraints "
#~ "<tvm.tir.analysis.apply_prim_func_arg_and_result_memory_constraints>`\\"
#~ " \\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns func written to capture the "
#~ "memory (aka storage) scope constraints "
#~ "for each of the func's parameters "
#~ "given by arg_and_result_memory_scopes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`calculate_workspace_bytes "
#~ "<tvm.tir.analysis.calculate_workspace_bytes>`\\ \\(func\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Calculate the workspace size in bytes"
#~ " needed by the TIR allocates inside"
#~ " the TIR PrimFunc."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`detect_buffer_access_lca "
#~ "<tvm.tir.analysis.detect_buffer_access_lca>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid ""
#~ "Detect the lowest common ancestor(LCA) "
#~ "of buffer access, including both "
#~ "high-level access(BufferLoad, BufferStore) and"
#~ " low-level access(Load, Store and "
#~ "opaque access)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`expr_deep_equal <tvm.tir.analysis.expr_deep_equal>`\\"
#~ " \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Deeply compare two nested expressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_block_access_region "
#~ "<tvm.tir.analysis.get_block_access_region>`\\ \\(block\\, "
#~ "buffer\\_var\\_map\\)"
#~ msgstr ""

#~ msgid "Detect which regions of tensors in this block are read or written to."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_block_read_write_region "
#~ "<tvm.tir.analysis.get_block_read_write_region>`\\ \\(block\\,"
#~ " ...\\)"
#~ msgstr ""

#~ msgid "Auto detect the block read/write region according to its body stmt."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_prim_func_arg_and_result_memory_constraints "
#~ "<tvm.tir.analysis.get_prim_func_arg_and_result_memory_constraints>`\\"
#~ " \\(...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns the memory (aka storage) scope"
#~ " constraints for all the arguments "
#~ "and result of func."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`verify_gpu_code <tvm.tir.analysis.verify_gpu_code>`\\"
#~ " \\(func\\, constraints\\)"
#~ msgstr ""

#~ msgid "Verify if module contains illegal host side direct memory access."
#~ msgstr ""

#~ msgid ":py:obj:`verify_memory <tvm.tir.analysis.verify_memory>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid ":py:obj:`verify_ssa <tvm.tir.analysis.verify_ssa>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid "Verify if the func is in SSA form."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`access_ptr <tvm.tir.analysis.Buffer.access_ptr>`\\ "
#~ "\\(access\\_mask\\[\\, ptr\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`scope <tvm.tir.analysis.Buffer.scope>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`vload <tvm.tir.analysis.Buffer.vload>`\\ "
#~ "\\(begin\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`vstore <tvm.tir.analysis.Buffer.vstore>`\\ "
#~ "\\(begin\\, value\\)"
#~ msgstr ""

#~ msgid ""
#~ "PrimExpr is used in the low-level"
#~ " code optimizations and integer analysis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`script <tvm.tir.analysis.PrimFunc.script>`\\ "
#~ "\\(\\[tir\\_prefix\\, show\\_meta\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`specialize <tvm.tir.analysis.PrimFunc.specialize>`\\"
#~ " \\(param\\_map\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`with_body <tvm.tir.analysis.PrimFunc.with_body>`\\ "
#~ "\\(new\\_body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns func written to capture the "
#~ "memory (aka storage) scope constraints "
#~ "for each of the func's parameters "
#~ "given by arg_and_result_memory_scopes. However, "
#~ "arg_and_result_memory_scopes should be w.r.t. "
#~ "the func's representation as a Relay "
#~ "Function of relay_func_type before lowering"
#~ " and conversion to DPS."
#~ msgstr ""

#~ msgid "Visible for testing."
#~ msgstr ""

#~ msgid ""
#~ "CAUTION: This is experimental. The "
#~ "resulting PrimFunc may not have fully"
#~ " accounted for all new memory scopes."
#~ msgstr ""

#~ msgid "The function to retrieve constraints from."
#~ msgstr ""

#~ msgid "The type of the Relay Function from which the func was derived."
#~ msgstr ""

#~ msgid ""
#~ "Memory constraints for funcs args and"
#~ " result in Relay form. The empty "
#~ "string denotes 'no constraint'."
#~ msgstr ""

#~ msgid "**result** -- The rewritten func."
#~ msgstr ""

#~ msgid "The function to be detected."
#~ msgstr ""

#~ msgid "The byte alignment required for each tensor"
#~ msgstr ""

#~ msgid "**result** -- Workspace size in bytes."
#~ msgstr ""

#~ msgid ""
#~ "Detect the lowest common ancestor(LCA) "
#~ "of buffer access, including both "
#~ "high-level access(BufferLoad, BufferStore) and"
#~ " low-level access(Load, Store and "
#~ "opaque access). The LCA may be a"
#~ " For loop or a Block."
#~ msgstr ""

#~ msgid "**result** -- Map from buffer to the LCA of all access to it."
#~ msgstr ""

#~ msgid "The left operand."
#~ msgstr ""

#~ msgid "The right operand."
#~ msgstr ""

#~ msgid "**result** -- The comparison result"
#~ msgstr ""

#~ msgid ""
#~ "This function does not remap variable"
#~ " bindings, it will not return true"
#~ " for (let x = 1 in x +"
#~ " 1) vs (let y = 1 in y"
#~ " + 1), unless x.same_as(y). Use "
#~ "py:func:`tvm.ir.structural_equal` to handle "
#~ "structural variable remapping."
#~ msgstr ""

#~ msgid ""
#~ "Due to the restriction of not "
#~ "remapping variables, this function can "
#~ "run faster than StructuralEqual and can"
#~ " be used as a utility function "
#~ "during arithmetic simplifications."
#~ msgstr ""

#~ msgid ""
#~ "Always consider py:func:`tvm.ir.structural_equal` "
#~ "first, which handles the structural "
#~ "remapping."
#~ msgstr ""

#~ msgid ":obj:`tvm.ir.structural_equal`"
#~ msgstr ""

#~ msgid "Regions are sorted by order of appearance in the AST."
#~ msgstr ""

#~ msgid "The block in which we are detecting read/write regions."
#~ msgstr ""

#~ msgid ""
#~ "The outside buffers which may access "
#~ "the block. Mapping from buffer var "
#~ "to the buffer"
#~ msgstr ""

#~ msgid ""
#~ "**result** --  Array of access regions."
#~ " There are three arrays of "
#~ "BufferRegion:     - first: read regions"
#~ "     - second: write regions     - "
#~ "third: opaque regions"
#~ msgstr ""

#~ msgid "**result** --"
#~ msgstr ""

#~ msgid "Array of access regions. There are three arrays of BufferRegion:"
#~ msgstr ""

#~ msgid "first: read regions"
#~ msgstr ""

#~ msgid "second: write regions"
#~ msgstr ""

#~ msgid "third: opaque regions"
#~ msgstr ""

#~ msgid "An opaque access will be counted as both a read and a write access"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- An array only consisting"
#~ " of the read regions and write "
#~ "regions of the input block"
#~ msgstr ""

#~ msgid ""
#~ "Returns the memory (aka storage) scope"
#~ " constraints for all the arguments "
#~ "and result of func. However the "
#~ "result will be w.r.t. the func's "
#~ "representation as a Relay Function of"
#~ " relay_func_type before lowering and "
#~ "conversion to DPS."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- Memory scope constraints "
#~ "for funcs args and result in Relay"
#~ " form. The empty string denotes 'no"
#~ " constraint'."
#~ msgstr ""

#~ msgid "The module to be verified."
#~ msgstr ""

#~ msgid "The attribute constraints."
#~ msgstr ""

#~ msgid "**result** -- The result of verification."
#~ msgstr ""

#~ msgid "Statement functor utilities for IR transformations"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ir_transform <tvm.tir.stmt_functor.ir_transform>`\\ "
#~ "\\(stmt\\, preorder\\, postorder\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Recursively visit and transform ir nodes in post DFS order."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`post_order_visit "
#~ "<tvm.tir.stmt_functor.post_order_visit>`\\ \\(stmt\\, "
#~ "fvisit\\)"
#~ msgstr ""

#~ msgid "Recursively visit the ir in post DFS order node, apply fvisit"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`substitute <tvm.tir.stmt_functor.substitute>`\\ "
#~ "\\(node\\, vmap\\)"
#~ msgstr ""

#~ msgid "Substitute the var specified by vmap."
#~ msgstr ""

#~ msgid "The input to be transformed."
#~ msgstr ""

#~ msgid ""
#~ "The function called in before recursive"
#~ " mutation If preorder returns None, "
#~ "then the transform will proceed to "
#~ "recursive call. If preorder returns a"
#~ " not None tvm.tir.Stmt/Expr, the "
#~ "transformer will simply return it and"
#~ " won't do further recursion."
#~ msgstr ""

#~ msgid "The function called after recursive mutation."
#~ msgstr ""

#~ msgid "List of types that we only enable."
#~ msgstr ""

#~ msgid "**result** -- The result."
#~ msgstr ""

#~ msgid "Each node is guaranteed to be visited only once."
#~ msgstr ""

#~ msgid "The visitor function."
#~ msgstr ""

#~ msgid "The input."
#~ msgstr ""

#~ msgid "The variable mapping."
#~ msgstr ""

