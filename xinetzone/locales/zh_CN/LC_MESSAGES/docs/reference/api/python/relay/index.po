# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-01-20 16:06+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../doc/docs/reference/api/python/relay/index.rst:19
msgid "tvm.relay"
msgstr ""

#: of tvm.relay:1
msgid "The Relay IR namespace containing the IR definition and compiler."
msgstr ""

#: of tvm.relay:1
msgid "**Classes:**"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":py:obj:`Call <tvm.relay.Call>`\\ \\(op\\, args\\[\\, attrs\\, "
"type\\_args\\, span\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1 tvm.relay:1:<autosummary>:1
msgid "Function call node in Relay."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`Clause <tvm.relay.Clause>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.adt.Clause:1 tvm.relay:1:<autosummary>:1
msgid "Clause for pattern matching in Relay."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`Constant <tvm.relay.Constant>`\\ \\(data\\[\\, span\\]\\)"
msgstr ""

#: of tvm.relay.expr.Constant:1 tvm.relay:1:<autosummary>:1
msgid "A constant expression in Relay."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`Expr <tvm.relay.Expr>`\\"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:class:`~tvm.ir.expr.RelayExpr` 的别名"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`ExprFunctor <tvm.relay.ExprFunctor>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.expr_functor.ExprFunctor:1 tvm.relay:1:<autosummary>:1
msgid "An abstract visitor defined over Expr."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`ExprMutator <tvm.relay.ExprMutator>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.expr_functor.ExprMutator:1 tvm.relay:1:<autosummary>:1
msgid "A functional visitor over Expr."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`ExprVisitor <tvm.relay.ExprVisitor>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.expr_functor.ExprVisitor:1 tvm.relay:1:<autosummary>:1
msgid "A visitor over Expr."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":py:obj:`Function <tvm.relay.Function>`\\ \\(params\\, body\\[\\, "
"ret\\_type\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.function.Function:1 tvm.relay:1:<autosummary>:1
msgid "A function declaration expression."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":py:obj:`If <tvm.relay.If>`\\ \\(cond\\, true\\_branch\\, "
"false\\_branch\\[\\, span\\]\\)"
msgstr ""

#: of tvm.relay.expr.If:1 tvm.relay:1:<autosummary>:1
msgid "A conditional expression in Relay."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":py:obj:`Let <tvm.relay.Let>`\\ \\(variable\\, value\\, body\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.relay.expr.Let:1 tvm.relay:1:<autosummary>:1
msgid "Let variable binding expression."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":py:obj:`Match <tvm.relay.Match>`\\ \\(data\\, clauses\\[\\, "
"complete\\]\\)"
msgstr ""

#: of tvm.relay.adt.Match:1 tvm.relay:1:<autosummary>:1
msgid "Pattern matching expression in Relay."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`Pattern <tvm.relay.Pattern>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.adt.Pattern:1 tvm.relay:1:<autosummary>:1
msgid "Base type for pattern matching constructs."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":py:obj:`PatternConstructor <tvm.relay.PatternConstructor>`\\ "
"\\(constructor\\[\\, patterns\\]\\)"
msgstr ""

#: of tvm.relay.adt.PatternConstructor:1 tvm.relay:1:<autosummary>:1
msgid ""
"Constructor pattern in Relay: Matches an ADT of the given constructor, "
"binds recursively."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`PatternTuple <tvm.relay.PatternTuple>`\\ \\(\\[patterns\\]\\)"
msgstr ""

#: of tvm.relay.adt.PatternTuple:1 tvm.relay:1:<autosummary>:1
msgid "Constructor pattern in Relay: Matches a tuple, binds recursively."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`PatternVar <tvm.relay.PatternVar>`\\ \\(var\\)"
msgstr ""

#: of tvm.relay.adt.PatternVar:1 tvm.relay:1:<autosummary>:1
msgid "Variable pattern in Relay: Matches anything and binds it to the variable."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`PatternWildcard <tvm.relay.PatternWildcard>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.adt.PatternWildcard:1 tvm.relay:1:<autosummary>:1
msgid "Wildcard pattern in Relay: Matches any ADT and binds nothing."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`Prelude <tvm.relay.Prelude>`\\ \\(\\[mod\\]\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude:1 tvm.relay:1:<autosummary>:1
msgid "Contains standard definitions."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`RefCreate <tvm.relay.RefCreate>`\\ \\(value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
"Create a new reference from initial value. Parameters ---------- value: "
"tvm.relay.Expr    The initial value."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`RefRead <tvm.relay.RefRead>`\\ \\(ref\\[\\, span\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
"Get the value inside the reference. Parameters ---------- ref: "
"tvm.relay.Expr      The reference."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`RefType <tvm.relay.RefType>`\\"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:class:`~tvm.ir.type.RelayRefType` 的别名"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`RefWrite <tvm.relay.RefWrite>`\\ \\(ref\\, value\\[\\, span\\]\\)"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
"Update the value inside the reference. The whole expression will evaluate"
" to an empty tuple. Parameters ---------- ref: tvm.relay.Expr     The "
"reference."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`ScopeBuilder <tvm.relay.ScopeBuilder>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder:1 tvm.relay:1:<autosummary>:1
msgid "Scope builder class."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`SequentialSpan <tvm.relay.SequentialSpan>`\\ \\(spans\\)"
msgstr ""

#: of tvm.ir.base.SequentialSpan:1 tvm.relay:1:<autosummary>:1
msgid "A sequence of source spans"
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`Tuple <tvm.relay.Tuple>`\\ \\(fields\\[\\, span\\]\\)"
msgstr ""

#: of tvm.relay.expr.Tuple:1 tvm.relay:1:<autosummary>:1
msgid "Tuple expression that groups several fields together."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":py:obj:`TupleGetItem <tvm.relay.TupleGetItem>`\\ \\(tuple\\_value\\, "
"index\\[\\, span\\]\\)"
msgstr ""

#: of tvm.relay.expr.TupleGetItem:1 tvm.relay:1:<autosummary>:1
msgid "Get index-th item from a tuple."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":py:obj:`TupleWrapper <tvm.relay.TupleWrapper>`\\ \\(tuple\\_value\\, "
"size\\)"
msgstr ""

#: of tvm.relay.expr.TupleWrapper:1 tvm.relay:1:<autosummary>:1
msgid "TupleWrapper."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ""
":py:obj:`TypeData <tvm.relay.TypeData>`\\ \\(header\\, type\\_vars\\, "
"constructors\\)"
msgstr ""

#: of tvm.ir.adt.TypeData:1 tvm.relay:1:<autosummary>:1
msgid "Stores the definition for an Algebraic Data Type (ADT) in Relay."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`TypeFunctor <tvm.relay.TypeFunctor>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.type_functor.TypeFunctor:1 tvm.relay:1:<autosummary>:1
msgid "An abstract visitor defined over Type."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`TypeMutator <tvm.relay.TypeMutator>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.type_functor.TypeMutator:1 tvm.relay:1:<autosummary>:1
msgid "A functional visitor over Type."
msgstr ""

#: of tvm.relay:1:<autosummary>:1
msgid ":py:obj:`TypeVisitor <tvm.relay.TypeVisitor>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.type_functor.TypeVisitor:1 tvm.relay:1:<autosummary>:1
msgid "A visitor over Type."
msgstr ""

#: of tvm.relay:1
msgid "**Functions:**"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`ShapeVar <tvm.relay.ShapeVar>`\\ \\(name\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.ty.ShapeVar:1
msgid "A helper which constructs a type var of which the shape kind."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`SpanCheck <tvm.relay.SpanCheck>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.parser.SpanCheck:1
msgid "A debugging utility for reporting missing span information."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`abs <tvm.relay.abs>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.abs:1
#: tvm.relay.op.tensor.sign:1
msgid "Compute element-wise absolute of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`acos <tvm.relay.acos>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.acos:1
msgid "Compute elementwise acos of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`acosh <tvm.relay.acosh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.acosh:1
msgid "Compute elementwise acosh of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`add <tvm.relay.add>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.add:1
msgid "Addition with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`adv_index <tvm.relay.adv_index>`\\ \\(inputs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Numpy style advanced indexing."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`all <tvm.relay.all>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.all:1
msgid "Computes the logical AND of boolean array elements over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`any <tvm.relay.any>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.any:1
msgid "Computes the logical OR of boolean array elements over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`arange <tvm.relay.arange>`\\ \\(start\\[\\, stop\\, step\\, "
"dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.arange:1
msgid "Return evenly spaced values within a given interval."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`argmax <tvm.relay.argmax>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.argmax:1
msgid "Returns the indices of the maximum values along an axis."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`argmin <tvm.relay.argmin>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.argmin:1
msgid "Returns the indices of the minimum values along an axis."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`argsort <tvm.relay.argsort>`\\ \\(data\\[\\, axis\\, "
"is\\_ascend\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.algorithm.argsort:1
msgid ""
"Performs sorting along the given axis and returns an array of indices "
"having same shape as an input array that index data in sorted order."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`argwhere <tvm.relay.argwhere>`\\ \\(condition\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.argwhere:1
msgid "Find the indices of elements of a tensor that are non-zero."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`asin <tvm.relay.asin>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.asin:1
msgid "Compute elementwise asin of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`asinh <tvm.relay.asinh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.asinh:1
msgid "Compute elementwise asinh of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`astext <tvm.relay.astext>`\\ \\(obj\\[\\, show\\_meta\\_data\\, "
"annotate\\]\\)"
msgstr ""

#: of tvm.relay.base.astext:1 tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.function.Function.__call__:1:<autosummary>:1
#: tvm.relay.function.Function.astext:1
msgid "Get the text format of the expression."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`atan <tvm.relay.atan>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.atan:1
msgid "Compute elementwise atan of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`atanh <tvm.relay.atanh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.atanh:1
msgid "Compute elementwise atanh of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`bind <tvm.relay.bind>`\\ \\(expr\\, binds\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.expr.bind:1
msgid "Bind an free variables in expr or function arguments."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`bitwise_and <tvm.relay.bitwise_and>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.bitwise_and:1
msgid "bitwise AND with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`bitwise_not <tvm.relay.bitwise_not>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.bitwise_not:1
msgid "Compute element-wise bitwise not of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`bitwise_or <tvm.relay.bitwise_or>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.bitwise_or:1
msgid "bitwise OR with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`bitwise_xor <tvm.relay.bitwise_xor>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.bitwise_xor:1
msgid "bitwise XOR with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`broadcast_to <tvm.relay.broadcast_to>`\\ \\(data\\, shape\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.broadcast_to:1
msgid ""
"Return a scalar value array with the same type, broadcasted to the "
"provided shape."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`broadcast_to_like <tvm.relay.broadcast_to_like>`\\ \\(data\\, "
"broadcast\\_type\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.broadcast_to_like:1
#: tvm.relay.op.transform.collapse_sum_like:1
#: tvm.relay.op.transform.full_like:1
msgid ""
"Return a scalar value array with the same shape and type as the input "
"array."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`build <tvm.relay.build>`\\ \\(ir\\_mod\\[\\, target\\, "
"target\\_host\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.build_module.build:1 tvm.relay.expr.Call:1:<autosummary>:1
msgid "Helper function that builds a Relay function to run on TVM graph executor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`build_config <tvm.relay.build_config>`\\ \\(\\[opt\\_level\\, "
"required\\_pass\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Configure the build behavior by setting config variables."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`cast <tvm.relay.cast>`\\ \\(data\\, dtype\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.cast:1
msgid "Cast input tensor to data type."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`cast_like <tvm.relay.cast_like>`\\ \\(data\\, dtype\\_like\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.cast_like:1
msgid "Cast input tensor to data type of another tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`ceil <tvm.relay.ceil>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.ceil:1
msgid "Compute element-wise ceil of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`clip <tvm.relay.clip>`\\ \\(a\\, a\\_min\\, a\\_max\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Clip the elements in `a` between `a_min` and `a_max`."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`collapse_sum_like <tvm.relay.collapse_sum_like>`\\ \\(data\\, "
"collapse\\_type\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`collapse_sum_to <tvm.relay.collapse_sum_to>`\\ \\(data\\, "
"shape\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.collapse_sum_to:1
msgid "Return a summation of data to the specified shape."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`concatenate <tvm.relay.concatenate>`\\ \\(data\\, axis\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.concatenate:1
msgid "Concatenate the input tensors along the given axis."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`const <tvm.relay.const>`\\ \\(value\\[\\, dtype\\, span\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.expr.const:1
msgid "Create a constant value."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`copy <tvm.relay.copy>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.copy:1
msgid "Copy a tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`copy_shape_func <tvm.relay.copy_shape_func>`\\ \\(attrs\\, "
"inputs\\, \\_\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.tensor.copy_shape_func:1
msgid "Shape function for copy op."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`cos <tvm.relay.cos>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.cos:1
msgid "Compute elementwise cos of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`cosh <tvm.relay.cosh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.cosh:1
msgid "Compute elementwise cosh of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`create_executor <tvm.relay.create_executor>`\\ \\(\\[kind\\, "
"mod\\, device\\, target\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.build_module.create_executor:1
#: tvm.relay.expr.Call:1:<autosummary>:1
msgid "Factory function to create an executor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`cumprod <tvm.relay.cumprod>`\\ \\(data\\[\\, axis\\, dtype\\, "
"exclusive\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Numpy style cumprod op."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`cumsum <tvm.relay.cumsum>`\\ \\(data\\[\\, axis\\, dtype\\, "
"exclusive\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Numpy style cumsum op."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`device_copy <tvm.relay.device_copy>`\\ \\(data\\, "
"src\\_device\\, dst\\_device\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Copy data from the source device to the destination device."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`dft <tvm.relay.dft>`\\ \\(re\\_data\\, im\\_data\\[\\, "
"inverse\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
"Computes the discrete Fourier transform of input (calculation along the "
"last axis)."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`divide <tvm.relay.divide>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.divide:1
msgid "Division with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`einsum <tvm.relay.einsum>`\\ \\(data\\, equation\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.einsum:1
msgid "Evaluates the Einstein summation convention on data"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`equal <tvm.relay.equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.equal:1
msgid "Broadcasted elementwise test for (lhs == rhs)."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`erf <tvm.relay.erf>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.erf:1
msgid "Compute elementwise error function of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`exp <tvm.relay.exp>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.exp:1
msgid "Compute elementwise exp of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`expand_dims <tvm.relay.expand_dims>`\\ \\(data\\, axis\\[\\, "
"num\\_newaxis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.expand_dims:1
msgid "Insert `num_newaxis` axes at the position given by `axis`."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`fixed_point_multiply <tvm.relay.fixed_point_multiply>`\\ "
"\\(data\\, multiplier\\, shift\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.tensor.fixed_point_multiply:1
msgid ""
"Fixed point multiplication between data and a fixed point constant "
"expressed as multiplier * 2^(-shift), where multiplier is a Q-number with"
" 31 fractional bits"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`floor <tvm.relay.floor>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.floor:1
msgid "Compute element-wise floor of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`floor_divide <tvm.relay.floor_divide>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.floor_divide:1
msgid "Floor division with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`floor_mod <tvm.relay.floor_mod>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.floor_mod:1
msgid "Floor mod with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`full <tvm.relay.full>`\\ \\(fill\\_value\\[\\, shape\\, "
"dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.full:1
msgid "Fill array with scalar value."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`full_like <tvm.relay.full_like>`\\ \\(data\\, fill\\_value\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`gather <tvm.relay.gather>`\\ \\(data\\, axis\\, indices\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.gather:1
msgid "Gather values along given axis from given indices."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`gather_nd <tvm.relay.gather_nd>`\\ \\(data\\, indices\\[\\, "
"batch\\_dims\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.gather_nd:1
msgid ""
"Gather elements or slices from data and store them to a tensor whose "
"shape is defined by indices."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`greater <tvm.relay.greater>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.greater:1
msgid "Broadcasted elementwise test for (lhs > rhs)."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`greater_equal <tvm.relay.greater_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.greater_equal:1
msgid "Broadcasted elementwise test for (lhs >= rhs)."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`invert_permutation <tvm.relay.invert_permutation>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Computes the inverse permutation of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`isfinite <tvm.relay.isfinite>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.isfinite:1
msgid "Compute element-wise finiteness of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`isinf <tvm.relay.isinf>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.isinf:1
msgid "Compute element-wise infiniteness of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`isnan <tvm.relay.isnan>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.isnan:1
msgid "Check nan in input data element-wise."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`layout_transform <tvm.relay.layout_transform>`\\ \\(data\\, "
"src\\_layout\\, dst\\_layout\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.layout_transform:1
msgid "Transform the layout of a tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`left_shift <tvm.relay.left_shift>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.left_shift:1
msgid "Left shift with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`less <tvm.relay.less>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.less:1
msgid "Broadcasted elementwise test for (lhs < rhs)."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`less_equal <tvm.relay.less_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.less_equal:1
msgid "Broadcasted elementwise test for (lhs <= rhs)."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`load_param_dict <tvm.relay.load_param_dict>`\\ "
"\\(param\\_bytes\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.param_dict.load_param_dict:1
msgid "Load parameter dictionary to binary bytes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`log <tvm.relay.log>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.log:1
msgid "Compute elementwise log of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`log10 <tvm.relay.log10>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.log10:1
msgid "Compute elementwise log to the base 10 of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`log2 <tvm.relay.log2>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.log2:1
msgid "Compute elementwise log to the base 2 of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`logical_and <tvm.relay.logical_and>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.logical_and:1
msgid "logical AND with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`logical_not <tvm.relay.logical_not>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.logical_not:1
msgid "Compute element-wise logical not of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`logical_or <tvm.relay.logical_or>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.logical_or:1
msgid "logical OR with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`logical_xor <tvm.relay.logical_xor>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.logical_xor:1
msgid "logical XOR with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`logsumexp <tvm.relay.logsumexp>`\\ \\(data\\[\\, axis\\, "
"keepdims\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.logsumexp:1
msgid ""
"Compute the log of the sum of exponentials of input elements over given "
"axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`matrix_set_diag <tvm.relay.matrix_set_diag>`\\ \\(data\\, "
"diagonal\\[\\, k\\, align\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.matrix_set_diag:1
msgid ""
"Returns a tensor with the diagonals of input tensor replaced with the "
"provided diagonal values."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`max <tvm.relay.max>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.max:1
msgid "Computes the max of array elements over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`maximum <tvm.relay.maximum>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.maximum:1
msgid "Maximum with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`mean <tvm.relay.mean>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.mean:1
msgid "Computes the mean of array elements over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`mean_std <tvm.relay.mean_std>`\\ \\(data\\[\\, axis\\, "
"keepdims\\, exclude\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.mean_std:1
msgid "Computes the mean and standard deviation of data over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`mean_variance <tvm.relay.mean_variance>`\\ \\(data\\[\\, axis\\,"
" keepdims\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.mean_variance:1
msgid "Computes the mean and variance of data over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`meshgrid <tvm.relay.meshgrid>`\\ \\(data\\[\\, indexing\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.meshgrid:1
msgid "Create coordinate matrices from coordinate vectors."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`min <tvm.relay.min>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.min:1
msgid "Computes the min of array elements over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`minimum <tvm.relay.minimum>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.minimum:1
msgid "Minimum with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`mod <tvm.relay.mod>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.mod:1
msgid "Mod with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`multiply <tvm.relay.multiply>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.multiply:1
msgid "Multiplication with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`ndarray_size <tvm.relay.ndarray_size>`\\ \\(data\\[\\, "
"dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.ndarray_size:1
msgid "Get number of elements of input tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`negative <tvm.relay.negative>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.negative:1
msgid "Compute element-wise negative of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`not_equal <tvm.relay.not_equal>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.not_equal:1
msgid "Broadcasted elementwise test for (lhs != rhs)."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`one_hot <tvm.relay.one_hot>`\\ \\(indices\\, on\\_value\\, "
"off\\_value\\, depth\\, ...\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
"Returns a one-hot tensor where the locations represented by indices take "
"value on_value, and other locations take value off_value."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`ones <tvm.relay.ones>`\\ \\(shape\\, dtype\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.ones:1
msgid "Fill array with ones."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`ones_like <tvm.relay.ones_like>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.ones_like:1
msgid "Returns an array of ones, with same type and shape as the input."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`optimize <tvm.relay.optimize>`\\ \\(mod\\[\\, target\\, "
"params\\]\\)"
msgstr ""

#: of tvm.relay.build_module.optimize:1 tvm.relay.expr.Call:1:<autosummary>:1
msgid "Helper function that optimizes a Relay module."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`power <tvm.relay.power>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.power:1
msgid "Power with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`pretty_print <tvm.relay.pretty_print>`\\ \\(obj\\)"
msgstr ""

#: of tvm.relay.base.pretty_print:1 tvm.relay.expr.Call:1:<autosummary>:1
msgid "Pretty print the object."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`prod <tvm.relay.prod>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.prod:1
msgid "Computes the products of array elements over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`reinterpret <tvm.relay.reinterpret>`\\ \\(data\\, dtype\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.reinterpret:1
msgid "Reinterpret input tensor to data type."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`repeat <tvm.relay.repeat>`\\ \\(data\\, repeats\\, axis\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Repeats elements of an array."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`reshape <tvm.relay.reshape>`\\ \\(data\\, newshape\\[\\, "
"allowzero\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.reshape:1
msgid "Reshape the input array."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`reshape_like <tvm.relay.reshape_like>`\\ \\(data\\, "
"shape\\_like\\[\\, lhs\\_begin\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Reshapes the input tensor by the size of another tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`reverse <tvm.relay.reverse>`\\ \\(data\\, axis\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.reverse:1
msgid ""
"Reverses the order of elements along given axis while preserving array "
"shape."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`reverse_reshape <tvm.relay.reverse_reshape>`\\ \\(data\\, "
"newshape\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.reverse_reshape:1
msgid ""
"Reshapes the input array where the special values are inferred from right"
" to left."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`reverse_sequence <tvm.relay.reverse_sequence>`\\ \\(data\\, "
"seq\\_lengths\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Reverse the tensor for variable length slices."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`right_shift <tvm.relay.right_shift>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.right_shift:1
msgid "Right shift with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`round <tvm.relay.round>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.round:1
msgid "Compute element-wise round of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`rsqrt <tvm.relay.rsqrt>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.rsqrt:1
msgid "Compute elementwise rsqrt of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`save_param_dict <tvm.relay.save_param_dict>`\\ \\(params\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.param_dict.save_param_dict:1
msgid "Save parameter dictionary to binary bytes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`scalar_type <tvm.relay.scalar_type>`\\ \\(dtype\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.ty.scalar_type:1
msgid "Creates a scalar type."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`scatter_elements <tvm.relay.scatter_elements>`\\ \\(data\\, "
"indices\\, updates\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.scatter_elements:1
msgid ""
"Scatter elements with updating data by reduction of values in updates at "
"positions defined by indices."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`scatter_nd <tvm.relay.scatter_nd>`\\ \\(data\\, indices\\, "
"updates\\[\\, mode\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.scatter_nd:1
msgid "Scatter values from an array and update."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`script <tvm.relay.script>`\\ \\(pyfunc\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.te.hybrid.script:1
msgid "Decorate a python function as hybrid script."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`searchsorted <tvm.relay.searchsorted>`\\ \\(sorted\\_sequence\\,"
" values\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.algorithm.searchsorted:3
msgid "Find indices where elements should be inserted to maintain order."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`segment_sum <tvm.relay.segment_sum>`\\ \\(data\\, "
"segment\\_ids\\[\\, num\\_segments\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Computes the sum along segment_ids along axis 0."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`sequence_mask <tvm.relay.sequence_mask>`\\ \\(data\\, "
"valid\\_length\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.sequence_mask:1
msgid ""
"Sets all elements outside the expected length of the sequence to a "
"constant value."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`setrecursionlimit <tvm.relay.setrecursionlimit>`\\ \\(limit\\, "
"\\/\\)"
msgstr ""

#: of sys.setrecursionlimit:1 tvm.relay.expr.Call:1:<autosummary>:1
msgid "Set the maximum depth of the Python interpreter stack to n."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`shape_of <tvm.relay.shape_of>`\\ \\(data\\[\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.shape_of:1
msgid "Get shape of a tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`sigmoid <tvm.relay.sigmoid>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.sigmoid:1
msgid "Compute elementwise sigmoid of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`sign <tvm.relay.sign>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`sin <tvm.relay.sin>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.sin:1
msgid "Compute elementwise sin of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`sinh <tvm.relay.sinh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.sinh:1
msgid "Compute elementwise sinh of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`slice_like <tvm.relay.slice_like>`\\ \\(data\\, "
"shape\\_like\\[\\, axes\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.slice_like:1
msgid "Slice the first input with respect to the second input."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`sliding_window <tvm.relay.sliding_window>`\\ \\(data\\, axis\\, "
"window\\_shape\\, strides\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.sliding_window:1
msgid "Slide a window over the data tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`sort <tvm.relay.sort>`\\ \\(data\\[\\, axis\\, is\\_ascend\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.algorithm.sort:1
msgid "Performs sorting along the given axis and returns data in sorted order."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`sparse_fill_empty_rows <tvm.relay.sparse_fill_empty_rows>`\\ "
"\\(sparse\\_indices\\, ...\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Fill rows in a sparse matrix that do not contain any values."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`sparse_reshape <tvm.relay.sparse_reshape>`\\ "
"\\(sparse\\_indices\\, prev\\_shape\\, ...\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Reshape a sparse tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`sparse_to_dense <tvm.relay.sparse_to_dense>`\\ "
"\\(sparse\\_indices\\, ...\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.sparse_to_dense:1
msgid "Converts a sparse representation into a dense tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`split <tvm.relay.split>`\\ \\(data\\, "
"indices\\_or\\_sections\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.split:1
msgid "Split input tensor along axis by sections or indices."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`sqrt <tvm.relay.sqrt>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.sqrt:1
msgid "Compute elementwise sqrt of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`squeeze <tvm.relay.squeeze>`\\ \\(data\\[\\, axis\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.squeeze:1
msgid "Squeeze axes in the array."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`stack <tvm.relay.stack>`\\ \\(data\\, axis\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.stack:1
msgid "Join a sequence of arrays along a new axis."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`std <tvm.relay.std>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\, unbiased\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.std:1
msgid "Computes the standard deviation of data over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`stft <tvm.relay.stft>`\\ \\(data\\, n\\_fft\\[\\, "
"hop\\_length\\, win\\_length\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
"The STFT computes the Fourier transform of short overlapping windows of "
"the input."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`strided_set <tvm.relay.strided_set>`\\ \\(data\\, v\\, begin\\, "
"end\\[\\, strides\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.strided_set:1
msgid "Strided set of an array."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`strided_slice <tvm.relay.strided_slice>`\\ \\(data\\, begin\\, "
"end\\[\\, strides\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.strided_slice:1
msgid "Strided slice of an array."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`subtract <tvm.relay.subtract>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.subtract:1
msgid "Subtraction with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`sum <tvm.relay.sum>`\\ \\(data\\[\\, axis\\, keepdims\\, "
"exclude\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.sum:1
msgid "Computes the sum of array elements over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`take <tvm.relay.take>`\\ \\(data\\, indices\\[\\, axis\\, "
"batch\\_dims\\, mode\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.take:1
msgid "Take elements from an array along an axis."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`tan <tvm.relay.tan>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.tan:1
msgid "Compute elementwise tan of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`tanh <tvm.relay.tanh>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.tanh:1
msgid "Compute element-wise tanh of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`tile <tvm.relay.tile>`\\ \\(data\\, reps\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.tile:1
msgid "Repeats the whole array multiple times."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`topk <tvm.relay.topk>`\\ \\(data\\[\\, k\\, axis\\, "
"ret\\_type\\, is\\_ascend\\, dtype\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.algorithm.topk:1
msgid "Get the top k elements in an input tensor along the given axis."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`transpose <tvm.relay.transpose>`\\ \\(data\\[\\, axes\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.transpose:1
msgid "Permutes the dimensions of an array."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`trilu <tvm.relay.trilu>`\\ \\(data\\, k\\[\\, upper\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.trilu:1
msgid ""
"Given a 2-D matrix or batches of 2-D matrices, returns the upper or lower"
" triangular part of the tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`trunc <tvm.relay.trunc>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.trunc:1
msgid "Compute element-wise trunc of data."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`trunc_divide <tvm.relay.trunc_divide>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.trunc_divide:1
msgid "Trunc division with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`trunc_mod <tvm.relay.trunc_mod>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.trunc_mod:1
msgid "Trunc mod with numpy-style broadcasting."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`unique <tvm.relay.unique>`\\ \\(data\\[\\, is\\_sorted\\, "
"return\\_counts\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid "Find the unique elements of a 1-D tensor."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`unravel_index <tvm.relay.unravel_index>`\\ \\(indices\\, shape\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
#: tvm.relay.op.transform.unravel_index:1
msgid ""
"Convert a flat index or array of flat indices into a tuple of coordinate "
"arrays."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`var <tvm.relay.var>`\\ \\(name\\_hint\\[\\, type\\_annotation\\,"
" shape\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.expr.var:1
msgid "Create a new tvm.relay.Var."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ""
":py:obj:`variance <tvm.relay.variance>`\\ \\(data\\[\\, axis\\, "
"keepdims\\, exclude\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.reduce.variance:1
msgid "Computes the variance of data over given axes."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`where <tvm.relay.where>`\\ \\(condition\\, x\\, y\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.transform.where:1
msgid ""
"Selecting elements from either x or y depending on the value of the "
"condition."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`zeros <tvm.relay.zeros>`\\ \\(shape\\, dtype\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.zeros:1
msgid "Fill array with zeros."
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1
msgid ":py:obj:`zeros_like <tvm.relay.zeros_like>`\\ \\(data\\)"
msgstr ""

#: of tvm.relay.expr.Call:1:<autosummary>:1 tvm.relay.op.tensor.zeros_like:1
msgid "Returns an array of zeros, with same type and shape as the input."
msgstr ""

#: of tvm.relay.expr.Call:3
msgid ""
"Call node corresponds the operator application node in computational "
"graph terminology."
msgstr ""

#: of tvm.ir.adt.TypeData:9 tvm.ir.base.SequentialSpan:7
#: tvm.relay.adt.Clause.__init__:4 tvm.relay.adt.Match.__init__:4
#: tvm.relay.adt.PatternConstructor.__init__:4
#: tvm.relay.adt.PatternTuple.__init__:4 tvm.relay.adt.PatternVar.__init__:4
#: tvm.relay.adt.PatternWildcard.__init__:4 tvm.relay.base.astext:4
#: tvm.relay.build_module.build:4 tvm.relay.build_module.create_executor:18
#: tvm.relay.build_module.optimize:4 tvm.relay.expr.Call:7
#: tvm.relay.expr.Constant:4 tvm.relay.expr.If:4 tvm.relay.expr.Let:4
#: tvm.relay.expr.Tuple:4 tvm.relay.expr.Tuple.astype:4
#: tvm.relay.expr.TupleGetItem:4 tvm.relay.expr.TupleWrapper:8
#: tvm.relay.expr.bind:6 tvm.relay.expr.const:4 tvm.relay.expr.var:7
#: tvm.relay.function.Function:4 tvm.relay.function.Function.__call__:4
#: tvm.relay.function.Function.astext:4 tvm.relay.op.algorithm.argsort:5
#: tvm.relay.op.algorithm.searchsorted:6 tvm.relay.op.algorithm.sort:4
#: tvm.relay.op.algorithm.topk:6 tvm.relay.op.reduce.all:4
#: tvm.relay.op.reduce.any:4 tvm.relay.op.reduce.argmax:4
#: tvm.relay.op.reduce.argmin:4 tvm.relay.op.reduce.logsumexp:8
#: tvm.relay.op.reduce.max:4 tvm.relay.op.reduce.mean:4
#: tvm.relay.op.reduce.mean_std:4 tvm.relay.op.reduce.mean_variance:4
#: tvm.relay.op.reduce.min:4 tvm.relay.op.reduce.prod:4
#: tvm.relay.op.reduce.std:4 tvm.relay.op.reduce.sum:4
#: tvm.relay.op.reduce.variance:4 tvm.relay.op.tensor.abs:4
#: tvm.relay.op.tensor.acos:4 tvm.relay.op.tensor.acosh:4
#: tvm.relay.op.tensor.add:4 tvm.relay.op.tensor.asin:4
#: tvm.relay.op.tensor.asinh:4 tvm.relay.op.tensor.atan:4
#: tvm.relay.op.tensor.atanh:4 tvm.relay.op.tensor.bitwise_and:4
#: tvm.relay.op.tensor.bitwise_not:4 tvm.relay.op.tensor.bitwise_or:4
#: tvm.relay.op.tensor.bitwise_xor:4 tvm.relay.op.tensor.ceil:4
#: tvm.relay.op.tensor.clip:5 tvm.relay.op.tensor.concatenate:4
#: tvm.relay.op.tensor.copy:4 tvm.relay.op.tensor.cos:4
#: tvm.relay.op.tensor.cosh:4 tvm.relay.op.tensor.device_copy:6
#: tvm.relay.op.tensor.divide:4 tvm.relay.op.tensor.einsum:4
#: tvm.relay.op.tensor.equal:4 tvm.relay.op.tensor.erf:4
#: tvm.relay.op.tensor.exp:4 tvm.relay.op.tensor.fixed_point_multiply:6
#: tvm.relay.op.tensor.floor:4 tvm.relay.op.tensor.floor_divide:4
#: tvm.relay.op.tensor.floor_mod:4 tvm.relay.op.tensor.greater:4
#: tvm.relay.op.tensor.greater_equal:4 tvm.relay.op.tensor.isfinite:4
#: tvm.relay.op.tensor.isinf:4 tvm.relay.op.tensor.isnan:4
#: tvm.relay.op.tensor.left_shift:4 tvm.relay.op.tensor.less:4
#: tvm.relay.op.tensor.less_equal:4 tvm.relay.op.tensor.log:4
#: tvm.relay.op.tensor.log10:4 tvm.relay.op.tensor.log2:4
#: tvm.relay.op.tensor.logical_and:4 tvm.relay.op.tensor.logical_not:4
#: tvm.relay.op.tensor.logical_or:4 tvm.relay.op.tensor.logical_xor:4
#: tvm.relay.op.tensor.maximum:4 tvm.relay.op.tensor.minimum:4
#: tvm.relay.op.tensor.mod:4 tvm.relay.op.tensor.multiply:4
#: tvm.relay.op.tensor.ndarray_size:4 tvm.relay.op.tensor.negative:4
#: tvm.relay.op.tensor.not_equal:4 tvm.relay.op.tensor.ones:4
#: tvm.relay.op.tensor.ones_like:4 tvm.relay.op.tensor.power:4
#: tvm.relay.op.tensor.right_shift:4 tvm.relay.op.tensor.round:4
#: tvm.relay.op.tensor.rsqrt:8 tvm.relay.op.tensor.shape_of:4
#: tvm.relay.op.tensor.sigmoid:4 tvm.relay.op.tensor.sign:4
#: tvm.relay.op.tensor.sin:4 tvm.relay.op.tensor.sinh:4
#: tvm.relay.op.tensor.sqrt:4 tvm.relay.op.tensor.stack:4
#: tvm.relay.op.tensor.subtract:4 tvm.relay.op.tensor.tan:4
#: tvm.relay.op.tensor.tanh:4 tvm.relay.op.tensor.trunc:4
#: tvm.relay.op.tensor.trunc_divide:4 tvm.relay.op.tensor.trunc_mod:4
#: tvm.relay.op.tensor.zeros:4 tvm.relay.op.tensor.zeros_like:4
#: tvm.relay.op.transform.adv_index:4 tvm.relay.op.transform.arange:11
#: tvm.relay.op.transform.argwhere:5 tvm.relay.op.transform.broadcast_to:5
#: tvm.relay.op.transform.broadcast_to_like:4 tvm.relay.op.transform.cast:4
#: tvm.relay.op.transform.cast_like:4
#: tvm.relay.op.transform.collapse_sum_like:4
#: tvm.relay.op.transform.collapse_sum_to:4 tvm.relay.op.transform.cumprod:5
#: tvm.relay.op.transform.cumsum:5 tvm.relay.op.transform.dft:5
#: tvm.relay.op.transform.expand_dims:4 tvm.relay.op.transform.full:4
#: tvm.relay.op.transform.full_like:4 tvm.relay.op.transform.gather:15
#: tvm.relay.op.transform.gather_nd:5
#: tvm.relay.op.transform.invert_permutation:10
#: tvm.relay.op.transform.layout_transform:4
#: tvm.relay.op.transform.matrix_set_diag:5 tvm.relay.op.transform.meshgrid:7
#: tvm.relay.op.transform.one_hot:6 tvm.relay.op.transform.reinterpret:4
#: tvm.relay.op.transform.repeat:5 tvm.relay.op.transform.reshape:54
#: tvm.relay.op.transform.reshape_like:11 tvm.relay.op.transform.reverse:4
#: tvm.relay.op.transform.reverse_reshape:14
#: tvm.relay.op.transform.reverse_sequence:5
#: tvm.relay.op.transform.scatter_elements:5
#: tvm.relay.op.transform.scatter_nd:6 tvm.relay.op.transform.segment_sum:11
#: tvm.relay.op.transform.sequence_mask:7 tvm.relay.op.transform.slice_like:7
#: tvm.relay.op.transform.sliding_window:4
#: tvm.relay.op.transform.sparse_fill_empty_rows:6
#: tvm.relay.op.transform.sparse_reshape:4
#: tvm.relay.op.transform.sparse_to_dense:4 tvm.relay.op.transform.split:10
#: tvm.relay.op.transform.squeeze:4 tvm.relay.op.transform.stft:5
#: tvm.relay.op.transform.strided_set:4 tvm.relay.op.transform.strided_slice:4
#: tvm.relay.op.transform.take:4 tvm.relay.op.transform.tile:4
#: tvm.relay.op.transform.transpose:4 tvm.relay.op.transform.trilu:5
#: tvm.relay.op.transform.unique:5 tvm.relay.op.transform.unravel_index:4
#: tvm.relay.op.transform.where:10 tvm.relay.param_dict.load_param_dict:7
#: tvm.relay.param_dict.save_param_dict:10
#: tvm.relay.scope_builder.ScopeBuilder.if_scope:4
#: tvm.relay.scope_builder.ScopeBuilder.let:4
#: tvm.relay.scope_builder.ScopeBuilder.ret:4
#: tvm.relay.scope_builder.ScopeBuilder.type_of:4
#: tvm.relay.transform.transform.build_config:6 tvm.relay.ty.ShapeVar:4
#: tvm.relay.ty.scalar_type:6
msgid "Parameters"
msgstr ""

#: of tvm.relay.expr.Call:9
msgid "op: tvm.ir.Op or any tvm.relay.Expr with function type."
msgstr ""

#: of tvm.relay.expr.Call:9
msgid "The operation to be called."
msgstr ""

#: of tvm.relay.expr.Call:12
msgid "args: List[tvm.relay.Expr]"
msgstr ""

#: of tvm.relay.expr.Call:12
msgid "The arguments to the call."
msgstr ""

#: of tvm.relay.expr.Call:15
msgid "attrs: Optional[tvm.Attrs]"
msgstr ""

#: of tvm.relay.expr.Call:15
msgid "Attributes to the call, can be None"
msgstr ""

#: of tvm.relay.expr.Call:19
msgid "type_args: Optional[List[tvm.relay.Type]]"
msgstr ""

#: of tvm.relay.expr.Call:18
msgid ""
"The additional type arguments, this is only used in advanced usecase of "
"template functions."
msgstr ""

#: of tvm.relay.expr.Call:21 tvm.relay.expr.Constant:8 tvm.relay.expr.If:14
#: tvm.relay.expr.Let:14 tvm.relay.expr.RefCreate:7 tvm.relay.expr.RefRead:7
#: tvm.relay.expr.RefWrite:11 tvm.relay.expr.Tuple:9
#: tvm.relay.expr.TupleGetItem:11 tvm.relay.expr.const:12 tvm.relay.expr.var:24
#: tvm.relay.function.Function:19
msgid "span: Optional[tvm.relay.Span]"
msgstr ""

#: of tvm.relay.expr.Call:22 tvm.relay.expr.Constant:9 tvm.relay.expr.If:15
#: tvm.relay.expr.Let:15 tvm.relay.expr.RefCreate:8 tvm.relay.expr.RefRead:8
#: tvm.relay.expr.RefWrite:12 tvm.relay.expr.Tuple:9
#: tvm.relay.expr.TupleGetItem:12 tvm.relay.expr.const:12 tvm.relay.expr.var:24
#: tvm.relay.function.Function:19
msgid "Span that points to original source code."
msgstr ""

#: of tvm.relay.adt.Clause:1 tvm.relay.adt.Match:1
#: tvm.relay.adt.PatternConstructor:1 tvm.relay.adt.PatternTuple:1
#: tvm.relay.adt.PatternVar:1 tvm.relay.adt.PatternWildcard:1
#: tvm.relay.expr.Tuple:1 tvm.relay.expr.TupleWrapper:1
#: tvm.relay.expr_functor.ExprFunctor:1 tvm.relay.function.Function:1
#: tvm.relay.prelude.Prelude:1 tvm.relay.scope_builder.ScopeBuilder:1
#: tvm.relay.type_functor.TypeFunctor:1
msgid "**Methods:**"
msgstr ""

#: of tvm.relay.adt.Clause.__init__:1:<autosummary>:1
msgid ":py:obj:`__init__ <tvm.relay.Clause.__init__>`\\ \\(lhs\\, rhs\\)"
msgstr ""

#: of tvm.relay.adt.Clause.__init__:1
#: tvm.relay.adt.Clause.__init__:1:<autosummary>:1
msgid "Construct a clause."
msgstr ""

#: of tvm.relay.adt.Clause.__init__:5
msgid "lhs: tvm.relay.Pattern"
msgstr ""

#: of tvm.relay.adt.Clause.__init__:6
msgid "Left-hand side of match clause."
msgstr ""

#: of tvm.relay.adt.Clause.__init__:8
msgid "rhs: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.adt.Clause.__init__:8
msgid "Right-hand side of match clause."
msgstr ""

#: of tvm.relay.adt.Clause.__init__:11 tvm.relay.adt.Match.__init__:16
#: tvm.relay.adt.PatternConstructor.__init__:12
#: tvm.relay.adt.PatternTuple.__init__:10 tvm.relay.adt.PatternVar.__init__:8
#: tvm.relay.adt.PatternWildcard.__init__:8 tvm.relay.base.astext:15
#: tvm.relay.build_module.build:42 tvm.relay.build_module.create_executor:40
#: tvm.relay.build_module.optimize:18 tvm.relay.expr.Tuple.astype:13
#: tvm.relay.expr.TupleWrapper.astext:4 tvm.relay.expr.bind:14
#: tvm.relay.function.Function.astext:14 tvm.relay.op.algorithm.argsort:22
#: tvm.relay.op.algorithm.searchsorted:26 tvm.relay.op.algorithm.sort:15
#: tvm.relay.op.algorithm.topk:29 tvm.relay.op.reduce.all:23
#: tvm.relay.op.reduce.any:23 tvm.relay.op.reduce.argmax:27
#: tvm.relay.op.reduce.argmin:27 tvm.relay.op.reduce.logsumexp:22
#: tvm.relay.op.reduce.max:23 tvm.relay.op.reduce.mean:23
#: tvm.relay.op.reduce.mean_std:23 tvm.relay.op.reduce.mean_variance:26
#: tvm.relay.op.reduce.min:24 tvm.relay.op.reduce.prod:23
#: tvm.relay.op.reduce.std:26 tvm.relay.op.reduce.sum:23
#: tvm.relay.op.reduce.variance:29 tvm.relay.op.tensor.abs:9
#: tvm.relay.op.tensor.acos:9 tvm.relay.op.tensor.acosh:9
#: tvm.relay.op.tensor.add:11 tvm.relay.op.tensor.asin:9
#: tvm.relay.op.tensor.asinh:9 tvm.relay.op.tensor.atan:9
#: tvm.relay.op.tensor.atanh:9 tvm.relay.op.tensor.bitwise_and:11
#: tvm.relay.op.tensor.bitwise_not:9 tvm.relay.op.tensor.bitwise_or:11
#: tvm.relay.op.tensor.bitwise_xor:11 tvm.relay.op.tensor.ceil:9
#: tvm.relay.op.tensor.clip:14 tvm.relay.op.tensor.concatenate:11
#: tvm.relay.op.tensor.copy:9 tvm.relay.op.tensor.cos:9
#: tvm.relay.op.tensor.cosh:9 tvm.relay.op.tensor.device_copy:17
#: tvm.relay.op.tensor.divide:11 tvm.relay.op.tensor.einsum:11
#: tvm.relay.op.tensor.equal:11 tvm.relay.op.tensor.erf:9
#: tvm.relay.op.tensor.exp:9 tvm.relay.op.tensor.fixed_point_multiply:15
#: tvm.relay.op.tensor.floor:9 tvm.relay.op.tensor.floor_divide:11
#: tvm.relay.op.tensor.floor_mod:11 tvm.relay.op.tensor.greater:11
#: tvm.relay.op.tensor.greater_equal:11 tvm.relay.op.tensor.isfinite:9
#: tvm.relay.op.tensor.isinf:9 tvm.relay.op.tensor.isnan:9
#: tvm.relay.op.tensor.left_shift:11 tvm.relay.op.tensor.less:11
#: tvm.relay.op.tensor.less_equal:11 tvm.relay.op.tensor.log:9
#: tvm.relay.op.tensor.log10:9 tvm.relay.op.tensor.log2:9
#: tvm.relay.op.tensor.logical_and:11 tvm.relay.op.tensor.logical_not:9
#: tvm.relay.op.tensor.logical_or:11 tvm.relay.op.tensor.logical_xor:11
#: tvm.relay.op.tensor.maximum:11 tvm.relay.op.tensor.minimum:11
#: tvm.relay.op.tensor.mod:11 tvm.relay.op.tensor.multiply:11
#: tvm.relay.op.tensor.ndarray_size:12 tvm.relay.op.tensor.negative:9
#: tvm.relay.op.tensor.not_equal:11 tvm.relay.op.tensor.ones:12
#: tvm.relay.op.tensor.ones_like:9 tvm.relay.op.tensor.power:11
#: tvm.relay.op.tensor.right_shift:11 tvm.relay.op.tensor.round:9
#: tvm.relay.op.tensor.rsqrt:13 tvm.relay.op.tensor.shape_of:12
#: tvm.relay.op.tensor.sigmoid:9 tvm.relay.op.tensor.sign:9
#: tvm.relay.op.tensor.sin:9 tvm.relay.op.tensor.sinh:9
#: tvm.relay.op.tensor.sqrt:9 tvm.relay.op.tensor.stack:12
#: tvm.relay.op.tensor.subtract:11 tvm.relay.op.tensor.tan:9
#: tvm.relay.op.tensor.tanh:9 tvm.relay.op.tensor.trunc:9
#: tvm.relay.op.tensor.trunc_divide:11 tvm.relay.op.tensor.trunc_mod:11
#: tvm.relay.op.tensor.zeros:12 tvm.relay.op.tensor.zeros_like:9
#: tvm.relay.op.transform.adv_index:10 tvm.relay.op.transform.arange:26
#: tvm.relay.op.transform.argwhere:10 tvm.relay.op.transform.broadcast_to:13
#: tvm.relay.op.transform.broadcast_to_like:12 tvm.relay.op.transform.cast:12
#: tvm.relay.op.transform.cast_like:12
#: tvm.relay.op.transform.collapse_sum_like:12
#: tvm.relay.op.transform.collapse_sum_to:12 tvm.relay.op.transform.cumprod:24
#: tvm.relay.op.transform.cumsum:24 tvm.relay.op.transform.dft:17
#: tvm.relay.op.transform.expand_dims:18 tvm.relay.op.transform.full:15
#: tvm.relay.op.transform.full_like:12 tvm.relay.op.transform.gather_nd:20
#: tvm.relay.op.transform.invert_permutation:15
#: tvm.relay.op.transform.layout_transform:15
#: tvm.relay.op.transform.matrix_set_diag:28 tvm.relay.op.transform.meshgrid:15
#: tvm.relay.op.transform.one_hot:26 tvm.relay.op.transform.reinterpret:12
#: tvm.relay.op.transform.repeat:18 tvm.relay.op.transform.reshape:65
#: tvm.relay.op.transform.reshape_like:34 tvm.relay.op.transform.reverse:12
#: tvm.relay.op.transform.reverse_reshape:22
#: tvm.relay.op.transform.reverse_sequence:22
#: tvm.relay.op.transform.scatter_elements:29
#: tvm.relay.op.transform.scatter_nd:26 tvm.relay.op.transform.segment_sum:27
#: tvm.relay.op.transform.sequence_mask:21 tvm.relay.op.transform.slice_like:19
#: tvm.relay.op.transform.sliding_window:23
#: tvm.relay.op.transform.sparse_fill_empty_rows:22
#: tvm.relay.op.transform.sparse_reshape:16
#: tvm.relay.op.transform.sparse_to_dense:19 tvm.relay.op.transform.split:21
#: tvm.relay.op.transform.squeeze:14 tvm.relay.op.transform.stft:30
#: tvm.relay.op.transform.strided_set:22
#: tvm.relay.op.transform.strided_slice:32 tvm.relay.op.transform.take:25
#: tvm.relay.op.transform.tile:12 tvm.relay.op.transform.transpose:12
#: tvm.relay.op.transform.trilu:19 tvm.relay.op.transform.unique:16
#: tvm.relay.op.transform.unravel_index:12 tvm.relay.op.transform.where:21
#: tvm.relay.param_dict.load_param_dict:12
#: tvm.relay.param_dict.save_param_dict:15
#: tvm.relay.scope_builder.ScopeBuilder.else_scope:4
#: tvm.relay.scope_builder.ScopeBuilder.get:4
#: tvm.relay.scope_builder.ScopeBuilder.if_scope:9
#: tvm.relay.transform.transform.build_config:38 tvm.relay.ty.ShapeVar:8
#: tvm.relay.ty.scalar_type:11 tvm.te.hybrid.script:7
msgid "Returns"
msgstr ""

#: of tvm.relay.adt.Clause.__init__:12
msgid "clause: Clause"
msgstr ""

#: of tvm.relay.adt.Clause.__init__:13
msgid "The Clause."
msgstr ""

#: of tvm.relay.expr.Constant:6 tvm.relay.op.algorithm.argsort:7
#: tvm.relay.op.algorithm.sort:6 tvm.relay.op.algorithm.topk:8
#: tvm.relay.op.reduce.all:6 tvm.relay.op.reduce.any:6
#: tvm.relay.op.reduce.argmax:6 tvm.relay.op.reduce.argmin:6
#: tvm.relay.op.reduce.logsumexp:10 tvm.relay.op.reduce.max:6
#: tvm.relay.op.reduce.mean:6 tvm.relay.op.reduce.mean_std:6
#: tvm.relay.op.reduce.mean_variance:6 tvm.relay.op.reduce.min:6
#: tvm.relay.op.reduce.prod:6 tvm.relay.op.reduce.std:6
#: tvm.relay.op.reduce.sum:6 tvm.relay.op.reduce.variance:6
#: tvm.relay.op.tensor.abs:6 tvm.relay.op.tensor.acos:6
#: tvm.relay.op.tensor.acosh:6 tvm.relay.op.tensor.asin:6
#: tvm.relay.op.tensor.asinh:6 tvm.relay.op.tensor.atan:6
#: tvm.relay.op.tensor.atanh:6 tvm.relay.op.tensor.bitwise_not:6
#: tvm.relay.op.tensor.ceil:6 tvm.relay.op.tensor.concatenate:5
#: tvm.relay.op.tensor.copy:6 tvm.relay.op.tensor.cos:6
#: tvm.relay.op.tensor.cosh:6 tvm.relay.op.tensor.device_copy:8
#: tvm.relay.op.tensor.einsum:5 tvm.relay.op.tensor.erf:6
#: tvm.relay.op.tensor.exp:6 tvm.relay.op.tensor.fixed_point_multiply:7
#: tvm.relay.op.tensor.floor:6 tvm.relay.op.tensor.isfinite:6
#: tvm.relay.op.tensor.isinf:6 tvm.relay.op.tensor.isnan:6
#: tvm.relay.op.tensor.log:6 tvm.relay.op.tensor.log10:6
#: tvm.relay.op.tensor.log2:6 tvm.relay.op.tensor.logical_not:6
#: tvm.relay.op.tensor.ndarray_size:6 tvm.relay.op.tensor.negative:6
#: tvm.relay.op.tensor.ones_like:6 tvm.relay.op.tensor.round:6
#: tvm.relay.op.tensor.rsqrt:10 tvm.relay.op.tensor.shape_of:6
#: tvm.relay.op.tensor.sigmoid:6 tvm.relay.op.tensor.sign:6
#: tvm.relay.op.tensor.sin:6 tvm.relay.op.tensor.sinh:6
#: tvm.relay.op.tensor.sqrt:6 tvm.relay.op.tensor.stack:6
#: tvm.relay.op.tensor.tan:6 tvm.relay.op.tensor.tanh:6
#: tvm.relay.op.tensor.trunc:6 tvm.relay.op.tensor.zeros_like:6
#: tvm.relay.op.transform.broadcast_to:7
#: tvm.relay.op.transform.broadcast_to_like:6 tvm.relay.op.transform.cast:6
#: tvm.relay.op.transform.cast_like:6
#: tvm.relay.op.transform.collapse_sum_like:6
#: tvm.relay.op.transform.collapse_sum_to:6 tvm.relay.op.transform.cumprod:7
#: tvm.relay.op.transform.cumsum:7 tvm.relay.op.transform.expand_dims:6
#: tvm.relay.op.transform.full_like:6 tvm.relay.op.transform.gather:17
#: tvm.relay.op.transform.gather_nd:7
#: tvm.relay.op.transform.invert_permutation:12
#: tvm.relay.op.transform.layout_transform:6
#: tvm.relay.op.transform.matrix_set_diag:7 tvm.relay.op.transform.meshgrid:9
#: tvm.relay.op.transform.reinterpret:6 tvm.relay.op.transform.repeat:7
#: tvm.relay.op.transform.reshape:56 tvm.relay.op.transform.reshape_like:13
#: tvm.relay.op.transform.reverse:6 tvm.relay.op.transform.reverse_reshape:16
#: tvm.relay.op.transform.reverse_sequence:7
#: tvm.relay.op.transform.scatter_elements:7
#: tvm.relay.op.transform.scatter_nd:8 tvm.relay.op.transform.segment_sum:13
#: tvm.relay.op.transform.sequence_mask:9 tvm.relay.op.transform.slice_like:9
#: tvm.relay.op.transform.sliding_window:6 tvm.relay.op.transform.split:12
#: tvm.relay.op.transform.squeeze:6 tvm.relay.op.transform.stft:7
#: tvm.relay.op.transform.strided_set:6 tvm.relay.op.transform.strided_slice:6
#: tvm.relay.op.transform.take:6 tvm.relay.op.transform.tile:6
#: tvm.relay.op.transform.transpose:6 tvm.relay.op.transform.trilu:8
#: tvm.relay.op.transform.unique:7
msgid "data"
msgstr ""

#: of tvm.relay.expr.Constant:-1
msgid "tvm.nd.NDArray"
msgstr ""

#: of tvm.relay.expr.Constant:6
msgid "The data content of the constant expression."
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid ":py:obj:`checked_type <tvm.relay.Expr.checked_type>`\\"
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid "Get the checked type of tvm.relay.Expr."
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid ":py:obj:`struct_info <tvm.relay.Expr.struct_info>`\\"
msgstr ""

#: of tvm.ir.expr.RelayExpr:1:<autosummary>:1
msgid "Get the struct info field"
msgstr ""

#: of tvm.relay.expr_functor.ExprFunctor:3
msgid "Defines the default dispatch over expressions, and implements memoization."
msgstr ""

#: of tvm.relay.expr_functor.ExprFunctor.visit:1:<autosummary>:1
msgid ":py:obj:`visit <tvm.relay.ExprFunctor.visit>`\\ \\(expr\\)"
msgstr ""

#: of tvm.relay.expr_functor.ExprFunctor.visit:1
#: tvm.relay.expr_functor.ExprFunctor.visit:1:<autosummary>:1
msgid "Apply the visitor to an expression."
msgstr ""

#: of tvm.relay.expr_functor.ExprMutator:3 tvm.relay.type_functor.TypeMutator:3
msgid ""
"The default behavior recursively traverses the AST and reconstructs the "
"AST."
msgstr ""

#: of tvm.relay.expr_functor.ExprVisitor:3 tvm.relay.type_functor.TypeVisitor:3
msgid "The default behavior recursively traverses the AST."
msgstr ""

#: of tvm.relay.function.Function:6
msgid "params: List[tvm.relay.Var]"
msgstr ""

#: of tvm.relay.function.Function:6
msgid "List of input parameters to the function."
msgstr ""

#: of tvm.relay.expr.Let:12 tvm.relay.function.Function:9
msgid "body: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.function.Function:9
msgid "The body of the function."
msgstr ""

#: of tvm.relay.function.Function:12
msgid "ret_type: Optional[tvm.relay.Type]"
msgstr ""

#: of tvm.relay.function.Function:12
msgid "The return type annotation of the function."
msgstr ""

#: of tvm.relay.function.Function:16
msgid "type_params: Optional[List[tvm.relay.TypeParam]]"
msgstr ""

#: of tvm.relay.function.Function:15
msgid ""
"The additional type parameters, this is only used in advanced usecase of "
"template functions."
msgstr ""

#: of tvm.relay.function.Function.__call__:1:<autosummary>:1
msgid ":py:obj:`__call__ <tvm.relay.Function.__call__>`\\ \\(\\*args\\)"
msgstr ""

#: of tvm.relay.function.Function.__call__:1
#: tvm.relay.function.Function.__call__:1:<autosummary>:1
msgid "Invoke the global function."
msgstr ""

#: of tvm.relay.function.Function.__call__:1:<autosummary>:1
msgid ""
":py:obj:`astext <tvm.relay.Function.astext>`\\ "
"\\(\\[show\\_meta\\_data\\, annotate\\]\\)"
msgstr ""

#: of tvm.relay.function.Function.__call__:5
msgid "args: List[relay.Expr]"
msgstr ""

#: of tvm.relay.function.Function.__call__:6
msgid "Arguments."
msgstr ""

#: of tvm.relay.base.astext:8 tvm.relay.function.Function.astext:7
msgid "show_meta_data"
msgstr ""

#: of tvm.relay.base.astext:-1 tvm.relay.function.Function.astext:-1
#: tvm.relay.op.reduce.all:-1 tvm.relay.op.reduce.any:-1
#: tvm.relay.op.reduce.argmax:-1 tvm.relay.op.reduce.argmin:-1
#: tvm.relay.op.reduce.logsumexp:-1 tvm.relay.op.reduce.max:-1
#: tvm.relay.op.reduce.mean:-1 tvm.relay.op.reduce.mean_std:-1
#: tvm.relay.op.reduce.mean_variance:-1 tvm.relay.op.reduce.min:-1
#: tvm.relay.op.reduce.prod:-1 tvm.relay.op.reduce.std:-1
#: tvm.relay.op.reduce.sum:-1 tvm.relay.op.reduce.variance:-1
#: tvm.relay.op.transform.dft:-1
msgid "bool"
msgstr ""

#: of tvm.relay.base.astext:8 tvm.relay.function.Function.astext:6
msgid "Whether to include meta data section in the text if there is meta data."
msgstr ""

#: of tvm.relay.base.astext:12 tvm.relay.function.Function.astext:11
msgid "annotate: Optional[Object->str]"
msgstr ""

#: of tvm.relay.base.astext:11 tvm.relay.function.Function.astext:10
msgid ""
"Optionally annotate function to provide additional information in the "
"comment block."
msgstr ""

#: of tvm.relay.base.astext:17 tvm.relay.expr.TupleWrapper.astext:5
#: tvm.relay.function.Function.astext:16
msgid "text"
msgstr ""

#: of tvm.relay.base.astext:-1 tvm.relay.build_module.create_executor:-1
#: tvm.relay.expr.Tuple.astype:-1 tvm.relay.expr.TupleWrapper.astext:-1
#: tvm.relay.function.Function.astext:-1 tvm.relay.op.tensor.einsum:-1
#: tvm.relay.op.transform.cast:-1 tvm.relay.op.transform.layout_transform:-1
#: tvm.relay.op.transform.one_hot:-1 tvm.relay.op.transform.reinterpret:-1
#: tvm.relay.ty.scalar_type:-1
msgid "str"
msgstr ""

#: of tvm.relay.base.astext:17 tvm.relay.function.Function.astext:16
msgid "The text format of the expression."
msgstr ""

#: of tvm.relay.base.astext:20 tvm.relay.function.Function.astext:19
#: tvm.relay.op.transform.tile:30
msgid "Notes"
msgstr ""

#: of tvm.relay.base.astext:21 tvm.relay.function.Function.astext:20
msgid ""
"The meta data section is necessary to fully parse the text format. "
"However, it can contain dumps that are big (e.g constant weights), so it "
"can be helpful to skip printing the meta data section."
msgstr ""

#: of tvm.relay.expr.If:6
msgid "cond: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.If:6
msgid "The condition."
msgstr ""

#: of tvm.relay.expr.If:9
msgid "true_branch: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.If:9
msgid "The expression evaluated when condition is true."
msgstr ""

#: of tvm.relay.expr.If:12
msgid "false_branch: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.If:12
msgid "The expression evaluated when condition is false."
msgstr ""

#: of tvm.relay.expr.Let:6
msgid "variable: tvm.relay.Var"
msgstr ""

#: of tvm.relay.expr.Let:6
msgid "The local variable to be bound."
msgstr ""

#: of tvm.relay.expr.Let:9 tvm.relay.expr.RefWrite:9
#: tvm.relay.scope_builder.ScopeBuilder.let:8
msgid "value: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.Let:9
msgid "The value to be bound."
msgstr ""

#: of tvm.relay.expr.Let:12
msgid "The body of the let binding."
msgstr ""

#: of tvm.relay.adt.Match.__init__:1:<autosummary>:1
msgid ""
":py:obj:`__init__ <tvm.relay.Match.__init__>`\\ \\(data\\, clauses\\[\\, "
"complete\\]\\)"
msgstr ""

#: of tvm.relay.adt.Match.__init__:1
#: tvm.relay.adt.Match.__init__:1:<autosummary>:1
msgid "Construct a Match."
msgstr ""

#: of tvm.relay.adt.Match.__init__:6
msgid "data: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.adt.Match.__init__:6
msgid "The value being deconstructed and matched."
msgstr ""

#: of tvm.relay.adt.Match.__init__:9
msgid "clauses: List[tvm.relay.Clause]"
msgstr ""

#: of tvm.relay.adt.Match.__init__:9
msgid "The pattern match clauses."
msgstr ""

#: of tvm.relay.adt.Match.__init__:13
msgid "complete: Optional[Bool]"
msgstr ""

#: of tvm.relay.adt.Match.__init__:12
msgid ""
"Should the match be complete (cover all cases)? If yes, the type checker "
"will generate an error if there are any missing cases."
msgstr ""

#: of tvm.relay.adt.Match.__init__:17
msgid "match: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.adt.Match.__init__:18
msgid "The match expression."
msgstr ""

#: of tvm.relay.adt.PatternConstructor.__init__:1:<autosummary>:1
msgid ""
":py:obj:`__init__ <tvm.relay.PatternConstructor.__init__>`\\ "
"\\(constructor\\[\\, patterns\\]\\)"
msgstr ""

#: of tvm.relay.adt.PatternConstructor.__init__:1
#: tvm.relay.adt.PatternConstructor.__init__:1:<autosummary>:1
msgid "Construct a constructor pattern."
msgstr ""

#: of tvm.relay.adt.PatternConstructor.__init__:5
msgid "constructor: Constructor"
msgstr ""

#: of tvm.relay.adt.PatternConstructor.__init__:6
msgid "The constructor."
msgstr ""

#: of tvm.relay.adt.PatternConstructor.__init__:9
#: tvm.relay.adt.PatternTuple.__init__:7
msgid "patterns: Optional[List[Pattern]]"
msgstr ""

#: of tvm.relay.adt.PatternConstructor.__init__:8
#: tvm.relay.adt.PatternTuple.__init__:6
msgid ""
"Optional subpatterns: for each field of the constructor, match to the "
"given subpattern (treated as a variable pattern by default)."
msgstr ""

#: of tvm.relay.adt.PatternConstructor.__init__:13
#: tvm.relay.adt.PatternTuple.__init__:11
#: tvm.relay.adt.PatternWildcard.__init__:9
msgid "wildcard: PatternWildcard"
msgstr ""

#: of tvm.relay.adt.PatternConstructor.__init__:14
#: tvm.relay.adt.PatternTuple.__init__:12
#: tvm.relay.adt.PatternWildcard.__init__:10
msgid "a wildcard pattern."
msgstr ""

#: of tvm.relay.adt.PatternTuple.__init__:1:<autosummary>:1
msgid ""
":py:obj:`__init__ <tvm.relay.PatternTuple.__init__>`\\ "
"\\(\\[patterns\\]\\)"
msgstr ""

#: of tvm.relay.adt.PatternTuple.__init__:1
#: tvm.relay.adt.PatternTuple.__init__:1:<autosummary>:1
msgid "Construct a tuple pattern."
msgstr ""

#: of tvm.relay.adt.PatternVar.__init__:1:<autosummary>:1
msgid ":py:obj:`__init__ <tvm.relay.PatternVar.__init__>`\\ \\(var\\)"
msgstr ""

#: of tvm.relay.adt.PatternVar.__init__:1
#: tvm.relay.adt.PatternVar.__init__:1:<autosummary>:1
msgid "Construct a variable pattern."
msgstr ""

#: of tvm.relay.adt.PatternVar.__init__:5
msgid "var: tvm.relay.Var"
msgstr ""

#: of tvm.relay.adt.PatternVar.__init__:9
msgid "pv: PatternVar"
msgstr ""

#: of tvm.relay.adt.PatternVar.__init__:10
msgid "A variable pattern."
msgstr ""

#: of tvm.relay.adt.PatternWildcard.__init__:1:<autosummary>:1
msgid ":py:obj:`__init__ <tvm.relay.PatternWildcard.__init__>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.adt.PatternWildcard.__init__:1
#: tvm.relay.adt.PatternWildcard.__init__:1:<autosummary>:1
msgid "Constructs a wildcard pattern."
msgstr ""

#: of tvm.relay.adt.PatternWildcard.__init__:5
msgid "None"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ""
":py:obj:`get_ctor <tvm.relay.Prelude.get_ctor>`\\ \\(ty\\_name\\, "
"canonical\\, dtype\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1
#: tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.get_ctor_static:1
#: tvm.relay.prelude.Prelude.get_tensor_ctor_static:1
msgid "Get constructor corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ""
":py:obj:`get_ctor_static <tvm.relay.Prelude.get_ctor_static>`\\ "
"\\(ty\\_name\\, name\\, dtype\\, shape\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ""
":py:obj:`get_global_var <tvm.relay.Prelude.get_global_var>`\\ "
"\\(canonical\\, dtype\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.get_global_var:1
msgid "Get global var corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ""
":py:obj:`get_global_var_static "
"<tvm.relay.Prelude.get_global_var_static>`\\ \\(canonical\\, dtype\\, "
"shape\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.get_global_var_static:1
msgid "Get var corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ":py:obj:`get_name <tvm.relay.Prelude.get_name>`\\ \\(canonical\\, dtype\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.get_name:1
#: tvm.relay.prelude.Prelude.get_name_static:1
msgid "Get name corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ""
":py:obj:`get_name_static <tvm.relay.Prelude.get_name_static>`\\ "
"\\(canonical\\, dtype\\, shape\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ""
":py:obj:`get_tensor_ctor_static "
"<tvm.relay.Prelude.get_tensor_ctor_static>`\\ \\(name\\, dtype\\, "
"shape\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ":py:obj:`get_type <tvm.relay.Prelude.get_type>`\\ \\(canonical\\, dtype\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.get_type:1
#: tvm.relay.prelude.Prelude.get_type_static:1
msgid "Get type corresponding to the canonical name"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ""
":py:obj:`get_type_static <tvm.relay.Prelude.get_type_static>`\\ "
"\\(canonical\\, dtype\\, shape\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
msgid ":py:obj:`load_prelude <tvm.relay.Prelude.load_prelude>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.prelude.Prelude.get_ctor:1:<autosummary>:1
#: tvm.relay.prelude.Prelude.load_prelude:1
msgid "Parses the Prelude from Relay's text format into a module."
msgstr ""

#: of tvm.relay.expr.RefCreate:1
msgid ""
"Create a new reference from initial value. Parameters ---------- value: "
"tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.RefCreate:5
msgid "The initial value."
msgstr ""

#: of tvm.relay.expr.RefRead:1
msgid ""
"Get the value inside the reference. Parameters ---------- ref: "
"tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.RefRead:5 tvm.relay.expr.RefWrite:6
msgid "The reference."
msgstr ""

#: of tvm.relay.expr.RefWrite:1
msgid ""
"Update the value inside the reference. The whole expression will evaluate"
" to an empty tuple. Parameters ---------- ref: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.RefWrite:9
msgid "The new value."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder:3
msgid "Enables users to build up a nested scope(let, if) expression easily."
msgstr ""

#: of tvm.relay.expr.var:27 tvm.relay.op.reduce.all:28
#: tvm.relay.op.reduce.any:28 tvm.relay.op.tensor.add:16
#: tvm.relay.op.tensor.clip:19 tvm.relay.op.transform.arange:31
#: tvm.relay.op.transform.argwhere:15 tvm.relay.op.transform.cumprod:30
#: tvm.relay.op.transform.cumsum:30 tvm.relay.op.transform.gather:26
#: tvm.relay.op.transform.gather_nd:25
#: tvm.relay.op.transform.invert_permutation:20
#: tvm.relay.op.transform.matrix_set_diag:33 tvm.relay.op.transform.meshgrid:20
#: tvm.relay.op.transform.one_hot:31 tvm.relay.op.transform.repeat:23
#: tvm.relay.op.transform.reshape_like:39 tvm.relay.op.transform.reverse:17
#: tvm.relay.op.transform.reverse_sequence:27
#: tvm.relay.op.transform.segment_sum:32
#: tvm.relay.op.transform.sequence_mask:26
#: tvm.relay.op.transform.sliding_window:28
#: tvm.relay.op.transform.sparse_fill_empty_rows:43
#: tvm.relay.op.transform.sparse_reshape:21
#: tvm.relay.op.transform.sparse_to_dense:24 tvm.relay.op.transform.stft:36
#: tvm.relay.op.transform.tile:17 tvm.relay.op.transform.trilu:24
#: tvm.relay.op.transform.unique:35 tvm.relay.op.transform.unravel_index:17
#: tvm.relay.op.transform.where:27 tvm.relay.param_dict.save_param_dict:20
#: tvm.relay.scope_builder.ScopeBuilder:7
msgid "Examples"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
msgid ":py:obj:`else_scope <tvm.relay.ScopeBuilder.else_scope>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1
#: tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
msgid "Create a new else scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
msgid ":py:obj:`get <tvm.relay.ScopeBuilder.get>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
#: tvm.relay.scope_builder.ScopeBuilder.get:1
msgid "Get the generated result."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
msgid ":py:obj:`if_scope <tvm.relay.ScopeBuilder.if_scope>`\\ \\(cond\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
#: tvm.relay.scope_builder.ScopeBuilder.if_scope:1
msgid "Create a new if scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
msgid ":py:obj:`let <tvm.relay.ScopeBuilder.let>`\\ \\(var\\, value\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
#: tvm.relay.scope_builder.ScopeBuilder.let:1
msgid "Create a new let binding."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
msgid ":py:obj:`ret <tvm.relay.ScopeBuilder.ret>`\\ \\(value\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
#: tvm.relay.scope_builder.ScopeBuilder.ret:1
msgid "Set the return value of this scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
msgid ":py:obj:`type_of <tvm.relay.ScopeBuilder.type_of>`\\ \\(expr\\)"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:1:<autosummary>:1
#: tvm.relay.scope_builder.ScopeBuilder.type_of:1
msgid "Compute the type of an expression."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:5
#: tvm.relay.scope_builder.ScopeBuilder.if_scope:11
msgid "scope: WithScope"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.else_scope:6
#: tvm.relay.scope_builder.ScopeBuilder.if_scope:11
msgid "The if scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.get:5
#: tvm.relay.scope_builder.ScopeBuilder.ret:5
msgid "value: tvm.relay.expr.Expr"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.get:6
msgid "The final result of the expression."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.if_scope:6
msgid "cond: tvm.relay.expr.Expr"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.if_scope:6
msgid "The condition"
msgstr ""

#: of tvm.relay.expr.Tuple.astype:9 tvm.relay.expr.const:15
#: tvm.relay.op.transform.sparse_fill_empty_rows:35
#: tvm.relay.scope_builder.ScopeBuilder.if_scope:14
msgid "Note"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.if_scope:15
msgid "The user must follows with an else scope."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:6
msgid "var: Union[Tuple[str, relay.Type], tvm.relay.Var]"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:6
msgid "The variable or name of variable."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.let:9
msgid "The value to be bound"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.ret:6
msgid "The return value."
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.type_of:5
msgid "expr: relay.Expr"
msgstr ""

#: of tvm.relay.scope_builder.ScopeBuilder.type_of:6
msgid "The expression to compute the type of."
msgstr ""

#: of tvm.ir.base.SequentialSpan:3
msgid ""
"This span is specific for an expression, which is from multiple "
"expressions after an IR transform."
msgstr ""

#: of tvm.ir.base.SequentialSpan:8
msgid "spans"
msgstr ""

#: of tvm.ir.base.SequentialSpan:-1
msgid "Array"
msgstr ""

#: of tvm.ir.base.SequentialSpan:9
msgid "The array of spans."
msgstr ""

#: of tvm.relay.ty.ShapeVar:5
msgid "name : str"
msgstr ""

#: of tvm.relay.ty.ShapeVar:9
msgid "type_var"
msgstr ""

#: of tvm.relay.ty.ShapeVar:-1
msgid "tvm.relay.TypeVar"
msgstr ""

#: of tvm.relay.ty.ShapeVar:10
msgid "The shape variable."
msgstr ""

#: of tvm.relay.expr.Tuple:6
msgid "fields"
msgstr ""

#: of tvm.relay.expr.Tuple:-1
msgid "List[tvm.relay.Expr]"
msgstr ""

#: of tvm.relay.expr.Tuple:6
msgid "The fields in the tuple."
msgstr ""

#: of tvm.relay.expr.Tuple.astype:1:<autosummary>:1
msgid ":py:obj:`astype <tvm.relay.Tuple.astype>`\\ \\(\\_\\)"
msgstr ""

#: of tvm.relay.expr.Tuple.astype:1
#: tvm.relay.expr.Tuple.astype:1:<autosummary>:1
msgid "Cast the content type of the current data to dtype."
msgstr ""

#: of tvm.relay.expr.Tuple.astype:6 tvm.relay.op.algorithm.argsort:19
#: tvm.relay.op.algorithm.searchsorted:23 tvm.relay.op.algorithm.topk:26
#: tvm.relay.op.tensor.ndarray_size:9 tvm.relay.op.tensor.ones:9
#: tvm.relay.op.tensor.shape_of:9 tvm.relay.op.tensor.zeros:9
#: tvm.relay.op.transform.arange:23 tvm.relay.op.transform.cast:9
#: tvm.relay.op.transform.cumprod:15 tvm.relay.op.transform.cumsum:15
#: tvm.relay.op.transform.full:12 tvm.relay.op.transform.one_hot:23
#: tvm.relay.op.transform.reinterpret:9 tvm.relay.ty.scalar_type:8
msgid "dtype"
msgstr ""

#: of tvm.relay.expr.Tuple.astype:6 tvm.relay.op.tensor.ndarray_size:9
#: tvm.relay.op.tensor.shape_of:9 tvm.relay.op.transform.arange:23
#: tvm.relay.op.transform.cast:9 tvm.relay.op.transform.reinterpret:9
msgid "The target data type."
msgstr ""

#: of tvm.relay.expr.Tuple.astype:10
msgid "This function only works for TensorType Exprs."
msgstr ""

#: of tvm.relay.expr.Tuple.astype:14 tvm.relay.expr.bind:15
#: tvm.relay.op.reduce.all:25 tvm.relay.op.reduce.any:25
#: tvm.relay.op.reduce.argmax:28 tvm.relay.op.reduce.argmin:28
#: tvm.relay.op.reduce.logsumexp:23 tvm.relay.op.reduce.max:24
#: tvm.relay.op.reduce.mean:24 tvm.relay.op.reduce.mean_std:24
#: tvm.relay.op.reduce.mean_variance:27 tvm.relay.op.reduce.min:25
#: tvm.relay.op.reduce.prod:24 tvm.relay.op.reduce.std:27
#: tvm.relay.op.reduce.sum:24 tvm.relay.op.reduce.variance:30
#: tvm.relay.op.tensor.abs:10 tvm.relay.op.tensor.acos:10
#: tvm.relay.op.tensor.acosh:10 tvm.relay.op.tensor.add:13
#: tvm.relay.op.tensor.asin:10 tvm.relay.op.tensor.asinh:10
#: tvm.relay.op.tensor.atan:10 tvm.relay.op.tensor.atanh:10
#: tvm.relay.op.tensor.bitwise_and:12 tvm.relay.op.tensor.bitwise_not:10
#: tvm.relay.op.tensor.bitwise_or:12 tvm.relay.op.tensor.bitwise_xor:12
#: tvm.relay.op.tensor.ceil:10 tvm.relay.op.tensor.clip:16
#: tvm.relay.op.tensor.cos:10 tvm.relay.op.tensor.cosh:10
#: tvm.relay.op.tensor.device_copy:18 tvm.relay.op.tensor.divide:12
#: tvm.relay.op.tensor.einsum:12 tvm.relay.op.tensor.equal:12
#: tvm.relay.op.tensor.erf:10 tvm.relay.op.tensor.exp:10
#: tvm.relay.op.tensor.fixed_point_multiply:16 tvm.relay.op.tensor.floor:10
#: tvm.relay.op.tensor.floor_divide:12 tvm.relay.op.tensor.floor_mod:12
#: tvm.relay.op.tensor.greater:12 tvm.relay.op.tensor.greater_equal:12
#: tvm.relay.op.tensor.isfinite:10 tvm.relay.op.tensor.isinf:10
#: tvm.relay.op.tensor.isnan:10 tvm.relay.op.tensor.left_shift:12
#: tvm.relay.op.tensor.less:12 tvm.relay.op.tensor.less_equal:12
#: tvm.relay.op.tensor.log:10 tvm.relay.op.tensor.log10:10
#: tvm.relay.op.tensor.log2:10 tvm.relay.op.tensor.logical_and:12
#: tvm.relay.op.tensor.logical_not:10 tvm.relay.op.tensor.logical_or:12
#: tvm.relay.op.tensor.logical_xor:12 tvm.relay.op.tensor.maximum:12
#: tvm.relay.op.tensor.minimum:12 tvm.relay.op.tensor.mod:12
#: tvm.relay.op.tensor.multiply:12 tvm.relay.op.tensor.ndarray_size:13
#: tvm.relay.op.tensor.negative:10 tvm.relay.op.tensor.not_equal:12
#: tvm.relay.op.tensor.ones:13 tvm.relay.op.tensor.ones_like:10
#: tvm.relay.op.tensor.power:12 tvm.relay.op.tensor.right_shift:12
#: tvm.relay.op.tensor.round:10 tvm.relay.op.tensor.rsqrt:14
#: tvm.relay.op.tensor.shape_of:13 tvm.relay.op.tensor.sigmoid:10
#: tvm.relay.op.tensor.sign:10 tvm.relay.op.tensor.sin:10
#: tvm.relay.op.tensor.sinh:10 tvm.relay.op.tensor.sqrt:10
#: tvm.relay.op.tensor.subtract:12 tvm.relay.op.tensor.tan:10
#: tvm.relay.op.tensor.tanh:10 tvm.relay.op.tensor.trunc:10
#: tvm.relay.op.tensor.trunc_divide:12 tvm.relay.op.tensor.trunc_mod:12
#: tvm.relay.op.tensor.zeros:13 tvm.relay.op.tensor.zeros_like:10
#: tvm.relay.op.transform.adv_index:11 tvm.relay.op.transform.arange:28
#: tvm.relay.op.transform.argwhere:12 tvm.relay.op.transform.broadcast_to:14
#: tvm.relay.op.transform.broadcast_to_like:13 tvm.relay.op.transform.cast:13
#: tvm.relay.op.transform.cast_like:13
#: tvm.relay.op.transform.collapse_sum_like:13
#: tvm.relay.op.transform.collapse_sum_to:13 tvm.relay.op.transform.cumprod:27
#: tvm.relay.op.transform.cumsum:27 tvm.relay.op.transform.expand_dims:19
#: tvm.relay.op.transform.full:16 tvm.relay.op.transform.full_like:13
#: tvm.relay.op.transform.matrix_set_diag:30
#: tvm.relay.op.transform.reinterpret:13 tvm.relay.op.transform.reshape:66
#: tvm.relay.op.transform.reverse_reshape:23
#: tvm.relay.op.transform.segment_sum:29 tvm.relay.op.transform.slice_like:20
#: tvm.relay.op.transform.sliding_window:25
#: tvm.relay.op.transform.sparse_to_dense:21 tvm.relay.op.transform.squeeze:15
#: tvm.relay.op.transform.transpose:13 tvm.relay.op.transform.unravel_index:14
#: tvm.relay.op.transform.where:24
msgid "result"
msgstr ""

#: of tvm.relay.expr.Tuple.astype:-1 tvm.relay.expr.bind:-1
#: tvm.relay.op.tensor.device_copy:-1 tvm.relay.op.tensor.ndarray_size:-1
#: tvm.relay.op.tensor.shape_of:-1
msgid "tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.Tuple.astype:15
msgid "The result expression."
msgstr ""

#: of tvm.relay.expr.TupleGetItem:6 tvm.relay.expr.TupleWrapper:10
msgid "tuple_value: tvm.relay.Expr"
msgstr ""

#: of tvm.relay.expr.TupleGetItem:6
msgid "The input tuple expression."
msgstr ""

#: of tvm.relay.expr.TupleGetItem:9
msgid "index: int"
msgstr ""

#: of tvm.relay.expr.TupleGetItem:9
msgid "The index."
msgstr ""

#: of tvm.relay.expr.TupleWrapper:3
msgid ""
"This class is a Python wrapper for a Relay tuple of known size. It allows"
" for accessing the fields of the Relay tuple as though it were a Python "
"tuple."
msgstr ""

#: of tvm.relay.expr.TupleWrapper:10
msgid "The input tuple"
msgstr ""

#: of tvm.relay.expr.TupleWrapper:13
msgid "size: int"
msgstr ""

#: of tvm.relay.expr.TupleWrapper:13
msgid "The size of the tuple."
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astext:1:<autosummary>:1
msgid ":py:obj:`astext <tvm.relay.TupleWrapper.astext>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astext:1
#: tvm.relay.expr.TupleWrapper.astext:1:<autosummary>:1
msgid "Get the text format of the tuple expression."
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astext:1:<autosummary>:1
msgid ":py:obj:`astuple <tvm.relay.TupleWrapper.astuple>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astext:1:<autosummary>:1
#: tvm.relay.expr.TupleWrapper.astuple:1
msgid ""
"Returns the underlying Relay tuple if this wrapper is passed as an "
"argument to an FFI function."
msgstr ""

#: of tvm.relay.expr.TupleWrapper.astext:6
msgid "The text format of the tuple expression."
msgstr ""

#: of tvm.ir.adt.TypeData:3
msgid ""
"Note that ADT definitions are treated as type-level functions because the"
" type parameters need to be given for an instance of the ADT. Thus, any "
"global type var that is an ADT header needs to be wrapped in a type call "
"that passes in the type params."
msgstr ""

#: of tvm.ir.adt.TypeData:13
msgid "header: GlobalTypeVar"
msgstr ""

#: of tvm.ir.adt.TypeData:11
msgid ""
"The name of the ADT. ADTs with the same constructors but different names "
"are treated as different types."
msgstr ""

#: of tvm.ir.adt.TypeData:16
msgid "type_vars: List[TypeVar]"
msgstr ""

#: of tvm.ir.adt.TypeData:16
msgid "Type variables that appear in constructors."
msgstr ""

#: of tvm.ir.adt.TypeData:18
msgid "constructors: List[Constructor]"
msgstr ""

#: of tvm.ir.adt.TypeData:19
msgid "The constructors for the ADT."
msgstr ""

#: of tvm.relay.type_functor.TypeFunctor:3
msgid "Defines the default dispatch over types."
msgstr ""

#: of tvm.relay.type_functor.TypeFunctor.visit:1:<autosummary>:1
msgid ":py:obj:`visit <tvm.relay.TypeFunctor.visit>`\\ \\(typ\\)"
msgstr ""

#: of tvm.relay.type_functor.TypeFunctor.visit:1
#: tvm.relay.type_functor.TypeFunctor.visit:1:<autosummary>:1
msgid "Apply the visitor to a type."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:-1 tvm.relay.op.algorithm.searchsorted:-1
#: tvm.relay.op.algorithm.sort:-1 tvm.relay.op.algorithm.topk:-1
#: tvm.relay.op.reduce.all:-1 tvm.relay.op.reduce.any:-1
#: tvm.relay.op.reduce.argmax:-1 tvm.relay.op.reduce.argmin:-1
#: tvm.relay.op.reduce.logsumexp:-1 tvm.relay.op.reduce.max:-1
#: tvm.relay.op.reduce.mean:-1 tvm.relay.op.reduce.mean_std:-1
#: tvm.relay.op.reduce.mean_variance:-1 tvm.relay.op.reduce.min:-1
#: tvm.relay.op.reduce.prod:-1 tvm.relay.op.reduce.std:-1
#: tvm.relay.op.reduce.sum:-1 tvm.relay.op.reduce.variance:-1
#: tvm.relay.op.tensor.abs:-1 tvm.relay.op.tensor.acos:-1
#: tvm.relay.op.tensor.acosh:-1 tvm.relay.op.tensor.add:-1
#: tvm.relay.op.tensor.asin:-1 tvm.relay.op.tensor.asinh:-1
#: tvm.relay.op.tensor.atan:-1 tvm.relay.op.tensor.atanh:-1
#: tvm.relay.op.tensor.bitwise_and:-1 tvm.relay.op.tensor.bitwise_not:-1
#: tvm.relay.op.tensor.bitwise_or:-1 tvm.relay.op.tensor.bitwise_xor:-1
#: tvm.relay.op.tensor.ceil:-1 tvm.relay.op.tensor.clip:-1
#: tvm.relay.op.tensor.copy:-1 tvm.relay.op.tensor.cos:-1
#: tvm.relay.op.tensor.cosh:-1 tvm.relay.op.tensor.divide:-1
#: tvm.relay.op.tensor.einsum:-1 tvm.relay.op.tensor.equal:-1
#: tvm.relay.op.tensor.erf:-1 tvm.relay.op.tensor.exp:-1
#: tvm.relay.op.tensor.fixed_point_multiply:-1 tvm.relay.op.tensor.floor:-1
#: tvm.relay.op.tensor.floor_divide:-1 tvm.relay.op.tensor.floor_mod:-1
#: tvm.relay.op.tensor.greater:-1 tvm.relay.op.tensor.greater_equal:-1
#: tvm.relay.op.tensor.isfinite:-1 tvm.relay.op.tensor.isinf:-1
#: tvm.relay.op.tensor.isnan:-1 tvm.relay.op.tensor.left_shift:-1
#: tvm.relay.op.tensor.less:-1 tvm.relay.op.tensor.less_equal:-1
#: tvm.relay.op.tensor.log:-1 tvm.relay.op.tensor.log10:-1
#: tvm.relay.op.tensor.log2:-1 tvm.relay.op.tensor.logical_and:-1
#: tvm.relay.op.tensor.logical_not:-1 tvm.relay.op.tensor.logical_or:-1
#: tvm.relay.op.tensor.logical_xor:-1 tvm.relay.op.tensor.maximum:-1
#: tvm.relay.op.tensor.minimum:-1 tvm.relay.op.tensor.mod:-1
#: tvm.relay.op.tensor.multiply:-1 tvm.relay.op.tensor.negative:-1
#: tvm.relay.op.tensor.not_equal:-1 tvm.relay.op.tensor.ones:-1
#: tvm.relay.op.tensor.ones_like:-1 tvm.relay.op.tensor.power:-1
#: tvm.relay.op.tensor.right_shift:-1 tvm.relay.op.tensor.round:-1
#: tvm.relay.op.tensor.rsqrt:-1 tvm.relay.op.tensor.sigmoid:-1
#: tvm.relay.op.tensor.sign:-1 tvm.relay.op.tensor.sin:-1
#: tvm.relay.op.tensor.sinh:-1 tvm.relay.op.tensor.sqrt:-1
#: tvm.relay.op.tensor.stack:-1 tvm.relay.op.tensor.subtract:-1
#: tvm.relay.op.tensor.tan:-1 tvm.relay.op.tensor.tanh:-1
#: tvm.relay.op.tensor.trunc:-1 tvm.relay.op.tensor.trunc_divide:-1
#: tvm.relay.op.tensor.trunc_mod:-1 tvm.relay.op.tensor.zeros:-1
#: tvm.relay.op.tensor.zeros_like:-1 tvm.relay.op.transform.adv_index:-1
#: tvm.relay.op.transform.arange:-1 tvm.relay.op.transform.argwhere:-1
#: tvm.relay.op.transform.broadcast_to:-1
#: tvm.relay.op.transform.broadcast_to_like:-1 tvm.relay.op.transform.cast:-1
#: tvm.relay.op.transform.cast_like:-1
#: tvm.relay.op.transform.collapse_sum_like:-1
#: tvm.relay.op.transform.collapse_sum_to:-1 tvm.relay.op.transform.cumprod:-1
#: tvm.relay.op.transform.cumsum:-1 tvm.relay.op.transform.dft:-1
#: tvm.relay.op.transform.expand_dims:-1 tvm.relay.op.transform.full:-1
#: tvm.relay.op.transform.full_like:-1 tvm.relay.op.transform.gather:-1
#: tvm.relay.op.transform.gather_nd:-1
#: tvm.relay.op.transform.invert_permutation:-1
#: tvm.relay.op.transform.layout_transform:-1
#: tvm.relay.op.transform.matrix_set_diag:-1 tvm.relay.op.transform.one_hot:-1
#: tvm.relay.op.transform.reinterpret:-1 tvm.relay.op.transform.repeat:-1
#: tvm.relay.op.transform.reshape:-1 tvm.relay.op.transform.reshape_like:-1
#: tvm.relay.op.transform.reverse:-1 tvm.relay.op.transform.reverse_reshape:-1
#: tvm.relay.op.transform.reverse_sequence:-1
#: tvm.relay.op.transform.scatter_elements:-1
#: tvm.relay.op.transform.scatter_nd:-1 tvm.relay.op.transform.segment_sum:-1
#: tvm.relay.op.transform.sequence_mask:-1 tvm.relay.op.transform.slice_like:-1
#: tvm.relay.op.transform.sliding_window:-1
#: tvm.relay.op.transform.sparse_fill_empty_rows:-1
#: tvm.relay.op.transform.sparse_reshape:-1
#: tvm.relay.op.transform.sparse_to_dense:-1 tvm.relay.op.transform.split:-1
#: tvm.relay.op.transform.squeeze:-1 tvm.relay.op.transform.stft:-1
#: tvm.relay.op.transform.strided_set:-1
#: tvm.relay.op.transform.strided_slice:-1 tvm.relay.op.transform.take:-1
#: tvm.relay.op.transform.tile:-1 tvm.relay.op.transform.transpose:-1
#: tvm.relay.op.transform.trilu:-1 tvm.relay.op.transform.unique:-1
#: tvm.relay.op.transform.unravel_index:-1 tvm.relay.op.transform.where:-1
msgid "relay.Expr"
msgstr ""

#: of tvm.relay.op.reduce.argmax:6 tvm.relay.op.reduce.argmin:6
#: tvm.relay.op.reduce.logsumexp:10 tvm.relay.op.reduce.max:6
#: tvm.relay.op.reduce.mean:6 tvm.relay.op.reduce.mean_std:6
#: tvm.relay.op.reduce.mean_variance:6 tvm.relay.op.reduce.min:6
#: tvm.relay.op.reduce.prod:6 tvm.relay.op.reduce.std:6
#: tvm.relay.op.reduce.sum:6 tvm.relay.op.reduce.variance:6
#: tvm.relay.op.tensor.abs:6 tvm.relay.op.tensor.acos:6
#: tvm.relay.op.tensor.acosh:6 tvm.relay.op.tensor.asin:6
#: tvm.relay.op.tensor.asinh:6 tvm.relay.op.tensor.atan:6
#: tvm.relay.op.tensor.atanh:6 tvm.relay.op.tensor.bitwise_not:6
#: tvm.relay.op.tensor.ceil:6 tvm.relay.op.tensor.cos:6
#: tvm.relay.op.tensor.cosh:6 tvm.relay.op.tensor.erf:6
#: tvm.relay.op.tensor.exp:6 tvm.relay.op.tensor.floor:6
#: tvm.relay.op.tensor.isfinite:6 tvm.relay.op.tensor.isinf:6
#: tvm.relay.op.tensor.isnan:6 tvm.relay.op.tensor.log:6
#: tvm.relay.op.tensor.log10:6 tvm.relay.op.tensor.log2:6
#: tvm.relay.op.tensor.logical_not:6 tvm.relay.op.tensor.negative:6
#: tvm.relay.op.tensor.ones_like:6 tvm.relay.op.tensor.round:6
#: tvm.relay.op.tensor.rsqrt:10 tvm.relay.op.tensor.sigmoid:6
#: tvm.relay.op.tensor.sign:6 tvm.relay.op.tensor.sin:6
#: tvm.relay.op.tensor.sinh:6 tvm.relay.op.tensor.sqrt:6
#: tvm.relay.op.tensor.tan:6 tvm.relay.op.tensor.tanh:6
#: tvm.relay.op.tensor.trunc:6 tvm.relay.op.tensor.zeros_like:6
msgid "The input data"
msgstr ""

#: of tvm.relay.op.algorithm.topk:31 tvm.relay.op.reduce.all:25
#: tvm.relay.op.reduce.any:25 tvm.relay.op.reduce.argmax:29
#: tvm.relay.op.reduce.argmin:29 tvm.relay.op.reduce.logsumexp:24
#: tvm.relay.op.reduce.max:25 tvm.relay.op.reduce.mean:25
#: tvm.relay.op.reduce.mean_std:25 tvm.relay.op.reduce.mean_variance:28
#: tvm.relay.op.reduce.min:26 tvm.relay.op.reduce.prod:25
#: tvm.relay.op.reduce.std:28 tvm.relay.op.reduce.sum:25
#: tvm.relay.op.reduce.variance:31 tvm.relay.op.tensor.abs:11
#: tvm.relay.op.tensor.acos:11 tvm.relay.op.tensor.acosh:11
#: tvm.relay.op.tensor.add:13 tvm.relay.op.tensor.asin:11
#: tvm.relay.op.tensor.asinh:11 tvm.relay.op.tensor.atan:11
#: tvm.relay.op.tensor.atanh:11 tvm.relay.op.tensor.bitwise_and:13
#: tvm.relay.op.tensor.bitwise_not:11 tvm.relay.op.tensor.bitwise_or:13
#: tvm.relay.op.tensor.bitwise_xor:13 tvm.relay.op.tensor.ceil:11
#: tvm.relay.op.tensor.cos:11 tvm.relay.op.tensor.cosh:11
#: tvm.relay.op.tensor.divide:13 tvm.relay.op.tensor.equal:13
#: tvm.relay.op.tensor.erf:11 tvm.relay.op.tensor.exp:11
#: tvm.relay.op.tensor.floor:11 tvm.relay.op.tensor.floor_divide:13
#: tvm.relay.op.tensor.floor_mod:13 tvm.relay.op.tensor.greater:13
#: tvm.relay.op.tensor.greater_equal:13 tvm.relay.op.tensor.isfinite:11
#: tvm.relay.op.tensor.isinf:11 tvm.relay.op.tensor.isnan:11
#: tvm.relay.op.tensor.left_shift:13 tvm.relay.op.tensor.less:13
#: tvm.relay.op.tensor.less_equal:13 tvm.relay.op.tensor.log:11
#: tvm.relay.op.tensor.log10:11 tvm.relay.op.tensor.log2:11
#: tvm.relay.op.tensor.logical_and:13 tvm.relay.op.tensor.logical_not:11
#: tvm.relay.op.tensor.logical_or:13 tvm.relay.op.tensor.logical_xor:13
#: tvm.relay.op.tensor.maximum:13 tvm.relay.op.tensor.minimum:13
#: tvm.relay.op.tensor.mod:13 tvm.relay.op.tensor.multiply:13
#: tvm.relay.op.tensor.negative:11 tvm.relay.op.tensor.not_equal:13
#: tvm.relay.op.tensor.ones_like:11 tvm.relay.op.tensor.power:13
#: tvm.relay.op.tensor.right_shift:13 tvm.relay.op.tensor.round:11
#: tvm.relay.op.tensor.rsqrt:15 tvm.relay.op.tensor.sigmoid:11
#: tvm.relay.op.tensor.sign:11 tvm.relay.op.tensor.sin:11
#: tvm.relay.op.tensor.sinh:11 tvm.relay.op.tensor.sqrt:11
#: tvm.relay.op.tensor.subtract:13 tvm.relay.op.tensor.tan:11
#: tvm.relay.op.tensor.tanh:11 tvm.relay.op.tensor.trunc:11
#: tvm.relay.op.tensor.trunc_divide:13 tvm.relay.op.tensor.trunc_mod:13
#: tvm.relay.op.tensor.zeros_like:11 tvm.relay.op.transform.gather_nd:22
#: tvm.relay.op.transform.meshgrid:17 tvm.relay.op.transform.repeat:20
#: tvm.relay.op.transform.reshape_like:36 tvm.relay.op.transform.reverse:14
#: tvm.relay.op.transform.scatter_elements:31
#: tvm.relay.op.transform.scatter_nd:28 tvm.relay.op.transform.sequence_mask:23
#: tvm.relay.op.transform.slice_like:21 tvm.relay.op.transform.split:23
#: tvm.relay.op.transform.strided_set:24
#: tvm.relay.op.transform.strided_slice:34 tvm.relay.op.transform.take:27
#: tvm.relay.op.transform.tile:14
msgid "The computed result."
msgstr ""

#: of tvm.relay.op.tensor.add:5 tvm.relay.op.tensor.bitwise_and:5
#: tvm.relay.op.tensor.bitwise_or:5 tvm.relay.op.tensor.bitwise_xor:5
#: tvm.relay.op.tensor.divide:5 tvm.relay.op.tensor.equal:5
#: tvm.relay.op.tensor.floor_divide:5 tvm.relay.op.tensor.floor_mod:5
#: tvm.relay.op.tensor.greater:5 tvm.relay.op.tensor.greater_equal:5
#: tvm.relay.op.tensor.left_shift:5 tvm.relay.op.tensor.less:5
#: tvm.relay.op.tensor.less_equal:5 tvm.relay.op.tensor.logical_and:5
#: tvm.relay.op.tensor.logical_or:5 tvm.relay.op.tensor.logical_xor:5
#: tvm.relay.op.tensor.maximum:5 tvm.relay.op.tensor.minimum:5
#: tvm.relay.op.tensor.mod:5 tvm.relay.op.tensor.multiply:5
#: tvm.relay.op.tensor.not_equal:5 tvm.relay.op.tensor.power:5
#: tvm.relay.op.tensor.right_shift:5 tvm.relay.op.tensor.subtract:5
#: tvm.relay.op.tensor.trunc_divide:5 tvm.relay.op.tensor.trunc_mod:5
msgid "lhs"
msgstr ""

#: of tvm.relay.op.tensor.add:6 tvm.relay.op.tensor.bitwise_and:6
#: tvm.relay.op.tensor.bitwise_or:6 tvm.relay.op.tensor.bitwise_xor:6
#: tvm.relay.op.tensor.divide:6 tvm.relay.op.tensor.equal:6
#: tvm.relay.op.tensor.floor_divide:6 tvm.relay.op.tensor.floor_mod:6
#: tvm.relay.op.tensor.greater:6 tvm.relay.op.tensor.greater_equal:6
#: tvm.relay.op.tensor.left_shift:6 tvm.relay.op.tensor.less:6
#: tvm.relay.op.tensor.less_equal:6 tvm.relay.op.tensor.logical_and:6
#: tvm.relay.op.tensor.logical_or:6 tvm.relay.op.tensor.logical_xor:6
#: tvm.relay.op.tensor.maximum:6 tvm.relay.op.tensor.minimum:6
#: tvm.relay.op.tensor.mod:6 tvm.relay.op.tensor.multiply:6
#: tvm.relay.op.tensor.not_equal:6 tvm.relay.op.tensor.power:6
#: tvm.relay.op.tensor.right_shift:6 tvm.relay.op.tensor.subtract:6
#: tvm.relay.op.tensor.trunc_divide:6 tvm.relay.op.tensor.trunc_mod:6
msgid "The left hand side input data"
msgstr ""

#: of tvm.relay.op.tensor.add:8 tvm.relay.op.tensor.bitwise_and:8
#: tvm.relay.op.tensor.bitwise_or:8 tvm.relay.op.tensor.bitwise_xor:8
#: tvm.relay.op.tensor.divide:8 tvm.relay.op.tensor.equal:8
#: tvm.relay.op.tensor.floor_divide:8 tvm.relay.op.tensor.floor_mod:8
#: tvm.relay.op.tensor.greater:8 tvm.relay.op.tensor.greater_equal:8
#: tvm.relay.op.tensor.left_shift:8 tvm.relay.op.tensor.less:8
#: tvm.relay.op.tensor.less_equal:8 tvm.relay.op.tensor.logical_and:8
#: tvm.relay.op.tensor.logical_or:8 tvm.relay.op.tensor.logical_xor:8
#: tvm.relay.op.tensor.maximum:8 tvm.relay.op.tensor.minimum:8
#: tvm.relay.op.tensor.mod:8 tvm.relay.op.tensor.multiply:8
#: tvm.relay.op.tensor.not_equal:8 tvm.relay.op.tensor.power:8
#: tvm.relay.op.tensor.right_shift:8 tvm.relay.op.tensor.subtract:8
#: tvm.relay.op.tensor.trunc_divide:8 tvm.relay.op.tensor.trunc_mod:8
msgid "rhs"
msgstr ""

#: of tvm.relay.op.tensor.add:8 tvm.relay.op.tensor.bitwise_and:8
#: tvm.relay.op.tensor.bitwise_or:8 tvm.relay.op.tensor.bitwise_xor:8
#: tvm.relay.op.tensor.divide:8 tvm.relay.op.tensor.equal:8
#: tvm.relay.op.tensor.floor_divide:8 tvm.relay.op.tensor.floor_mod:8
#: tvm.relay.op.tensor.greater:8 tvm.relay.op.tensor.greater_equal:8
#: tvm.relay.op.tensor.left_shift:8 tvm.relay.op.tensor.less:8
#: tvm.relay.op.tensor.less_equal:8 tvm.relay.op.tensor.logical_and:8
#: tvm.relay.op.tensor.logical_or:8 tvm.relay.op.tensor.logical_xor:8
#: tvm.relay.op.tensor.maximum:8 tvm.relay.op.tensor.minimum:8
#: tvm.relay.op.tensor.mod:8 tvm.relay.op.tensor.multiply:8
#: tvm.relay.op.tensor.not_equal:8 tvm.relay.op.tensor.power:8
#: tvm.relay.op.tensor.right_shift:8 tvm.relay.op.tensor.subtract:8
#: tvm.relay.op.tensor.trunc_divide:8 tvm.relay.op.tensor.trunc_mod:8
msgid "The right hand side input data"
msgstr ""

#: of tvm.relay.op.transform.adv_index:1
msgid "Numpy style advanced indexing. Index with a list of tensors."
msgstr ""

#: of tvm.relay.op.transform.adv_index:7
msgid "inputs"
msgstr ""

#: of tvm.relay.op.tensor.concatenate:-1 tvm.relay.op.tensor.einsum:-1
#: tvm.relay.op.transform.adv_index:-1 tvm.relay.op.transform.meshgrid:-1
msgid "Union(List[relay.Expr], Tuple[relay.Expr])"
msgstr ""

#: of tvm.relay.op.transform.adv_index:6
msgid ""
"Input tensor and indices. The first tensor is the input data and the rest"
" are the indices."
msgstr ""

#: of tvm.relay.op.transform.adv_index:12 tvm.relay.op.transform.segment_sum:29
#: tvm.relay.op.transform.sparse_reshape:18
msgid "Output tensor."
msgstr ""

#: of tvm.relay.op.reduce.all:6 tvm.relay.op.reduce.any:6
msgid "The input boolean tensor"
msgstr ""

#: of tvm.relay.op.algorithm.argsort:13 tvm.relay.op.algorithm.sort:9
#: tvm.relay.op.algorithm.topk:14 tvm.relay.op.reduce.all:11
#: tvm.relay.op.reduce.any:11 tvm.relay.op.reduce.argmax:11
#: tvm.relay.op.reduce.argmin:11 tvm.relay.op.reduce.logsumexp:15
#: tvm.relay.op.reduce.max:11 tvm.relay.op.reduce.mean:11
#: tvm.relay.op.reduce.mean_std:11 tvm.relay.op.reduce.mean_variance:11
#: tvm.relay.op.reduce.min:12 tvm.relay.op.reduce.prod:11
#: tvm.relay.op.reduce.std:11 tvm.relay.op.reduce.sum:11
#: tvm.relay.op.reduce.variance:11 tvm.relay.op.tensor.concatenate:8
#: tvm.relay.op.tensor.stack:9 tvm.relay.op.transform.cumprod:11
#: tvm.relay.op.transform.cumsum:11 tvm.relay.op.transform.expand_dims:12
#: tvm.relay.op.transform.gather:20 tvm.relay.op.transform.one_hot:20
#: tvm.relay.op.transform.scatter_elements:16
#: tvm.relay.op.transform.sequence_mask:18
#: tvm.relay.op.transform.sliding_window:12 tvm.relay.op.transform.split:18
#: tvm.relay.op.transform.squeeze:11 tvm.relay.op.transform.take:13
msgid "axis"
msgstr ""

#: of tvm.relay.op.reduce.all:-1 tvm.relay.op.reduce.any:-1
#: tvm.relay.op.reduce.argmax:-1 tvm.relay.op.reduce.argmin:-1
#: tvm.relay.op.reduce.logsumexp:-1 tvm.relay.op.reduce.max:-1
#: tvm.relay.op.reduce.mean:-1 tvm.relay.op.reduce.mean_std:-1
#: tvm.relay.op.reduce.mean_variance:-1 tvm.relay.op.reduce.min:-1
#: tvm.relay.op.reduce.prod:-1 tvm.relay.op.reduce.std:-1
#: tvm.relay.op.reduce.sum:-1 tvm.relay.op.reduce.variance:-1
msgid "None or int or tuple of int"
msgstr ""

#: of tvm.relay.op.reduce.all:9 tvm.relay.op.reduce.any:9
#: tvm.relay.op.reduce.sum:9
msgid ""
"Axis or axes along which a sum is performed. The default, axis=None, will"
" sum all of the elements of the input array. If axis is negative it "
"counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.reduce.all:16 tvm.relay.op.reduce.any:16
#: tvm.relay.op.reduce.argmax:16 tvm.relay.op.reduce.argmin:16
#: tvm.relay.op.reduce.logsumexp:19 tvm.relay.op.reduce.max:16
#: tvm.relay.op.reduce.mean:16 tvm.relay.op.reduce.mean_std:16
#: tvm.relay.op.reduce.mean_variance:16 tvm.relay.op.reduce.min:17
#: tvm.relay.op.reduce.prod:16 tvm.relay.op.reduce.std:16
#: tvm.relay.op.reduce.sum:16 tvm.relay.op.reduce.variance:16
msgid "keepdims"
msgstr ""

#: of tvm.relay.op.reduce.all:14 tvm.relay.op.reduce.any:14
#: tvm.relay.op.reduce.argmax:14 tvm.relay.op.reduce.argmin:14
#: tvm.relay.op.reduce.max:14 tvm.relay.op.reduce.mean:14
#: tvm.relay.op.reduce.mean_std:14 tvm.relay.op.reduce.mean_variance:14
#: tvm.relay.op.reduce.min:15 tvm.relay.op.reduce.prod:14
#: tvm.relay.op.reduce.std:14 tvm.relay.op.reduce.sum:14
#: tvm.relay.op.reduce.variance:14
msgid ""
"If this is set to True, the axes which are reduced are left in the result"
" as dimensions with size one. With this option, the result will broadcast"
" correctly against the input array."
msgstr ""

#: of tvm.relay.op.reduce.all:20 tvm.relay.op.reduce.any:20
#: tvm.relay.op.reduce.argmax:20 tvm.relay.op.reduce.argmin:20
#: tvm.relay.op.reduce.max:20 tvm.relay.op.reduce.mean:20
#: tvm.relay.op.reduce.mean_std:20 tvm.relay.op.reduce.mean_variance:20
#: tvm.relay.op.reduce.min:21 tvm.relay.op.reduce.prod:20
#: tvm.relay.op.reduce.std:20 tvm.relay.op.reduce.sum:20
#: tvm.relay.op.reduce.variance:20
msgid "exclude"
msgstr ""

#: of tvm.relay.op.reduce.all:19 tvm.relay.op.reduce.any:19
#: tvm.relay.op.reduce.argmax:19 tvm.relay.op.reduce.argmin:19
#: tvm.relay.op.reduce.max:19 tvm.relay.op.reduce.mean:19
#: tvm.relay.op.reduce.mean_std:19 tvm.relay.op.reduce.mean_variance:19
#: tvm.relay.op.reduce.min:20 tvm.relay.op.reduce.prod:19
#: tvm.relay.op.reduce.std:19 tvm.relay.op.reduce.sum:19
#: tvm.relay.op.reduce.variance:19
msgid ""
"If `exclude` is true, reduction will be performed on the axes that are "
"NOT in axis instead."
msgstr ""

#: of tvm.relay.op.transform.arange:4
msgid ""
"Similar to ``numpy.arange``. When only one argument is given, it is used "
"as `stop` instead of `start` while `start` takes default value 0."
msgstr ""

#: of tvm.relay.op.transform.arange:7
msgid ""
"Warning: Undefined behavior when dtype is incompatible with "
"start/stop/step. It could lead to different results compared to numpy, "
"MXNet, pytorch, etc."
msgstr ""

#: of tvm.relay.op.transform.arange:14
msgid "start"
msgstr ""

#: of tvm.relay.op.transform.arange:-1
#: tvm.relay.op.transform.sparse_to_dense:-1 tvm.relay.op.transform.stft:-1
#: tvm.relay.op.transform.unique:-1
msgid "relay.Expr, optional"
msgstr ""

#: of tvm.relay.op.transform.arange:13
msgid ""
"Start of interval. The interval includes this value. The default start "
"value is 0."
msgstr ""

#: of tvm.relay.op.transform.arange:17
msgid "stop"
msgstr ""

#: of tvm.relay.op.transform.arange:17
msgid "Stop of interval. The interval does not include this value."
msgstr ""

#: of tvm.relay.op.transform.arange:20
msgid "step"
msgstr ""

#: of tvm.relay.op.transform.arange:20
msgid "Spacing between values. The default step size is 1."
msgstr ""

#: of tvm.relay.op.tensor.ndarray_size:-1 tvm.relay.op.tensor.shape_of:-1
#: tvm.relay.op.transform.arange:-1 tvm.relay.op.transform.meshgrid:-1
#: tvm.relay.op.transform.strided_slice:-1 tvm.relay.op.transform.take:-1
msgid "str, optional"
msgstr ""

#: of tvm.relay.op.tensor.ones:14 tvm.relay.op.tensor.zeros:14
#: tvm.relay.op.transform.arange:28 tvm.relay.op.transform.broadcast_to:15
#: tvm.relay.op.transform.broadcast_to_like:14
#: tvm.relay.op.transform.collapse_sum_like:14
#: tvm.relay.op.transform.collapse_sum_to:14 tvm.relay.op.transform.full:17
#: tvm.relay.op.transform.full_like:14 tvm.relay.op.transform.sliding_window:25
msgid "The resulting tensor."
msgstr ""

#: of tvm.relay.op.reduce.argmax:9
msgid ""
"Axis or axes along which a argmax operation is performed. The default, "
"axis=None, will find the indices of the maximum element of the elements "
"of the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.relay.op.reduce.argmax:24 tvm.relay.op.reduce.argmin:24
msgid "select_last_index"
msgstr ""

#: of tvm.relay.op.reduce.argmax:23
msgid ""
"Whether to select the last index or the first index if the max element "
"appears in multiple indices, default is False (first index)."
msgstr ""

#: of tvm.relay.op.reduce.argmin:9
msgid ""
"Axis or axes along which a argmin operation is performed. The default, "
"axis=None, will find the indices of minimum element all of the elements "
"of the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.relay.op.reduce.argmin:23
msgid ""
"Whether to select the last index or the first index if the min element "
"appears in multiple indices, default is False (first index)."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:7 tvm.relay.op.algorithm.sort:6
#: tvm.relay.op.algorithm.topk:8
msgid "The input data tensor."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:10
msgid "valid_count"
msgstr ""

#: of tvm.relay.op.algorithm.argsort:-1
msgid "tvm.te.Tensor"
msgstr ""

#: of tvm.relay.op.algorithm.argsort:10
msgid "The number of valid elements to be sorted."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:-1 tvm.relay.op.algorithm.sort:-1
#: tvm.relay.op.algorithm.topk:-1 tvm.relay.op.transform.cumprod:-1
#: tvm.relay.op.transform.cumsum:-1 tvm.relay.op.transform.expand_dims:-1
#: tvm.relay.op.transform.gather_nd:-1 tvm.relay.op.transform.reshape_like:-1
#: tvm.relay.op.transform.reverse_sequence:-1
#: tvm.relay.op.transform.segment_sum:-1
#: tvm.relay.op.transform.sequence_mask:-1 tvm.relay.op.transform.split:-1
#: tvm.relay.op.transform.stft:-1 tvm.relay.op.transform.take:-1
msgid "int, optional"
msgstr ""

#: of tvm.relay.op.algorithm.argsort:13 tvm.relay.op.algorithm.sort:9
#: tvm.relay.op.algorithm.topk:14
msgid "Axis long which to sort the input tensor."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:16 tvm.relay.op.algorithm.sort:12
#: tvm.relay.op.algorithm.topk:23
msgid "is_ascend"
msgstr ""

#: of tvm.relay.op.algorithm.argsort:-1 tvm.relay.op.algorithm.sort:-1
#: tvm.relay.op.algorithm.topk:-1
msgid "boolean, optional"
msgstr ""

#: of tvm.relay.op.algorithm.argsort:16 tvm.relay.op.algorithm.sort:12
#: tvm.relay.op.algorithm.topk:23
msgid "Whether to sort in ascending or descending order."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:-1 tvm.relay.op.algorithm.searchsorted:-1
#: tvm.relay.op.algorithm.topk:-1 tvm.relay.op.transform.cumprod:-1
#: tvm.relay.op.transform.cumsum:-1 tvm.relay.op.transform.matrix_set_diag:-1
#: tvm.relay.op.transform.scatter_elements:-1
#: tvm.relay.op.transform.scatter_nd:-1
msgid "string, optional"
msgstr ""

#: of tvm.relay.op.algorithm.argsort:19 tvm.relay.op.algorithm.searchsorted:23
msgid "The data type of the output indices."
msgstr ""

#: of tvm.relay.op.algorithm.argsort:23 tvm.relay.op.algorithm.sort:16
#: tvm.relay.op.algorithm.topk:30
msgid "out"
msgstr ""

#: of tvm.relay.op.algorithm.argsort:24 tvm.relay.op.algorithm.sort:17
msgid "Tensor with same shape as data."
msgstr ""

#: of tvm.relay.op.transform.argwhere:7 tvm.relay.op.transform.where:12
msgid "condition"
msgstr ""

#: of tvm.relay.op.transform.argwhere:7
msgid "The input condition tensor."
msgstr ""

#: of tvm.relay.op.transform.argwhere:12
msgid "Tensor with the indices of elements that are non-zero."
msgstr ""

#: of tvm.relay.base.astext:5
msgid "obj"
msgstr ""

#: of tvm.relay.base.astext:-1
msgid "Object"
msgstr ""

#: of tvm.relay.base.astext:6
msgid "The object to be printed."
msgstr ""

#: of tvm.relay.expr.bind:3
msgid "We can bind parameters expr if it is a function."
msgstr ""

#: of tvm.relay.expr.bind:8
msgid "expr"
msgstr ""

#: of tvm.relay.expr.bind:8
msgid "The input expression."
msgstr ""

#: of tvm.relay.expr.bind:11
msgid "binds"
msgstr ""

#: of tvm.relay.expr.bind:-1
msgid "Map[tvm.relay.Var, tvm.relay.Expr]"
msgstr ""

#: of tvm.relay.expr.bind:11
msgid "The specific bindings."
msgstr ""

#: of tvm.relay.expr.bind:16
msgid "The expression or function after binding."
msgstr ""

#: of tvm.relay.op.tensor.clip:7 tvm.relay.op.tensor.fixed_point_multiply:8
#: tvm.relay.op.tensor.ndarray_size:6 tvm.relay.op.tensor.shape_of:6
#: tvm.relay.op.transform.broadcast_to:7
#: tvm.relay.op.transform.broadcast_to_like:6
#: tvm.relay.op.transform.collapse_sum_like:6
#: tvm.relay.op.transform.collapse_sum_to:6 tvm.relay.op.transform.full_like:6
#: tvm.relay.op.transform.repeat:7
msgid "The input tensor."
msgstr ""

#: of tvm.relay.op.tensor.ones:6 tvm.relay.op.tensor.zeros:6
#: tvm.relay.op.transform.broadcast_to:10
#: tvm.relay.op.transform.collapse_sum_to:9 tvm.relay.op.transform.full:9
#: tvm.relay.op.transform.unravel_index:9
msgid "shape"
msgstr ""

#: of tvm.relay.op.tensor.ones:-1 tvm.relay.op.tensor.zeros:-1
#: tvm.relay.op.transform.broadcast_to:-1 tvm.relay.op.transform.tile:-1
msgid "tuple of int or relay.Expr"
msgstr ""

#: of tvm.relay.op.transform.broadcast_to:10
#: tvm.relay.op.transform.broadcast_to_like:9
msgid "Provide the shape to broadcast to."
msgstr ""

#: of tvm.relay.op.transform.broadcast_to_like:9
msgid "broadcast_type"
msgstr ""

#: of tvm.relay.build_module.build:6
msgid "ir_mod : :py:class:`~tvm.IRModule`"
msgstr ""

#: of tvm.relay.build_module.build:5 tvm.relay.build_module.create_executor:23
#: tvm.relay.build_module.optimize:5 tvm.relay.build_module.optimize:19
msgid "IRModule"
msgstr ""

#: of tvm.relay.build_module.build:6
msgid "The IR module to build. Using relay.Function is deprecated."
msgstr ""

#: of tvm.relay.build_module.build:11 tvm.relay.build_module.create_executor:33
#: tvm.relay.build_module.optimize:11
msgid "target"
msgstr ""

#: of tvm.relay.build_module.build:-1 tvm.relay.build_module.optimize:-1
msgid "None, or any multi-target like object, see Target.canon_multi_target"
msgstr ""

#: of tvm.relay.build_module.build:9 tvm.relay.build_module.optimize:9
msgid ""
"For homogeneous compilation, the unique build target. For heterogeneous "
"compilation, a dictionary or list of possible build targets. Defaults to "
"the current target in the environment if None."
msgstr ""

#: of tvm.relay.build_module.build:14
msgid "target_host"
msgstr ""

#: of tvm.relay.build_module.build:-1
msgid "None, or any target like object, see Target.canon_target"
msgstr ""

#: of tvm.relay.build_module.build:14
msgid "Host compilation target, if target is device."
msgstr ""

#: of tvm.relay.build_module.build:18
msgid "executor"
msgstr ""

#: of tvm.relay.build_module.build:-1
msgid "Optional[Executor]"
msgstr ""

#: of tvm.relay.build_module.build:17
msgid ""
"The executor configuration with which to build the model. Defaults to "
"\"graph\" if no executor specified."
msgstr ""

#: of tvm.relay.build_module.build:22
msgid "runtime"
msgstr ""

#: of tvm.relay.build_module.build:-1
msgid "Optional[Runtime]"
msgstr ""

#: of tvm.relay.build_module.build:21
msgid ""
"Runtime configuration to use when building the model. Defaults to \"cpp\""
" if no runtime specified."
msgstr ""

#: of tvm.relay.build_module.build:27
msgid "workspace_memory_pools"
msgstr ""

#: of tvm.relay.build_module.build:-1
msgid "Optional[WorkspaceMemoryPools]"
msgstr ""

#: of tvm.relay.build_module.build:25
msgid ""
"The object that contains an Array of WorkspacePoolInfo objects that hold "
"properties of read-write workspace pools that could be used by the "
"inference."
msgstr ""

#: of tvm.relay.build_module.build:32
msgid "constant_memory_pools"
msgstr ""

#: of tvm.relay.build_module.build:-1
msgid "Optional[ConstantMemoryPools]"
msgstr ""

#: of tvm.relay.build_module.build:30
msgid ""
"The object that contains an Array of ConstantPoolInfo objects that hold "
"properties of read-only pools that could be used by the inference."
msgstr ""

#: of tvm.relay.build_module.build:36 tvm.relay.build_module.create_executor:37
#: tvm.relay.build_module.optimize:15 tvm.relay.build_module.optimize:22
#: tvm.relay.param_dict.load_param_dict:13
#: tvm.relay.param_dict.save_param_dict:12
msgid "params"
msgstr ""

#: of tvm.relay.build_module.build:-1 tvm.relay.build_module.create_executor:-1
#: tvm.relay.build_module.optimize:-1 tvm.relay.param_dict.load_param_dict:-1
#: tvm.relay.param_dict.save_param_dict:-1
msgid "dict of str to NDArray"
msgstr ""

#: of tvm.relay.build_module.build:35 tvm.relay.build_module.optimize:14
msgid ""
"Input parameters to the graph that do not change during inference time. "
"Used for constant folding."
msgstr ""

#: of tvm.relay.build_module.build:39
msgid "mod_name: Optional[str]"
msgstr ""

#: of tvm.relay.build_module.build:39
msgid "The module name we will build"
msgstr ""

#: of tvm.relay.build_module.build:43
msgid "factory_module"
msgstr ""

#: of tvm.relay.build_module.build:-1
msgid "tvm.relay.backend.executor_factory.ExecutorFactoryModule"
msgstr ""

#: of tvm.relay.build_module.build:44
msgid "The runtime factory for the TVM graph executor."
msgstr ""

#: of tvm.relay.transform.transform.build_config:1
msgid ""
"Configure the build behavior by setting config variables. This function "
"will be deprecated in TVM v0.7. Instead, we should directly use "
"tvm.transform.PassContext."
msgstr ""

#: of tvm.relay.transform.transform.build_config:26
msgid "opt_level: int, optional"
msgstr ""

#: of tvm.relay.transform.transform.build_config:8
msgid ""
"Optimization level. The optimization pass name and level are as the "
"following:"
msgstr ""

#: of tvm.relay.transform.transform.build_config:29
msgid "required_pass: set of str, optional"
msgstr ""

#: of tvm.relay.transform.transform.build_config:29
msgid "Optimization passes that are required regardless of optimization level."
msgstr ""

#: of tvm.relay.transform.transform.build_config:32
msgid "disabled_pass: set of str, optional"
msgstr ""

#: of tvm.relay.transform.transform.build_config:32
msgid "Optimization passes to be disabled during optimization."
msgstr ""

#: of tvm.relay.transform.transform.build_config:35
msgid "trace: Callable[[IRModule, PassInfo, bool], None]"
msgstr ""

#: of tvm.relay.transform.transform.build_config:35
msgid "A tracing function for debugging or introspection."
msgstr ""

#: of tvm.relay.transform.transform.build_config:39
msgid "pass_context: PassContext"
msgstr ""

#: of tvm.relay.transform.transform.build_config:40
msgid "The pass context for optimizations."
msgstr ""

#: of tvm.relay.op.transform.cast:6 tvm.relay.op.transform.cast_like:6
#: tvm.relay.op.transform.cumprod:7 tvm.relay.op.transform.cumsum:7
#: tvm.relay.op.transform.expand_dims:6 tvm.relay.op.transform.gather:17
#: tvm.relay.op.transform.gather_nd:7 tvm.relay.op.transform.reinterpret:6
#: tvm.relay.op.transform.reshape:56 tvm.relay.op.transform.reshape_like:13
#: tvm.relay.op.transform.reverse:6 tvm.relay.op.transform.reverse_reshape:16
#: tvm.relay.op.transform.scatter_elements:7
#: tvm.relay.op.transform.scatter_nd:8 tvm.relay.op.transform.sliding_window:6
#: tvm.relay.op.transform.squeeze:6 tvm.relay.op.transform.tile:6
#: tvm.relay.op.transform.transpose:6
msgid "The input data to the operator."
msgstr ""

#: of tvm.relay.op.transform.cast:14 tvm.relay.op.transform.cast_like:14
msgid "The casted result."
msgstr ""

#: of tvm.relay.op.transform.cast_like:9
msgid "dtype_like"
msgstr ""

#: of tvm.relay.op.transform.cast_like:9
msgid "The tensor to cast to."
msgstr ""

#: of tvm.relay.op.tensor.clip:1
msgid ""
"Clip the elements in `a` between `a_min` and `a_max`. `a_min` and `a_max`"
" are cast to `a`'s dtype."
msgstr ""

#: of tvm.relay.op.tensor.clip:6
msgid "a"
msgstr ""

#: of tvm.relay.op.tensor.clip:8
msgid "a_min"
msgstr ""

#: of tvm.relay.op.tensor.clip:-1
msgid "float"
msgstr ""

#: of tvm.relay.op.tensor.clip:9
msgid "The clip minimum."
msgstr ""

#: of tvm.relay.op.tensor.clip:11
msgid "a_max"
msgstr ""

#: of tvm.relay.op.tensor.clip:11
msgid "The clip maximum."
msgstr ""

#: of tvm.relay.op.tensor.clip:16
msgid "`a` with elements clipped between `a_min` and `a_max`."
msgstr ""

#: of tvm.relay.op.transform.collapse_sum_like:9
msgid "collapse_type"
msgstr ""

#: of tvm.relay.op.transform.collapse_sum_like:9
msgid "Provide the shape to collapse to."
msgstr ""

#: of tvm.relay.op.transform.collapse_sum_to:9
msgid "Shape to collapse to."
msgstr ""

#: of tvm.relay.op.tensor.concatenate:6 tvm.relay.op.tensor.einsum:6
msgid "A list of tensors."
msgstr ""

#: of tvm.relay.op.tensor.concatenate:-1
#: tvm.relay.op.tensor.fixed_point_multiply:-1 tvm.relay.op.tensor.stack:-1
#: tvm.relay.op.transform.gather:-1 tvm.relay.op.transform.one_hot:-1
#: tvm.relay.op.transform.repeat:-1 tvm.relay.op.transform.scatter_elements:-1
#: tvm.relay.op.transform.sliding_window:-1 tvm.relay.op.transform.stft:-1
#: tvm.relay.op.transform.trilu:-1
msgid "int"
msgstr ""

#: of tvm.relay.op.tensor.concatenate:8
msgid "The axis along which the tensors are concatenated."
msgstr ""

#: of tvm.relay.op.tensor.concatenate:12 tvm.relay.op.tensor.copy:10
#: tvm.relay.op.transform.sparse_reshape:18
msgid "result: relay.Expr"
msgstr ""

#: of tvm.relay.op.tensor.concatenate:13
msgid "The concatenated tensor."
msgstr ""

#: of tvm.relay.expr.const:6
msgid "value: Union[bool, int, float, numpy.ndarray, tvm.nd.NDArray]"
msgstr ""

#: of tvm.relay.expr.const:6
msgid "The constant value."
msgstr ""

#: of tvm.relay.expr.const:9 tvm.relay.expr.var:21
msgid "dtype: str, optional"
msgstr ""

#: of tvm.relay.expr.const:9
msgid "The data type of the resulting constant."
msgstr ""

#: of tvm.relay.expr.const:16
msgid "When dtype is None, we use the following rule:"
msgstr ""

#: of tvm.relay.expr.const:18
msgid "int maps to \"int32\""
msgstr ""

#: of tvm.relay.expr.const:19
msgid "float maps to \"float32\""
msgstr ""

#: of tvm.relay.expr.const:20
msgid "bool maps to \"bool\""
msgstr ""

#: of tvm.relay.expr.const:21
msgid "other using the same default rule as numpy."
msgstr ""

#: of tvm.relay.op.tensor.copy:6 tvm.relay.op.tensor.device_copy:8
msgid "The tensor to be copied."
msgstr ""

#: of tvm.relay.op.tensor.copy:11 tvm.relay.op.tensor.device_copy:19
msgid "The copied result."
msgstr ""

#: of tvm.relay.build_module.create_executor:4
msgid "Example"
msgstr ""

#: of tvm.relay.build_module.create_executor:21
msgid "kind"
msgstr ""

#: of tvm.relay.build_module.create_executor:20
msgid ""
"The type of executor. Avaliable options are `debug` for the interpreter, "
"`graph` for the graph executor, `aot` for the aot executor, and `vm` for "
"the virtual machine."
msgstr ""

#: of tvm.relay.build_module.create_executor:24
#: tvm.relay.build_module.optimize:6 tvm.relay.build_module.optimize:20
msgid "mod : :py:class:`~tvm.IRModule`"
msgstr ""

#: of tvm.relay.build_module.create_executor:24
msgid "The Relay module containing collection of functions"
msgstr ""

#: of tvm.relay.build_module.create_executor:27
msgid "device : :py:class:`Device`"
msgstr ""

#: of tvm.relay.build_module.create_executor:26
msgid "Device"
msgstr ""

#: of tvm.relay.build_module.create_executor:27
msgid "The device to execute the code."
msgstr ""

#: of tvm.relay.build_module.create_executor:-1
msgid "any multi-target like object, see Target.canon_multi_target"
msgstr ""

#: of tvm.relay.build_module.create_executor:30
msgid ""
"For homogeneous compilation, the unique build target. For heterogeneous "
"compilation, a dictionary or list of possible build targets. CAUTION: "
"Though this API allows multiple targets, it does not allow multiple "
"devices, so heterogenous compilation is not yet supported."
msgstr ""

#: of tvm.relay.build_module.create_executor:36
msgid "Input parameters to the graph that do not change during inference time."
msgstr ""

#: of tvm.relay.build_module.create_executor:41
msgid "executor : :py:class:`~tvm.relay.backend.interpreter.Executor`"
msgstr ""

#: of tvm.relay.op.transform.cumprod:1
msgid ""
"Numpy style cumprod op. Return the cumulative inclusive product of the "
"elements along a given axis."
msgstr ""

#: of tvm.relay.op.transform.cumprod:10
msgid ""
"Axis along which the cumulative product is computed. The default (None) "
"is to compute the cumprod over the flattened array."
msgstr ""

#: of tvm.relay.op.transform.cumprod:14
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are multiplied. If dtype is not specified, it defaults to the dtype of "
"data."
msgstr ""

#: of tvm.relay.op.transform.cumprod:21 tvm.relay.op.transform.cumsum:21
msgid "exclusive"
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:-1 tvm.relay.op.transform.cumprod:-1
#: tvm.relay.op.transform.cumsum:-1 tvm.relay.op.transform.stft:-1
#: tvm.relay.op.transform.unique:-1
msgid "bool, optional"
msgstr ""

#: of tvm.relay.op.transform.cumprod:18
msgid ""
"If true will return exclusive product in which the first element is not "
"included. In other terms, if true, the j-th output element would be the "
"product of the first (j-1) elements. Otherwise, it would be the product "
"of the first j elements. The product of zero elements will be 1."
msgstr ""

#: of tvm.relay.op.transform.cumprod:26 tvm.relay.op.transform.cumsum:26
msgid ""
"The result has the same size as data, and the same shape as data if axis "
"is not None. If axis is None, the result is a 1-d array."
msgstr ""

#: of tvm.relay.op.transform.cumsum:1
msgid ""
"Numpy style cumsum op. Return the cumulative inclusive sum of the "
"elements along a given axis."
msgstr ""

#: of tvm.relay.op.transform.cumsum:10
msgid ""
"Axis along which the cumulative sum is computed. The default (None) is to"
" compute the cumsum over the flattened array."
msgstr ""

#: of tvm.relay.op.transform.cumsum:14
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are summed. If dtype is not specified, it defaults to the dtype of data."
msgstr ""

#: of tvm.relay.op.transform.cumsum:18
msgid ""
"If true will return exclusive sum in which the first element is not "
"included. In other terms, if true, the j-th output element would be the "
"sum of the first (j-1) elements. Otherwise, it would be the sum of the "
"first j elements."
msgstr ""

#: of tvm.relay.op.tensor.device_copy:1
msgid ""
"Copy data from the source device to the destination device. This operator"
" helps data transferring between difference devices for heterogeneous "
"execution."
msgstr ""

#: of tvm.relay.op.tensor.device_copy:11
msgid "src_device : Union[:py:class:`Device`, str]"
msgstr ""

#: of tvm.relay.op.tensor.device_copy:-1
msgid "Union["
msgstr ""

#: of tvm.relay.op.tensor.device_copy:11
msgid "The source device where the data is copied from."
msgstr ""

#: of tvm.relay.op.tensor.device_copy:14
msgid "dst_device : Union[:py:class:`Device`, str]"
msgstr ""

#: of tvm.relay.op.tensor.device_copy:14
msgid "The destination device where the data is copied to."
msgstr ""

#: of tvm.relay.op.transform.dft:1
msgid ""
"Computes the discrete Fourier transform of input (calculation along the "
"last axis). This gives frequency components of the signal as they change "
"over time."
msgstr ""

#: of tvm.relay.op.transform.dft:7
msgid "re_data"
msgstr ""

#: of tvm.relay.op.transform.dft:7
msgid "N-D tensor, real part of the input signal."
msgstr ""

#: of tvm.relay.op.transform.dft:11
msgid "im_data"
msgstr ""

#: of tvm.relay.op.transform.dft:10
msgid ""
"N-D tensor, imaginary part of the input signal. If the signal is real, "
"then the values of this tensor are zeros."
msgstr ""

#: of tvm.relay.op.transform.dft:14
msgid "inverse"
msgstr ""

#: of tvm.relay.op.transform.dft:14
msgid "Whether to perform the inverse discrete fourier transform."
msgstr ""

#: of tvm.relay.op.transform.dft:18
msgid "re_output"
msgstr ""

#: of tvm.relay.op.transform.dft:19
msgid "The Fourier Transform of the input (Real part)."
msgstr ""

#: of tvm.relay.op.transform.dft:20
msgid "im_output"
msgstr ""

#: of tvm.relay.op.transform.dft:21
msgid "The Fourier Transform of the input (Imaginary part)."
msgstr ""

#: of tvm.relay.op.tensor.einsum:8
msgid "equation"
msgstr ""

#: of tvm.relay.op.tensor.einsum:8
msgid "The einsum expression string."
msgstr ""

#: of tvm.relay.op.tensor.einsum:13
msgid "The output tensor from the einsum op."
msgstr ""

#: of tvm.relay.op.transform.expand_dims:-1
msgid "Union[int, Expr]"
msgstr ""

#: of tvm.relay.op.transform.expand_dims:9
msgid ""
"The axis at which the input array is expanded. Should lie in range "
"`[-data.ndim - 1, data.ndim]`. If `axis < 0`, it is the first axis "
"inserted; If `axis >= 0`, it is the last axis inserted in Python's "
"negative indexing."
msgstr ""

#: of tvm.relay.op.transform.expand_dims:15
msgid "num_newaxis"
msgstr ""

#: of tvm.relay.op.transform.expand_dims:15
msgid "Number of axes to be inserted. Should be >= 0."
msgstr ""

#: of tvm.relay.op.transform.expand_dims:20 tvm.relay.op.transform.reshape:67
#: tvm.relay.op.transform.reverse_reshape:24
msgid "The reshaped result."
msgstr ""

#: of tvm.relay.op.tensor.fixed_point_multiply:9
msgid "multiplier"
msgstr ""

#: of tvm.relay.op.tensor.fixed_point_multiply:10
msgid "The integer multiplier of the fixed point constant."
msgstr ""

#: of tvm.relay.op.tensor.fixed_point_multiply:12
msgid "shift"
msgstr ""

#: of tvm.relay.op.tensor.fixed_point_multiply:12
msgid "The integer shift of the fixed point constant."
msgstr ""

#: of tvm.relay.op.tensor.fixed_point_multiply:17
msgid "The output of the fixed point multiplication"
msgstr ""

#: of tvm.relay.op.transform.full:6 tvm.relay.op.transform.full_like:9
msgid "fill_value"
msgstr ""

#: of tvm.relay.op.transform.full:6
msgid "The value to fill. Must be a scalar."
msgstr ""

#: of tvm.relay.op.transform.full:-1
msgid "tuple of int or relay.Expr, optional"
msgstr ""

#: of tvm.relay.op.tensor.ones:6 tvm.relay.op.tensor.zeros:6
#: tvm.relay.op.transform.full:9
msgid "The shape of the target."
msgstr ""

#: of tvm.relay.op.transform.full:-1
msgid "data type, optional (defaults to data type of the fill value)"
msgstr ""

#: of tvm.relay.op.tensor.ones:9 tvm.relay.op.tensor.zeros:9
#: tvm.relay.op.transform.full:12
msgid "The data type of the target."
msgstr ""

#: of tvm.relay.op.transform.full_like:9
msgid "The scalar value to fill."
msgstr ""

#: of tvm.relay.op.transform.gather:3
msgid "E.g. for a 3D tensor, output is computed as:"
msgstr ""

#: of tvm.relay.op.transform.gather:11
msgid ""
"``indices`` must have the same shape as ``data``, except at dimension "
"``axis`` which must just be not null. Output will have the same shape as "
"``indices``."
msgstr ""

#: of tvm.relay.op.transform.gather:20
msgid "The axis along which to index. Negative axis is supported."
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:28 tvm.relay.op.transform.gather:23
#: tvm.relay.op.transform.gather_nd:10 tvm.relay.op.transform.one_hot:8
#: tvm.relay.op.transform.scatter_elements:10
#: tvm.relay.op.transform.scatter_nd:11 tvm.relay.op.transform.take:9
#: tvm.relay.op.transform.unique:22 tvm.relay.op.transform.unravel_index:6
msgid "indices"
msgstr ""

#: of tvm.relay.op.transform.gather:23
msgid "The indices of values to gather."
msgstr ""

#: of tvm.relay.op.transform.gather_nd:10
msgid "The shape of output tensor."
msgstr ""

#: of tvm.relay.op.transform.gather_nd:13 tvm.relay.op.transform.take:16
msgid "batch_dims"
msgstr ""

#: of tvm.relay.op.transform.gather_nd:13
msgid "The number of batch dimensions."
msgstr ""

#: of tvm.relay.op.transform.gather_nd:17
msgid "index_rank"
msgstr ""

#: of tvm.relay.op.transform.gather_nd:16
msgid ""
"The size of an indexing tuple, which is a fixed value and the same as "
"indices.shape[0]. Only needed when other dimensions of indices are "
"dynamic."
msgstr ""

#: of tvm.relay.op.tensor.stack:13 tvm.relay.op.transform.gather_nd:22
#: tvm.relay.op.transform.invert_permutation:17
#: tvm.relay.op.transform.layout_transform:16
#: tvm.relay.op.transform.meshgrid:17 tvm.relay.op.transform.one_hot:28
#: tvm.relay.op.transform.repeat:20 tvm.relay.op.transform.reshape_like:36
#: tvm.relay.op.transform.reverse:14 tvm.relay.op.transform.reverse_sequence:24
#: tvm.relay.op.transform.scatter_elements:30
#: tvm.relay.op.transform.scatter_nd:27 tvm.relay.op.transform.sequence_mask:23
#: tvm.relay.op.transform.split:22 tvm.relay.op.transform.strided_set:23
#: tvm.relay.op.transform.strided_slice:33 tvm.relay.op.transform.take:26
#: tvm.relay.op.transform.tile:14 tvm.relay.op.transform.trilu:21
msgid "ret"
msgstr ""

#: of tvm.relay.op.transform.invert_permutation:1
msgid ""
"Computes the inverse permutation of data. This operation computes the "
"inverse of an index permutation. It takes a 1-D integer tensor x, which "
"represents the indices of a zero-based array and swaps each value with "
"its index position."
msgstr ""

#: of tvm.relay.op.transform.invert_permutation:6
msgid ""
"For an output tensor y and an input tensor x, this operation computes the"
" following: y[x[i]] = i for i in [0, 1, ..., len(x) - 1]"
msgstr ""

#: of tvm.relay.op.transform.invert_permutation:12
msgid "The source data to be invert permuted."
msgstr ""

#: of tvm.relay.op.transform.invert_permutation:17
msgid "Invert permuted data. Has the same type as data."
msgstr ""

#: of tvm.relay.op.transform.layout_transform:6
msgid "The source tensor to be transformed."
msgstr ""

#: of tvm.relay.op.transform.layout_transform:9
msgid "src_layout"
msgstr ""

#: of tvm.relay.op.transform.layout_transform:9
msgid "The source layout.  (e.g NCHW)"
msgstr ""

#: of tvm.relay.op.transform.layout_transform:12
msgid "dst_layout"
msgstr ""

#: of tvm.relay.op.transform.layout_transform:12
msgid "The destination layout.  (e.g. NCHW16c)"
msgstr ""

#: of tvm.relay.op.transform.layout_transform:17
msgid "The transformed tensor."
msgstr ""

#: of tvm.relay.param_dict.load_param_dict:3
msgid "Use :py:func:`tvm.runtime.load_param_dict` instead."
msgstr ""

#: of tvm.relay.param_dict.load_param_dict:9
#: tvm.relay.param_dict.save_param_dict:17
msgid "param_bytes: bytearray"
msgstr ""

#: of tvm.relay.param_dict.load_param_dict:9
#: tvm.relay.param_dict.save_param_dict:17
msgid "Serialized parameters."
msgstr ""

#: of tvm.relay.param_dict.load_param_dict:14
#: tvm.relay.param_dict.save_param_dict:12
msgid "The parameter dictionary."
msgstr ""

#: of tvm.relay.op.reduce.logsumexp:3
msgid ""
"This function is more numerically stable than log(sum(exp(input))). It "
"avoids overflows caused by taking the exp of large inputs and underflows "
"caused by taking the log of small inputs."
msgstr ""

#: of tvm.relay.op.reduce.logsumexp:13
msgid ""
"Axis or axes along which a standard deviation operation is performed. The"
" default, axis=None, will compute the log of the sum of exponentials of "
"all elements in the input array. If axis is negative it counts from the "
"last to the first axis."
msgstr ""

#: of tvm.relay.op.reduce.logsumexp:18
msgid ""
"If this is set to True, the axes which are reduced are left in the result"
" as dimensions with size one."
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:7
msgid "Input tensor."
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:10
msgid "diagonal"
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:10
msgid "Values to be filled in the diagonal."
msgstr ""

#: of tvm.relay.op.algorithm.topk:11 tvm.relay.op.transform.matrix_set_diag:17
#: tvm.relay.op.transform.trilu:12
msgid "k"
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:-1
msgid "int or tuple of int, optional"
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:13
msgid ""
"Diagonal offset(s). The diagonal or range of diagonals to set. (0 by "
"default) Positive value means superdiagonal, 0 refers to the main "
"diagonal, and negative value means subdiagonals. k can be a single "
"integer (for a single diagonal) or a pair of integers specifying the low "
"and high ends of a matrix band. k[0] must not be larger than k[1]."
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:25
msgid "align"
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:20
msgid ""
"Some diagonals are shorter than max_diag_len and need to be padded. align"
" is a string specifying how superdiagonals and subdiagonals should be "
"aligned, respectively. There are four possible alignments: \"RIGHT_LEFT\""
" (default), \"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\". "
"\"RIGHT_LEFT\" aligns superdiagonals to the right (left-pads the row) and"
" subdiagonals to the left (right-pads the row). It is the packing format "
"LAPACK uses. cuSPARSE uses \"LEFT_RIGHT\", which is the opposite "
"alignment."
msgstr ""

#: of tvm.relay.op.transform.matrix_set_diag:30
msgid "New tensor with given diagonal values."
msgstr ""

#: of tvm.relay.op.reduce.max:9
msgid ""
"Axis or axes along which the max operation is performed. The default, "
"axis=None, will find the max element from all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.relay.op.reduce.mean:9
msgid ""
"Axis or axes along which a mean operation is performed. The default, "
"axis=None, will compute the mean of all elements in the input array. If "
"axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.reduce.mean_std:9
msgid ""
"Axis or axes along which a mean and standard deviation operation is "
"performed. The default, axis=None, will compute the mean and standard "
"deviation of all elements in the input array. If axis is negative it "
"counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.reduce.mean_variance:9
msgid ""
"Axis or axes along which a mean and variance operation is performed. The "
"default, axis=None, will compute the mean and variance of all elements in"
" the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.relay.op.reduce.mean_variance:23 tvm.relay.op.reduce.std:23
#: tvm.relay.op.reduce.variance:23
msgid "unbiased"
msgstr ""

#: of tvm.relay.op.reduce.mean_variance:23 tvm.relay.op.reduce.std:23
#: tvm.relay.op.reduce.variance:23
msgid "If this is set to True, the unbiased estimation will be used."
msgstr ""

#: of tvm.relay.op.transform.meshgrid:4
msgid "Similar to ``numpy.meshgrid``."
msgstr ""

#: of tvm.relay.op.transform.meshgrid:9
msgid "A list of tensors, which must be either scalars or 1-D vectors."
msgstr ""

#: of tvm.relay.op.transform.meshgrid:12
msgid "indexing"
msgstr ""

#: of tvm.relay.op.transform.meshgrid:12
msgid ""
"Indexing mode, either \"ij\" for matrix indexing or \"xy\" for Cartesian "
"indexing."
msgstr ""

#: of tvm.relay.op.transform.meshgrid:-1 tvm.relay.op.transform.split:-1
msgid "relay.Tuple([relay.Expr, relay.Expr])"
msgstr ""

#: of tvm.relay.op.reduce.min:9
msgid ""
"Axis or axes along which a minimum operation is performed. The default, "
"axis=None, will find the minimum element from all of the elements of the "
"input array. If axis is negative it counts from the last to the first "
"axis."
msgstr ""

#: of tvm.relay.op.tensor.ndarray_size:14
msgid "The number of elements of input tensor."
msgstr ""

#: of tvm.relay.op.transform.one_hot:1
msgid ""
"Returns a one-hot tensor where the locations represented by indices take "
"value on_value, and other locations take value off_value. Final dimension"
" is <indices outer dimensions> x depth x <indices inner dimensions>."
msgstr ""

#: of tvm.relay.op.transform.one_hot:8
msgid "Locations to set to on_value."
msgstr ""

#: of tvm.relay.op.transform.one_hot:11
msgid "on_value"
msgstr ""

#: of tvm.relay.op.transform.one_hot:11
msgid "Value to fill at indices."
msgstr ""

#: of tvm.relay.op.transform.one_hot:14
msgid "off_value"
msgstr ""

#: of tvm.relay.op.transform.one_hot:14
msgid "Value to fill at all other positions besides indices."
msgstr ""

#: of tvm.relay.op.transform.one_hot:17
msgid "depth"
msgstr ""

#: of tvm.relay.op.transform.one_hot:-1
msgid "int or relay.Expr"
msgstr ""

#: of tvm.relay.op.transform.one_hot:17
msgid "Depth of the one-hot dimension."
msgstr ""

#: of tvm.relay.op.transform.one_hot:20
msgid "Axis to fill."
msgstr ""

#: of tvm.relay.op.transform.one_hot:23
msgid "Data type of the output tensor."
msgstr ""

#: of tvm.relay.op.transform.one_hot:28
msgid "The one-hot tensor."
msgstr ""

#: of tvm.relay.op.tensor.ones:-1 tvm.relay.op.tensor.zeros:-1
msgid "data type"
msgstr ""

#: of tvm.relay.build_module.optimize:6
msgid "The module to build. Using relay.Function is deprecated."
msgstr ""

#: of tvm.relay.build_module.optimize:20
msgid "The optimized relay module."
msgstr ""

#: of tvm.relay.build_module.optimize:-1
msgid "dict"
msgstr ""

#: of tvm.relay.build_module.optimize:23
msgid "The parameters of the final graph."
msgstr ""

#: of tvm.relay.op.reduce.prod:9
msgid ""
"Axis or axes along which a product is performed. The default, axis=None, "
"will find the indices of minimum element all of the elements of the input"
" array. If axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.transform.reinterpret:14
msgid "The reinterpreted result."
msgstr ""

#: of tvm.relay.op.transform.repeat:1
msgid ""
"Repeats elements of an array. By default, repeat flattens the input array"
" into 1-D and then repeats the elements."
msgstr ""

#: of tvm.relay.op.transform.repeat:10
msgid "repeats"
msgstr ""

#: of tvm.relay.op.transform.repeat:10
msgid "The number of repetitions for each element."
msgstr ""

#: of tvm.relay.op.transform.repeat:15 tvm.relay.op.transform.reverse:9
msgid "axis: int"
msgstr ""

#: of tvm.relay.op.transform.repeat:13
msgid ""
"The axis along which to repeat values. The negative numbers are "
"interpreted counting from the backward. By default, use the flattened "
"input array, and return a flat output array."
msgstr ""

#: of tvm.relay.op.transform.reshape:3
msgid ""
"To give user more convenience in without doing manual shape inference, "
"some dimensions of the shape can take special values from the set {0, -1,"
" -2, -3, -4}. The significance of each is explained below:"
msgstr ""

#: of tvm.relay.op.transform.reshape:7
msgid "``0`` copy this dimension from the input to the output shape."
msgstr ""

#: of tvm.relay.op.transform.reshape:14
msgid ""
"Note: If the parameter allowzero is manually set to true, it specifies a "
"special case where 0 actually means a true empty tensor."
msgstr ""

#: of tvm.relay.op.transform.reshape:17
msgid ""
"``-1`` infers the dimension of the output shape by using the remainder of"
" the input dimensions keeping the size of the new array same as that of "
"the input array. At most one dimension of shape can be -1."
msgstr ""

#: of tvm.relay.op.transform.reshape:27
msgid "``-2`` copy all/remainder of the input dimensions to the output shape."
msgstr ""

#: of tvm.relay.op.transform.reshape:35
msgid ""
"``-3`` use the product of two consecutive dimensions of the input shape "
"as the output dimension."
msgstr ""

#: of tvm.relay.op.transform.reshape:45
msgid ""
"``-4`` split one dimension of the input into two dimensions passed "
"subsequent to -4 in shape (can contain -1)."
msgstr ""

#: of tvm.relay.op.transform.reshape:59
#: tvm.relay.op.transform.reverse_reshape:19
msgid "newshape"
msgstr ""

#: of tvm.relay.op.transform.reshape:-1
msgid "Union[int, Tuple[int], List[int]] or relay.Expr"
msgstr ""

#: of tvm.relay.op.transform.reshape:59
#: tvm.relay.op.transform.reverse_reshape:19
msgid "The new shape. Should be compatible with the original shape."
msgstr ""

#: of tvm.relay.op.transform.reshape:62
msgid "allowzero"
msgstr ""

#: of tvm.relay.op.transform.reshape:-1
msgid "Bool, optional"
msgstr ""

#: of tvm.relay.op.transform.reshape:62
msgid ""
"If true, then treat zero as true empty tensor rather than a copy "
"instruction."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:1
msgid ""
"Reshapes the input tensor by the size of another tensor. For an input "
"tensor with shape ``(d0, d1, ..., d(k-1))``, `reshape_like` operation "
"reshapes the input tensor into an output tensor with the same shape as "
"the second input tensor, in particular reshaping the dimensions of `data`"
" in `[lhs_begin, lhs_end)` using the dimensions from `shape_like` in "
"`[rhs_begin, rhs_end)`."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:8
msgid "Sizes for `data` and the output tensor should be compatible."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:17
#: tvm.relay.op.transform.slice_like:12
msgid "shape_like"
msgstr ""

#: of tvm.relay.op.transform.reshape_like:16
msgid ""
"The tensor to reshape data like. Should be compatible with the original "
"shape on the reshaped dimensions."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:20
msgid "lhs_begin"
msgstr ""

#: of tvm.relay.op.transform.reshape_like:20
msgid "The axis of data to begin reshaping. Default is 0."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:24
msgid "lhs_end"
msgstr ""

#: of tvm.relay.op.transform.reshape_like:-1
msgid "int or None, optional"
msgstr ""

#: of tvm.relay.op.transform.reshape_like:23
msgid ""
"The axis of data where reshaping should stop, exclusive. Default is None "
"which reshapes to the end."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:27
msgid "rhs_begin"
msgstr ""

#: of tvm.relay.op.transform.reshape_like:27
msgid "The axis of shape_like where the target shape begins. Default is 0."
msgstr ""

#: of tvm.relay.op.transform.reshape_like:31
msgid "rhs_end"
msgstr ""

#: of tvm.relay.op.transform.reshape_like:30
msgid ""
"The axis of shape_like where the target shape ends, exclusive. Default is"
" None which extends to the end."
msgstr ""

#: of tvm.relay.op.transform.reverse:9
msgid "The axis along which to reverse elements."
msgstr ""

#: of tvm.relay.op.transform.reverse_reshape:4
msgid ""
"The special values have the same semantics as "
":py:class:`tvm.relay.reshape`. The difference is that special values are "
"inferred from right to left. It can be explained in the example below."
msgstr ""

#: of tvm.relay.op.transform.reverse_reshape:-1
msgid "Union[int, Tuple[int], List[int]]"
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:1
msgid ""
"Reverse the tensor for variable length slices. Input is first sliced "
"along batch axis and then elements are reversed along seq axis."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:7
msgid "The tensor to be reversed."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:13
msgid "seq_lengths"
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:10
msgid ""
"A 1D Tensor with length a.dims[batch_axis]. Must be one of the following "
"types: int32, int64. If seq_lengths[i] > a.dims[seq_axis], it is rounded "
"to a.dims[seq_axis]. If seq_lengths[i] < 1, it is rounded to 1."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:16
msgid "seq_axis"
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:16
msgid "The axis along which the elements will be reversed. Default is 1."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:19
msgid "batch_axis"
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:19
msgid "The axis along which the tensor will be sliced. Default is 0."
msgstr ""

#: of tvm.relay.op.transform.reverse_sequence:24
msgid "The computed result of same shape and type as of input."
msgstr ""

#: of tvm.relay.op.tensor.rsqrt:3
msgid "1/sqrt(x)"
msgstr ""

#: of tvm.relay.param_dict.save_param_dict:3
msgid ""
"The result binary bytes can be loaded by the GraphModule with API "
"\"load_params\"."
msgstr ""

#: of tvm.relay.param_dict.save_param_dict:6
msgid "Use :py:func:`tvm.runtime.save_param_dict` instead."
msgstr ""

#: of tvm.relay.ty.scalar_type:3
msgid "This function returns TensorType((), dtype)"
msgstr ""

#: of tvm.relay.ty.scalar_type:8
msgid "The content data type."
msgstr ""

#: of tvm.relay.ty.scalar_type:12
msgid "s_type"
msgstr ""

#: of tvm.relay.ty.scalar_type:-1
msgid "tvm.relay.TensorType"
msgstr ""

#: of tvm.relay.ty.scalar_type:13
msgid "The result type."
msgstr ""

#: of tvm.relay.op.transform.scatter_elements:10
#: tvm.relay.op.transform.scatter_nd:11
msgid "The index locations to update."
msgstr ""

#: of tvm.relay.op.transform.scatter_elements:13
#: tvm.relay.op.transform.scatter_nd:14
msgid "updates"
msgstr ""

#: of tvm.relay.op.transform.scatter_elements:13
#: tvm.relay.op.transform.scatter_nd:14
msgid "The values to update."
msgstr ""

#: of tvm.relay.op.transform.scatter_elements:16
msgid "The axis to scatter elements on. It is zero by default."
msgstr ""

#: of tvm.relay.op.transform.scatter_elements:26
msgid "reduction"
msgstr ""

#: of tvm.relay.op.transform.scatter_elements:19
msgid ""
"The reduction mode for scatter. Choise is from [\"update\", \"add\", "
"\"mul\", \"mean\", \"min\", max\"] If update, the update values will "
"replace the input data If add, the update values will be added to the "
"input data If mul, the input data will be multiplied on the update values"
" If mean, the input data will be mean between the update values and the "
"input data If min, there is choice of minimal between the update values "
"and the input data If max, there is choice of maximal between the update "
"values and the input data It is \"update\" by default"
msgstr ""

#: of tvm.relay.op.transform.scatter_nd:3
msgid "See :py:func:`tvm.topi.scatter` for how data is scattered."
msgstr ""

#: of tvm.relay.op.transform.scatter_nd:23 tvm.relay.op.transform.take:22
msgid "mode"
msgstr ""

#: of tvm.relay.op.transform.scatter_nd:17
msgid ""
"The accumulation mode for scatter. \"update\", \"add\", \"mul\", \"min\" "
"or \"max\" If update, the update values will replace the input data If "
"add, the update values will be added to the input data If mul, the update"
" values will be multiply to the input data If min, there is choice of "
"minimal between the update values and the input data If max, there is "
"choice of maximal between the update values and the input data It is "
"\"update\" by default"
msgstr ""

#: of tvm.te.hybrid.script:3
msgid ""
"The hybrid function support emulation mode and parsing to the internal "
"language IR."
msgstr ""

#: of tvm.te.hybrid.script:8
msgid "hybrid_func"
msgstr ""

#: of tvm.te.hybrid.script:-1
msgid "function"
msgstr ""

#: of tvm.te.hybrid.script:9
msgid "A decorated hybrid script function."
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:2
msgid ""
"If `sorted_sequence` is N-dimensional, the innermost dimension of "
"`values` are searched in the corresponding dimension of "
"`sorted_sequence`."
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:9
msgid "sorted_sequence"
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:8
msgid ""
"N-D or 1-D Tensor, containing monotonically increasing sequence on the "
"innermost dimension."
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:14
msgid "values"
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:12
msgid ""
"N-D Tensor containing the search values. When `sorted_sequence` is 1-D, "
"the shape of `values` can be arbitrary. Otherwise, ranks of "
"`sorted_sequence` and `values` must be the same, and outer N-1 axes must "
"have the same size."
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:20
msgid "right"
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:17
msgid ""
"Controls which index is returned if a value lands exactly on one of "
"sorted values. If False, the index of the first suitable location found "
"is given. If true, return the last such index. If there is no suitable "
"index, return either 0 or N (where N is the size of the innermost "
"dimension)."
msgstr ""

#: of tvm.relay.op.algorithm.searchsorted:28
msgid ""
"Tensor with same shape as values, representing the indices of elements of"
" `values` if they are inserted in `sorted_sequence`."
msgstr ""

#: of tvm.relay.op.transform.segment_sum:1
msgid ""
"Computes the sum along segment_ids along axis 0. If multiple segment_ids "
"reference the same location their contributions add up. result[index, j, "
"k, ...] = Σi... data[i, j, k,..] where index = segment_ids[i] This op is "
"much better understood with visualization articulated in the following "
"links and examples at the end of this docstring."
msgstr ""

#: of tvm.relay.op.transform.segment_sum:7
msgid ""
"https://www.tensorflow.org/api_docs/python/tf/math/unsorted_segment_sum "
"https://caffe2.ai/docs/sparse-operations.html#null__unsorted-segment-"
"reduction-ops"
msgstr ""

#: of tvm.relay.op.transform.segment_sum:13
msgid "Input tensor. It can be of any type and multi-dimensional."
msgstr ""

#: of tvm.relay.op.transform.segment_sum:20
msgid "segment_ids"
msgstr ""

#: of tvm.relay.op.transform.segment_sum:16
msgid ""
"A 1-D int32/int64 tensor containing the segment_ids of the rows to "
"calculate the output sum upon. It defines a mapping from the zeroth "
"dimension of data onto segment_ids. The segment_ids tensor should be the "
"size of the first dimension, d0, with consecutive IDs in the range 0 to "
"k, where k<d0. In particular, a segmentation of a matrix tensor is a "
"mapping of rows to segments. This tensor doesn't need to be sorted."
msgstr ""

#: of tvm.relay.op.transform.segment_sum:24
msgid "num_segments"
msgstr ""

#: of tvm.relay.op.transform.segment_sum:23
msgid ""
"An integer describing the shape of the zeroth dimension. If unspecified, "
"it is calculated equivalent to the number of unique segment_ids."
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:3
msgid ""
"This function takes an n-dimensional input array of the form [MAX_LENGTH,"
" batch_size, ...] or [batch_size, MAX_LENGTH, ...] and returns an array "
"of the same shape."
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:9
msgid "The input data."
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:12
msgid "valid_length"
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:12
msgid "The expected (valid) length of each sequence in the tensor."
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:15
msgid "mask_value"
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:-1
msgid "float, optional"
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:15
msgid "The masking value."
msgstr ""

#: of tvm.relay.op.transform.sequence_mask:18
msgid "The axis of the length dimension."
msgstr ""

#: of sys.setrecursionlimit:3
msgid ""
"This limit prevents infinite recursion from causing an overflow of the C "
"stack and crashing Python.  The highest possible limit is platform- "
"dependent."
msgstr ""

#: of tvm.relay.op.tensor.shape_of:14
msgid "The shape tensor."
msgstr ""

#: of tvm.relay.op.transform.slice_like:3
msgid ""
"For an input array with shape ``(d1, d2, ..., dk)``, `slice_like` "
"operation slices the input array corresponding to the size of the second "
"array. By default will slice on all axes."
msgstr ""

#: of tvm.relay.op.transform.slice_like:9 tvm.relay.op.transform.split:12
#: tvm.relay.op.transform.take:6
msgid "The source array."
msgstr ""

#: of tvm.relay.op.transform.slice_like:12
msgid "An array based on which shape, the result shape is computed."
msgstr ""

#: of tvm.relay.op.transform.slice_like:16
#: tvm.relay.op.transform.strided_slice:22 tvm.relay.op.transform.transpose:9
msgid "axes"
msgstr ""

#: of tvm.relay.op.transform.slice_like:-1
#: tvm.relay.op.transform.strided_slice:-1
msgid "Tuple[int] or List[int], optional"
msgstr ""

#: of tvm.relay.op.transform.slice_like:15
msgid ""
"List of axes on which input data will be sliced according to the "
"corresponding size of the second input. By default will slice on all "
"axes. Negative axes mean counting in reverse."
msgstr ""

#: of tvm.relay.op.transform.sliding_window:9
msgid ""
"What axis the window begins sliding over. Window will be slid over this "
"axis and all following axes. The axis value determines the window shape "
"(and thus, the number of strides): window shape and strides must both be "
"of length `data.ndim-axis`."
msgstr ""

#: of tvm.relay.op.transform.sliding_window:16
msgid "window_shape"
msgstr ""

#: of tvm.relay.op.transform.sliding_window:-1
msgid "List[int]"
msgstr ""

#: of tvm.relay.op.transform.sliding_window:15
msgid ""
"The window shape to form over the input. Window shape must be of length "
"`data.ndim-axis`."
msgstr ""

#: of tvm.relay.op.transform.sliding_window:20
#: tvm.relay.op.transform.strided_slice:16
msgid "strides"
msgstr ""

#: of tvm.relay.op.transform.sliding_window:19
msgid ""
"How to stride the window along each dimension. Strides must be of length "
"`data.ndim-axis`."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:1
msgid ""
"Fill rows in a sparse matrix that do not contain any values. Values are "
"placed in the first column of empty rows. The sparse array is in COO "
"format. It returns a TupleWrapper with 3 outputs."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:10
#: tvm.relay.op.transform.sparse_reshape:7
#: tvm.relay.op.transform.sparse_to_dense:6
msgid "sparse_indices"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:8
msgid ""
"A 2-D tensor[N, ndims] of integers containing the locations of sparse "
"values, where N is the number of sparse values and n_dim is the number of"
" dimensions of the dense_shape. The first column of this parameter must "
"be sorted in ascending order."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:13
#: tvm.relay.op.transform.sparse_to_dense:12
msgid "sparse_values"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:13
msgid "A 1-D tensor[N] containing the sparse values for the sparse indices."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:16
msgid "dense_shape"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:16
msgid "A 1-D tensor[ndims] which contains the shape of the dense output tensor."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:19
#: tvm.relay.op.transform.sparse_to_dense:16
msgid "default_value"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:19
msgid "A 1-D tensor[1] containing the default value for the remaining locations."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:25
msgid "new_sparse_indices"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:24
msgid ""
"A 2-D tensor[?, ndims] of integers containing location of new sparse "
"indices. The first column outputs must be sorted in ascending order."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:28
msgid "new_sparse_values"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:28
msgid "A 1-D tensor[?] containing the sparse values for the sparse indices."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:32
msgid "empty_row_indicator"
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:31
msgid ""
"A 1-D tensor[dense_shape[0]] filled with zeros and ones indicating "
"whether the particular row is empty or full respectively."
msgstr ""

#: of tvm.relay.op.transform.sparse_fill_empty_rows:36
msgid ""
"This op exactly follows the documentation here: "
"https://www.tensorflow.org/api_docs/python/tf/sparse/fill_empty_rows "
"There are two exceptions: 1. Input Sparse Indices are expected to be in "
"row-major order. 2. Empty Row Indicator has int64 output type with 1(for "
"True) and 0(for False)."
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:1
msgid "Reshape a sparse tensor. The sparse array is in COO format."
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:6
msgid ""
"A 2-D tensor[N, n_dim] of integers containing location of sparse values, "
"where N is the number of sparse values and n_dim is the number of "
"dimensions of the dense_shape."
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:10
msgid "prev_shape"
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:10
msgid "A 1-D tensor containing the previous shape of the dense tensor."
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:13
msgid "new_shape"
msgstr ""

#: of tvm.relay.op.transform.sparse_reshape:13
msgid "A 1-D tensor containing the new shape of the dense tensor."
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:6
msgid ""
"A 0-D, 1-D, or 2-D tensor of integers containing location of sparse "
"values."
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:9
msgid "output_shape"
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:9
msgid "A list of integers. Shape of the dense output tensor."
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:12
msgid "A 0-D or 1-D tensor containing the sparse values for the sparse indices."
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:15
msgid ""
"A 0-D tensor containing the default value for the remaining locations. "
"Defaults to 0."
msgstr ""

#: of tvm.relay.op.transform.sparse_to_dense:21
msgid "Dense tensor of shape output_shape. Has the same type as sparse_values."
msgstr ""

#: of tvm.relay.op.transform.split:3
msgid ""
"If indices_or_sections is an integer, the input will be divided equally "
"along given axis. If such a split is not possible, an error is raised."
msgstr ""

#: of tvm.relay.op.transform.split:6
msgid ""
"If indices_or_sections is a tuple of sorted integers, the entries "
"indicate where along axis the array is split."
msgstr ""

#: of tvm.relay.op.transform.split:15
msgid "indices_or_sections"
msgstr ""

#: of tvm.relay.op.transform.split:-1
msgid "int or tuple of int"
msgstr ""

#: of tvm.relay.op.transform.split:15
msgid "Indices or sections to split into. Accepts an int or a tuple."
msgstr ""

#: of tvm.relay.op.transform.split:18
msgid "The axis over which to split."
msgstr ""

#: of tvm.relay.op.transform.squeeze:-1
msgid "Union[None, int, Tuple[int], List[int]] or Expr"
msgstr ""

#: of tvm.relay.op.transform.squeeze:9
msgid ""
"The set of axes to remove. If axis = None, remove all axes of dimension "
"1. If any specified axis has dimension that does not equal 1, it is an "
"error."
msgstr ""

#: of tvm.relay.op.transform.squeeze:16
msgid "The squeezed result."
msgstr ""

#: of tvm.relay.op.tensor.stack:-1
msgid "Union(List[relay.Expr], relay.Expr)"
msgstr ""

#: of tvm.relay.op.tensor.stack:6
msgid ""
"A list of tensors or a Relay expression that evaluates to a tuple of "
"tensors."
msgstr ""

#: of tvm.relay.op.tensor.stack:9
msgid "The axis in the result array along which the input arrays are stacked."
msgstr ""

#: of tvm.relay.op.tensor.stack:14
msgid "The stacked tensor."
msgstr ""

#: of tvm.relay.op.reduce.std:9
msgid ""
"Axis or axes along which a standard deviation operation is performed. The"
" default, axis=None, will compute the standard deviation of all elements "
"in the input array. If axis is negative it counts from the last to the "
"first axis."
msgstr ""

#: of tvm.relay.op.transform.stft:1
msgid ""
"The STFT computes the Fourier transform of short overlapping windows of "
"the input. This gives frequency components of the signal as they change "
"over time."
msgstr ""

#: of tvm.relay.op.transform.stft:7
msgid "Either a 1-D tensor or a 2-D batch tensor."
msgstr ""

#: of tvm.relay.op.transform.stft:10
msgid "n_fft"
msgstr ""

#: of tvm.relay.op.transform.stft:10
msgid "The size of Fourier transform."
msgstr ""

#: of tvm.relay.op.transform.stft:14
msgid "hop_length"
msgstr ""

#: of tvm.relay.op.transform.stft:13
msgid ""
"The distance between neighboring sliding window frames. If is None, it is"
" treated as equal to floor(n_fft / 4)."
msgstr ""

#: of tvm.relay.op.transform.stft:17
msgid "win_length"
msgstr ""

#: of tvm.relay.op.transform.stft:17
msgid ""
"The size of window frame and STFT filter. If is None, it is treated as "
"equal to n_fft."
msgstr ""

#: of tvm.relay.op.transform.stft:21
msgid "window"
msgstr ""

#: of tvm.relay.op.transform.stft:20
msgid ""
"A 1-D tensor window frame. If is None (default), it is treated as if "
"having 1 everywhere in the window."
msgstr ""

#: of tvm.relay.op.transform.stft:24
msgid "normalized"
msgstr ""

#: of tvm.relay.op.transform.stft:24
msgid "Whether to return the normalized STFT results. Default value is False."
msgstr ""

#: of tvm.relay.op.transform.stft:27
msgid "onesided"
msgstr ""

#: of tvm.relay.op.transform.stft:27
msgid ""
"Whether to return onesided result or fill with conjugate symmetry. "
"Default value is True."
msgstr ""

#: of tvm.relay.op.transform.stft:33
msgid "output"
msgstr ""

#: of tvm.relay.op.transform.stft:32
msgid ""
"Tensor containing the STFT result with shape [batch, N, T, 2], where N is"
" the number of frequencies where STFT is applied and T is the total "
"number of frames used."
msgstr ""

#: of tvm.relay.op.transform.strided_set:6
#: tvm.relay.op.transform.strided_slice:6
msgid "The source array to be sliced."
msgstr ""

#: of tvm.relay.op.transform.strided_set:9
msgid "v"
msgstr ""

#: of tvm.relay.op.transform.strided_set:9
msgid "The data to be set."
msgstr ""

#: of tvm.relay.op.transform.strided_set:12
#: tvm.relay.op.transform.strided_slice:9
msgid "begin"
msgstr ""

#: of tvm.relay.op.transform.strided_set:-1
#: tvm.relay.op.transform.strided_slice:-1
msgid "relay.Expr, Tuple[int], or List[int]"
msgstr ""

#: of tvm.relay.op.transform.strided_set:12
#: tvm.relay.op.transform.strided_slice:9
msgid "The indices to begin with in the slicing."
msgstr ""

#: of tvm.relay.op.transform.strided_set:15
#: tvm.relay.op.transform.strided_slice:12
msgid "end"
msgstr ""

#: of tvm.relay.op.transform.strided_set:15
#: tvm.relay.op.transform.strided_slice:12
msgid "Indices indicating end of the slice."
msgstr ""

#: of tvm.relay.op.transform.strided_set:19
msgid "strides: relay.Expr, Tuple[int], or List[int], optional"
msgstr ""

#: of tvm.relay.op.transform.strided_set:18
#: tvm.relay.op.transform.strided_slice:15
msgid ""
"Specifies the stride values. It can be negative. In that case, the input "
"tensor will be reversed in that particular axis."
msgstr ""

#: of tvm.relay.op.transform.strided_slice:-1
msgid "relay.Expr, Tuple[int], or List[int], optional"
msgstr ""

#: of tvm.relay.op.transform.strided_slice:19
msgid ""
"Axes along which slicing is applied. When it is specified, the length of "
"begin, end, strides, and axes must be equal. Moreover, begin, end, "
"strides, and axes must be static (cannot be relay.Expr). Axes argument "
"for dynamic parameter slicing is not supported yet."
msgstr ""

#: of tvm.relay.op.transform.strided_slice:29
msgid "slice_mode"
msgstr ""

#: of tvm.relay.op.transform.strided_slice:25
msgid ""
"The slice mode [end, size]. end: The ending indices for the slice "
"[default]. size: The input strides will be ignored. Input end in this "
"mode indicates the size of a slice starting at the location specified by "
"begin. If end[i] is -1, all remaining elements in that dimension are "
"included in the slice."
msgstr ""

#: of tvm.relay.op.transform.take:9
msgid "The indices of the values to extract."
msgstr ""

#: of tvm.relay.op.transform.take:12
msgid ""
"The axis over which to select values. By default, the flattened input "
"array is used."
msgstr ""

#: of tvm.relay.op.transform.take:16
msgid "The number of batch dimensions. By default is 0."
msgstr ""

#: of tvm.relay.op.transform.take:19
msgid ""
"Specifies how out-of-bound indices will behave [clip, wrap, fast]. clip: "
"clip to the range (default). wrap: wrap around the indices. fast: no clip"
" or wrap around (user must make sure indices are in-bound)."
msgstr ""

#: of tvm.relay.op.transform.tile:9
msgid "reps"
msgstr ""

#: of tvm.relay.op.transform.tile:9
msgid "The number of times repeating the tensor data."
msgstr ""

#: of tvm.relay.op.transform.tile:31
msgid ""
"Each dim size of reps must be a positive integer. If reps has length d, "
"the result will have dimension of max(d, data.ndim); If data.ndim < d, "
"data is promoted to be d-dimensional by prepending new axes. If data.ndim"
" >=  d, reps is promoted to a.ndim by pre-pending 1's to it."
msgstr ""

#: of tvm.relay.op.algorithm.topk:3
msgid ""
"ret_type specifies the return type, can be one of (\"both\", \"values\", "
"\"indices\")."
msgstr ""

#: of tvm.relay.op.algorithm.topk:-1
msgid "int or relay.Expr, optional"
msgstr ""

#: of tvm.relay.op.algorithm.topk:11
msgid "Number of top elements to select. Return all elements if k < 1."
msgstr ""

#: of tvm.relay.op.algorithm.topk:20
msgid "ret_type: str, optional"
msgstr ""

#: of tvm.relay.op.algorithm.topk:17
msgid ""
"The return type [both, values, indices]. \"both\": return both top k data"
" and indices. \"values\": return top k data only. \"indices\": return top"
" k indices only."
msgstr ""

#: of tvm.relay.op.algorithm.topk:26
msgid "The data type of the indices output."
msgstr ""

#: of tvm.relay.op.algorithm.topk:-1
msgid "relay.Expr or List[relay.Expr]"
msgstr ""

#: of tvm.relay.op.transform.transpose:-1
msgid "None or List[int]"
msgstr ""

#: of tvm.relay.op.transform.transpose:9
msgid "The target axes order, reverse order if not specified."
msgstr ""

#: of tvm.relay.op.transform.transpose:14
msgid "The transposed result."
msgstr ""

#: of tvm.relay.op.transform.trilu:7
msgid ""
"The tensor that trilu will be applied to. Must be either a 2D matrix or a"
" tensor of batches of 2D matrices."
msgstr ""

#: of tvm.relay.op.transform.trilu:11
msgid ""
"The number of diagonals above or below the main diagonal to exclude or "
"include."
msgstr ""

#: of tvm.relay.op.transform.trilu:16
msgid "upper: bool, optional"
msgstr ""

#: of tvm.relay.op.transform.trilu:15
msgid ""
"If True, only upper triangular values of input are kept, if False, the "
"lower triangular values are kept."
msgstr ""

#: of tvm.relay.op.transform.trilu:21
msgid "The new tensor with appropriate diagonals set to zero."
msgstr ""

#: of tvm.relay.op.transform.unique:1
msgid ""
"Find the unique elements of a 1-D tensor. Please note `output` and "
"`counts` are all padded to have the same length of `data` and element "
"with index >= num_unique[0] has undefined value."
msgstr ""

#: of tvm.relay.op.transform.unique:7
msgid "A 1-D tensor of integers."
msgstr ""

#: of tvm.relay.op.transform.unique:10
msgid "is_sorted"
msgstr ""

#: of tvm.relay.op.transform.unique:10
msgid ""
"Whether to sort the unique elements in ascending order before returning "
"as output."
msgstr ""

#: of tvm.relay.op.transform.unique:13
msgid "return_counts"
msgstr ""

#: of tvm.relay.op.transform.unique:13
msgid "Whether to return the count of each unique element."
msgstr ""

#: of tvm.relay.op.transform.unique:18
msgid "unique"
msgstr ""

#: of tvm.relay.op.transform.unique:18
msgid "A 1-D tensor containing the unique elements of the input data tensor."
msgstr ""

#: of tvm.relay.op.transform.unique:21
msgid ""
"A 1-D tensor containing the indeces of the first occurence of each unique"
" value in the input tensor."
msgstr ""

#: of tvm.relay.op.transform.unique:26
msgid "inverse_indices"
msgstr ""

#: of tvm.relay.op.transform.unique:25
msgid ""
"A 1-D tensor. For each entry in data, it contains the index of that data "
"element in the unique array."
msgstr ""

#: of tvm.relay.op.transform.unique:29
msgid "num_unique"
msgstr ""

#: of tvm.relay.op.transform.unique:29
msgid ""
"A 1-D tensor with size=1 containing the number of unique elements in the "
"input data tensor."
msgstr ""

#: of tvm.relay.op.transform.unique:32
msgid "counts"
msgstr ""

#: of tvm.relay.op.transform.unique:32
msgid "A 1-D tensor containing the count of each unique element in the output."
msgstr ""

#: of tvm.relay.op.transform.unravel_index:6
msgid "An integer array containing indices."
msgstr ""

#: of tvm.relay.op.transform.unravel_index:9
msgid "The shape of the array."
msgstr ""

#: of tvm.relay.op.transform.unravel_index:14
msgid "The tuple of coordinate arrays."
msgstr ""

#: of tvm.relay.expr.var:3
msgid ""
"This is a simple wrapper function that allows specify shape and dtype "
"directly."
msgstr ""

#: of tvm.relay.expr.var:11
msgid "name_hint: str"
msgstr ""

#: of tvm.relay.expr.var:9
msgid ""
"The name of the variable. This name only acts as a hint, and is not used "
"for equality."
msgstr ""

#: of tvm.relay.expr.var:15
msgid "type_annotation: Optional[tvm.relay.Type, str]"
msgstr ""

#: of tvm.relay.expr.var:14
msgid ""
"The type annotation on the variable. When type_annotation is a str, we "
"will create a scalar variable."
msgstr ""

#: of tvm.relay.expr.var:18
msgid "shape: Optional[List[tvm.Expr]]"
msgstr ""

#: of tvm.relay.expr.var:18
msgid "The shape of the tensor type."
msgstr ""

#: of tvm.relay.expr.var:21
msgid "The data type of the tensor."
msgstr ""

#: of tvm.relay.op.reduce.variance:9
msgid ""
"Axis or axes along which a variance operation is performed. The default, "
"axis=None, will compute the variance of all elements in the input array. "
"If axis is negative it counts from the last to the first axis."
msgstr ""

#: of tvm.relay.op.reduce.variance:26
msgid "with_mean"
msgstr ""

#: of tvm.relay.op.reduce.variance:-1
msgid "Optional[relay.Expr]"
msgstr ""

#: of tvm.relay.op.reduce.variance:26
msgid "To compute variance given an already computed mean"
msgstr ""

#: of tvm.relay.op.transform.where:5
msgid ""
"Shapes of condition, x, and y must be broadcastable to a common shape. "
"Semantics follow numpy where function "
"https://numpy.org/doc/stable/reference/generated/numpy.where.html"
msgstr ""

#: of tvm.relay.op.transform.where:12
msgid "Where True, yield x, otherwise yield y"
msgstr ""

#: of tvm.relay.op.transform.where:15
msgid "x"
msgstr ""

#: of tvm.relay.op.transform.where:15
msgid "The first array or scalar to be selected."
msgstr ""

#: of tvm.relay.op.transform.where:18
msgid "y"
msgstr ""

#: of tvm.relay.op.transform.where:18
msgid "The second array or scalar to be selected."
msgstr ""

#: of tvm.relay.op.transform.where:23
msgid ""
"The selected array. The output shape is the broadcasted shape from "
"condition, x, and y."
msgstr ""

#~ msgid ":py:obj:`Pattern <tvm.relay.Pattern>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sliding_window <tvm.relay.sliding_window>`\\ "
#~ "\\(data\\, axis\\, window\\_shape\\, strides\\)"
#~ msgstr ""

#~ msgid "Slide a window over the data tensor."
#~ msgstr ""

#~ msgid ""
#~ "What axis the window begins sliding "
#~ "over. Window will be slid over "
#~ "this axis and all following axes. "
#~ "The axis value determines the window "
#~ "shape (and thus, the number of "
#~ "strides): window shape and strides must"
#~ " both be of length `data.ndim-axis`."
#~ msgstr ""

#~ msgid ""
#~ "The window shape to form over the"
#~ " input. Window shape must be of "
#~ "length `data.ndim-axis`."
#~ msgstr ""

#~ msgid ""
#~ "How to stride the window along "
#~ "each dimension. Strides must be of "
#~ "length `data.ndim-axis`."
#~ msgstr ""

#~ msgid ":py:obj:`Pattern <tvm.relay.Pattern>`\\"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`TupleGetItem <tvm.relay.TupleGetItem>`\\ "
#~ "\\(tuple\\_value\\, index\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`mean_variance <tvm.relay.mean_variance>`\\ "
#~ "\\(data\\[\\, axis\\, keepdims\\, exclude\\]\\)"
#~ msgstr ""

#~ msgid "The Relay IR namespace containing the IR definition and compiler."
#~ msgstr ""

#~ msgid "**Classes:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Call <tvm.relay.Call>`\\ \\(op\\, "
#~ "args\\[\\, attrs\\, type\\_args\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Function call node in Relay."
#~ msgstr ""

#~ msgid ":py:obj:`Clause <tvm.relay.Clause>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Clause for pattern matching in Relay."
#~ msgstr ""

#~ msgid ":py:obj:`Constant <tvm.relay.Constant>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "A constant expression in Relay."
#~ msgstr ""

#~ msgid ":py:obj:`Expr <tvm.relay.Expr>`\\"
#~ msgstr ""

#~ msgid "alias of :py:class:`tvm.ir.expr.RelayExpr`"
#~ msgstr ""

#~ msgid ":py:obj:`ExprFunctor <tvm.relay.ExprFunctor>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "An abstract visitor defined over Expr."
#~ msgstr ""

#~ msgid ":py:obj:`ExprMutator <tvm.relay.ExprMutator>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "A functional visitor over Expr."
#~ msgstr ""

#~ msgid ":py:obj:`ExprVisitor <tvm.relay.ExprVisitor>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "A visitor over Expr."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Function <tvm.relay.Function>`\\ \\(params\\, "
#~ "body\\[\\, ret\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "A function declaration expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`If <tvm.relay.If>`\\ \\(cond\\, "
#~ "true\\_branch\\, false\\_branch\\)"
#~ msgstr ""

#~ msgid "A conditional expression in Relay."
#~ msgstr ""

#~ msgid ":py:obj:`Let <tvm.relay.Let>`\\ \\(variable\\, value\\, body\\)"
#~ msgstr ""

#~ msgid "Let variable binding expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Match <tvm.relay.Match>`\\ \\(data\\, "
#~ "clauses\\[\\, complete\\]\\)"
#~ msgstr ""

#~ msgid "Pattern matching expression in Relay."
#~ msgstr ""

#~ msgid "Base type for pattern matching constructs."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PatternConstructor <tvm.relay.PatternConstructor>`\\"
#~ " \\(constructor\\[\\, patterns\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Constructor pattern in Relay: Matches an"
#~ " ADT of the given constructor, binds"
#~ " recursively."
#~ msgstr ""

#~ msgid ":py:obj:`PatternTuple <tvm.relay.PatternTuple>`\\ \\(\\[patterns\\]\\)"
#~ msgstr ""

#~ msgid "Constructor pattern in Relay: Matches a tuple, binds recursively."
#~ msgstr ""

#~ msgid ":py:obj:`PatternVar <tvm.relay.PatternVar>`\\ \\(var\\)"
#~ msgstr ""

#~ msgid ""
#~ "Variable pattern in Relay: Matches "
#~ "anything and binds it to the "
#~ "variable."
#~ msgstr ""

#~ msgid ":py:obj:`PatternWildcard <tvm.relay.PatternWildcard>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Wildcard pattern in Relay: Matches any ADT and binds nothing."
#~ msgstr ""

#~ msgid ":py:obj:`Prelude <tvm.relay.Prelude>`\\ \\(\\[mod\\]\\)"
#~ msgstr ""

#~ msgid "Contains standard definitions."
#~ msgstr ""

#~ msgid ":py:obj:`RefCreate <tvm.relay.RefCreate>`\\ \\(value\\)"
#~ msgstr ""

#~ msgid "Create a new reference from initial value."
#~ msgstr ""

#~ msgid ":py:obj:`RefRead <tvm.relay.RefRead>`\\ \\(ref\\)"
#~ msgstr ""

#~ msgid "Get the value inside the reference."
#~ msgstr ""

#~ msgid ":py:obj:`RefType <tvm.relay.RefType>`\\"
#~ msgstr ""

#~ msgid "alias of :py:class:`tvm.ir.type.RelayRefType`"
#~ msgstr ""

#~ msgid ":py:obj:`RefWrite <tvm.relay.RefWrite>`\\ \\(ref\\, value\\)"
#~ msgstr ""

#~ msgid "Update the value inside the reference."
#~ msgstr ""

#~ msgid ":py:obj:`ScopeBuilder <tvm.relay.ScopeBuilder>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Scope builder class."
#~ msgstr ""

#~ msgid ":py:obj:`Tuple <tvm.relay.Tuple>`\\ \\(fields\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Tuple expression that groups several fields together."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`TupleGetItem <tvm.relay.TupleGetItem>`\\ "
#~ "\\(tuple\\_value\\, index\\)"
#~ msgstr ""

#~ msgid "Get index-th item from a tuple."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`TupleWrapper <tvm.relay.TupleWrapper>`\\ "
#~ "\\(tuple\\_value\\, size\\)"
#~ msgstr ""

#~ msgid "TupleWrapper."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`TypeData <tvm.relay.TypeData>`\\ \\(header\\, "
#~ "type\\_vars\\, constructors\\)"
#~ msgstr ""

#~ msgid "Stores the definition for an Algebraic Data Type (ADT) in Relay."
#~ msgstr ""

#~ msgid ":py:obj:`TypeFunctor <tvm.relay.TypeFunctor>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "An abstract visitor defined over Type."
#~ msgstr ""

#~ msgid ":py:obj:`TypeMutator <tvm.relay.TypeMutator>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "A functional visitor over Type."
#~ msgstr ""

#~ msgid ":py:obj:`TypeVisitor <tvm.relay.TypeVisitor>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "A visitor over Type."
#~ msgstr ""

#~ msgid "**Functions:**"
#~ msgstr ""

#~ msgid ":py:obj:`ShapeVar <tvm.relay.ShapeVar>`\\ \\(name\\)"
#~ msgstr ""

#~ msgid "A helper which constructs a type var of which the shape kind."
#~ msgstr ""

#~ msgid ":py:obj:`abs <tvm.relay.abs>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise absolute of data."
#~ msgstr ""

#~ msgid ":py:obj:`acos <tvm.relay.acos>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise acos of data."
#~ msgstr ""

#~ msgid ":py:obj:`acosh <tvm.relay.acosh>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise acosh of data."
#~ msgstr ""

#~ msgid ":py:obj:`add <tvm.relay.add>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Addition with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`adv_index <tvm.relay.adv_index>`\\ \\(inputs\\)"
#~ msgstr ""

#~ msgid "Numpy style advanced indexing."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`all <tvm.relay.all>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\]\\)"
#~ msgstr ""

#~ msgid "Computes the logical AND of boolean array elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`any <tvm.relay.any>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\]\\)"
#~ msgstr ""

#~ msgid "Computes the logical OR of boolean array elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`arange <tvm.relay.arange>`\\ \\(start\\[\\, "
#~ "stop\\, step\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Return evenly spaced values within a given interval."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`argmax <tvm.relay.argmax>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Returns the indices of the maximum values along an axis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`argmin <tvm.relay.argmin>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Returns the indices of the minimum values along an axis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`argsort <tvm.relay.argsort>`\\ \\(data\\[\\, "
#~ "axis\\, is\\_ascend\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Performs sorting along the given axis"
#~ " and returns an array of indicies "
#~ "having same shape as an input "
#~ "array that index data in sorted "
#~ "order."
#~ msgstr ""

#~ msgid ":py:obj:`argwhere <tvm.relay.argwhere>`\\ \\(condition\\)"
#~ msgstr ""

#~ msgid "Find the indices of elements of a tensor that are non-zero."
#~ msgstr ""

#~ msgid ":py:obj:`asin <tvm.relay.asin>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise asin of data."
#~ msgstr ""

#~ msgid ":py:obj:`asinh <tvm.relay.asinh>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise asinh of data."
#~ msgstr ""

#~ msgid ":py:obj:`atan <tvm.relay.atan>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise atan of data."
#~ msgstr ""

#~ msgid ":py:obj:`atanh <tvm.relay.atanh>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise atanh of data."
#~ msgstr ""

#~ msgid ":py:obj:`bind <tvm.relay.bind>`\\ \\(expr\\, binds\\)"
#~ msgstr ""

#~ msgid "Bind an free variables in expr or function arguments."
#~ msgstr ""

#~ msgid ":py:obj:`bitwise_and <tvm.relay.bitwise_and>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "bitwise AND with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`bitwise_not <tvm.relay.bitwise_not>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise bitwise not of data."
#~ msgstr ""

#~ msgid ":py:obj:`bitwise_or <tvm.relay.bitwise_or>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "bitwise OR with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`bitwise_xor <tvm.relay.bitwise_xor>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "bitwise XOR with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`broadcast_to <tvm.relay.broadcast_to>`\\ \\(data\\, shape\\)"
#~ msgstr ""

#~ msgid ""
#~ "Return a scalar value array with "
#~ "the same type, broadcast to the "
#~ "provided shape."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`broadcast_to_like <tvm.relay.broadcast_to_like>`\\ "
#~ "\\(data\\, broadcast\\_type\\)"
#~ msgstr ""

#~ msgid ""
#~ "Return a scalar value array with "
#~ "the same shape and type as the "
#~ "input array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`build <tvm.relay.build>`\\ \\(ir\\_mod\\[\\, "
#~ "target\\, target\\_host\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Helper function that builds a Relay "
#~ "function to run on TVM graph "
#~ "executor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`build_config <tvm.relay.build_config>`\\ "
#~ "\\(\\[opt\\_level\\, required\\_pass\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Configure the build behavior by setting config variables."
#~ msgstr ""

#~ msgid ":py:obj:`cast <tvm.relay.cast>`\\ \\(data\\, dtype\\)"
#~ msgstr ""

#~ msgid "Cast input tensor to data type."
#~ msgstr ""

#~ msgid ":py:obj:`cast_like <tvm.relay.cast_like>`\\ \\(data\\, dtype\\_like\\)"
#~ msgstr ""

#~ msgid "Cast input tensor to data type of another tensor."
#~ msgstr ""

#~ msgid ":py:obj:`ceil <tvm.relay.ceil>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise ceil of data."
#~ msgstr ""

#~ msgid ":py:obj:`clip <tvm.relay.clip>`\\ \\(a\\, a\\_min\\, a\\_max\\)"
#~ msgstr ""

#~ msgid "Clip the elements in `a` between `a_min` and `a_max`."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`collapse_sum_like <tvm.relay.collapse_sum_like>`\\ "
#~ "\\(data\\, collapse\\_type\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`collapse_sum_to <tvm.relay.collapse_sum_to>`\\ "
#~ "\\(data\\, shape\\)"
#~ msgstr ""

#~ msgid "Return a summation of data to the specified shape."
#~ msgstr ""

#~ msgid ":py:obj:`concatenate <tvm.relay.concatenate>`\\ \\(data\\, axis\\)"
#~ msgstr ""

#~ msgid "Concatenate the input tensors along the given axis."
#~ msgstr ""

#~ msgid ":py:obj:`const <tvm.relay.const>`\\ \\(value\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Create a constant value."
#~ msgstr ""

#~ msgid ":py:obj:`copy <tvm.relay.copy>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Copy a tensor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`copy_shape_func <tvm.relay.copy_shape_func>`\\ "
#~ "\\(attrs\\, inputs\\, \\_\\)"
#~ msgstr ""

#~ msgid "Shape function for copy op."
#~ msgstr ""

#~ msgid ":py:obj:`cos <tvm.relay.cos>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise cos of data."
#~ msgstr ""

#~ msgid ":py:obj:`cosh <tvm.relay.cosh>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise cosh of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`create_executor <tvm.relay.create_executor>`\\ "
#~ "\\(\\[kind\\, mod\\, device\\, target\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Factory function to create an executor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cumprod <tvm.relay.cumprod>`\\ \\(data\\[\\, "
#~ "axis\\, dtype\\, exclusive\\]\\)"
#~ msgstr ""

#~ msgid "Numpy style cumprod op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cumsum <tvm.relay.cumsum>`\\ \\(data\\[\\, "
#~ "axis\\, dtype\\, exclusive\\]\\)"
#~ msgstr ""

#~ msgid "Numpy style cumsum op."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`device_copy <tvm.relay.device_copy>`\\ \\(data\\,"
#~ " src\\_device\\, dst\\_device\\)"
#~ msgstr ""

#~ msgid "Copy data from the source device to the destination device."
#~ msgstr ""

#~ msgid ":py:obj:`divide <tvm.relay.divide>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Division with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`einsum <tvm.relay.einsum>`\\ \\(data\\, equation\\)"
#~ msgstr ""

#~ msgid "Evaluates the Einstein summation convention on data"
#~ msgstr ""

#~ msgid ":py:obj:`equal <tvm.relay.equal>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Broadcasted elementwise test for (lhs == rhs)."
#~ msgstr ""

#~ msgid ":py:obj:`erf <tvm.relay.erf>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise error function of data."
#~ msgstr ""

#~ msgid ":py:obj:`exp <tvm.relay.exp>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise exp of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`expand_dims <tvm.relay.expand_dims>`\\ \\(data\\,"
#~ " axis\\[\\, num\\_newaxis\\]\\)"
#~ msgstr ""

#~ msgid "Insert `num_newaxis` axes at the position given by `axis`."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`fixed_point_multiply "
#~ "<tvm.relay.fixed_point_multiply>`\\ \\(data\\, "
#~ "multiplier\\, shift\\)"
#~ msgstr ""

#~ msgid ""
#~ "Fixed point multiplication between data "
#~ "and a fixed point constant expressed "
#~ "as multiplier * 2^(-shift), where "
#~ "multiplier is a Q-number with 31 "
#~ "fractional bits"
#~ msgstr ""

#~ msgid ":py:obj:`floor <tvm.relay.floor>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise floor of data."
#~ msgstr ""

#~ msgid ":py:obj:`floor_divide <tvm.relay.floor_divide>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Floor division with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`floor_mod <tvm.relay.floor_mod>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Floor mod with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`full <tvm.relay.full>`\\ \\(fill\\_value\\[\\,"
#~ " shape\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Fill array with scalar value."
#~ msgstr ""

#~ msgid ":py:obj:`full_like <tvm.relay.full_like>`\\ \\(data\\, fill\\_value\\)"
#~ msgstr ""

#~ msgid ":py:obj:`gather <tvm.relay.gather>`\\ \\(data\\, axis\\, indices\\)"
#~ msgstr ""

#~ msgid "Gather values along given axis from given indices."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`gather_nd <tvm.relay.gather_nd>`\\ \\(data\\, "
#~ "indices\\[\\, batch\\_dims\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Gather elements or slices from data "
#~ "and store to a tensor whose shape"
#~ " is defined by indices."
#~ msgstr ""

#~ msgid ":py:obj:`greater <tvm.relay.greater>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Broadcasted elementwise test for (lhs > rhs)."
#~ msgstr ""

#~ msgid ":py:obj:`greater_equal <tvm.relay.greater_equal>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Broadcasted elementwise test for (lhs >= rhs)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`invert_permutation <tvm.relay.invert_permutation>`\\"
#~ " \\(data\\)"
#~ msgstr ""

#~ msgid "Computes the inverse permutation of data."
#~ msgstr ""

#~ msgid ":py:obj:`isfinite <tvm.relay.isfinite>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise finiteness of data."
#~ msgstr ""

#~ msgid ":py:obj:`isinf <tvm.relay.isinf>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise infiniteness of data."
#~ msgstr ""

#~ msgid ":py:obj:`isnan <tvm.relay.isnan>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Check nan in input data element-wise."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`layout_transform <tvm.relay.layout_transform>`\\ "
#~ "\\(data\\, src\\_layout\\, dst\\_layout\\)"
#~ msgstr ""

#~ msgid "Transform the layout of a tensor"
#~ msgstr ""

#~ msgid ":py:obj:`left_shift <tvm.relay.left_shift>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Left shift with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`less <tvm.relay.less>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Broadcasted elementwise test for (lhs < rhs)."
#~ msgstr ""

#~ msgid ":py:obj:`less_equal <tvm.relay.less_equal>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Broadcasted elementwise test for (lhs <= rhs)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`load_param_dict <tvm.relay.load_param_dict>`\\ "
#~ "\\(param\\_bytes\\)"
#~ msgstr ""

#~ msgid "Load parameter dictionary to binary bytes."
#~ msgstr ""

#~ msgid ":py:obj:`log <tvm.relay.log>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise log of data."
#~ msgstr ""

#~ msgid ":py:obj:`log10 <tvm.relay.log10>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise log to the base 10 of data."
#~ msgstr ""

#~ msgid ":py:obj:`log2 <tvm.relay.log2>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise log to the base 2 of data."
#~ msgstr ""

#~ msgid ":py:obj:`logical_and <tvm.relay.logical_and>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "logical AND with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`logical_not <tvm.relay.logical_not>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise logical not of data."
#~ msgstr ""

#~ msgid ":py:obj:`logical_or <tvm.relay.logical_or>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "logical OR with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`logical_xor <tvm.relay.logical_xor>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "logical XOR with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`logsumexp <tvm.relay.logsumexp>`\\ \\(data\\[\\,"
#~ " axis\\, keepdims\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Compute the log of the sum of "
#~ "exponentials of input elements over "
#~ "given axes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`matrix_set_diag <tvm.relay.matrix_set_diag>`\\ "
#~ "\\(data\\, diagonal\\[\\, k\\, align\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns a tensor with the diagonals "
#~ "of input tensor replaced with the "
#~ "provided diagonal values."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`max <tvm.relay.max>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\]\\)"
#~ msgstr ""

#~ msgid "Computes the max of array elements over given axes."
#~ msgstr ""

#~ msgid ":py:obj:`maximum <tvm.relay.maximum>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Maximum with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`mean <tvm.relay.mean>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\]\\)"
#~ msgstr ""

#~ msgid "Computes the mean of array elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`mean_std <tvm.relay.mean_std>`\\ \\(data\\[\\,"
#~ " axis\\, keepdims\\, exclude\\]\\)"
#~ msgstr ""

#~ msgid "Computes the mean and standard deviation of data over given axes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`mean_variance <tvm.relay.mean_variance>`\\ "
#~ "\\(data\\[\\, axis\\, keepdims\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Computes the mean and variance of data over given axes."
#~ msgstr ""

#~ msgid ":py:obj:`meshgrid <tvm.relay.meshgrid>`\\ \\(data\\[\\, indexing\\]\\)"
#~ msgstr ""

#~ msgid "Create coordinate matrices from coordinate vectors."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`min <tvm.relay.min>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\]\\)"
#~ msgstr ""

#~ msgid "Computes the min of array elements over given axes."
#~ msgstr ""

#~ msgid ":py:obj:`minimum <tvm.relay.minimum>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Minimum with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`mod <tvm.relay.mod>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Mod with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`multiply <tvm.relay.multiply>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Multiplication with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ndarray_size <tvm.relay.ndarray_size>`\\ "
#~ "\\(data\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Get number of elements of input tensor."
#~ msgstr ""

#~ msgid ":py:obj:`negative <tvm.relay.negative>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise negative of data."
#~ msgstr ""

#~ msgid ":py:obj:`not_equal <tvm.relay.not_equal>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Broadcasted elementwise test for (lhs != rhs)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`one_hot <tvm.relay.one_hot>`\\ \\(indices\\, "
#~ "on\\_value\\, off\\_value\\, depth\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns a one-hot tensor where the"
#~ " locations repsented by indices take "
#~ "value on_value, other locations take "
#~ "value off_value."
#~ msgstr ""

#~ msgid ":py:obj:`ones <tvm.relay.ones>`\\ \\(shape\\, dtype\\)"
#~ msgstr ""

#~ msgid "Fill array with ones."
#~ msgstr ""

#~ msgid ":py:obj:`ones_like <tvm.relay.ones_like>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Returns an array of ones, with same type and shape as the input."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`optimize <tvm.relay.optimize>`\\ \\(mod\\[\\, "
#~ "target\\, params\\]\\)"
#~ msgstr ""

#~ msgid "Helper function that optimizes a Relay module."
#~ msgstr ""

#~ msgid ":py:obj:`power <tvm.relay.power>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Power with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`prod <tvm.relay.prod>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\]\\)"
#~ msgstr ""

#~ msgid "Computes the products of array elements over given axes."
#~ msgstr ""

#~ msgid ":py:obj:`reinterpret <tvm.relay.reinterpret>`\\ \\(data\\, dtype\\)"
#~ msgstr ""

#~ msgid "Reinterpret input tensor to data type."
#~ msgstr ""

#~ msgid ":py:obj:`repeat <tvm.relay.repeat>`\\ \\(data\\, repeats\\, axis\\)"
#~ msgstr ""

#~ msgid "Repeats elements of an array."
#~ msgstr ""

#~ msgid ":py:obj:`reshape <tvm.relay.reshape>`\\ \\(data\\, newshape\\)"
#~ msgstr ""

#~ msgid "Reshape the input array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reshape_like <tvm.relay.reshape_like>`\\ "
#~ "\\(data\\, shape\\_like\\[\\, lhs\\_begin\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Reshapes the input tensor by the size of another tensor."
#~ msgstr ""

#~ msgid ":py:obj:`reverse <tvm.relay.reverse>`\\ \\(data\\, axis\\)"
#~ msgstr ""

#~ msgid ""
#~ "Reverses the order of elements along "
#~ "given axis while preserving array shape."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reverse_reshape <tvm.relay.reverse_reshape>`\\ "
#~ "\\(data\\, newshape\\)"
#~ msgstr ""

#~ msgid ""
#~ "Reshapes the input array where the "
#~ "special values are inferred from right"
#~ " to left."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reverse_sequence <tvm.relay.reverse_sequence>`\\ "
#~ "\\(data\\, seq\\_lengths\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Reverse the tensor for variable length slices."
#~ msgstr ""

#~ msgid ":py:obj:`right_shift <tvm.relay.right_shift>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Right shift with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`round <tvm.relay.round>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise round of data."
#~ msgstr ""

#~ msgid ":py:obj:`rsqrt <tvm.relay.rsqrt>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise rsqrt of data."
#~ msgstr ""

#~ msgid ":py:obj:`save_param_dict <tvm.relay.save_param_dict>`\\ \\(params\\)"
#~ msgstr ""

#~ msgid "Save parameter dictionary to binary bytes."
#~ msgstr ""

#~ msgid ":py:obj:`scalar_type <tvm.relay.scalar_type>`\\ \\(dtype\\)"
#~ msgstr ""

#~ msgid "Creates a scalar type."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scatter <tvm.relay.scatter>`\\ \\(data\\, "
#~ "indices\\, updates\\, axis\\)"
#~ msgstr ""

#~ msgid "Update data at positions defined by indices with values in updates"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scatter_add <tvm.relay.scatter_add>`\\ \\(data\\,"
#~ " indices\\, updates\\, axis\\)"
#~ msgstr ""

#~ msgid "Update data by adding values in updates at positions defined by indices"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scatter_nd <tvm.relay.scatter_nd>`\\ \\(data\\,"
#~ " indices\\, updates\\[\\, mode\\]\\)"
#~ msgstr ""

#~ msgid "Scatter values from an array and update."
#~ msgstr ""

#~ msgid ":py:obj:`script <tvm.relay.script>`\\ \\(pyfunc\\)"
#~ msgstr ""

#~ msgid "Decorate a python function function as hybrid script."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`searchsorted <tvm.relay.searchsorted>`\\ "
#~ "\\(sorted\\_sequence\\, values\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Find indices where elements should be inserted to maintain order."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`segment_sum <tvm.relay.segment_sum>`\\ \\(data\\,"
#~ " segment\\_ids\\[\\, num\\_segments\\]\\)"
#~ msgstr ""

#~ msgid "Computes the sum along segment_ids along axis 0."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sequence_mask <tvm.relay.sequence_mask>`\\ "
#~ "\\(data\\, valid\\_length\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Sets all elements outside the expected"
#~ " length of the sequence to a "
#~ "constant value."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`setrecursionlimit <tvm.relay.setrecursionlimit>`\\ "
#~ "\\(limit\\, \\/\\)"
#~ msgstr ""

#~ msgid "Set the maximum depth of the Python interpreter stack to n."
#~ msgstr ""

#~ msgid ":py:obj:`shape_of <tvm.relay.shape_of>`\\ \\(data\\[\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Get shape of a tensor."
#~ msgstr ""

#~ msgid ":py:obj:`sigmoid <tvm.relay.sigmoid>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise sigmoid of data."
#~ msgstr ""

#~ msgid ":py:obj:`sign <tvm.relay.sign>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid ":py:obj:`sin <tvm.relay.sin>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise sin of data."
#~ msgstr ""

#~ msgid ":py:obj:`sinh <tvm.relay.sinh>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise sinh of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`slice_like <tvm.relay.slice_like>`\\ \\(data\\,"
#~ " shape\\_like\\[\\, axes\\]\\)"
#~ msgstr ""

#~ msgid "Slice the first input with respect to the second input."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sort <tvm.relay.sort>`\\ \\(data\\[\\, "
#~ "axis\\, is\\_ascend\\]\\)"
#~ msgstr ""

#~ msgid "Performs sorting along the given axis and returns data in sorted order."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_fill_empty_rows "
#~ "<tvm.relay.sparse_fill_empty_rows>`\\ \\(sparse\\_indices\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Fill rows in a sparse matrix that do no contain any values."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_reshape <tvm.relay.sparse_reshape>`\\ "
#~ "\\(sparse\\_indices\\, prev\\_shape\\, ...\\)"
#~ msgstr ""

#~ msgid "Reshape a Sparse Tensor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sparse_to_dense <tvm.relay.sparse_to_dense>`\\ "
#~ "\\(sparse\\_indices\\, ...\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Converts a sparse representation into a dense tensor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`split <tvm.relay.split>`\\ \\(data\\, "
#~ "indices\\_or\\_sections\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Split input tensor along axis by sections or indices."
#~ msgstr ""

#~ msgid ":py:obj:`sqrt <tvm.relay.sqrt>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise sqrt of data."
#~ msgstr ""

#~ msgid ":py:obj:`squeeze <tvm.relay.squeeze>`\\ \\(data\\[\\, axis\\]\\)"
#~ msgstr ""

#~ msgid "Squeeze axes in the array."
#~ msgstr ""

#~ msgid ":py:obj:`stack <tvm.relay.stack>`\\ \\(data\\, axis\\)"
#~ msgstr ""

#~ msgid "Join a sequence of arrays along a new axis."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`std <tvm.relay.std>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\, unbiased\\]\\)"
#~ msgstr ""

#~ msgid "Computes the standard deviation of data over given axes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`strided_set <tvm.relay.strided_set>`\\ \\(data\\,"
#~ " v\\, begin\\, end\\[\\, strides\\]\\)"
#~ msgstr ""

#~ msgid "Strided set of an array."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`strided_slice <tvm.relay.strided_slice>`\\ "
#~ "\\(data\\, begin\\, end\\[\\, strides\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Strided slice of an array."
#~ msgstr ""

#~ msgid ":py:obj:`subtract <tvm.relay.subtract>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Subtraction with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sum <tvm.relay.sum>`\\ \\(data\\[\\, "
#~ "axis\\, keepdims\\, exclude\\]\\)"
#~ msgstr ""

#~ msgid "Computes the sum of array elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`take <tvm.relay.take>`\\ \\(data\\, "
#~ "indices\\[\\, axis\\, batch\\_dims\\, mode\\]\\)"
#~ msgstr ""

#~ msgid "Take elements from an array along an axis."
#~ msgstr ""

#~ msgid ":py:obj:`tan <tvm.relay.tan>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute elementwise tan of data."
#~ msgstr ""

#~ msgid ":py:obj:`tanh <tvm.relay.tanh>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise tanh of data."
#~ msgstr ""

#~ msgid ":py:obj:`tile <tvm.relay.tile>`\\ \\(data\\, reps\\)"
#~ msgstr ""

#~ msgid "Repeats the whole array multiple times."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`topk <tvm.relay.topk>`\\ \\(data\\[\\, k\\,"
#~ " axis\\, ret\\_type\\, is\\_ascend\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Get the top k elements in an input tensor along the given axis."
#~ msgstr ""

#~ msgid ":py:obj:`transpose <tvm.relay.transpose>`\\ \\(data\\[\\, axes\\]\\)"
#~ msgstr ""

#~ msgid "Permutes the dimensions of an array."
#~ msgstr ""

#~ msgid ":py:obj:`trunc <tvm.relay.trunc>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Compute element-wise trunc of data."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`unique <tvm.relay.unique>`\\ \\(data\\[\\, "
#~ "is\\_sorted\\, return\\_counts\\]\\)"
#~ msgstr ""

#~ msgid "Find the unique elements of a 1-D tensor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`unravel_index <tvm.relay.unravel_index>`\\ "
#~ "\\(indices\\, shape\\)"
#~ msgstr ""

#~ msgid ""
#~ "Convert a flat index or array of"
#~ " flat indices into a tuple of "
#~ "coordinate arrays."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`var <tvm.relay.var>`\\ \\(name\\_hint\\[\\, "
#~ "type\\_annotation\\, shape\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Create a new tvm.relay.Var."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`variance <tvm.relay.variance>`\\ \\(data\\[\\,"
#~ " axis\\, keepdims\\, exclude\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Computes the variance of data over given axes."
#~ msgstr ""

#~ msgid ":py:obj:`where <tvm.relay.where>`\\ \\(condition\\, x\\, y\\)"
#~ msgstr ""

#~ msgid ""
#~ "Selecting elements from either x or "
#~ "y depending on the value of the"
#~ " condition."
#~ msgstr ""

#~ msgid ":py:obj:`zeros <tvm.relay.zeros>`\\ \\(shape\\, dtype\\)"
#~ msgstr ""

#~ msgid "Fill array with zeros."
#~ msgstr ""

#~ msgid ":py:obj:`zeros_like <tvm.relay.zeros_like>`\\ \\(data\\)"
#~ msgstr ""

#~ msgid "Returns an array of zeros, with same type and shape as the input."
#~ msgstr ""

#~ msgid ""
#~ "Call node corresponds the operator "
#~ "application node in computational graph "
#~ "terminology."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "The operation to be called."
#~ msgstr ""

#~ msgid "The arguments to the call."
#~ msgstr ""

#~ msgid "Attributes to the call, can be None"
#~ msgstr ""

#~ msgid ""
#~ "The additional type arguments, this is"
#~ " only used in advanced usecase of "
#~ "template functions."
#~ msgstr ""

#~ msgid "Span that points to original source code"
#~ msgstr ""

#~ msgid "The data content of the constant expression."
#~ msgstr ""

#~ msgid ":py:obj:`checked_type <tvm.relay.Expr.checked_type>`\\"
#~ msgstr ""

#~ msgid "Get the checked type of tvm.relay.Expr."
#~ msgstr ""

#~ msgid ""
#~ "Defines the default dispatch over "
#~ "expressions, and implements memoization."
#~ msgstr ""

#~ msgid "**Methods:**"
#~ msgstr ""

#~ msgid ":py:obj:`visit <tvm.relay.ExprFunctor.visit>`\\ \\(expr\\)"
#~ msgstr ""

#~ msgid "Apply the visitor to an expression."
#~ msgstr ""

#~ msgid ""
#~ "The default behavior recursively traverses "
#~ "the AST and reconstructs the AST."
#~ msgstr ""

#~ msgid "The default behavior recursively traverses the AST."
#~ msgstr ""

#~ msgid "List of input parameters to the function."
#~ msgstr ""

#~ msgid "The body of the function."
#~ msgstr ""

#~ msgid "The return type annotation of the function."
#~ msgstr ""

#~ msgid ""
#~ "The additional type parameters, this is"
#~ " only used in advanced usecase of "
#~ "template functions."
#~ msgstr ""

#~ msgid "The condition."
#~ msgstr ""

#~ msgid "The expression evaluated when condition is true."
#~ msgstr ""

#~ msgid "The expression evaluated when condition is false."
#~ msgstr ""

#~ msgid "The local variable to be bound."
#~ msgstr ""

#~ msgid "The value to be bound."
#~ msgstr ""

#~ msgid "The body of the let binding."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_ctor <tvm.relay.Prelude.get_ctor>`\\ "
#~ "\\(ty\\_name\\, canonical\\, dtype\\)"
#~ msgstr ""

#~ msgid "Get constructor corresponding to the canonical name"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_ctor_static <tvm.relay.Prelude.get_ctor_static>`\\"
#~ " \\(ty\\_name\\, name\\, dtype\\, shape\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_global_var <tvm.relay.Prelude.get_global_var>`\\"
#~ " \\(canonical\\, dtype\\)"
#~ msgstr ""

#~ msgid "Get global var corresponding to the canonical name"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_global_var_static "
#~ "<tvm.relay.Prelude.get_global_var_static>`\\ \\(canonical\\, "
#~ "dtype\\, shape\\)"
#~ msgstr ""

#~ msgid "Get var corresponding to the canonical name"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_name <tvm.relay.Prelude.get_name>`\\ "
#~ "\\(canonical\\, dtype\\)"
#~ msgstr ""

#~ msgid "Get name corresponding to the canonical name"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_name_static <tvm.relay.Prelude.get_name_static>`\\"
#~ " \\(canonical\\, dtype\\, shape\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_tensor_ctor_static "
#~ "<tvm.relay.Prelude.get_tensor_ctor_static>`\\ \\(name\\, "
#~ "dtype\\, shape\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_type <tvm.relay.Prelude.get_type>`\\ "
#~ "\\(canonical\\, dtype\\)"
#~ msgstr ""

#~ msgid "Get type corresponding to the canonical name"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_type_static <tvm.relay.Prelude.get_type_static>`\\"
#~ " \\(canonical\\, dtype\\, shape\\)"
#~ msgstr ""

#~ msgid ":py:obj:`load_prelude <tvm.relay.Prelude.load_prelude>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Parses the Prelude from Relay's text format into a module."
#~ msgstr ""

#~ msgid ""
#~ "Create a new reference from initial "
#~ "value. :param value: The initial value."
#~ " :type value: tvm.relay.Expr"
#~ msgstr ""

#~ msgid ""
#~ "Get the value inside the reference. "
#~ ":param ref: The reference. :type ref:"
#~ " tvm.relay.Expr"
#~ msgstr ""

#~ msgid ""
#~ "Update the value inside the reference."
#~ " The whole expression will evaluate "
#~ "to an empty tuple. :param ref: The"
#~ " reference. :type ref: tvm.relay.Expr "
#~ ":param value: The new value. :type "
#~ "value: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "Enables users to build up a nested scope(let, if) expression easily."
#~ msgstr ""

#~ msgid "实际案例"
#~ msgstr ""

#~ msgid ":py:obj:`else_scope <tvm.relay.ScopeBuilder.else_scope>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Create a new else scope."
#~ msgstr ""

#~ msgid ":py:obj:`get <tvm.relay.ScopeBuilder.get>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Get the generated result."
#~ msgstr ""

#~ msgid ":py:obj:`if_scope <tvm.relay.ScopeBuilder.if_scope>`\\ \\(cond\\)"
#~ msgstr ""

#~ msgid "Create a new if scope."
#~ msgstr ""

#~ msgid ":py:obj:`let <tvm.relay.ScopeBuilder.let>`\\ \\(var\\, value\\)"
#~ msgstr ""

#~ msgid "Create a new let binding."
#~ msgstr ""

#~ msgid ":py:obj:`ret <tvm.relay.ScopeBuilder.ret>`\\ \\(value\\)"
#~ msgstr ""

#~ msgid "Set the return value of this scope."
#~ msgstr ""

#~ msgid ":py:obj:`type_of <tvm.relay.ScopeBuilder.type_of>`\\ \\(expr\\)"
#~ msgstr ""

#~ msgid "Compute the type of an expression."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "**scope** -- The if scope."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "**value** -- The final result of the expression."
#~ msgstr ""

#~ msgid "The condition"
#~ msgstr ""

#~ msgid "The user must follows with an else scope."
#~ msgstr ""

#~ msgid "The variable or name of variable."
#~ msgstr ""

#~ msgid "The value to be bound"
#~ msgstr ""

#~ msgid "The return value."
#~ msgstr ""

#~ msgid "The expression to compute the type of."
#~ msgstr ""

#~ msgid "**type_var** -- The shape variable."
#~ msgstr ""

#~ msgid "The fields in the tuple."
#~ msgstr ""

#~ msgid ":py:obj:`astype <tvm.relay.Tuple.astype>`\\ \\(\\_\\)"
#~ msgstr ""

#~ msgid "Cast the content type of the current data to dtype."
#~ msgstr ""

#~ msgid "The target data type."
#~ msgstr ""

#~ msgid "This function only works for TensorType Exprs."
#~ msgstr ""

#~ msgid "**result** -- The result expression."
#~ msgstr ""

#~ msgid "The input tuple expression."
#~ msgstr ""

#~ msgid "The index."
#~ msgstr ""

#~ msgid ""
#~ "This class is a Python wrapper for"
#~ " a Relay tuple of known size. "
#~ "It allows for accessing the fields "
#~ "of the Relay tuple as though it"
#~ " were a Python tuple."
#~ msgstr ""

#~ msgid "The input tuple"
#~ msgstr ""

#~ msgid "The size of the tuple."
#~ msgstr ""

#~ msgid ":py:obj:`astext <tvm.relay.TupleWrapper.astext>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Get the text format of the tuple expression."
#~ msgstr ""

#~ msgid ":py:obj:`astuple <tvm.relay.TupleWrapper.astuple>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Returns the underlying Relay tuple if"
#~ " this wrapper is passed as an "
#~ "argument to an FFI function."
#~ msgstr ""

#~ msgid "**text** -- The text format of the tuple expression."
#~ msgstr ""

#~ msgid ""
#~ "Note that ADT definitions are treated"
#~ " as type-level functions because the"
#~ " type parameters need to be given "
#~ "for an instance of the ADT. Thus,"
#~ " any global type var that is an"
#~ " ADT header needs to be wrapped "
#~ "in a type call that passes in "
#~ "the type params."
#~ msgstr ""

#~ msgid ""
#~ "The name of the ADT. ADTs with "
#~ "the same constructors but different "
#~ "names are treated as different types."
#~ msgstr ""

#~ msgid "Type variables that appear in constructors."
#~ msgstr ""

#~ msgid "The constructors for the ADT."
#~ msgstr ""

#~ msgid "Defines the default dispatch over types."
#~ msgstr ""

#~ msgid ":py:obj:`visit <tvm.relay.TypeFunctor.visit>`\\ \\(typ\\)"
#~ msgstr ""

#~ msgid "Apply the visitor to a type."
#~ msgstr ""

#~ msgid "The input data"
#~ msgstr ""

#~ msgid "**result** -- The computed result."
#~ msgstr ""

#~ msgid "The left hand side input data"
#~ msgstr ""

#~ msgid "The right hand side input data"
#~ msgstr ""

#~ msgid "Numpy style advanced indexing. Index with a list of tensors."
#~ msgstr ""

#~ msgid ""
#~ "Input tensor and indices. The first "
#~ "tensor is input data and rests are"
#~ " indices."
#~ msgstr ""

#~ msgid "**result** -- Output tensor."
#~ msgstr ""

#~ msgid "The input boolean tensor"
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a sum "
#~ "is performed. The default, axis=None, "
#~ "will sum all of the elements of"
#~ " the input array. If axis is "
#~ "negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid ""
#~ "If this is set to True, the "
#~ "axes which are reduced are left in"
#~ " the result as dimensions with size"
#~ " one. With this option, the result"
#~ " will broadcast correctly against the "
#~ "input array."
#~ msgstr ""

#~ msgid ""
#~ "If `exclude` is true, reduction will "
#~ "be performed on the axes that are"
#~ " NOT in axis instead."
#~ msgstr ""

#~ msgid ""
#~ "Similar to ``numpy.arange``, when only "
#~ "one argument is given, it is used"
#~ " as `stop` instead of `start` while"
#~ " `start` takes default value 0."
#~ msgstr ""

#~ msgid ""
#~ "Warning: Undefined behavior when dtype "
#~ "is incompatible with start/stop/step. It "
#~ "could lead to different results compared"
#~ " to numpy, MXNet, pytorch, etc."
#~ msgstr ""

#~ msgid ""
#~ "Start of interval. The interval includes"
#~ " this value. The default start value"
#~ " is 0."
#~ msgstr ""

#~ msgid "Stop of interval. The interval does not include this value."
#~ msgstr ""

#~ msgid "Spacing between values. The default step size is 1."
#~ msgstr ""

#~ msgid "**result** -- The resulting tensor."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a argmax"
#~ " operation is performed. The default, "
#~ "axis=None, will find the indices of "
#~ "the maximum element of the elements "
#~ "of the input array. If axis is "
#~ "negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid ""
#~ "Whether to select the last index "
#~ "or the first index if the max "
#~ "element appears in multiple indices, "
#~ "default is False (first index)."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a argmin"
#~ " operation is performed. The default, "
#~ "axis=None, will find the indices of "
#~ "minimum element all of the elements "
#~ "of the input array. If axis is "
#~ "negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid ""
#~ "Whether to select the last index "
#~ "or the first index if the min "
#~ "element appears in multiple indices, "
#~ "default is False (first index)."
#~ msgstr ""

#~ msgid "The input data tensor."
#~ msgstr ""

#~ msgid "The number of valid elements to be sorted."
#~ msgstr ""

#~ msgid "Axis long which to sort the input tensor."
#~ msgstr ""

#~ msgid "Whether to sort in ascending or descending order."
#~ msgstr ""

#~ msgid "The data type of the output indices."
#~ msgstr ""

#~ msgid "**out** -- Tensor with same shape as data."
#~ msgstr ""

#~ msgid "The input condition tensor."
#~ msgstr ""

#~ msgid "**out** -- Tensor with the indices of elements that are non-zero."
#~ msgstr ""

#~ msgid "We can bind parameters expr if it is a function."
#~ msgstr ""

#~ msgid "The input expression."
#~ msgstr ""

#~ msgid "The specific bindings."
#~ msgstr ""

#~ msgid "**result** -- The expression or function after binding."
#~ msgstr ""

#~ msgid "The input tensor."
#~ msgstr ""

#~ msgid "Provide the shape to broadcast to."
#~ msgstr ""

#~ msgid "Provide the type to broadcast to."
#~ msgstr ""

#~ msgid "The IR module to build. Using relay.Function is deprecated."
#~ msgstr ""

#~ msgid ""
#~ "For heterogeneous compilation, it is a"
#~ " dictionary indicating context to target"
#~ " mapping. For homogeneous compilation, it"
#~ " is a build target."
#~ msgstr ""

#~ msgid ""
#~ "Host compilation target, if target is"
#~ " device. When TVM compiles device "
#~ "specific program such as CUDA, we "
#~ "also need host(CPU) side code to "
#~ "interact with the driver setup the "
#~ "dimensions and parameters correctly. "
#~ "target_host is used to specify the "
#~ "host side codegen target. By default,"
#~ " llvm is used if it is enabled,"
#~ " otherwise a stackvm interpreter is "
#~ "used."
#~ msgstr ""

#~ msgid ""
#~ "The executor configuration with which to"
#~ " build the model. Defaults to "
#~ "\"graph\" if no executor specified."
#~ msgstr ""

#~ msgid ""
#~ "Runtime configuration to use when "
#~ "building the model. Defaults to \"cpp\""
#~ " if no runtime specified."
#~ msgstr ""

#~ msgid ""
#~ "Input parameters to the graph that "
#~ "do not change during inference time. "
#~ "Used for constant folding."
#~ msgstr ""

#~ msgid "The module name we will build"
#~ msgstr ""

#~ msgid "**factory_module** -- The runtime factory for the TVM graph executor."
#~ msgstr ""

#~ msgid ""
#~ "Configure the build behavior by setting"
#~ " config variables. This function will "
#~ "be deprecated in TVM v0.7. Instead, "
#~ "we should directly use "
#~ "tvm.transform.PassContext."
#~ msgstr ""

#~ msgid ""
#~ "Optimization level. The optimization pass "
#~ "name and level are as the "
#~ "following:  .. code-block:: python      "
#~ "OPT_PASS_LEVEL = {         \"SimplifyInference\":"
#~ " 0,         \"OpFusion\": 1,         "
#~ "\"FoldConstant\": 2,         \"FoldScaleAxis\": 3,"
#~ "         \"AlterOpLayout\": 3,         "
#~ "\"CanonicalizeOps\": 3,         \"CanonicalizeCast\": "
#~ "3,         \"EliminateCommonSubexpr\": 3,         "
#~ "\"CombineParallelConv2D\": 4,         "
#~ "\"CombineParallelDense\": 4,         "
#~ "\"CombineParallelBatchMatmul\": 4,         \"FastMath\":"
#~ " 4     }"
#~ msgstr ""

#~ msgid ""
#~ "Optimization level. The optimization pass "
#~ "name and level are as the "
#~ "following:"
#~ msgstr ""

#~ msgid "Optimization passes that are required regardless of optimization level."
#~ msgstr ""

#~ msgid "Optimization passes to be disabled during optimization."
#~ msgstr ""

#~ msgid "A tracing function for debugging or introspection."
#~ msgstr ""

#~ msgid "**pass_context** -- The pass context for optimizations."
#~ msgstr ""

#~ msgid "The input data to the operator."
#~ msgstr ""

#~ msgid "The target data type"
#~ msgstr ""

#~ msgid "**result** -- The casted result."
#~ msgstr ""

#~ msgid "The tensor to cast to."
#~ msgstr ""

#~ msgid ""
#~ "Clip the elements in `a` between "
#~ "`a_min` and `a_max`. `a_min` and `a_max`"
#~ " are cast to `a`'s dtype."
#~ msgstr ""

#~ msgid "The clip minimum."
#~ msgstr ""

#~ msgid "The clip maximum."
#~ msgstr ""

#~ msgid "**result** -- `a` with elements clipped between `a_min` and `a_max`."
#~ msgstr ""

#~ msgid "Provide the type to collapse to."
#~ msgstr ""

#~ msgid "Shape to collapse to."
#~ msgstr ""

#~ msgid "A list of tensors."
#~ msgstr ""

#~ msgid "The axis along which the tensors are concatenated."
#~ msgstr ""

#~ msgid "**result** -- The concatenated tensor."
#~ msgstr ""

#~ msgid "The constant value."
#~ msgstr ""

#~ msgid "The data type of the resulting constant."
#~ msgstr ""

#~ msgid "When dtype is None, we use the following rule:"
#~ msgstr ""

#~ msgid "int maps to \"int32\""
#~ msgstr ""

#~ msgid "float maps to \"float32\""
#~ msgstr ""

#~ msgid "bool maps to \"bool\""
#~ msgstr ""

#~ msgid "other using the same default rule as numpy."
#~ msgstr ""

#~ msgid "The tensor to be copied."
#~ msgstr ""

#~ msgid "**result** -- The copied result."
#~ msgstr ""

#~ msgid "示例"
#~ msgstr ""

#~ msgid ""
#~ "The type of executor. Avaliable options"
#~ " are `debug` for the interpreter, "
#~ "`graph` for the graph executor, and "
#~ "`vm` for the virtual machine."
#~ msgstr ""

#~ msgid "The Relay module containing collection of functions"
#~ msgstr ""

#~ msgid "The device to execute the code."
#~ msgstr ""

#~ msgid "The corresponding context"
#~ msgstr ""

#~ msgid "Input parameters to the graph that do not change during inference time."
#~ msgstr ""

#~ msgid "**executor**"
#~ msgstr ""

#~ msgid ":py:class:`~tvm.relay.backend.interpreter.Executor`"
#~ msgstr ""

#~ msgid ""
#~ "Numpy style cumprod op. Return the "
#~ "cumulative inclusive product of the "
#~ "elements along a given axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which the cumulative product"
#~ " is computed. The default (None) is"
#~ " to compute the cumprod over the "
#~ "flattened array."
#~ msgstr ""

#~ msgid ""
#~ "Type of the returned array and of"
#~ " the accumulator in which the "
#~ "elements are multiplied. If dtype is "
#~ "not specified, it defaults to the "
#~ "dtype of data."
#~ msgstr ""

#~ msgid ""
#~ "If true will return exclusive product"
#~ " in which the first element is "
#~ "not included. In other terms, if "
#~ "true, the j-th output element would "
#~ "be the product of the first (j-1)"
#~ " elements. Otherwise, it would be the"
#~ " product of the first j elements. "
#~ "The product of zero elements will "
#~ "be 1."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The result has the "
#~ "same size as data, and the same"
#~ " shape as data if axis is not"
#~ " None. If axis is None, the "
#~ "result is a 1-d array."
#~ msgstr ""

#~ msgid ""
#~ "Numpy style cumsum op. Return the "
#~ "cumulative inclusive sum of the elements"
#~ " along a given axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which the cumulative sum "
#~ "is computed. The default (None) is "
#~ "to compute the cumsum over the "
#~ "flattened array."
#~ msgstr ""

#~ msgid ""
#~ "Type of the returned array and of"
#~ " the accumulator in which the "
#~ "elements are summed. If dtype is "
#~ "not specified, it defaults to the "
#~ "dtype of data."
#~ msgstr ""

#~ msgid ""
#~ "If true will return exclusive sum "
#~ "in which the first element is not"
#~ " included. In other terms, if true,"
#~ " the j-th output element would be "
#~ "the sum of the first (j-1) "
#~ "elements. Otherwise, it would be the "
#~ "sum of the first j elements."
#~ msgstr ""

#~ msgid ""
#~ "Copy data from the source device "
#~ "to the destination device. This operator"
#~ " helps data transferring between difference"
#~ " devices for heterogeneous execution."
#~ msgstr ""

#~ msgid "The source device where the data is copied from."
#~ msgstr ""

#~ msgid "The destination device where the data is copied to."
#~ msgstr ""

#~ msgid "The einsum expression string."
#~ msgstr ""

#~ msgid "**result** -- The output tensor from the einsum op."
#~ msgstr ""

#~ msgid ""
#~ "The axis at which the input array"
#~ " is expanded. Should lie in range "
#~ "`[-data.ndim - 1, data.ndim]`. If `axis"
#~ " < 0`, it is the first axis "
#~ "inserted; If `axis >= 0`, it is"
#~ " the last axis inserted in Python's"
#~ " negative indexing."
#~ msgstr ""

#~ msgid "Number of axes to be inserted. Should be >= 0."
#~ msgstr ""

#~ msgid "**result** -- The reshaped result."
#~ msgstr ""

#~ msgid "The integer multiplier of the fixed point constant."
#~ msgstr ""

#~ msgid "The integer shift of the fixed point constant."
#~ msgstr ""

#~ msgid "**result** -- The output of the fixed point multiplication"
#~ msgstr ""

#~ msgid "The value to fill. Must be a scalar."
#~ msgstr ""

#~ msgid "The shape of the target."
#~ msgstr ""

#~ msgid "The data type of the target."
#~ msgstr ""

#~ msgid "The scalar value to fill."
#~ msgstr ""

#~ msgid "E.g. for a 3D tensor, output is computed as:"
#~ msgstr ""

#~ msgid ""
#~ "``indices`` must have same shape as "
#~ "``data``, except at dimension ``axis`` "
#~ "which must just be not null. "
#~ "Output will have same shape as "
#~ "``indices``."
#~ msgstr ""

#~ msgid "The axis along which to index. negative axis is supported."
#~ msgstr ""

#~ msgid "The indices of values to gather."
#~ msgstr ""

#~ msgid "The shape of output tensor."
#~ msgstr ""

#~ msgid "The number of batch dimensions."
#~ msgstr ""

#~ msgid ""
#~ "The size of an indexing tuple, "
#~ "which is a fixed value and the "
#~ "same as indices.shape[0] Only needed "
#~ "when other dimensions of indices are "
#~ "dynamic."
#~ msgstr ""

#~ msgid "**ret** -- The computed result."
#~ msgstr ""

#~ msgid ""
#~ "Computes the inverse permutation of "
#~ "data. This operation computes the "
#~ "inverse of an index permutation. It "
#~ "takes a 1-D integer tensor x, "
#~ "which represents the indices of a "
#~ "zero-based array and swaps each value"
#~ " with its index position."
#~ msgstr ""

#~ msgid ""
#~ "For an output tensor y and an "
#~ "input tensor x, this operation computes"
#~ " the following: y[x[i]] = i for "
#~ "i in [0, 1, ..., len(x) - 1]"
#~ msgstr ""

#~ msgid "The source data to be invert permuated."
#~ msgstr ""

#~ msgid "**ret** -- Invert permuated data. Has the same type as data."
#~ msgstr ""

#~ msgid "The source tensor to be transformed"
#~ msgstr ""

#~ msgid "The source layout.  (e.g NCHW)"
#~ msgstr ""

#~ msgid "The destination layout.  (e.g. NCHW16c)"
#~ msgstr ""

#~ msgid "**ret** -- The transformed tensor."
#~ msgstr ""

#~ msgid "Use :py:func:`tvm.runtime.load_param_dict` instead."
#~ msgstr ""

#~ msgid "Serialized parameters."
#~ msgstr ""

#~ msgid "**params** -- The parameter dictionary."
#~ msgstr ""

#~ msgid ""
#~ "This function is more numerically stable"
#~ " than log(sum(exp(input))). It avoids "
#~ "overflows caused by taking the exp "
#~ "of large inputs and underflows caused"
#~ " by taking the log of small "
#~ "inputs."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a "
#~ "standard deviation operation is performed. "
#~ "The default, axis=None, will compute the"
#~ " log of the sum of exponentials "
#~ "of all elements in the input "
#~ "array. If axis is negative it "
#~ "counts from the last to the first"
#~ " axis."
#~ msgstr ""

#~ msgid ""
#~ "If this is set to True, the "
#~ "axes which are reduced are left in"
#~ " the result as dimensions with size"
#~ " one."
#~ msgstr ""

#~ msgid "Input Tensor."
#~ msgstr ""

#~ msgid "Values to be filled in the diagonal."
#~ msgstr ""

#~ msgid ""
#~ "Diagonal Offset(s). The diagonal or "
#~ "range of diagonals to set. (0 by"
#~ " default) Positive value means "
#~ "superdiagonal, 0 refers to the main "
#~ "diagonal, and negative value means "
#~ "subdiagonals. k can be a single "
#~ "integer (for a single diagonal) or "
#~ "a pair of integers specifying the "
#~ "low and high ends of a matrix "
#~ "band. k[0] must not be larger than"
#~ " k[1]."
#~ msgstr ""

#~ msgid ""
#~ "Some diagonals are shorter than "
#~ "max_diag_len and need to be padded. "
#~ "align is a string specifying how "
#~ "superdiagonals and subdiagonals should be "
#~ "aligned, respectively. There are four "
#~ "possible alignments: \"RIGHT_LEFT\" (default), "
#~ "\"LEFT_RIGHT\", \"LEFT_LEFT\", and \"RIGHT_RIGHT\"."
#~ " \"RIGHT_LEFT\" aligns superdiagonals to "
#~ "the right (left-pads the row) and"
#~ " subdiagonals to the left (right-pads"
#~ " the row). It is the packing "
#~ "format LAPACK uses. cuSPARSE uses "
#~ "\"LEFT_RIGHT\", which is the opposite "
#~ "alignment."
#~ msgstr ""

#~ msgid "**result** -- New tensor with given diagonal values."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which the max"
#~ " operation is performed. The default, "
#~ "axis=None, will find the max element "
#~ "from all of the elements of the"
#~ " input array. If axis is negative "
#~ "it counts from the last to the "
#~ "first axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a mean"
#~ " operation is performed. The default, "
#~ "axis=None, will compute the mean of "
#~ "all elements in the input array. "
#~ "If axis is negative it counts from"
#~ " the last to the first axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a mean"
#~ " and standard deviation operation is "
#~ "performed. The default, axis=None, will "
#~ "compute the mean and standard deviation"
#~ " of all elements in the input "
#~ "array. If axis is negative it "
#~ "counts from the last to the first"
#~ " axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a mean"
#~ " and variance operation is performed. "
#~ "The default, axis=None, will compute the"
#~ " mean and variance of all elements"
#~ " in the input array. If axis is"
#~ " negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid "If this is set to True, the unbiased estimation will be used."
#~ msgstr ""

#~ msgid "Similar to ``numpy.meshgrid``."
#~ msgstr ""

#~ msgid "A list of tensors, which must be either scalars or 1-D vectors."
#~ msgstr ""

#~ msgid ""
#~ "Indexing mode, either \"ij\" for matrix"
#~ " indexing or \"xy\" for Cartesian "
#~ "indexing."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a minimum"
#~ " operation is performed. The default, "
#~ "axis=None, will find the minimum element"
#~ " from all of the elements of "
#~ "the input array. If axis is "
#~ "negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid "**result** -- The number of elements of input tensor."
#~ msgstr ""

#~ msgid ""
#~ "Returns a one-hot tensor where the"
#~ " locations repsented by indices take "
#~ "value on_value, other locations take "
#~ "value off_value. Final dimension is "
#~ "<indices outer dimensions> x depth x "
#~ "<indices inner dimensions>."
#~ msgstr ""

#~ msgid "Locations to set to on_value."
#~ msgstr ""

#~ msgid "Value to fill at indices."
#~ msgstr ""

#~ msgid "Value to fill at all other positions besides indices."
#~ msgstr ""

#~ msgid "Depth of the one-hot dimension."
#~ msgstr ""

#~ msgid "Axis to fill."
#~ msgstr ""

#~ msgid "Data type of the output tensor."
#~ msgstr ""

#~ msgid "**ret** -- The one-hot tensor."
#~ msgstr ""

#~ msgid "The module to build. Using relay.Function is deprecated."
#~ msgstr ""

#~ msgid ""
#~ "* **mod** (:py:class:`~tvm.IRModule`) -- The"
#~ " optimized relay module. * **params** "
#~ "(*dict*) -- The parameters of the "
#~ "final graph."
#~ msgstr ""

#~ msgid "**mod** (:py:class:`~tvm.IRModule`) -- The optimized relay module."
#~ msgstr ""

#~ msgid "**params** (*dict*) -- The parameters of the final graph."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a product"
#~ " is performed. The default, axis=None, "
#~ "will find the indices of minimum "
#~ "element all of the elements of the"
#~ " input array. If axis is negative "
#~ "it counts from the last to the "
#~ "first axis."
#~ msgstr ""

#~ msgid "**result** -- The reinterpreted result."
#~ msgstr ""

#~ msgid ""
#~ "Repeats elements of an array. By "
#~ "default, repeat flattens the input array"
#~ " into 1-D and then repeats the "
#~ "elements."
#~ msgstr ""

#~ msgid "repeats"
#~ msgstr ""

#~ msgid "int"
#~ msgstr ""

#~ msgid "The number of repetitions for each element."
#~ msgstr ""

#~ msgid "axis: int"
#~ msgstr ""

#~ msgid ""
#~ "The axis along which to repeat "
#~ "values. The negative numbers are "
#~ "interpreted counting from the backward. "
#~ "By default, use the flattened input "
#~ "array, and return a flat output "
#~ "array."
#~ msgstr ""

#~ msgid ""
#~ "To give user more convenience in "
#~ "without doing manual shape inference, "
#~ "some dimensions of the shape can "
#~ "take special values from the set "
#~ "{0, -1, -2, -3, -4}. The "
#~ "significance of each is explained below:"
#~ msgstr ""

#~ msgid "``0`` copy this dimension from the input to the output shape."
#~ msgstr ""

#~ msgid ""
#~ "``-1`` infers the dimension of the "
#~ "output shape by using the remainder "
#~ "of the input dimensions keeping the "
#~ "size of the new array same as "
#~ "that of the input array. At most"
#~ " one dimension of shape can be "
#~ "-1."
#~ msgstr ""

#~ msgid "``-2`` copy all/remainder of the input dimensions to the output shape."
#~ msgstr ""

#~ msgid ""
#~ "``-3`` use the product of two "
#~ "consecutive dimensions of the input "
#~ "shape as the output dimension."
#~ msgstr ""

#~ msgid ""
#~ "``-4`` split one dimension of the "
#~ "input into two dimensions passed "
#~ "subsequent to -4 in shape (can "
#~ "contain -1)."
#~ msgstr ""

#~ msgid "The new shape. Should be compatible with the original shape."
#~ msgstr ""

#~ msgid ""
#~ "Reshapes the input tensor by the "
#~ "size of another tensor. For an "
#~ "input tensor with shape ``(d0, d1, "
#~ "..., d(k-1))``, `reshape_like` operation "
#~ "reshapes the input tensor into an "
#~ "output tensor with the same shape "
#~ "as the second input tensor, in "
#~ "particular reshaping the dimensions of "
#~ "`data` in `[lhs_begin, lhs_end)` using "
#~ "the dimensions from `shape_like` in "
#~ "`[rhs_begin, rhs_end)`."
#~ msgstr ""

#~ msgid "Sizes for `data` and the output tensor should be compatible."
#~ msgstr ""

#~ msgid ""
#~ "The tensor to reshape data like. "
#~ "Should be compatible with the original"
#~ " shape on the reshaped dimensions."
#~ msgstr ""

#~ msgid "The axis of data to begin reshaping. Default is 0."
#~ msgstr ""

#~ msgid ""
#~ "The axis of data where reshaping "
#~ "should stop, exclusive. Default is None"
#~ " which reshapes to the end."
#~ msgstr ""

#~ msgid "The axis of shape_like where the target shape begins. Default is 0."
#~ msgstr ""

#~ msgid ""
#~ "The axis of shape_like where the "
#~ "target shape ends, exclusive. Default is"
#~ " None which extends to the end."
#~ msgstr ""

#~ msgid ""
#~ "Reverses the order of elements along "
#~ "given axis while preserving array shape."
#~ " By default, repeat flattens the "
#~ "input array into 1-D and then "
#~ "repeats the elements."
#~ msgstr ""

#~ msgid "The axis along which to reverse elements."
#~ msgstr ""

#~ msgid ""
#~ "The special values have the same "
#~ "semantics as :py:class:`tvm.relay.reshape`. The "
#~ "difference is that special values are"
#~ " inferred from right to left. It "
#~ "can be explained in the example "
#~ "below."
#~ msgstr ""

#~ msgid ""
#~ "Reverse the tensor for variable length"
#~ " slices. Input is first sliced along"
#~ " batch axis and then elements are "
#~ "reversed along seq axis."
#~ msgstr ""

#~ msgid "The tensor to be reversed."
#~ msgstr ""

#~ msgid ""
#~ "A 1D Tensor with length "
#~ "a.dims[batch_axis] Must be one of the"
#~ " following types: int32, int64 if "
#~ "seq_lengths[i] > a.dims[seq_axis], it is "
#~ "rounded to a.dims[seq_axis] if seq_lengths[i]"
#~ " < 1, it is rounded to 1"
#~ msgstr ""

#~ msgid "The axis along which the elements will be reversed. Default is 1."
#~ msgstr ""

#~ msgid "The axis along which the tensor will be sliced. Default is 0."
#~ msgstr ""

#~ msgid "**ret** -- The computed result of same shape and type as of input."
#~ msgstr ""

#~ msgid "1/sqrt(x)"
#~ msgstr ""

#~ msgid ""
#~ "The result binary bytes can be "
#~ "loaded by the GraphModule with API "
#~ "\"load_params\"."
#~ msgstr ""

#~ msgid "Use :py:func:`tvm.runtime.save_param_dict` instead."
#~ msgstr ""

#~ msgid "The parameter dictionary."
#~ msgstr ""

#~ msgid "**param_bytes** -- Serialized parameters."
#~ msgstr ""

#~ msgid "This function returns TensorType((), dtype)"
#~ msgstr ""

#~ msgid "The content data type."
#~ msgstr ""

#~ msgid "**s_type** -- The result type."
#~ msgstr ""

#~ msgid "The index locations to update."
#~ msgstr ""

#~ msgid "The values to update."
#~ msgstr ""

#~ msgid "The axis to scatter on"
#~ msgstr ""

#~ msgid "The values to add."
#~ msgstr ""

#~ msgid "The axis to scatter_add on"
#~ msgstr ""

#~ msgid "See :py:func:`tvm.topi.scatter` for how data is scattered."
#~ msgstr ""

#~ msgid "The accumulation mode for scatter. \"update\" or \"add\""
#~ msgstr ""

#~ msgid ""
#~ "The hybrid function support emulation "
#~ "mode and parsing to the internal "
#~ "language IR."
#~ msgstr ""

#~ msgid "**hybrid_func** -- A decorated hybrid script function."
#~ msgstr ""

#~ msgid ""
#~ "If `sorted_sequence` is N-dimensional, the "
#~ "innermost dimension of `values` are "
#~ "searched in the corresponding dimension "
#~ "of `sorted_sequence`."
#~ msgstr ""

#~ msgid ""
#~ "N-D or 1-D Tensor, containing "
#~ "monotonically increasing sequence on the "
#~ "innermost dimension."
#~ msgstr ""

#~ msgid ""
#~ "N-D Tensor containing the search values."
#~ " When `sorted_sequence` is 1-D, the "
#~ "shape of `values` can be arbitrary. "
#~ "Otherwise, ranks of `sorted_sequence` and "
#~ "`values` must be the same, and "
#~ "outer N-1 axes must have the same"
#~ " size."
#~ msgstr ""

#~ msgid ""
#~ "Controls which index is returned if "
#~ "a value lands exactly on one of"
#~ " sorted values. If False, the index"
#~ " of the first suitable location found"
#~ " is given. If true, return the "
#~ "last such index. If there is no"
#~ " suitable index, return either 0 or"
#~ " N (where N is the size of "
#~ "the innermost dimension)."
#~ msgstr ""

#~ msgid ""
#~ "**indices** -- Tensor with same shape"
#~ " as values, representing the indices "
#~ "of elements of `values` if they "
#~ "are inserted in `sorted_sequence`."
#~ msgstr ""

#~ msgid ""
#~ "Computes the sum along segment_ids along"
#~ " axis 0. If multiple segment_ids "
#~ "reference the same location their "
#~ "contributions add up. result[index, j, "
#~ "k, ...] = Σi... data[i, j, k,..]"
#~ " where index = segment_ids[i] This op"
#~ " is much better understood with "
#~ "visualization articulated in the following "
#~ "links and examples at the end of"
#~ " this docstring."
#~ msgstr ""

#~ msgid ""
#~ "https://www.tensorflow.org/api_docs/python/tf/math/unsorted_segment_sum"
#~ " https://caffe2.ai/docs/sparse-operations.html"
#~ "#null__unsorted-segment-reduction-ops"
#~ msgstr ""

#~ msgid "Input Tensor. It can be of any type and multi-dimensional"
#~ msgstr ""

#~ msgid ""
#~ "A 1-D int32/int64 tensor containing the"
#~ " segment_ids of the rows to calculate"
#~ " the output sum upon. It defines "
#~ "a mapping from the zeroth dimension "
#~ "of data onto segment_ids. The "
#~ "segment_ids tensor should be the size"
#~ " of the first dimension, d0, with "
#~ "consecutive IDs in the range 0 to"
#~ " k, where k<d0. In particular, a "
#~ "segmentation of a matrix tensor is "
#~ "a mapping of rows to segments. "
#~ "This tensor doesn't need to be "
#~ "sorted"
#~ msgstr ""

#~ msgid ""
#~ "An integer describing the shape of "
#~ "the zeroth dimension. If unspecified, "
#~ "its calculated equivalent to the number"
#~ " of unique segment_ids"
#~ msgstr ""

#~ msgid ""
#~ "This function takes an n-dimensional "
#~ "input array of the form [MAX_LENGTH, "
#~ "batch_size, ...] or [batch_size, MAX_LENGTH,"
#~ " ...] and returns an array of "
#~ "the same shape."
#~ msgstr ""

#~ msgid "The input data."
#~ msgstr ""

#~ msgid "The expected (valid) length of each sequence in the tensor."
#~ msgstr ""

#~ msgid "The masking value."
#~ msgstr ""

#~ msgid "The axis of the length dimension."
#~ msgstr ""

#~ msgid ""
#~ "This limit prevents infinite recursion "
#~ "from causing an overflow of the C"
#~ " stack and crashing Python.  The "
#~ "highest possible limit is platform- "
#~ "dependent."
#~ msgstr ""

#~ msgid "**result** -- The shape tensor."
#~ msgstr ""

#~ msgid ""
#~ "For an input array with shape "
#~ "``(d1, d2, ..., dk)``, `slice_like` "
#~ "operation slices the the input array "
#~ "corresponding size of second array. By"
#~ " default will slice on all axes."
#~ msgstr ""

#~ msgid "The source array."
#~ msgstr ""

#~ msgid "The new shape."
#~ msgstr ""

#~ msgid ""
#~ "List of axes on which input data"
#~ " will be sliced according to the "
#~ "corresponding size of the second input."
#~ " By default will slice on all "
#~ "axes. Negative axes mean counting in "
#~ "reverse."
#~ msgstr ""

#~ msgid ""
#~ "Fill rows in a sparse matrix that"
#~ " do no contain any values. Values "
#~ "are placed in the first column of"
#~ " empty rows. The sparse array is "
#~ "in COO format. It returns a "
#~ "TupleWrapper with 3 outputs"
#~ msgstr ""

#~ msgid ""
#~ "A 2-D tensor[N, ndims] of integers "
#~ "containing location of sparse values, "
#~ "where N is the number of sparse"
#~ " values and n_dim is the number "
#~ "of dimensions of the dense_shape. The"
#~ " first column of this relay parameter"
#~ " must be sorted in ascending order."
#~ msgstr ""

#~ msgid "A 1-D tensor[N] containing the sparse values for the sparse indices."
#~ msgstr ""

#~ msgid "A 1-D tensor[ndims] which contains shape of the dense output tensor."
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor[1] containing the default"
#~ " value for the remaining locations."
#~ msgstr ""

#~ msgid ""
#~ "* **new_sparse_indices** (*relay.Expr*) -- A"
#~ " 2-D tensor[?, ndims] of integers "
#~ "containing location of new sparse   "
#~ "indices. The first column outputs must"
#~ " be sorted in ascending order. * "
#~ "**new_sparse_values** (*relay.Expr*) -- A 1-D"
#~ " tensor[?] containing the sparse values "
#~ "for the sparse indices. * "
#~ "**empty_row_indicator** (*relay.Expr*) -- A "
#~ "1-D tensor[dense_shape[0]] filled with zeros"
#~ " and ones   indicating whether the "
#~ "particular row is empty or full "
#~ "respectively"
#~ msgstr ""

#~ msgid ""
#~ "**new_sparse_indices** (*relay.Expr*) -- A 2-D"
#~ " tensor[?, ndims] of integers containing"
#~ " location of new sparse indices. The"
#~ " first column outputs must be sorted"
#~ " in ascending order."
#~ msgstr ""

#~ msgid ""
#~ "**new_sparse_values** (*relay.Expr*) -- A 1-D"
#~ " tensor[?] containing the sparse values "
#~ "for the sparse indices."
#~ msgstr ""

#~ msgid ""
#~ "**empty_row_indicator** (*relay.Expr*) -- A "
#~ "1-D tensor[dense_shape[0]] filled with zeros"
#~ " and ones indicating whether the "
#~ "particular row is empty or full "
#~ "respectively"
#~ msgstr ""

#~ msgid ""
#~ "This op exactly follows the "
#~ "documentation here: "
#~ "https://www.tensorflow.org/api_docs/python/tf/sparse/fill_empty_rows"
#~ " There are two exceptions: 1. Input"
#~ " Sparse Indices are expected to be"
#~ " in row-major order. 2. Empty "
#~ "Row Indicator has int64 output type "
#~ "with 1(for True) and 0(for False)."
#~ msgstr ""

#~ msgid "Reshape a Sparse Tensor. The sparse array is in COO format."
#~ msgstr ""

#~ msgid ""
#~ "A 2-D tensor[N, n_dim] of integers "
#~ "containing location of sparse values, "
#~ "where N is the number of sparse"
#~ " values and n_dim is the number "
#~ "of dimensions of the dense_shape"
#~ msgstr ""

#~ msgid "A 1-D tensor containing the previous shape of the dense tensor"
#~ msgstr ""

#~ msgid "A 1-D tensor containing the new shape of the dense tensor"
#~ msgstr ""

#~ msgid ""
#~ "Example:: -   sparse_to_dense([[0, 0], [1, "
#~ "1]], [2, 2], [3, 3], 0) = "
#~ "[[3, 0], [0, 3]]"
#~ msgstr ""

#~ msgid ""
#~ "A 0-D, 1-D, or 2-D tensor of "
#~ "integers containing location of sparse "
#~ "values."
#~ msgstr ""

#~ msgid "A list of integers. Shape of the dense output tensor."
#~ msgstr ""

#~ msgid ""
#~ "A 0-D or 1-D tensor containing the"
#~ " sparse values for the sparse "
#~ "indices."
#~ msgstr ""

#~ msgid ""
#~ "A 0-D tensor containing the default "
#~ "value for the remaining locations. "
#~ "Defaults to 0."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- Dense tensor of shape "
#~ "output_shape. Has the same type as "
#~ "sparse_values."
#~ msgstr ""

#~ msgid ""
#~ "If indices_or_sections is an integer, "
#~ "the input will be divided equally "
#~ "along given axis. If such a split"
#~ " is not possible, an error is "
#~ "raised."
#~ msgstr ""

#~ msgid ""
#~ "If indices_or_sections is a tuple of "
#~ "sorted integers, the entries indicate "
#~ "where along axis the array is "
#~ "split."
#~ msgstr ""

#~ msgid "Indices or sections to split into. Accepts an int or a tuple"
#~ msgstr ""

#~ msgid "The axis over which to split."
#~ msgstr ""

#~ msgid ""
#~ "The set of axes to remove. If "
#~ "axis = None, remove all axis of"
#~ " dimensions 1. If any specified axis"
#~ " has dimension that does not equal"
#~ " 1, it is an error."
#~ msgstr ""

#~ msgid "**result** -- The squeezed result."
#~ msgstr ""

#~ msgid ""
#~ "A list of tensors or a Relay "
#~ "expression that evaluates to a tuple "
#~ "of tensors."
#~ msgstr ""

#~ msgid "The axis in the result array along which the input arrays are stacked."
#~ msgstr ""

#~ msgid "**ret** -- The stacked tensor."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a "
#~ "standard deviation operation is performed. "
#~ "The default, axis=None, will compute the"
#~ " standard deviation of all elements "
#~ "in the input array. If axis is "
#~ "negative it counts from the last "
#~ "to the first axis."
#~ msgstr ""

#~ msgid "The source array to be sliced."
#~ msgstr ""

#~ msgid "The data to be set."
#~ msgstr ""

#~ msgid "The indices to begin with in the slicing."
#~ msgstr ""

#~ msgid "Indices indicating end of the slice."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the stride values, it can "
#~ "be negative in that case, the "
#~ "input tensor will be reversed in "
#~ "that particular axis."
#~ msgstr ""

#~ msgid ""
#~ "Axes along which slicing is applied. "
#~ "When it is specified, the length "
#~ "of begin, end, strides, and axes "
#~ "must be equal. Moreover, begin, end, "
#~ "strides, and axes must be static "
#~ "(cannot be relay.Expr). Axes argument "
#~ "for dynamic parameter slicing is not "
#~ "supported yet."
#~ msgstr ""

#~ msgid ""
#~ "The slice mode [end, size]. end: "
#~ "The ending indices for the slice "
#~ "[default]. size: The input strides will"
#~ " be ignored, input end in this "
#~ "mode indicates the size of a slice"
#~ " starting at the location specified "
#~ "by begin. If end[i] is -1, all "
#~ "remaining elements in that dimension are"
#~ " included in the slice."
#~ msgstr ""

#~ msgid "The indices of the values to extract."
#~ msgstr ""

#~ msgid ""
#~ "The axis over which to select "
#~ "values. By default, the flattened input"
#~ " array is used."
#~ msgstr ""

#~ msgid "The number of batch dimensions. By default is 0."
#~ msgstr ""

#~ msgid ""
#~ "Specifies how out-of-bound indices "
#~ "will behave [clip, wrap, fast]. clip:"
#~ " clip to the range (default). wrap:"
#~ " wrap around the indices. fast: no"
#~ " clip or wrap around (user must "
#~ "make sure indices are in-bound)."
#~ msgstr ""

#~ msgid "The number of times repeating the tensor data."
#~ msgstr ""

#~ msgid "提示"
#~ msgstr ""

#~ msgid ""
#~ "Each dim size of reps must be "
#~ "a positive integer. If reps has "
#~ "length d, the result will have "
#~ "dimension of max(d, data.ndim); If "
#~ "data.ndim < d, data is promoted to"
#~ " be d-dimensional by prepending new "
#~ "axes. If data.ndim >=  d, reps is"
#~ " promoted to a.ndim by pre-pending"
#~ " 1's to it."
#~ msgstr ""

#~ msgid ""
#~ "ret_type specifies the return type, can"
#~ " be one of (\"both\", \"values\", "
#~ "\"indices\")."
#~ msgstr ""

#~ msgid "Number of top elements to select. Return all elements if k < 1."
#~ msgstr ""

#~ msgid ""
#~ "The return type [both, values, indices]."
#~ " \"both\": return both top k data "
#~ "and indices. \"values\": return top k"
#~ " data only. \"indices\": return top k"
#~ " indices only."
#~ msgstr ""

#~ msgid "The data type of the indices output."
#~ msgstr ""

#~ msgid "**out** -- The computed result."
#~ msgstr ""

#~ msgid "The target axes order, reverse order if not specified."
#~ msgstr ""

#~ msgid "**result** -- The transposed result."
#~ msgstr ""

#~ msgid ""
#~ "Find the unique elements of a 1-D"
#~ " tensor. Please note `output` and "
#~ "`counts` are all padded to have "
#~ "the same length of `data` and "
#~ "element with index >= num_unique[0] has"
#~ " undefined value."
#~ msgstr ""

#~ msgid "A 1-D tensor of integers."
#~ msgstr ""

#~ msgid ""
#~ "Whether to sort the unique elements "
#~ "in ascending order before returning as"
#~ " output."
#~ msgstr ""

#~ msgid "Whether to return the count of each unique element."
#~ msgstr ""

#~ msgid ""
#~ "* **unique** (*relay.Expr*) -- A 1-D "
#~ "tensor containing the unique elements of"
#~ " the input data tensor. * **indices**"
#~ " (*relay.Expr*) -- A 1-D tensor "
#~ "containing the index of each data "
#~ "element in the output tensor. * "
#~ "**inverse_indices** (*relay.Expr*) -- A 1-D"
#~ " tensor. For each entry in data, "
#~ "it contains the index of that data"
#~ " element in the   unique array. * "
#~ "**num_unique** (*relay.Expr*) -- A 1-D "
#~ "tensor with size=1 containing the number"
#~ " of unique elements in the input "
#~ "data tensor. * **counts (optional)** "
#~ "(*relay.Expr*) -- A 1-D tensor "
#~ "containing the count of each unique "
#~ "element in the output."
#~ msgstr ""

#~ msgid ""
#~ "**unique** (*relay.Expr*) -- A 1-D "
#~ "tensor containing the unique elements of"
#~ " the input data tensor."
#~ msgstr ""

#~ msgid ""
#~ "**indices** (*relay.Expr*) -- A 1-D "
#~ "tensor containing the index of each "
#~ "data element in the output tensor."
#~ msgstr ""

#~ msgid ""
#~ "**inverse_indices** (*relay.Expr*) -- A 1-D"
#~ " tensor. For each entry in data, "
#~ "it contains the index of that data"
#~ " element in the unique array."
#~ msgstr ""

#~ msgid ""
#~ "**num_unique** (*relay.Expr*) -- A 1-D "
#~ "tensor with size=1 containing the number"
#~ " of unique elements in the input "
#~ "data tensor."
#~ msgstr ""

#~ msgid ""
#~ "**counts (optional)** (*relay.Expr*) -- A "
#~ "1-D tensor containing the count of "
#~ "each unique element in the output."
#~ msgstr ""

#~ msgid ""
#~ "Example:: -   unravel_index([22, 41, 37], "
#~ "[7, 6]) = [[3, 6, 6],[4, 5, "
#~ "1]]"
#~ msgstr ""

#~ msgid "An integer array containing indices."
#~ msgstr ""

#~ msgid "The shape of the array."
#~ msgstr ""

#~ msgid "**result** -- The tuple of coordinate arrays."
#~ msgstr ""

#~ msgid ""
#~ "This is a simple wrapper function "
#~ "that allows specify shape and dtype "
#~ "directly."
#~ msgstr ""

#~ msgid ""
#~ "The name of the variable. This "
#~ "name only acts as a hint, and "
#~ "is not used for equality."
#~ msgstr ""

#~ msgid ""
#~ "The type annotation on the variable. "
#~ "When type_annotation is a str, we "
#~ "will create a scalar variable."
#~ msgstr ""

#~ msgid "The shape of the tensor type."
#~ msgstr ""

#~ msgid "The data type of the tensor."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a "
#~ "variance operation is performed. The "
#~ "default, axis=None, will compute the "
#~ "variance of all elements in the "
#~ "input array. If axis is negative "
#~ "it counts from the last to the "
#~ "first axis."
#~ msgstr ""

#~ msgid ""
#~ "Shapes of condition, x, and y must"
#~ " be broadcastable to a common shape."
#~ " Semantics follow numpy where function "
#~ "https://numpy.org/doc/stable/reference/generated/numpy.where.html"
#~ msgstr ""

#~ msgid "Where True, yield x, otherwise yield y"
#~ msgstr ""

#~ msgid "The first array or scalar to be selected."
#~ msgstr ""

#~ msgid "The second array or scalar to be selected."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The selected array. The"
#~ " output shape is the broadcasted "
#~ "shape from condition, x, and y."
#~ msgstr ""

#~ msgid ":py:obj:`Constant <tvm.relay.Constant>`\\ \\(data\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ":py:class:`~tvm.ir.expr.RelayExpr` 的别名"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`If <tvm.relay.If>`\\ \\(cond\\, "
#~ "true\\_branch\\, false\\_branch\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Let <tvm.relay.Let>`\\ \\(variable\\, "
#~ "value\\, body\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`RefCreate <tvm.relay.RefCreate>`\\ \\(value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Create a new reference from initial "
#~ "value. Parameters ---------- value: "
#~ "tvm.relay.Expr    The initial value."
#~ msgstr ""

#~ msgid ":py:obj:`RefRead <tvm.relay.RefRead>`\\ \\(ref\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Get the value inside the reference. "
#~ "Parameters ---------- ref: tvm.relay.Expr      "
#~ "The reference."
#~ msgstr ""

#~ msgid ":py:class:`~tvm.ir.type.RelayRefType` 的别名"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RefWrite <tvm.relay.RefWrite>`\\ \\(ref\\, "
#~ "value\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Update the value inside the reference."
#~ " The whole expression will evaluate "
#~ "to an empty tuple. Parameters ----------"
#~ " ref: tvm.relay.Expr     The reference."
#~ msgstr ""

#~ msgid ":py:obj:`SequentialSpan <tvm.relay.SequentialSpan>`\\ \\(spans\\)"
#~ msgstr ""

#~ msgid "A sequence of source spans"
#~ msgstr ""

#~ msgid ":py:obj:`SpanCheck <tvm.relay.SpanCheck>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "A debugging utility for reporting missing span information."
#~ msgstr ""

#~ msgid ""
#~ "Performs sorting along the given axis"
#~ " and returns an array of indices "
#~ "having same shape as an input "
#~ "array that index data in sorted "
#~ "order."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`astext <tvm.relay.astext>`\\ \\(obj\\[\\, "
#~ "show\\_meta\\_data\\, annotate\\]\\)"
#~ msgstr ""

#~ msgid "Get the text format of the expression."
#~ msgstr ""

#~ msgid ""
#~ "Return a scalar value array with "
#~ "the same type, broadcasted to the "
#~ "provided shape."
#~ msgstr ""

#~ msgid ":py:obj:`const <tvm.relay.const>`\\ \\(value\\[\\, dtype\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`dft <tvm.relay.dft>`\\ \\(re\\_data\\, "
#~ "im\\_data\\[\\, inverse\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Computes the discrete Fourier transform "
#~ "of input (calculation along the last "
#~ "axis)."
#~ msgstr ""

#~ msgid ""
#~ "Gather elements or slices from data "
#~ "and store them to a tensor whose"
#~ " shape is defined by indices."
#~ msgstr ""

#~ msgid "Transform the layout of a tensor."
#~ msgstr ""

#~ msgid ""
#~ "Returns a one-hot tensor where the"
#~ " locations represented by indices take "
#~ "value on_value, and other locations take"
#~ " value off_value."
#~ msgstr ""

#~ msgid ":py:obj:`pretty_print <tvm.relay.pretty_print>`\\ \\(obj\\)"
#~ msgstr ""

#~ msgid "Pretty print the object."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reshape <tvm.relay.reshape>`\\ \\(data\\, "
#~ "newshape\\[\\, allowzero\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scatter_elements <tvm.relay.scatter_elements>`\\ "
#~ "\\(data\\, indices\\, updates\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Scatter elements with updating data by"
#~ " reduction of values in updates at"
#~ " positions defined by indices."
#~ msgstr ""

#~ msgid "Decorate a python function as hybrid script."
#~ msgstr ""

#~ msgid "Fill rows in a sparse matrix that do not contain any values."
#~ msgstr ""

#~ msgid "Reshape a sparse tensor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`stft <tvm.relay.stft>`\\ \\(data\\, "
#~ "n\\_fft\\[\\, hop\\_length\\, win\\_length\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "The STFT computes the Fourier transform"
#~ " of short overlapping windows of the"
#~ " input."
#~ msgstr ""

#~ msgid ":py:obj:`trilu <tvm.relay.trilu>`\\ \\(data\\, k\\[\\, upper\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Given a 2-D matrix or batches of"
#~ " 2-D matrices, returns the upper or"
#~ " lower triangular part of the tensor."
#~ msgstr ""

#~ msgid ":py:obj:`trunc_divide <tvm.relay.trunc_divide>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Trunc division with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ":py:obj:`trunc_mod <tvm.relay.trunc_mod>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Trunc mod with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`var <tvm.relay.var>`\\ \\(name\\_hint\\[\\, "
#~ "type\\_annotation\\, shape\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Parameters"
#~ msgstr ""

#~ msgid "op: tvm.ir.Op or any tvm.relay.Expr with function type."
#~ msgstr ""

#~ msgid "args: List[tvm.relay.Expr]"
#~ msgstr ""

#~ msgid "attrs: Optional[tvm.Attrs]"
#~ msgstr ""

#~ msgid "type_args: Optional[List[tvm.relay.Type]]"
#~ msgstr ""

#~ msgid "span: Optional[tvm.relay.Span]"
#~ msgstr ""

#~ msgid "Span that points to original source code."
#~ msgstr ""

#~ msgid ":py:obj:`__init__ <tvm.relay.Clause.__init__>`\\ \\(lhs\\, rhs\\)"
#~ msgstr ""

#~ msgid "Construct a clause."
#~ msgstr ""

#~ msgid "lhs: tvm.relay.Pattern"
#~ msgstr ""

#~ msgid "Left-hand side of match clause."
#~ msgstr ""

#~ msgid "rhs: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "Right-hand side of match clause."
#~ msgstr ""

#~ msgid "Returns"
#~ msgstr ""

#~ msgid "clause: Clause"
#~ msgstr ""

#~ msgid "The Clause."
#~ msgstr ""

#~ msgid "data"
#~ msgstr ""

#~ msgid "tvm.nd.NDArray"
#~ msgstr ""

#~ msgid ":py:obj:`struct_info <tvm.relay.Expr.struct_info>`\\"
#~ msgstr ""

#~ msgid "Get the struct info field"
#~ msgstr ""

#~ msgid "params: List[tvm.relay.Var]"
#~ msgstr ""

#~ msgid "body: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "ret_type: Optional[tvm.relay.Type]"
#~ msgstr ""

#~ msgid "type_params: Optional[List[tvm.relay.TypeParam]]"
#~ msgstr ""

#~ msgid ":py:obj:`__call__ <tvm.relay.Function.__call__>`\\ \\(\\*args\\)"
#~ msgstr ""

#~ msgid "Invoke the global function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`astext <tvm.relay.Function.astext>`\\ "
#~ "\\(\\[show\\_meta\\_data\\, annotate\\]\\)"
#~ msgstr ""

#~ msgid "args: List[relay.Expr]"
#~ msgstr ""

#~ msgid "Arguments."
#~ msgstr ""

#~ msgid "show_meta_data"
#~ msgstr ""

#~ msgid "bool"
#~ msgstr ""

#~ msgid "Whether to include meta data section in the text if there is meta data."
#~ msgstr ""

#~ msgid "annotate: Optional[Object->str]"
#~ msgstr ""

#~ msgid ""
#~ "Optionally annotate function to provide "
#~ "additional information in the comment "
#~ "block."
#~ msgstr ""

#~ msgid "text"
#~ msgstr ""

#~ msgid "str"
#~ msgstr ""

#~ msgid "The text format of the expression."
#~ msgstr ""

#~ msgid "Notes"
#~ msgstr ""

#~ msgid ""
#~ "The meta data section is necessary "
#~ "to fully parse the text format. "
#~ "However, it can contain dumps that "
#~ "are big (e.g constant weights), so "
#~ "it can be helpful to skip printing"
#~ " the meta data section."
#~ msgstr ""

#~ msgid "cond: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "true_branch: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "false_branch: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "variable: tvm.relay.Var"
#~ msgstr ""

#~ msgid "value: tvm.relay.Expr"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`__init__ <tvm.relay.Match.__init__>`\\ \\(data\\,"
#~ " clauses\\[\\, complete\\]\\)"
#~ msgstr ""

#~ msgid "Construct a Match."
#~ msgstr ""

#~ msgid "data: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "The value being deconstructed and matched."
#~ msgstr ""

#~ msgid "clauses: List[tvm.relay.Clause]"
#~ msgstr ""

#~ msgid "The pattern match clauses."
#~ msgstr ""

#~ msgid "complete: Optional[Bool]"
#~ msgstr ""

#~ msgid ""
#~ "Should the match be complete (cover "
#~ "all cases)? If yes, the type "
#~ "checker will generate an error if "
#~ "there are any missing cases."
#~ msgstr ""

#~ msgid "match: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "The match expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`__init__ <tvm.relay.PatternConstructor.__init__>`\\ "
#~ "\\(constructor\\[\\, patterns\\]\\)"
#~ msgstr ""

#~ msgid "Construct a constructor pattern."
#~ msgstr ""

#~ msgid "constructor: Constructor"
#~ msgstr ""

#~ msgid "The constructor."
#~ msgstr ""

#~ msgid "patterns: Optional[List[Pattern]]"
#~ msgstr ""

#~ msgid ""
#~ "Optional subpatterns: for each field of"
#~ " the constructor, match to the given"
#~ " subpattern (treated as a variable "
#~ "pattern by default)."
#~ msgstr ""

#~ msgid "wildcard: PatternWildcard"
#~ msgstr ""

#~ msgid "a wildcard pattern."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`__init__ <tvm.relay.PatternTuple.__init__>`\\ "
#~ "\\(\\[patterns\\]\\)"
#~ msgstr ""

#~ msgid "Construct a tuple pattern."
#~ msgstr ""

#~ msgid ":py:obj:`__init__ <tvm.relay.PatternVar.__init__>`\\ \\(var\\)"
#~ msgstr ""

#~ msgid "Construct a variable pattern."
#~ msgstr ""

#~ msgid "var: tvm.relay.Var"
#~ msgstr ""

#~ msgid "pv: PatternVar"
#~ msgstr ""

#~ msgid "A variable pattern."
#~ msgstr ""

#~ msgid ":py:obj:`__init__ <tvm.relay.PatternWildcard.__init__>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Constructs a wildcard pattern."
#~ msgstr ""

#~ msgid "None"
#~ msgstr ""

#~ msgid ""
#~ "Create a new reference from initial "
#~ "value. Parameters ---------- value: "
#~ "tvm.relay.Expr"
#~ msgstr ""

#~ msgid "The initial value."
#~ msgstr ""

#~ msgid ""
#~ "Get the value inside the reference. "
#~ "Parameters ---------- ref: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "The reference."
#~ msgstr ""

#~ msgid ""
#~ "Update the value inside the reference."
#~ " The whole expression will evaluate "
#~ "to an empty tuple. Parameters ----------"
#~ " ref: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "The new value."
#~ msgstr ""

#~ msgid "Examples"
#~ msgstr ""

#~ msgid "scope: WithScope"
#~ msgstr ""

#~ msgid "The if scope."
#~ msgstr ""

#~ msgid "value: tvm.relay.expr.Expr"
#~ msgstr ""

#~ msgid "The final result of the expression."
#~ msgstr ""

#~ msgid "cond: tvm.relay.expr.Expr"
#~ msgstr ""

#~ msgid "Note"
#~ msgstr ""

#~ msgid "var: Union[Tuple[str, relay.Type], tvm.relay.Var]"
#~ msgstr ""

#~ msgid "expr: relay.Expr"
#~ msgstr ""

#~ msgid ""
#~ "This span is specific for an "
#~ "expression, which is from multiple "
#~ "expressions after an IR transform."
#~ msgstr ""

#~ msgid "spans"
#~ msgstr ""

#~ msgid "Array"
#~ msgstr ""

#~ msgid "The array of spans."
#~ msgstr ""

#~ msgid "name : str"
#~ msgstr ""

#~ msgid "type_var"
#~ msgstr ""

#~ msgid "tvm.relay.TypeVar"
#~ msgstr ""

#~ msgid "The shape variable."
#~ msgstr ""

#~ msgid "fields"
#~ msgstr ""

#~ msgid "List[tvm.relay.Expr]"
#~ msgstr ""

#~ msgid "dtype"
#~ msgstr ""

#~ msgid "result"
#~ msgstr ""

#~ msgid "tvm.relay.Expr"
#~ msgstr ""

#~ msgid "The result expression."
#~ msgstr ""

#~ msgid "tuple_value: tvm.relay.Expr"
#~ msgstr ""

#~ msgid "index: int"
#~ msgstr ""

#~ msgid "size: int"
#~ msgstr ""

#~ msgid "The text format of the tuple expression."
#~ msgstr ""

#~ msgid "header: GlobalTypeVar"
#~ msgstr ""

#~ msgid "type_vars: List[TypeVar]"
#~ msgstr ""

#~ msgid "constructors: List[Constructor]"
#~ msgstr ""

#~ msgid "relay.Expr"
#~ msgstr ""

#~ msgid "The computed result."
#~ msgstr ""

#~ msgid "lhs"
#~ msgstr ""

#~ msgid "rhs"
#~ msgstr ""

#~ msgid "inputs"
#~ msgstr ""

#~ msgid "Union(List[relay.Expr], Tuple[relay.Expr])"
#~ msgstr ""

#~ msgid ""
#~ "Input tensor and indices. The first "
#~ "tensor is the input data and the"
#~ " rest are the indices."
#~ msgstr ""

#~ msgid "Output tensor."
#~ msgstr ""

#~ msgid "axis"
#~ msgstr ""

#~ msgid "None or int or tuple of int"
#~ msgstr ""

#~ msgid "keepdims"
#~ msgstr ""

#~ msgid "exclude"
#~ msgstr ""

#~ msgid ""
#~ "Similar to ``numpy.arange``. When only "
#~ "one argument is given, it is used"
#~ " as `stop` instead of `start` while"
#~ " `start` takes default value 0."
#~ msgstr ""

#~ msgid "start"
#~ msgstr ""

#~ msgid "relay.Expr, optional"
#~ msgstr ""

#~ msgid "stop"
#~ msgstr ""

#~ msgid "step"
#~ msgstr ""

#~ msgid "str, optional"
#~ msgstr ""

#~ msgid "The resulting tensor."
#~ msgstr ""

#~ msgid "select_last_index"
#~ msgstr ""

#~ msgid "valid_count"
#~ msgstr ""

#~ msgid "tvm.te.Tensor"
#~ msgstr ""

#~ msgid "int, optional"
#~ msgstr ""

#~ msgid "is_ascend"
#~ msgstr ""

#~ msgid "boolean, optional"
#~ msgstr ""

#~ msgid "string, optional"
#~ msgstr ""

#~ msgid "out"
#~ msgstr ""

#~ msgid "Tensor with same shape as data."
#~ msgstr ""

#~ msgid "condition"
#~ msgstr ""

#~ msgid "Tensor with the indices of elements that are non-zero."
#~ msgstr ""

#~ msgid "obj"
#~ msgstr ""

#~ msgid "Object"
#~ msgstr ""

#~ msgid "The object to be printed."
#~ msgstr ""

#~ msgid "expr"
#~ msgstr ""

#~ msgid "binds"
#~ msgstr ""

#~ msgid "Map[tvm.relay.Var, tvm.relay.Expr]"
#~ msgstr ""

#~ msgid "The expression or function after binding."
#~ msgstr ""

#~ msgid "shape"
#~ msgstr ""

#~ msgid "tuple of int or relay.Expr"
#~ msgstr ""

#~ msgid "broadcast_type"
#~ msgstr ""

#~ msgid "ir_mod : :py:class:`~tvm.IRModule`"
#~ msgstr ""

#~ msgid "IRModule"
#~ msgstr ""

#~ msgid "target"
#~ msgstr ""

#~ msgid "None, or any multi-target like object, see Target.canon_multi_target"
#~ msgstr ""

#~ msgid ""
#~ "For homogeneous compilation, the unique "
#~ "build target. For heterogeneous compilation,"
#~ " a dictionary or list of possible "
#~ "build targets. Defaults to the current"
#~ " target in the environment if None."
#~ msgstr ""

#~ msgid "target_host"
#~ msgstr ""

#~ msgid "None, or any target like object, see Target.canon_target"
#~ msgstr ""

#~ msgid "Host compilation target, if target is device."
#~ msgstr ""

#~ msgid "executor"
#~ msgstr ""

#~ msgid "Optional[Executor]"
#~ msgstr ""

#~ msgid "runtime"
#~ msgstr ""

#~ msgid "Optional[Runtime]"
#~ msgstr ""

#~ msgid "workspace_memory_pools"
#~ msgstr ""

#~ msgid "Optional[WorkspaceMemoryPools]"
#~ msgstr ""

#~ msgid ""
#~ "The object that contains an Array "
#~ "of WorkspacePoolInfo objects that hold "
#~ "properties of read-write workspace pools"
#~ " that could be used by the "
#~ "inference."
#~ msgstr ""

#~ msgid "constant_memory_pools"
#~ msgstr ""

#~ msgid "Optional[ConstantMemoryPools]"
#~ msgstr ""

#~ msgid ""
#~ "The object that contains an Array "
#~ "of ConstantPoolInfo objects that hold "
#~ "properties of read-only pools that "
#~ "could be used by the inference."
#~ msgstr ""

#~ msgid "params"
#~ msgstr ""

#~ msgid "dict of str to NDArray"
#~ msgstr ""

#~ msgid "mod_name: Optional[str]"
#~ msgstr ""

#~ msgid "factory_module"
#~ msgstr ""

#~ msgid "tvm.relay.backend.executor_factory.ExecutorFactoryModule"
#~ msgstr ""

#~ msgid "The runtime factory for the TVM graph executor."
#~ msgstr ""

#~ msgid "opt_level: int, optional"
#~ msgstr ""

#~ msgid "required_pass: set of str, optional"
#~ msgstr ""

#~ msgid "disabled_pass: set of str, optional"
#~ msgstr ""

#~ msgid "trace: Callable[[IRModule, PassInfo, bool], None]"
#~ msgstr ""

#~ msgid "pass_context: PassContext"
#~ msgstr ""

#~ msgid "The pass context for optimizations."
#~ msgstr ""

#~ msgid "The casted result."
#~ msgstr ""

#~ msgid "dtype_like"
#~ msgstr ""

#~ msgid "a"
#~ msgstr ""

#~ msgid "a_min"
#~ msgstr ""

#~ msgid "float"
#~ msgstr ""

#~ msgid "a_max"
#~ msgstr ""

#~ msgid "`a` with elements clipped between `a_min` and `a_max`."
#~ msgstr ""

#~ msgid "collapse_type"
#~ msgstr ""

#~ msgid "Provide the shape to collapse to."
#~ msgstr ""

#~ msgid "result: relay.Expr"
#~ msgstr ""

#~ msgid "The concatenated tensor."
#~ msgstr ""

#~ msgid "value: Union[bool, int, float, numpy.ndarray, tvm.nd.NDArray]"
#~ msgstr ""

#~ msgid "dtype: str, optional"
#~ msgstr ""

#~ msgid "The copied result."
#~ msgstr ""

#~ msgid "Example"
#~ msgstr ""

#~ msgid "kind"
#~ msgstr ""

#~ msgid ""
#~ "The type of executor. Avaliable options"
#~ " are `debug` for the interpreter, "
#~ "`graph` for the graph executor, `aot`"
#~ " for the aot executor, and `vm` "
#~ "for the virtual machine."
#~ msgstr ""

#~ msgid "mod : :py:class:`~tvm.IRModule`"
#~ msgstr ""

#~ msgid "device : :py:class:`Device`"
#~ msgstr ""

#~ msgid "Device"
#~ msgstr ""

#~ msgid "any multi-target like object, see Target.canon_multi_target"
#~ msgstr ""

#~ msgid ""
#~ "For homogeneous compilation, the unique "
#~ "build target. For heterogeneous compilation,"
#~ " a dictionary or list of possible "
#~ "build targets. CAUTION: Though this API"
#~ " allows multiple targets, it does not"
#~ " allow multiple devices, so heterogenous"
#~ " compilation is not yet supported."
#~ msgstr ""

#~ msgid "executor : :py:class:`~tvm.relay.backend.interpreter.Executor`"
#~ msgstr ""

#~ msgid "exclusive"
#~ msgstr ""

#~ msgid "bool, optional"
#~ msgstr ""

#~ msgid ""
#~ "The result has the same size as"
#~ " data, and the same shape as "
#~ "data if axis is not None. If "
#~ "axis is None, the result is a "
#~ "1-d array."
#~ msgstr ""

#~ msgid "src_device : Union[:py:class:`Device`, str]"
#~ msgstr ""

#~ msgid "Union["
#~ msgstr ""

#~ msgid "dst_device : Union[:py:class:`Device`, str]"
#~ msgstr ""

#~ msgid ""
#~ "Computes the discrete Fourier transform "
#~ "of input (calculation along the last "
#~ "axis). This gives frequency components "
#~ "of the signal as they change over"
#~ " time."
#~ msgstr ""

#~ msgid "re_data"
#~ msgstr ""

#~ msgid "N-D tensor, real part of the input signal."
#~ msgstr ""

#~ msgid "im_data"
#~ msgstr ""

#~ msgid ""
#~ "N-D tensor, imaginary part of the "
#~ "input signal. If the signal is "
#~ "real, then the values of this "
#~ "tensor are zeros."
#~ msgstr ""

#~ msgid "inverse"
#~ msgstr ""

#~ msgid "Whether to perform the inverse discrete fourier transform."
#~ msgstr ""

#~ msgid "re_output"
#~ msgstr ""

#~ msgid "The Fourier Transform of the input (Real part)."
#~ msgstr ""

#~ msgid "im_output"
#~ msgstr ""

#~ msgid "The Fourier Transform of the input (Imaginary part)."
#~ msgstr ""

#~ msgid "equation"
#~ msgstr ""

#~ msgid "The output tensor from the einsum op."
#~ msgstr ""

#~ msgid "Union[int, Expr]"
#~ msgstr ""

#~ msgid "num_newaxis"
#~ msgstr ""

#~ msgid "The reshaped result."
#~ msgstr ""

#~ msgid "multiplier"
#~ msgstr ""

#~ msgid "shift"
#~ msgstr ""

#~ msgid "The output of the fixed point multiplication"
#~ msgstr ""

#~ msgid "fill_value"
#~ msgstr ""

#~ msgid "tuple of int or relay.Expr, optional"
#~ msgstr ""

#~ msgid "data type, optional (defaults to data type of the fill value)"
#~ msgstr ""

#~ msgid ""
#~ "``indices`` must have the same shape "
#~ "as ``data``, except at dimension "
#~ "``axis`` which must just be not "
#~ "null. Output will have the same "
#~ "shape as ``indices``."
#~ msgstr ""

#~ msgid "The axis along which to index. Negative axis is supported."
#~ msgstr ""

#~ msgid "indices"
#~ msgstr ""

#~ msgid "batch_dims"
#~ msgstr ""

#~ msgid "index_rank"
#~ msgstr ""

#~ msgid ""
#~ "The size of an indexing tuple, "
#~ "which is a fixed value and the "
#~ "same as indices.shape[0]. Only needed "
#~ "when other dimensions of indices are "
#~ "dynamic."
#~ msgstr ""

#~ msgid "ret"
#~ msgstr ""

#~ msgid "The source data to be invert permuted."
#~ msgstr ""

#~ msgid "Invert permuted data. Has the same type as data."
#~ msgstr ""

#~ msgid "The source tensor to be transformed."
#~ msgstr ""

#~ msgid "src_layout"
#~ msgstr ""

#~ msgid "dst_layout"
#~ msgstr ""

#~ msgid "The transformed tensor."
#~ msgstr ""

#~ msgid "param_bytes: bytearray"
#~ msgstr ""

#~ msgid "Input tensor."
#~ msgstr ""

#~ msgid "diagonal"
#~ msgstr ""

#~ msgid "k"
#~ msgstr ""

#~ msgid "int or tuple of int, optional"
#~ msgstr ""

#~ msgid ""
#~ "Diagonal offset(s). The diagonal or "
#~ "range of diagonals to set. (0 by"
#~ " default) Positive value means "
#~ "superdiagonal, 0 refers to the main "
#~ "diagonal, and negative value means "
#~ "subdiagonals. k can be a single "
#~ "integer (for a single diagonal) or "
#~ "a pair of integers specifying the "
#~ "low and high ends of a matrix "
#~ "band. k[0] must not be larger than"
#~ " k[1]."
#~ msgstr ""

#~ msgid "align"
#~ msgstr ""

#~ msgid "New tensor with given diagonal values."
#~ msgstr ""

#~ msgid "unbiased"
#~ msgstr ""

#~ msgid "indexing"
#~ msgstr ""

#~ msgid "relay.Tuple([relay.Expr, relay.Expr])"
#~ msgstr ""

#~ msgid "The number of elements of input tensor."
#~ msgstr ""

#~ msgid ""
#~ "Returns a one-hot tensor where the"
#~ " locations represented by indices take "
#~ "value on_value, and other locations take"
#~ " value off_value. Final dimension is "
#~ "<indices outer dimensions> x depth x "
#~ "<indices inner dimensions>."
#~ msgstr ""

#~ msgid "on_value"
#~ msgstr ""

#~ msgid "off_value"
#~ msgstr ""

#~ msgid "depth"
#~ msgstr ""

#~ msgid "int or relay.Expr"
#~ msgstr ""

#~ msgid "The one-hot tensor."
#~ msgstr ""

#~ msgid "data type"
#~ msgstr ""

#~ msgid "The optimized relay module."
#~ msgstr ""

#~ msgid "dict"
#~ msgstr ""

#~ msgid "The parameters of the final graph."
#~ msgstr ""

#~ msgid "The reinterpreted result."
#~ msgstr ""

#~ msgid ""
#~ "Note: If the parameter allowzero is "
#~ "manually set to true, it specifies "
#~ "a special case where 0 actually "
#~ "means a true empty tensor."
#~ msgstr ""

#~ msgid "newshape"
#~ msgstr ""

#~ msgid "Union[int, Tuple[int], List[int]] or relay.Expr"
#~ msgstr ""

#~ msgid "allowzero"
#~ msgstr ""

#~ msgid "Bool, optional"
#~ msgstr ""

#~ msgid ""
#~ "If true, then treat zero as true"
#~ " empty tensor rather than a copy "
#~ "instruction."
#~ msgstr ""

#~ msgid "shape_like"
#~ msgstr ""

#~ msgid "lhs_begin"
#~ msgstr ""

#~ msgid "lhs_end"
#~ msgstr ""

#~ msgid "int or None, optional"
#~ msgstr ""

#~ msgid "rhs_begin"
#~ msgstr ""

#~ msgid "rhs_end"
#~ msgstr ""

#~ msgid "Union[int, Tuple[int], List[int]]"
#~ msgstr ""

#~ msgid "seq_lengths"
#~ msgstr ""

#~ msgid ""
#~ "A 1D Tensor with length "
#~ "a.dims[batch_axis]. Must be one of the"
#~ " following types: int32, int64. If "
#~ "seq_lengths[i] > a.dims[seq_axis], it is "
#~ "rounded to a.dims[seq_axis]. If seq_lengths[i]"
#~ " < 1, it is rounded to 1."
#~ msgstr ""

#~ msgid "seq_axis"
#~ msgstr ""

#~ msgid "batch_axis"
#~ msgstr ""

#~ msgid "The computed result of same shape and type as of input."
#~ msgstr ""

#~ msgid "s_type"
#~ msgstr ""

#~ msgid "tvm.relay.TensorType"
#~ msgstr ""

#~ msgid "The result type."
#~ msgstr ""

#~ msgid "updates"
#~ msgstr ""

#~ msgid "The axis to scatter elements on. It is zero by default."
#~ msgstr ""

#~ msgid "reduction"
#~ msgstr ""

#~ msgid ""
#~ "The reduction mode for scatter. Choise"
#~ " is from [\"update\", \"add\", \"mul\", "
#~ "\"mean\", \"min\", max\"] If update, the"
#~ " update values will replace the input"
#~ " data If add, the update values "
#~ "will be added to the input data"
#~ " If mul, the input data will be"
#~ " multiplied on the update values If"
#~ " mean, the input data will be "
#~ "mean between the update values and "
#~ "the input data If min, there is"
#~ " choice of minimal between the update"
#~ " values and the input data If "
#~ "max, there is choice of maximal "
#~ "between the update values and the "
#~ "input data It is \"update\" by "
#~ "default"
#~ msgstr ""

#~ msgid "mode"
#~ msgstr ""

#~ msgid ""
#~ "The accumulation mode for scatter. "
#~ "\"update\", \"add\", \"mul\", \"min\" or "
#~ "\"max\" If update, the update values "
#~ "will replace the input data If "
#~ "add, the update values will be "
#~ "added to the input data If mul,"
#~ " the update values will be multiply"
#~ " to the input data If min, "
#~ "there is choice of minimal between "
#~ "the update values and the input "
#~ "data If max, there is choice of"
#~ " maximal between the update values "
#~ "and the input data It is "
#~ "\"update\" by default"
#~ msgstr ""

#~ msgid "hybrid_func"
#~ msgstr ""

#~ msgid "function"
#~ msgstr ""

#~ msgid "A decorated hybrid script function."
#~ msgstr ""

#~ msgid "sorted_sequence"
#~ msgstr ""

#~ msgid "values"
#~ msgstr ""

#~ msgid "right"
#~ msgstr ""

#~ msgid ""
#~ "Tensor with same shape as values, "
#~ "representing the indices of elements of"
#~ " `values` if they are inserted in "
#~ "`sorted_sequence`."
#~ msgstr ""

#~ msgid "Input tensor. It can be of any type and multi-dimensional."
#~ msgstr ""

#~ msgid "segment_ids"
#~ msgstr ""

#~ msgid ""
#~ "A 1-D int32/int64 tensor containing the"
#~ " segment_ids of the rows to calculate"
#~ " the output sum upon. It defines "
#~ "a mapping from the zeroth dimension "
#~ "of data onto segment_ids. The "
#~ "segment_ids tensor should be the size"
#~ " of the first dimension, d0, with "
#~ "consecutive IDs in the range 0 to"
#~ " k, where k<d0. In particular, a "
#~ "segmentation of a matrix tensor is "
#~ "a mapping of rows to segments. "
#~ "This tensor doesn't need to be "
#~ "sorted."
#~ msgstr ""

#~ msgid "num_segments"
#~ msgstr ""

#~ msgid ""
#~ "An integer describing the shape of "
#~ "the zeroth dimension. If unspecified, it"
#~ " is calculated equivalent to the "
#~ "number of unique segment_ids."
#~ msgstr ""

#~ msgid "valid_length"
#~ msgstr ""

#~ msgid "mask_value"
#~ msgstr ""

#~ msgid "float, optional"
#~ msgstr ""

#~ msgid "The shape tensor."
#~ msgstr ""

#~ msgid ""
#~ "For an input array with shape "
#~ "``(d1, d2, ..., dk)``, `slice_like` "
#~ "operation slices the input array "
#~ "corresponding to the size of the "
#~ "second array. By default will slice "
#~ "on all axes."
#~ msgstr ""

#~ msgid "An array based on which shape, the result shape is computed."
#~ msgstr ""

#~ msgid "axes"
#~ msgstr ""

#~ msgid "Tuple[int] or List[int], optional"
#~ msgstr ""

#~ msgid "window_shape"
#~ msgstr ""

#~ msgid "List[int]"
#~ msgstr ""

#~ msgid "strides"
#~ msgstr ""

#~ msgid ""
#~ "Fill rows in a sparse matrix that"
#~ " do not contain any values. Values"
#~ " are placed in the first column "
#~ "of empty rows. The sparse array is"
#~ " in COO format. It returns a "
#~ "TupleWrapper with 3 outputs."
#~ msgstr ""

#~ msgid "sparse_indices"
#~ msgstr ""

#~ msgid ""
#~ "A 2-D tensor[N, ndims] of integers "
#~ "containing the locations of sparse "
#~ "values, where N is the number of"
#~ " sparse values and n_dim is the "
#~ "number of dimensions of the dense_shape."
#~ " The first column of this parameter"
#~ " must be sorted in ascending order."
#~ msgstr ""

#~ msgid "sparse_values"
#~ msgstr ""

#~ msgid "dense_shape"
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor[ndims] which contains the"
#~ " shape of the dense output tensor."
#~ msgstr ""

#~ msgid "default_value"
#~ msgstr ""

#~ msgid "new_sparse_indices"
#~ msgstr ""

#~ msgid ""
#~ "A 2-D tensor[?, ndims] of integers "
#~ "containing location of new sparse "
#~ "indices. The first column outputs must"
#~ " be sorted in ascending order."
#~ msgstr ""

#~ msgid "new_sparse_values"
#~ msgstr ""

#~ msgid "A 1-D tensor[?] containing the sparse values for the sparse indices."
#~ msgstr ""

#~ msgid "empty_row_indicator"
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor[dense_shape[0]] filled with "
#~ "zeros and ones indicating whether the"
#~ " particular row is empty or full "
#~ "respectively."
#~ msgstr ""

#~ msgid "Reshape a sparse tensor. The sparse array is in COO format."
#~ msgstr ""

#~ msgid ""
#~ "A 2-D tensor[N, n_dim] of integers "
#~ "containing location of sparse values, "
#~ "where N is the number of sparse"
#~ " values and n_dim is the number "
#~ "of dimensions of the dense_shape."
#~ msgstr ""

#~ msgid "prev_shape"
#~ msgstr ""

#~ msgid "A 1-D tensor containing the previous shape of the dense tensor."
#~ msgstr ""

#~ msgid "new_shape"
#~ msgstr ""

#~ msgid "A 1-D tensor containing the new shape of the dense tensor."
#~ msgstr ""

#~ msgid "output_shape"
#~ msgstr ""

#~ msgid "Dense tensor of shape output_shape. Has the same type as sparse_values."
#~ msgstr ""

#~ msgid "indices_or_sections"
#~ msgstr ""

#~ msgid "int or tuple of int"
#~ msgstr ""

#~ msgid "Indices or sections to split into. Accepts an int or a tuple."
#~ msgstr ""

#~ msgid "Union[None, int, Tuple[int], List[int]] or Expr"
#~ msgstr ""

#~ msgid ""
#~ "The set of axes to remove. If "
#~ "axis = None, remove all axes of"
#~ " dimension 1. If any specified axis"
#~ " has dimension that does not equal"
#~ " 1, it is an error."
#~ msgstr ""

#~ msgid "The squeezed result."
#~ msgstr ""

#~ msgid "Union(List[relay.Expr], relay.Expr)"
#~ msgstr ""

#~ msgid "The stacked tensor."
#~ msgstr ""

#~ msgid ""
#~ "The STFT computes the Fourier transform"
#~ " of short overlapping windows of the"
#~ " input. This gives frequency components "
#~ "of the signal as they change over"
#~ " time."
#~ msgstr ""

#~ msgid "Either a 1-D tensor or a 2-D batch tensor."
#~ msgstr ""

#~ msgid "n_fft"
#~ msgstr ""

#~ msgid "The size of Fourier transform."
#~ msgstr ""

#~ msgid "hop_length"
#~ msgstr ""

#~ msgid ""
#~ "The distance between neighboring sliding "
#~ "window frames. If is None, it is"
#~ " treated as equal to floor(n_fft /"
#~ " 4)."
#~ msgstr ""

#~ msgid "win_length"
#~ msgstr ""

#~ msgid ""
#~ "The size of window frame and STFT"
#~ " filter. If is None, it is "
#~ "treated as equal to n_fft."
#~ msgstr ""

#~ msgid "window"
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor window frame. If is"
#~ " None (default), it is treated as "
#~ "if having 1 everywhere in the "
#~ "window."
#~ msgstr ""

#~ msgid "normalized"
#~ msgstr ""

#~ msgid "Whether to return the normalized STFT results. Default value is False."
#~ msgstr ""

#~ msgid "onesided"
#~ msgstr ""

#~ msgid ""
#~ "Whether to return onesided result or "
#~ "fill with conjugate symmetry. Default "
#~ "value is True."
#~ msgstr ""

#~ msgid "output"
#~ msgstr ""

#~ msgid ""
#~ "Tensor containing the STFT result with"
#~ " shape [batch, N, T, 2], where "
#~ "N is the number of frequencies "
#~ "where STFT is applied and T is "
#~ "the total number of frames used."
#~ msgstr ""

#~ msgid "v"
#~ msgstr ""

#~ msgid "begin"
#~ msgstr ""

#~ msgid "relay.Expr, Tuple[int], or List[int]"
#~ msgstr ""

#~ msgid "end"
#~ msgstr ""

#~ msgid "strides: relay.Expr, Tuple[int], or List[int], optional"
#~ msgstr ""

#~ msgid ""
#~ "Specifies the stride values. It can "
#~ "be negative. In that case, the "
#~ "input tensor will be reversed in "
#~ "that particular axis."
#~ msgstr ""

#~ msgid "relay.Expr, Tuple[int], or List[int], optional"
#~ msgstr ""

#~ msgid "slice_mode"
#~ msgstr ""

#~ msgid ""
#~ "The slice mode [end, size]. end: "
#~ "The ending indices for the slice "
#~ "[default]. size: The input strides will"
#~ " be ignored. Input end in this "
#~ "mode indicates the size of a slice"
#~ " starting at the location specified "
#~ "by begin. If end[i] is -1, all "
#~ "remaining elements in that dimension are"
#~ " included in the slice."
#~ msgstr ""

#~ msgid "reps"
#~ msgstr ""

#~ msgid "int or relay.Expr, optional"
#~ msgstr ""

#~ msgid "ret_type: str, optional"
#~ msgstr ""

#~ msgid "relay.Expr or List[relay.Expr]"
#~ msgstr ""

#~ msgid "None or List[int]"
#~ msgstr ""

#~ msgid "The transposed result."
#~ msgstr ""

#~ msgid ""
#~ "The tensor that trilu will be "
#~ "applied to. Must be either a 2D"
#~ " matrix or a tensor of batches "
#~ "of 2D matrices."
#~ msgstr ""

#~ msgid ""
#~ "The number of diagonals above or "
#~ "below the main diagonal to exclude "
#~ "or include."
#~ msgstr ""

#~ msgid "upper: bool, optional"
#~ msgstr ""

#~ msgid ""
#~ "If True, only upper triangular values"
#~ " of input are kept, if False, "
#~ "the lower triangular values are kept."
#~ msgstr ""

#~ msgid "The new tensor with appropriate diagonals set to zero."
#~ msgstr ""

#~ msgid "is_sorted"
#~ msgstr ""

#~ msgid "return_counts"
#~ msgstr ""

#~ msgid "unique"
#~ msgstr ""

#~ msgid "A 1-D tensor containing the unique elements of the input data tensor."
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor containing the indeces "
#~ "of the first occurence of each "
#~ "unique value in the input tensor."
#~ msgstr ""

#~ msgid "inverse_indices"
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor. For each entry in"
#~ " data, it contains the index of "
#~ "that data element in the unique "
#~ "array."
#~ msgstr ""

#~ msgid "num_unique"
#~ msgstr ""

#~ msgid ""
#~ "A 1-D tensor with size=1 containing "
#~ "the number of unique elements in "
#~ "the input data tensor."
#~ msgstr ""

#~ msgid "counts"
#~ msgstr ""

#~ msgid "A 1-D tensor containing the count of each unique element in the output."
#~ msgstr ""

#~ msgid "The tuple of coordinate arrays."
#~ msgstr ""

#~ msgid "name_hint: str"
#~ msgstr ""

#~ msgid "type_annotation: Optional[tvm.relay.Type, str]"
#~ msgstr ""

#~ msgid "shape: Optional[List[tvm.Expr]]"
#~ msgstr ""

#~ msgid "with_mean"
#~ msgstr ""

#~ msgid "Optional[relay.Expr]"
#~ msgstr ""

#~ msgid "To compute variance given an already computed mean"
#~ msgstr ""

#~ msgid "x"
#~ msgstr ""

#~ msgid "y"
#~ msgstr ""

#~ msgid ""
#~ "The selected array. The output shape "
#~ "is the broadcasted shape from condition,"
#~ " x, and y."
#~ msgstr ""

