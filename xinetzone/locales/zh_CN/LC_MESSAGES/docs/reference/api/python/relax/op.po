# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-09-17 13:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../doc/docs/reference/api/python/relax/op.rst:19
#: ../../doc/docs/reference/api/python/relax/op.rst:22
msgid "tvm.relax.op"
msgstr ""

#: of tvm.relax.op:1
msgid "Relax core operators."
msgstr ""

#: of tvm.relax.op.unary.abs:1
msgid "Compute element-wise absolute value of the input data."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst of
#: tvm.relax.op.nn.nn.pixel_shuffle
msgid "参数"
msgstr ""

#: of tvm.relax.op.builtin.builtin.stop_lift_params:4 tvm.relax.op.nn.nn.gelu:8
#: tvm.relax.op.nn.nn.gelu_tanh:6 tvm.relax.op.nn.nn.leakyrelu:6
#: tvm.relax.op.nn.nn.relu:6 tvm.relax.op.nn.nn.relu6:6
#: tvm.relax.op.nn.nn.silu:6 tvm.relax.op.unary.abs:3 tvm.relax.op.unary.acos:3
#: tvm.relax.op.unary.acosh:3 tvm.relax.op.unary.asin:3
#: tvm.relax.op.unary.asinh:3 tvm.relax.op.unary.atan:3
#: tvm.relax.op.unary.atanh:3 tvm.relax.op.unary.bitwise_not:3
#: tvm.relax.op.unary.ceil:3 tvm.relax.op.unary.clip:3 tvm.relax.op.unary.cos:3
#: tvm.relax.op.unary.cosh:3 tvm.relax.op.unary.erf:3 tvm.relax.op.unary.exp:3
#: tvm.relax.op.unary.floor:3 tvm.relax.op.unary.isfinite:3
#: tvm.relax.op.unary.isinf:3 tvm.relax.op.unary.isnan:3
#: tvm.relax.op.unary.log:3 tvm.relax.op.unary.logical_not:3
#: tvm.relax.op.unary.negative:3 tvm.relax.op.unary.round:3
#: tvm.relax.op.unary.rsqrt:7 tvm.relax.op.unary.sigmoid:3
#: tvm.relax.op.unary.sign:3 tvm.relax.op.unary.sin:3 tvm.relax.op.unary.sinh:3
#: tvm.relax.op.unary.sqrt:3 tvm.relax.op.unary.square:3
#: tvm.relax.op.unary.tan:3 tvm.relax.op.unary.tanh:3
msgid "The input data"
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst of
#: tvm.relax.op.nn.nn.pixel_shuffle
msgid "返回"
msgstr ""

#: of tvm.relax.op.binary.add:8 tvm.relax.op.binary.bitwise_and:7
#: tvm.relax.op.binary.bitwise_or:7 tvm.relax.op.binary.bitwise_xor:7
#: tvm.relax.op.binary.divide:8 tvm.relax.op.binary.equal:8
#: tvm.relax.op.binary.floor_divide:8 tvm.relax.op.binary.greater:8
#: tvm.relax.op.binary.greater_equal:8 tvm.relax.op.binary.left_shift:7
#: tvm.relax.op.binary.less:8 tvm.relax.op.binary.less_equal:8
#: tvm.relax.op.binary.logical_and:7 tvm.relax.op.binary.logical_or:7
#: tvm.relax.op.binary.logical_xor:7 tvm.relax.op.binary.maximum:8
#: tvm.relax.op.binary.minimum:8 tvm.relax.op.binary.multiply:8
#: tvm.relax.op.binary.not_equal:8 tvm.relax.op.binary.power:8
#: tvm.relax.op.binary.right_shift:7 tvm.relax.op.binary.subtract:8
#: tvm.relax.op.linear_algebra.linear:18 tvm.relax.op.linear_algebra.matmul:14
#: tvm.relax.op.manipulate.one_hot:14 tvm.relax.op.nn.nn.adaptive_avg_pool1d:33
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:36
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:36 tvm.relax.op.nn.nn.avg_pool1d:34
#: tvm.relax.op.nn.nn.avg_pool2d:42 tvm.relax.op.nn.nn.avg_pool3d:35
#: tvm.relax.op.nn.nn.batch_norm:81 tvm.relax.op.nn.nn.conv1d:50
#: tvm.relax.op.nn.nn.conv1d_transpose:38 tvm.relax.op.nn.nn.conv2d:50
#: tvm.relax.op.nn.nn.conv2d_transpose:48 tvm.relax.op.nn.nn.conv3d:52
#: tvm.relax.op.nn.nn.cross_entropy_with_logits:16 tvm.relax.op.nn.nn.gelu:11
#: tvm.relax.op.nn.nn.gelu_tanh:9 tvm.relax.op.nn.nn.group_norm:25
#: tvm.relax.op.nn.nn.instance_norm:18 tvm.relax.op.nn.nn.layer_norm:34
#: tvm.relax.op.nn.nn.leakyrelu:12 tvm.relax.op.nn.nn.log_softmax:17
#: tvm.relax.op.nn.nn.max_pool1d:34 tvm.relax.op.nn.nn.max_pool2d:41
#: tvm.relax.op.nn.nn.max_pool3d:35 tvm.relax.op.nn.nn.nll_loss:25
#: tvm.relax.op.nn.nn.pad:21 tvm.relax.op.nn.nn.prelu:14
#: tvm.relax.op.nn.nn.relu:9 tvm.relax.op.nn.nn.relu6:9
#: tvm.relax.op.nn.nn.rms_norm:19 tvm.relax.op.nn.nn.selu:14
#: tvm.relax.op.nn.nn.silu:9 tvm.relax.op.nn.nn.softmax:12
#: tvm.relax.op.nn.nn.softplus:13 tvm.relax.op.qdq.dequantize:18
#: tvm.relax.op.qdq.quantize:18 tvm.relax.op.search.argmax:14
#: tvm.relax.op.search.argmin:14 tvm.relax.op.statistical.max:14
#: tvm.relax.op.statistical.mean:14 tvm.relax.op.statistical.min:14
#: tvm.relax.op.statistical.prod:14 tvm.relax.op.statistical.std:14
#: tvm.relax.op.statistical.sum:14 tvm.relax.op.statistical.variance:14
#: tvm.relax.op.ternary.ewise_fma:11 tvm.relax.op.unary.abs:6
#: tvm.relax.op.unary.acos:6 tvm.relax.op.unary.acosh:6
#: tvm.relax.op.unary.asin:6 tvm.relax.op.unary.asinh:6
#: tvm.relax.op.unary.atan:6 tvm.relax.op.unary.atanh:6
#: tvm.relax.op.unary.bitwise_not:6 tvm.relax.op.unary.ceil:6
#: tvm.relax.op.unary.clip:10 tvm.relax.op.unary.cos:6
#: tvm.relax.op.unary.cosh:6 tvm.relax.op.unary.exp:6
#: tvm.relax.op.unary.floor:6 tvm.relax.op.unary.isfinite:6
#: tvm.relax.op.unary.isinf:6 tvm.relax.op.unary.isnan:6
#: tvm.relax.op.unary.log:6 tvm.relax.op.unary.logical_not:6
#: tvm.relax.op.unary.round:6 tvm.relax.op.unary.rsqrt:10
#: tvm.relax.op.unary.sigmoid:6 tvm.relax.op.unary.sign:6
#: tvm.relax.op.unary.sin:6 tvm.relax.op.unary.sinh:6 tvm.relax.op.unary.sqrt:6
#: tvm.relax.op.unary.square:6 tvm.relax.op.unary.tan:6
#: tvm.relax.op.unary.tanh:6 tvm.relax.op.unary.trunc:5
msgid "**result** -- The computed result."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst of
#: tvm.relax.op.nn.nn.pixel_shuffle
msgid "返回类型"
msgstr ""

#: of tvm.relax.op.unary.acos:1
msgid "Compute element-wise arc cos of the input data."
msgstr ""

#: of tvm.relax.op.nn.nn.gelu:14 tvm.relax.op.nn.nn.gelu_tanh:12
#: tvm.relax.op.nn.nn.silu:12 tvm.relax.op.nn.nn.softmax:15
#: tvm.relax.op.unary.acos:9 tvm.relax.op.unary.acosh:9
#: tvm.relax.op.unary.asin:9 tvm.relax.op.unary.asinh:9
#: tvm.relax.op.unary.atan:9 tvm.relax.op.unary.atanh:9
#: tvm.relax.op.unary.cos:9 tvm.relax.op.unary.cosh:9 tvm.relax.op.unary.exp:9
#: tvm.relax.op.unary.log:9 tvm.relax.op.unary.rsqrt:13
#: tvm.relax.op.unary.sigmoid:9 tvm.relax.op.unary.sin:9
#: tvm.relax.op.unary.sinh:9 tvm.relax.op.unary.sqrt:9 tvm.relax.op.unary.tan:9
#: tvm.relax.op.unary.tanh:9
msgid "The input tensor is required to have float dtype"
msgstr ""

#: of tvm.relax.op.unary.acosh:1
msgid "Compute element-wise arc cosh of the input data."
msgstr ""

#: of tvm.relax.op.binary.add:1
msgid "Addition with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.binary.add:3 tvm.relax.op.binary.divide:3
#: tvm.relax.op.binary.equal:3 tvm.relax.op.binary.floor_divide:3
#: tvm.relax.op.binary.floor_mod:3 tvm.relax.op.binary.greater:3
#: tvm.relax.op.binary.greater_equal:3 tvm.relax.op.binary.less:3
#: tvm.relax.op.binary.less_equal:3 tvm.relax.op.binary.log_add_exp:3
#: tvm.relax.op.binary.maximum:3 tvm.relax.op.binary.minimum:3
#: tvm.relax.op.binary.mod:3 tvm.relax.op.binary.multiply:3
#: tvm.relax.op.binary.not_equal:3 tvm.relax.op.binary.power:3
#: tvm.relax.op.binary.subtract:3 tvm.relax.op.linear_algebra.matmul:6
msgid "The first input tensor."
msgstr ""

#: of tvm.relax.op.binary.add:5 tvm.relax.op.binary.divide:5
#: tvm.relax.op.binary.equal:5 tvm.relax.op.binary.floor_divide:5
#: tvm.relax.op.binary.floor_mod:5 tvm.relax.op.binary.greater:5
#: tvm.relax.op.binary.greater_equal:5 tvm.relax.op.binary.less:5
#: tvm.relax.op.binary.less_equal:5 tvm.relax.op.binary.log_add_exp:5
#: tvm.relax.op.binary.maximum:5 tvm.relax.op.binary.minimum:5
#: tvm.relax.op.binary.mod:5 tvm.relax.op.binary.multiply:5
#: tvm.relax.op.binary.not_equal:5 tvm.relax.op.binary.power:5
#: tvm.relax.op.binary.subtract:5 tvm.relax.op.linear_algebra.matmul:8
msgid "The second input tensor."
msgstr ""

#: of tvm.relax.op.binary.add:12 tvm.relax.op.manipulate.flip:12
#: tvm.relax.op.manipulate.gather_elements:14
#: tvm.relax.op.manipulate.gather_nd:14 tvm.relax.op.manipulate.index_put:19
#: tvm.relax.op.manipulate.index_tensor:33 tvm.relax.op.manipulate.one_hot:18
#: tvm.relax.op.manipulate.repeat:16
#: tvm.relax.op.manipulate.scatter_elements:37
#: tvm.relax.op.manipulate.scatter_nd:17 tvm.relax.op.manipulate.tile:21
#: tvm.relax.op.nn.nn.pixel_shuffle:20
#: tvm.relax.op.sampling.multinomial_from_uniform:31
#: tvm.relax.op.set.nonzero:12 tvm.relax.op.statistical.cumprod:21
#: tvm.relax.op.statistical.cumsum:21
msgid "示例"
msgstr ""

#: of tvm.relax.op.create.arange:1
msgid "Construct a tensor with evenly spaced elements."
msgstr ""

#: of tvm.relax.op.create.arange:3
msgid "The start of the interval."
msgstr ""

#: of tvm.relax.op.create.arange:5
msgid ""
"The end of the interval. If not given, it will be set to start, and start"
" will be set to 0."
msgstr ""

#: of tvm.relax.op.create.arange:8
msgid "The step size."
msgstr ""

#: of tvm.relax.op.create.arange:10 tvm.relax.op.create.eye:11
#: tvm.relax.op.create.ones:5 tvm.relax.op.create.zeros:5
#: tvm.relax.op.op_attrs.InitAttrs.dtype:1
msgid "The data type of the created tensor."
msgstr ""

#: of tvm.relax.op.create.arange:13 tvm.relax.op.create.eye:14
#: tvm.relax.op.create.eye_like:15 tvm.relax.op.create.full:11
#: tvm.relax.op.create.full_like:14 tvm.relax.op.create.ones:8
#: tvm.relax.op.create.ones_like:10 tvm.relax.op.create.zeros:8
#: tvm.relax.op.create.zeros_like:10 tvm.relax.op.search.where:18
msgid "**result** -- The result tensor."
msgstr ""

#: of tvm.relax.op.search.argmax:1
msgid "Computes the argmax of tensor elements over given axis."
msgstr ""

#: of tvm.relax.op.search.argmax:3 tvm.relax.op.search.argmin:3
#: tvm.relax.op.statistical.max:3 tvm.relax.op.statistical.mean:3
#: tvm.relax.op.statistical.min:3 tvm.relax.op.statistical.prod:3
#: tvm.relax.op.statistical.std:3 tvm.relax.op.statistical.sum:3
#: tvm.relax.op.statistical.variance:3
msgid "The input data tensor"
msgstr ""

#: of tvm.relax.op.search.argmax:5
msgid ""
"Axis along which an argmax operation is performed. The default, "
"axis=None, will compute the argmax of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.search.argmax:9 tvm.relax.op.search.argmin:9
msgid ""
"If this is set to True, the axis being reduced is left in the result as "
"dimensions with size one. With this option, the result will broadcast "
"correctly against the input tensor."
msgstr ""

#: of tvm.relax.op.search.argmin:1
msgid "Computes the argmin of tensor elements over given axis."
msgstr ""

#: of tvm.relax.op.search.argmin:5
msgid ""
"Axis along which an argmin operation is performed. The default, "
"axis=None, will compute the argmin of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.sorting.argsort:1
msgid ""
"Performs sorting along the given axis and returns an array of indices "
"having same shape as an input array that index data in sorted order."
msgstr ""

#: of tvm.relax.op.set.nonzero:3 tvm.relax.op.sorting.argsort:4
#: tvm.relax.op.sorting.topk:5
msgid "The input data tensor."
msgstr ""

#: of tvm.relax.op.sorting.argsort:6 tvm.relax.op.sorting.topk:9
msgid "Axis long which to sort the input tensor."
msgstr ""

#: of tvm.relax.op.sorting.argsort:8 tvm.relax.op.sorting.sort:9
msgid "Whether to sort in descending order, the default is False"
msgstr ""

#: of tvm.relax.op.sorting.argsort:10
msgid "The data type of the output indices."
msgstr ""

#: of tvm.relax.op.sorting.argsort:13
msgid "**out** -- Tensor with same shape as data."
msgstr ""

#: of tvm.relax.op.unary.asin:1
msgid "Compute element-wise arc sin of the input data."
msgstr ""

#: of tvm.relax.op.unary.asinh:1
msgid "Compute element-wise arc sinh of the input data."
msgstr ""

#: of tvm.relax.op.base.assert_op:1
msgid ""
"Create a call to Relax's assert_op operation (`assert` is reserved in "
"Python, so the name must be distinct)."
msgstr ""

#: of tvm.relax.op.base.assert_op:4
msgid "The assertion condition."
msgstr ""

#: of tvm.relax.op.base.assert_op:6
msgid "Format arguments for the error message if the condition fails."
msgstr ""

#: of tvm.relax.op.base.assert_op:8
msgid "The format string or StringImm for the error message."
msgstr ""

#: of tvm.relax.op.base.assert_op:11
msgid "**result** -- A Call to the Relax assert operation."
msgstr ""

#: of tvm.relax.op.datatype.astype:1
msgid "Cast input tensor to the given data type."
msgstr ""

#: of tvm.relax.op.datatype.astype:3 tvm.relax.op.image.image.resize2d:11
#: tvm.relax.op.manipulate.broadcast_to:3 tvm.relax.op.manipulate.expand_dims:3
#: tvm.relax.op.manipulate.flatten:3 tvm.relax.op.manipulate.flip:3
#: tvm.relax.op.manipulate.gather_elements:3
#: tvm.relax.op.manipulate.gather_nd:3 tvm.relax.op.manipulate.permute_dims:3
#: tvm.relax.op.manipulate.reshape:13
#: tvm.relax.op.manipulate.scatter_elements:21
#: tvm.relax.op.manipulate.squeeze:3 tvm.relax.op.manipulate.tile:12
#: tvm.relax.op.memory.view.view:11 tvm.relax.op.nn.nn.adaptive_avg_pool1d:22
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:25
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:25 tvm.relax.op.nn.nn.avg_pool1d:14
#: tvm.relax.op.nn.nn.avg_pool2d:22 tvm.relax.op.nn.nn.avg_pool3d:15
#: tvm.relax.op.nn.nn.batch_norm:55 tvm.relax.op.nn.nn.conv1d:26
#: tvm.relax.op.nn.nn.conv1d_transpose:12 tvm.relax.op.nn.nn.conv2d:26
#: tvm.relax.op.nn.nn.conv2d_transpose:22 tvm.relax.op.nn.nn.conv3d:28
#: tvm.relax.op.nn.nn.dropout:7 tvm.relax.op.nn.nn.log_softmax:10
#: tvm.relax.op.nn.nn.max_pool1d:14 tvm.relax.op.nn.nn.max_pool2d:21
#: tvm.relax.op.nn.nn.max_pool3d:15 tvm.relax.op.nn.nn.softmax:5
#: tvm.relax.op.statistical.cumprod:4 tvm.relax.op.statistical.cumsum:4
msgid "The input data to the operator."
msgstr ""

#: of tvm.relax.op.datatype.astype:5
msgid "The target data type"
msgstr ""

#: of tvm.relax.op.datatype.astype:8 tvm.relax.op.datatype.wrap_param:8
msgid "**result** -- The casted result."
msgstr ""

#: of tvm.relax.op.unary.atan:1
msgid "Compute element-wise arc tan of the input data."
msgstr ""

#: of tvm.relax.op.unary.atanh:1
msgid "Compute element-wise arc tanh of the input data."
msgstr ""

#: of tvm.relax.op.binary.bitwise_and:1
msgid ""
"Bitwise AND :param x1: The first input tensor. :type x1: relax.Expr "
":param x2: The second input tensor. :type x2: relax.Expr"
msgstr ""

#: of tvm.relax.op.unary.bitwise_not:1
msgid "Compute bitwise NOT of the input data."
msgstr ""

#: of tvm.relax.op.binary.bitwise_or:1
msgid ""
"Bitwise OR :param x1: The first input tensor. :type x1: relax.Expr :param"
" x2: The second input tensor. :type x2: relax.Expr"
msgstr ""

#: of tvm.relax.op.binary.bitwise_xor:1
msgid ""
"Bitwise XOR :param x1: The first input tensor. :type x1: relax.Expr "
":param x2: The second input tensor. :type x2: relax.Expr"
msgstr ""

#: of tvm.relax.op.manipulate.broadcast_to:1
msgid "Broadcasts a tensor to a specified shape."
msgstr ""

#: of tvm.relax.op.manipulate.broadcast_to:5
msgid "The target shape."
msgstr ""

#: of tvm.relax.op.manipulate.broadcast_to:8
msgid "**result** -- The broadcasted tensor."
msgstr ""

#: of tvm.relax.op.search.bucketize:1
msgid ""
"Returns the indices of the buckets to which each value in the input "
"belongs."
msgstr ""

#: of tvm.relax.op.search.bucketize:3
msgid "N-D tensor containing the search values."
msgstr ""

#: of tvm.relax.op.search.bucketize:5
msgid ""
"1-D tensor, must contain a strictly increasing sequence, or the return "
"value is undefined."
msgstr ""

#: of tvm.relax.op.search.bucketize:7
msgid ""
"Indicate the output data type. int32 if True, int64 otherwise. "
"Default=False"
msgstr ""

#: of tvm.relax.op.search.bucketize:9
msgid ""
"Determines the behavior for values in boundaries. Similar to "
"torch.bucketize"
msgstr ""

#: of tvm.relax.op.search.bucketize:12
msgid "**result** -- The computed result with same shape as input_tensor."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:1
msgid "Call a builtin function func."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:3
msgid "The builtin function to be called."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:5
#: tvm.relax.op.base.call_dps_packed:9 tvm.relax.op.base.call_tir:5
#: tvm.relax.op.base.call_tir_inplace:15 tvm.relax.op.base.call_tir_with_grad:7
#: tvm.relax.op.base.invoke_closure:5 tvm.relax.op.base.invoke_pure_closure:11
#: tvm.relax.op.base.make_closure:5
#: tvm.relax.op.distributed.distributed.call_tir_local_view:7
msgid "The input arguments."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:7
msgid "The struct info arguments to the call node."
msgstr ""

#: of tvm.relax.op.base.call_builtin_with_ctx:10 tvm.relax.op.base.null_value:3
msgid "**ret** -- The created call node."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:1
msgid "Call a destination-passing-style packed function and return the output."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:3
msgid ""
"Note: The called function is assumed to be _pure_ (other than modifying "
"the designated output arguments). If the function _does_ result in other "
"side effects, then the compiler may end up removing, reordering, or "
"repeating those effects--no guarantees can be made."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:7
msgid "The destination-passing-style function, can be ExternFunc."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:11
msgid ""
"The structure info of the call_dps_packed output. It should be a single "
"or a list of TensorStructInfo. Each one denotes the structure info of a "
"returned tensor."
msgstr ""

#: of tvm.relax.op.base.call_dps_packed:16
msgid "**ret** -- A call node for the call_dps_packed operator."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:1
msgid ""
"Construct a call to a packed function that consumes some of its arguments"
" \"in-place\" and returns the mutated arguments (aliased), but should be "
"considered to be otherwise pure. The `inplace_indices` argument indicates"
" which of the outputs are mutated arguments."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:5
#: tvm.relax.op.base.call_pure_packed:4
msgid ""
"The resulting call will have the same semantics as calling the packed "
"function directly."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:7
msgid ""
"Note: This should be used for cases when the user knows that calling the "
"packed function with these arguments will **in reality** not cause any "
"other side effects. If it is used for a call that **does** result in "
"other side effects, then the compiler may end up removing, reordering, or"
" repeating that call, with no guarantees made about any side effects from"
" the callee."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:13
msgid ""
"Warning: This operator as treated as pure by the type system even though "
"it *is* performing side effects (mutating some arguments). It is "
"therefore incumbent upon the user to ensure that it is being used safely "
"(viz., that mutated arguments are not live after the mutation, that they "
"do not alias values live after the mutation)."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:18
#: tvm.relax.op.base.call_pure_packed:12
msgid "The name (global symbol) for a PackedFunc or an ExternFunc node."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:20
#: tvm.relax.op.base.call_pure_packed:14
msgid "The arguments for the PackedFunc."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:22
#: tvm.relax.op.base.call_tir_inplace:17
msgid ""
"Specify which arguments should be used for in-place computations. If "
"`inplace_indices` is a single integer, it will be made into a singleton "
"list. Suppose `inplace_indices[i] = j`, where `j >= 0`. Then the `i`th "
"output will be an alias of `args[j]`. If `inplace_indices[i] = -1`, then "
"the `i`th output will be a freshly allocated tensor. At least one member "
"of `inplace_indices` must not be -1."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:29
#: tvm.relax.op.base.call_pure_packed:16
msgid ""
"The list of structure info arguments (giving the structural info for the "
"returned value)."
msgstr ""

#: of tvm.relax.op.base.call_inplace_packed:32
#: tvm.relax.op.base.call_pure_packed:19
msgid ""
"**result** -- A Relax call, corresponding to "
"`call_pure_packed(ExternFunc(func), args, DictAttrs(kwargs), sinfo_args)`"
msgstr ""

#: of tvm.relax.op.base.call_pure_packed:1
msgid ""
"Construct a call to a packed function that should be treated as pure, "
"even though packed calls are normally not treated as pure."
msgstr ""

#: of tvm.relax.op.base.call_pure_packed:6
msgid ""
"Note: This should be used for cases when the user knows that calling the "
"packed function with these arguments will **in reality** not cause any "
"side effects. If it is used for a call that **does** result in side "
"effects, then the compiler may end up removing, reordering, or repeating "
"that call, with no guarantees made about any side effects from the "
"callee."
msgstr ""

#: of tvm.relax.op.base.call_tir:1
msgid "Call a tir.prim_func and return the output."
msgstr ""

#: of tvm.relax.op.base.call_tir:3 tvm.relax.op.base.call_tir_with_grad:5
#: tvm.relax.op.distributed.distributed.call_tir_local_view:5
msgid "The GlobalVar referring to a tir PrimFunc."
msgstr ""

#: of tvm.relax.op.base.call_tir:7
msgid ""
"The structure info of the call_tir output. It should be a single or a "
"list of TensorStructInfo. Each one denotes the structure info of a "
"returned tensor."
msgstr ""

#: of tvm.relax.op.base.call_tir:11 tvm.relax.op.base.call_tir_inplace:29
#: tvm.relax.op.base.call_tir_with_grad:19
#: tvm.relax.op.distributed.distributed.call_tir_local_view:13
msgid ""
"ShapeExpr representing a tuple of integers to unpack when calling func. "
"Is null if not used"
msgstr ""

#: of tvm.relax.op.base.call_tir:14 tvm.relax.op.base.call_tir_inplace:32
msgid "**ret** -- A call node for the call_tir operator."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:1
msgid ""
"Call a TIR PrimFunc and return the result, doing the specified "
"computations in-place (based on the `inplace_indices` argument; outputs "
"will alias the inputs selected by in-place indices)."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:5
msgid ""
"Warning: This operator is considered pure by the type system but actually"
" mutates the arguments specified by `inplace_indices`. This operator "
"should not be used directly, but rather should be inserted by passes that"
" have checked whether it is safe to perform operations in-place (i.e., "
"none of the arguments specified as an output is aliased or is live after "
"calling call_tir_inplace)."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:11
msgid "Direct calls to this operator should be done for testing purposes only."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:13
msgid "The GlobalVar referring to a TIR PrimFunc."
msgstr ""

#: of tvm.relax.op.base.call_tir_inplace:24
msgid ""
"The structure info of the call_tir_inplace output. It should be a single "
"`TensorStructInfo` or a list of `TensorStructInfo`. Each one denotes the "
"structure info of a returned tensor. If a list of `TensorStructInfo` is "
"given, the result will be a tuple of `TensorStructInfo`."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:1
msgid ""
"Call a tir.prim_func and return the output. This intrinsic will bind a te"
" gradient function (refered by te_grad_name) to the call_tir_with_grad "
"node. The te gradient function will be called by the Gradient pass."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:9
msgid ""
"The structure info of the call_tir_with_grad output. It should be a "
"single or a list of TensorStructInfo. Each one denotes the structure info"
" of a returned tensor."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:13
msgid ""
"The registered name of the te gradient function associated with the "
"call_tir_with_grad node. Must be provided as a keyword argument."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:16
#, python-brace-format
msgid ""
"The keyword arguments passed to the te gradient function. Optionally "
"provided as a keyword argument. Default: {}."
msgstr ""

#: of tvm.relax.op.base.call_tir_with_grad:22
msgid "**ret** -- A call node for the call_tir_with_grad operator."
msgstr ""

#: of tvm.relax.op.unary.ceil:1
msgid "Take ceil of input data."
msgstr ""

#: of tvm.relax.op.unary.clip:1
msgid "Clips tensor values to a specified min and max."
msgstr ""

#: of tvm.relax.op.unary.clip:5
msgid "The minimum value"
msgstr ""

#: of tvm.relax.op.unary.clip:7
msgid "The maximum value"
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_like:1
msgid "Return a summation of data to the shape of collapse_target."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_like:3
msgid "For details, please see relax.op.collapse_sum_to."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:3 tvm.relax.op.ccl.ccl.allreduce:3
#: tvm.relax.op.distributed.distributed.annotate_sharding:3
#: tvm.relax.op.distributed.distributed.redistribute:3
#: tvm.relax.op.manipulate.collapse_sum_like:5
#: tvm.relax.op.manipulate.collapse_sum_to:14 tvm.relax.op.manipulate.repeat:3
#: tvm.relax.op.nn.nn.prelu:6 tvm.relax.op.set.unique:7
#: tvm.relax.op.sorting.sort:4
msgid "The input tensor."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_like:7
msgid "The tensor whose shape is the shape to collapse to."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_like:10
msgid "**result** -- The result tensor after summation."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:1
msgid "Return a summation of data to the given shape."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:3
msgid ""
"collapse_sum_to is intended as the backward operator of "
"tvm.relax.op.broadcast_to and other broadcast operators in the automatic "
"differentiation process."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:6
msgid ""
"We expect that data is the result of broadcasting some tensor of the "
"given shape in some broadcast operation. Thus the given `shape` and "
"`data.shape` must follow broadcast rules."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:9
msgid ""
"During computation, all axes of `data.shape` and `shape` are checked from"
" right to left. For an axis, if it follows these rules, `data` will be "
"summed over this axis: - the axis exists in `data.shape` but not in "
"`shape`, or - the axis exists in `data.shape` and equals to 1 in `shape`."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:16
msgid "The shape to collapse to."
msgstr ""

#: of tvm.relax.op.manipulate.collapse_sum_to:19
msgid "**result** -- The result tensor of the given shape after summation."
msgstr ""

#: of tvm.relax.op.manipulate.concat:1
msgid "Concatenate the input tensors along the given axis."
msgstr ""

#: of tvm.relax.op.manipulate.concat:3
msgid ""
"An Expr in Tuple type, containing the tensors to be concatenated, or a "
"list of Tensors."
msgstr ""

#: of tvm.relax.op.manipulate.concat:6
msgid ""
"The axis along which the tensors are concatenated. If `axis` is `None`, "
"the input tensor is required to be flattened before concatenation."
msgstr ""

#: of tvm.relax.op.manipulate.concat:10
msgid "**result** -- The concatenated tensor."
msgstr ""

#: of tvm.relax.op.unary.cos:1
msgid "Compute element-wise cos of the input data."
msgstr ""

#: of tvm.relax.op.unary.cosh:1
msgid "Compute element-wise cosh of the input data."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:1
msgid ""
"Numpy style cumprod op. Return the cumulative product of the elements "
"along a given axis."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:6
msgid ""
"Axis along which the cumulative product is computed. The default (None) "
"is to compute the cumprod over the flattened array."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:9
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are computed. If dtype is not specified, it defaults to the dtype of "
"data."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:12
msgid ""
"If false (default), all elements are included in the product.  If true, "
"the first element is excluded from the product."
msgstr ""

#: of tvm.relax.op.statistical.cumprod:16 tvm.relax.op.statistical.cumsum:16
msgid ""
"**result** -- The result has the same size as data, and the same shape as"
" data if axis is not None. If axis is None, the result is a 1-d array."
msgstr ""

#: of tvm.relax.op.statistical.cumsum:1
msgid ""
"Numpy style cumsum op. Return the cumulative inclusive sum of the "
"elements along a given axis."
msgstr ""

#: of tvm.relax.op.statistical.cumsum:6
msgid ""
"Axis along which the cumulative sum is computed. The default (None) is to"
" compute the cumsum over the flattened array."
msgstr ""

#: of tvm.relax.op.statistical.cumsum:9
msgid ""
"Type of the returned array and of the accumulator in which the elements "
"are summed. If dtype is not specified, it defaults to the dtype of data."
msgstr ""

#: of tvm.relax.op.statistical.cumsum:12
msgid ""
"If false (default), all elements are included in the sum.  If true, the "
"first element is excluded from the sum."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:1
msgid ""
"Dequantize op This operator takes input and produces dequantized output. "
"The input tensor can be of any shape. The output shape is the same as "
"input shape."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:5
msgid ""
"output = clamp(scale * (input_tensor - zero_point), out_dtype::min, "
"out_dtype::max)"
msgstr ""

#: of tvm.relax.op.qdq.dequantize:7
msgid "The input tensor to be dequantized."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:9
msgid "The input scale."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:11
msgid "The input zero_point."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:13
msgid ""
"The channel axis for dequantization. Default value is -1 which "
"corresponds to the last axis."
msgstr ""

#: of tvm.relax.op.qdq.dequantize:15 tvm.relax.op.qdq.quantize:15
#: tvm.relax.op.sampling.multinomial_from_uniform:24
msgid "The data type of the output tensor."
msgstr ""

#: of tvm.relax.op.binary.divide:1
msgid "Division with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:1
msgid ""
"Dynamic strided slice of a tensor. `begin`, `end`, `strides` can be "
"computed at runtime."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:3
#: tvm.relax.op.index.strided_slice:3
msgid "The source tensor to be sliced."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:5
#: tvm.relax.op.index.strided_slice:7
msgid "The indices to begin with in the slicing, inclusive."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:7
#: tvm.relax.op.index.strided_slice:9
msgid "The indices indicating end of the slice, exclusive."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:9
#: tvm.relax.op.index.strided_slice:11
msgid ""
"Specifies the stride values, it can be negative in that case, the input "
"tensor will be reversed in that particular axis. If not specified, it by "
"default is an list of ones of the same length as `axes`."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:14
#: tvm.relax.op.index.strided_slice:19
msgid "**ret** -- The sliced result."
msgstr ""

#: of tvm.relax.op.index.dynamic_strided_slice:19
msgid ""
"dyn_strided_slice require the input `begin`, `end` and `strides` to have "
"the same length as rank of `data` tensor."
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:1
msgid "Evaluates the Einstein summation convention on data"
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:3
msgid "A list of expression."
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:5
msgid "The einsum expression string."
msgstr ""

#: of tvm.relax.op.linear_algebra.einsum:8
msgid "**result** -- The output from the einsum op."
msgstr ""

#: of tvm.relax.op.binary.equal:1
msgid "Broadcasted element-wise test for (lhs == rhs)."
msgstr ""

#: of tvm.relax.op.unary.erf:1
msgid "Computes the error function of the input."
msgstr ""

#: of tvm.relax.op.unary.erf:6
msgid "**result** -- Computed error function for each element."
msgstr ""

#: of tvm.relax.op.ternary.ewise_fma:1
msgid ""
"Elementwise fused multiply-add operator Returns elementwise result of "
":math:`x1 * x2 + x3`"
msgstr ""

#: of tvm.relax.op.ternary.ewise_fma:4
msgid "The left hand operand of the multiplication"
msgstr ""

#: of tvm.relax.op.ternary.ewise_fma:6
msgid "The right hand operand of the multiplication"
msgstr ""

#: of tvm.relax.op.ternary.ewise_fma:8
msgid "The operand of the addition"
msgstr ""

#: of tvm.relax.op.unary.exp:1
msgid "Compute element-wise exp of data."
msgstr ""

#: of tvm.relax.op.manipulate.expand_dims:1
msgid "Insert new axes at the positions given by `axis`."
msgstr ""

#: of tvm.relax.op.manipulate.expand_dims:5
#: tvm.relax.op.op_attrs.ExpandDimsAttrs.axis:1
msgid ""
"The axes at which the input array are expanded. All values are required "
"to lie in range `[-data.ndim - 1, data.ndim]`, with the convention of "
"negative indexing."
msgstr ""

#: of tvm.relax.op.manipulate.expand_dims:10
msgid "**result** -- The transformed result."
msgstr ""

#: of tvm.relax.op.create.eye:1
msgid "Construct a 2-D tensor with ones on the diagonal and zeros elsewhere."
msgstr ""

#: of tvm.relax.op.create.eye:3
msgid "Number of rows in the output."
msgstr ""

#: of tvm.relax.op.create.eye:5
msgid "Number of columns in the output. If None, defaults to n."
msgstr ""

#: of tvm.relax.op.create.eye:7 tvm.relax.op.create.eye_like:7
msgid ""
"Index of the diagonal: 0 (the default) refers to the main diagonal, a "
"positive value refers to an upper diagonal, and a negative value to a "
"lower diagonal."
msgstr ""

#: of tvm.relax.op.create.eye_like:1
msgid ""
"Return a 2-D tensor with ones on the diagonal and zeros elsewhere, with "
"the same shape as the input tensor."
msgstr ""

#: of tvm.relax.op.create.eye_like:4 tvm.relax.op.create.full_like:5
#: tvm.relax.op.create.ones_like:3 tvm.relax.op.create.zeros_like:3
msgid ""
"The input tensor, which provides the shape, and dtype when the `dtype` "
"field is not specified."
msgstr ""

#: of tvm.relax.op.create.eye_like:11 tvm.relax.op.create.full_like:10
#: tvm.relax.op.create.ones_like:6 tvm.relax.op.create.zeros_like:6
msgid ""
"The data type of the created tensor. If dtype is not given, it will by "
"default use the dtype of the input tensor."
msgstr ""

#: of tvm.relax.op.manipulate.flatten:1
msgid "Flatten all the tensor dimensions into one."
msgstr ""

#: of tvm.relax.op.manipulate.flatten:6
msgid "**result** -- The flattened result."
msgstr ""

#: of tvm.relax.op.manipulate.flip:1
msgid ""
"Reverses the order of elements along given axis while preserving array "
"shape."
msgstr ""

#: of tvm.relax.op.manipulate.flip:5
msgid "axis to flip on"
msgstr ""

#: of tvm.relax.op.manipulate.flip:8 tvm.relax.op.manipulate.gather_elements:10
#: tvm.relax.op.manipulate.gather_nd:10 tvm.relax.op.manipulate.repeat:12
#: tvm.relax.op.manipulate.split:17 tvm.relax.op.manipulate.tile:17
msgid "**ret** -- The computed result."
msgstr ""

#: of tvm.relax.op.unary.floor:1
msgid "Take floor of input data."
msgstr ""

#: of tvm.relax.op.binary.floor_divide:1
msgid "Floor division with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.binary.floor_mod:1
msgid "Floor modulo with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.create.full:1
msgid "Fill array with scalar value."
msgstr ""

#: of tvm.relax.op.create.full:3 tvm.relax.op.create.ones:3
#: tvm.relax.op.create.zeros:3
msgid "The shape of the created tensor."
msgstr ""

#: of tvm.relax.op.create.full:5 tvm.relax.op.create.full_like:8
msgid "The value to fill. Must be a scalar tensor."
msgstr ""

#: of tvm.relax.op.create.full:7
msgid ""
"The data type of the created tensor. If dtype is not given, it will by "
"default use the dtype of fill_value."
msgstr ""

#: of tvm.relax.op.create.full_like:1
msgid ""
"Construct a tensor such that - its shape is the same as the input data "
"tensor's shape, - its value is filled with the input scalar fill value."
msgstr ""

#: of tvm.relax.op.manipulate.gather_elements:1
msgid "Gather elements from data according to indices along the specified axis."
msgstr ""

#: of tvm.relax.op.manipulate.gather_elements:5
#: tvm.relax.op.manipulate.gather_nd:5
msgid "The indices tensor, must have integer type."
msgstr ""

#: of tvm.relax.op.manipulate.gather_elements:7
msgid "The axis along which to index. Default is 0."
msgstr ""

#: of tvm.relax.op.manipulate.gather_nd:1
msgid "Update data at positions defined by indices with values in updates."
msgstr ""

#: of tvm.relax.op.manipulate.gather_nd:7
msgid "The number of batch dimensions. Default is 0."
msgstr ""

#: of tvm.relax.op.binary.greater:1
msgid "Broadcasted element-wise test for (lhs > rhs)."
msgstr ""

#: of tvm.relax.op.binary.greater_equal:1
msgid "Broadcasted element-wise test for (lhs >= rhs)."
msgstr ""

#: of tvm.relax.op.create.hamming_window:1
msgid "Hamming window function."
msgstr ""

#: of tvm.relax.op.create.hamming_window:3
msgid "The size of returned window."
msgstr ""

#: of tvm.relax.op.create.hamming_window:5
msgid ""
"If True, returns a window to be used as periodic function. If False, "
"return a symmetric window."
msgstr ""

#: of tvm.relax.op.create.hamming_window:8
msgid "The co-efficient alpha."
msgstr ""

#: of tvm.relax.op.create.hamming_window:10
msgid "The co-efficient beta."
msgstr ""

#: of tvm.relax.op.create.hamming_window:13 tvm.relax.op.create.tril:12
#: tvm.relax.op.create.triu:12
msgid "**ret** -- The result tensor."
msgstr ""

#: of tvm.relax.op.base.hint_on_device:1
msgid ""
"It provides a hint specifying the device on which the input data should "
"be executed. This hint is utilized by RealizeVDevice to propagate the "
"virtual device.\""
msgstr ""

#: of tvm.relax.op.base.hint_on_device:4 tvm.relax.op.base.to_vdevice:5
msgid "The tensor to be copied."
msgstr ""

#: of tvm.relax.op.base.hint_on_device:6
msgid "The destination device where the data is supposed to be executed."
msgstr ""

#: of tvm.relax.op.base.hint_on_device:9
msgid "**result** -- The result."
msgstr ""

#: of tvm.relax.op.manipulate.index_put:1
msgid ""
"This operation updates values in `data` at positions specified by "
"`indices` with corresponding values from `values`. The `indices` is a "
"tuple of tensors where each tensor corresponds to a dimension in `data`. "
"When `accumulate` is True, the operation performs accumulation (addition)"
" rather than replacement. The `reduction` parameter allows specifying "
"different reduction operations. :param data: The input tensor to be "
"modified :type data: relax.Expr :param indices: Tuple of index tensors "
"(one for each dimension) specifying positions to update :type indices: "
"Union[Expr, Tuple[Expr]] :param values: Values to place at the specified "
"indices :type values: relax.Expr :param accumulate: Whether to accumulate"
" (add) values rather than replace (default: False) :type accumulate: bool"
msgstr ""

#: of tvm.relax.op.manipulate.index_put:15
msgid ""
"**result** -- A new tensor with the same shape as data but with specified"
" positions updated"
msgstr ""

#: of tvm.relax.op.manipulate.index_tensor:1
msgid "Advanced‑tensor indexing (NumPy/PyTorch‐style)."
msgstr ""

#: of tvm.relax.op.manipulate.index_tensor:3
msgid ""
"Given k index tensors ``indices = (I0, I1, …, Ik‑1)`` this operator "
"selects elements from ``data`` as if one had written ``data[I0, I1, …, "
"Ik‑1]`` in NumPy/PyTorch:"
msgstr ""

#: of tvm.relax.op.manipulate.index_tensor:7
msgid "All index tensors must have an integer dtype."
msgstr ""

#: of tvm.relax.op.manipulate.index_tensor:9
msgid ""
"Their shapes are broadcast together to a common shape ``B`` in the usual "
"NumPy way."
msgstr ""

#: of tvm.relax.op.manipulate.index_tensor:12
msgid ""
"The result shape is ``B + data.shape[k:]`` (i.e. the broadcast shape "
"followed by the remaining axes of ``data`` that are *not* indexed)."
msgstr ""

#: of tvm.relax.op.manipulate.index_tensor:16
msgid ""
"At compile‑time Relax checks that the number of index tensors ``k`` does "
"not exceed ``data.ndim``, that the dtypes are integer, and that the "
"shapes are consitent (broadcast‑compatible)."
msgstr ""

#: of tvm.relax.op.manipulate.index_tensor:20
msgid "The input tensor to be indexed."
msgstr ""

#: of tvm.relax.op.manipulate.index_tensor:22
msgid ""
"A Tuple expression containing the index tensors, or a Python ``list`` / "
"``tuple`` that will be promoted to a tuple expression automatically. Each"
" tensor must have an integer dtype."
msgstr ""

#: of tvm.relax.op.manipulate.index_tensor:28
msgid ""
"**result** -- The tensor obtained after advanced indexing.  Its dtype "
"equals ``data.dtype``"
msgstr ""

#: of tvm.relax.op.base.invoke_closure:1
msgid "Invoke a closure."
msgstr ""

#: of tvm.relax.op.base.invoke_closure:3
#: tvm.relax.op.base.invoke_pure_closure:9
msgid "The VMClosure object."
msgstr ""

#: of tvm.relax.op.base.invoke_closure:7
#: tvm.relax.op.base.invoke_pure_closure:13
msgid "The structure info arguments of the CallNode"
msgstr ""

#: of tvm.relax.op.base.invoke_closure:10
msgid "**ret** -- A call to `invoke_closure`."
msgstr ""

#: of tvm.relax.op.base.invoke_pure_closure:1
msgid "Invoke a closure and indicate to the compiler that it is pure."
msgstr ""

#: of tvm.relax.op.base.invoke_pure_closure:3
msgid ""
"Note: This should be used for cases when the user knows that calling the "
"closure with these arguments will **in reality** not cause any side "
"effects. If it is used for a call that _does_ result in side effects, "
"then the compiler may end up removing, reordering, or repeating that "
"call, with no guarantees made about any side effects from the callee."
msgstr ""

#: of tvm.relax.op.base.invoke_pure_closure:16
msgid "**ret** -- A call to `invoke_pure_closure`."
msgstr ""

#: of tvm.relax.op.unary.isfinite:1
msgid "Check if input value is finite."
msgstr ""

#: of tvm.relax.op.unary.isinf:1
msgid "Check if input value is infinite."
msgstr ""

#: of tvm.relax.op.unary.isnan:1
msgid "Check if input value is Nan."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:1
msgid "Modifies the layout of a tensor."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:3
msgid "The input tensor to the operator."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:5
msgid "The transformation to apply."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:7
msgid ""
"The value used for padding if the transformation results in implicit "
"padding. If not specified, any value can be used."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:10
msgid "The axis_separators for index_map to create non flat buffers."
msgstr ""

#: of tvm.relax.op.manipulate.layout_transform:13
msgid "**result** -- The transformed tensor."
msgstr ""

#: of tvm.relax.op.binary.left_shift:1
msgid ""
"Bitwise Shift Left :param x1: The input tensor to be shifted. :type x1: "
"relax.Expr :param x2: The number of positions to shift. :type x2: "
"relax.Expr"
msgstr ""

#: of tvm.relax.op.binary.less:1
msgid "Broadcasted element-wise test for (lhs < rhs)."
msgstr ""

#: of tvm.relax.op.binary.less_equal:1
msgid "Broadcasted element-wise test for (lhs <= rhs)."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:1
msgid "Applies a linear transformation to the incoming data: y = xA^T + b"
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:3 tvm.relax.op.nn.nn.selu:11
#: tvm.relax.op.nn.nn.softplus:5
msgid "The input data."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:5
msgid "The weight tensor."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:7
msgid "The bias tensor."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:9
#: tvm.relax.op.linear_algebra.matmul:10
msgid ""
"The data type of the matmul result. When it is not specified, the output "
"dtype will be the same as input dtype."
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:14 tvm.relax.op.linear_algebra.outer:9
#: tvm.relax.op.sampling.multinomial_from_uniform:5
msgid "备注"
msgstr ""

#: of tvm.relax.op.linear_algebra.linear:15
msgid ""
"Relax does not regard the Linear Op as a primitive Op, while combine the "
"transpose, matmul and add op to implement it."
msgstr ""

#: of tvm.relax.op.unary.log:1
msgid "Compute element-wise natural logarithm of the input data."
msgstr ""

#: of tvm.relax.op.binary.log_add_exp:1
msgid "Compute the log of the sum of exponentials of the inputs, element-wise."
msgstr ""

#: of tvm.relax.op.binary.log_add_exp:8
msgid "The element-wise log-sum-exp of `x1` and `x2`."
msgstr ""

#: of tvm.relax.op.binary.logical_and:1
msgid ""
"Logical AND :param x1: The first input tensor. :type x1: relax.Expr "
":param x2: The second input tensor. :type x2: relax.Expr"
msgstr ""

#: of tvm.relax.op.unary.logical_not:1
msgid "Compute logical NOT of the input data."
msgstr ""

#: of tvm.relax.op.binary.logical_or:1
msgid ""
"Logical OR :param x1: The first input tensor. :type x1: relax.Expr :param"
" x2: The second input tensor. :type x2: relax.Expr"
msgstr ""

#: of tvm.relax.op.binary.logical_xor:1
msgid ""
"Logical XOR :param x1: The first input tensor. :type x1: relax.Expr "
":param x2: The second input tensor. :type x2: relax.Expr"
msgstr ""

#: of tvm.relax.op.base.make_closure:1
msgid "Create a closure with free variables and return the closure."
msgstr ""

#: of tvm.relax.op.base.make_closure:3
msgid "The closure, can be ExternFunc or PrimFunc."
msgstr ""

#: of tvm.relax.op.base.make_closure:8
msgid "**ret** -- The VMClosure."
msgstr ""

#: of tvm.relax.op.mask.masked_fill:1
msgid ""
"Fill a tensor by a specified value in places defined by a mask. :param x:"
" The input data to the operator. :type x: relax.Expr :param mask: The "
"mask. :type mask: relax.Expr :param value: The value to set in the input "
"tensor. :type value: relax.Expr"
msgstr ""

#: of tvm.relax.op.mask.masked_fill:9
msgid "**result** -- The filled tensor."
msgstr ""

#: of tvm.relax.op.linear_algebra.matmul:1
msgid ""
"General matrix multiplication of two tensors, with broadcasting on "
"batched dimensions."
msgstr ""

#: of tvm.relax.op.linear_algebra.matmul:3
msgid ""
"The semantics and output shape deduction rule is specified as https"
"://data-apis.org/array-"
"api/latest/API_specification/generated/array_api.matmul.html."
msgstr ""

#: of tvm.relax.op.statistical.max:1
msgid "Computes the max of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.max:5
msgid ""
"Axis or axes along which a max operation is performed. The default, "
"axis=None, will compute the max of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.statistical.max:9 tvm.relax.op.statistical.mean:9
#: tvm.relax.op.statistical.min:9 tvm.relax.op.statistical.prod:9
#: tvm.relax.op.statistical.std:9 tvm.relax.op.statistical.sum:9
#: tvm.relax.op.statistical.variance:9
msgid ""
"If this is set to True, the axes which are reduced are left in the result"
" as dimensions with size one. With this option, the result will broadcast"
" correctly against the input tensor."
msgstr ""

#: of tvm.relax.op.binary.maximum:1
msgid "Element-wise maximum"
msgstr ""

#: of tvm.relax.op.statistical.mean:1
msgid "Computes the mean of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.mean:5
msgid ""
"Axis or axes along which a mean operation is performed. The default, "
"axis=None, will compute the mean of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.manipulate.meshgrid:1
msgid "Generate coordinate grids from input tensors."
msgstr ""

#: of tvm.relax.op.manipulate.meshgrid:3
msgid ""
"An Expr in Tuple type, containing 1D tensors (or scalars promoted to 1D) "
"to generate coordinate grids from, or a list of such tensors."
msgstr ""

#: of tvm.relax.op.manipulate.meshgrid:6
msgid ""
"The indexing mode, either \"ij\" (matrix indexing) or \"xy\" (Cartesian "
"indexing). Defaults to \"ij\"."
msgstr ""

#: of tvm.relax.op.manipulate.meshgrid:10
msgid "**result** -- A Tuple of tensors representing the coordinate grids."
msgstr ""

#: of tvm.relax.op.statistical.min:1
msgid "Computes the min of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.min:5
msgid ""
"Axis or axes along which a min operation is performed. The default, "
"axis=None, will compute the min of all elements in the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.binary.minimum:1
msgid "Element-wise minimum"
msgstr ""

#: of tvm.relax.op.binary.mod:1
msgid "Modulo with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:1
msgid ""
"Returns a tensor where each row contains the index sampled from the "
"multinomial probability distribution located in the corresponding row of "
"tensor prob."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:6
msgid ""
"For better cpu performance, use 'vm.builtin.multinomial_from_uniform'. "
"For accurate results, ensure probabilities are between 0 and 1 and sum to"
" 1."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:9
msgid ""
"A 2-D tensor of shape (batch, vocab_size) representing probability "
"distributions. Each row is a distribution across vocabulary for a batch, "
"where: Values range from [0, 1], indicating the probability of each "
"vocabulary item. The sum of values in each row is 1, forming a valid "
"distribution."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:14
msgid ""
"The uniformly sampled 2-D tensor with the shape (n, 1). Values range from"
" 0 to 1, indicating probabilities sampled uniformly."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:17
msgid ""
"The 2-D tensor with the shape [n, 1], which indicates the specific "
"probability distribution to sample from. The value of sample_indices[i] "
"determines that the ith token should be sampled from the "
"sample_indices[i]th probability distribution. For instance, if there are "
"3 distinct probability distributions and the requirement is to sample 2, "
"3, and 4 tokens from each, then sample_indices would be [0, 0, 1, 1, 1, "
"2, 2, 2, 2]."
msgstr ""

#: of tvm.relax.op.sampling.multinomial_from_uniform:27
msgid "**result** -- The computed tensor with shape (n, 1)."
msgstr ""

#: of tvm.relax.op.binary.multiply:1
msgid "Multiplication with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.unary.negative:1
msgid "Compute element-wise negative of the input data."
msgstr ""

#: of tvm.relax.op.unary.negative:6
msgid "**result** -- The computed result"
msgstr ""

#: of tvm.relax.op.set.nonzero:1
msgid "Find the indices of elements of a tensor that are non-zero."
msgstr ""

#: of tvm.relax.op.set.nonzero:6
msgid "**result** -- A 2-D tensor containing indices of non-zero elements."
msgstr ""

#: of tvm.relax.op.set.nonzero:9
msgid "This function is equivalent to `onnx.nonzero`."
msgstr ""

#: of tvm.relax.op.binary.not_equal:1
msgid "Broadcasted element-wise test for (lhs != rhs)."
msgstr ""

#: of tvm.relax.op.base.null_value:1
msgid "Create a call node that represents a null value object."
msgstr ""

#: of tvm.relax.op.manipulate.one_hot:1
msgid "Returns a one-hot tensor."
msgstr ""

#: of tvm.relax.op.manipulate.one_hot:3
msgid "The indices to set to `on_value`."
msgstr ""

#: of tvm.relax.op.manipulate.one_hot:5
msgid "The value to fill at `indices`."
msgstr ""

#: of tvm.relax.op.manipulate.one_hot:7
msgid "The value to fill at other locations."
msgstr ""

#: of tvm.relax.op.manipulate.one_hot:9
msgid "The depth of the one-hot dimension."
msgstr ""

#: of tvm.relax.op.manipulate.one_hot:11
msgid "The axis to fill. Default is -1 which adds a new dimension at the end."
msgstr ""

#: of tvm.relax.op.create.ones:1
msgid "Construct a tensor of all ones, with the input shape and dtype."
msgstr ""

#: of tvm.relax.op.create.ones_like:1
msgid "Construct a tensor with all ones, with shape of the input tensor shape."
msgstr ""

#: of tvm.relax.op.linear_algebra.outer:1
msgid "Computes the outer product of two input expressions."
msgstr ""

#: of tvm.relax.op.linear_algebra.outer:3
msgid "The first input expression."
msgstr ""

#: of tvm.relax.op.linear_algebra.outer:5
msgid "The second input expression."
msgstr ""

#: of tvm.relax.op.linear_algebra.outer:10
msgid ""
"This operation computes the outer product between two expressions, "
"resulting in a tensor where each element is the product of elements from "
"`x1` and `x2`. It is commonly used in tensor and matrix operations to "
"expand lower-dimensional inputs into higher-dimensional representations."
msgstr ""

#: of tvm.relax.op.linear_algebra.outer:15
msgid "**result** -- The resulting expression representing the outer product."
msgstr ""

#: of tvm.relax.op.manipulate.permute_dims:1
msgid "Permutes the dimensions of an array."
msgstr ""

#: of tvm.relax.op.manipulate.permute_dims:5
msgid ""
"The target axes order. If not specified, permute_dims will reverse the "
"order of all axes."
msgstr ""

#: of tvm.relax.op.manipulate.permute_dims:8
msgid "**result** -- The transposed result."
msgstr ""

#: of tvm.relax.op.binary.power:1
msgid "Power with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.base.print:1
msgid "Print op to print the values"
msgstr ""

#: of tvm.relax.op.base.print:3
msgid "The values to print."
msgstr ""

#: of tvm.relax.op.base.print:5
msgid "The format string or StringImm."
msgstr ""

#: of tvm.relax.op.base.print:8
msgid "**result** -- A relax Call, which will print the value during runtime."
msgstr ""

#: of tvm.relax.op.statistical.prod:1
msgid "Computes the product of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.prod:5
msgid ""
"Axis or axes along which a product is performed. The default, axis=None, "
"will compute the product of all elements of the input tensor. Negative "
"indexing is supported."
msgstr ""

#: of tvm.relax.op.qdq.quantize:1
msgid ""
"Quantize op This operator takes input and produces quantized output. The "
"input tensor can be of any shape. The output shape is the same as input "
"shape."
msgstr ""

#: of tvm.relax.op.qdq.quantize:5
msgid ""
"Q_output = clamp((round(input_tensor/scale) + zero_point), "
"out_dtype::min, out_dtype::max)"
msgstr ""

#: of tvm.relax.op.qdq.quantize:7
msgid "The input tensor to be quantized."
msgstr ""

#: of tvm.relax.op.qdq.quantize:9
msgid "The output scale."
msgstr ""

#: of tvm.relax.op.qdq.quantize:11
msgid "The output zero_point."
msgstr ""

#: of tvm.relax.op.qdq.quantize:13
msgid ""
"The channel axis for quantization. Default value is -1 which corresponds "
"to the last axis."
msgstr ""

#: of tvm.relax.op.base.register_gradient:1
msgid "Register operator gradient function for a relax operator."
msgstr ""

#: of tvm.relax.op.base.register_gradient:3
msgid "The name of the op."
msgstr ""

#: of tvm.relax.op.base.register_gradient:5
msgid "-> partials: List[Expr] The gradient function being used."
msgstr ""

#: of tvm.relax.op.base.register_gradient:8
msgid "The priority level"
msgstr ""

#: of tvm.relax.op.manipulate.repeat:1
msgid "Repeats elements of an array."
msgstr ""

#: of tvm.relax.op.manipulate.repeat:5
#: tvm.relax.op.op_attrs.RepeatAttrs.repeats:1
msgid "The number of repetitions."
msgstr ""

#: of tvm.relax.op.manipulate.repeat:7 tvm.relax.op.op_attrs.RepeatAttrs.axis:1
msgid ""
"The axis along which to repeat values. The negative numbers are "
"interpreted counting from the backward. By default, use the flattened "
"input array, and return a flat output array."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:1
msgid "Reshape the input array."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:3
msgid ""
"``-1`` infers the dimension of the output shape by using the remainder of"
" the input dimensions keeping the size of the new array same as that of "
"the input array. At most one dimension of shape can be -1."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:15
msgid "The new shape. Should be compatible with the original shape."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:18
msgid "**result** -- The reshaped result."
msgstr ""

#: of tvm.relax.op.manipulate.reshape:23
msgid ""
"The ``-1`` inference is only performed at compile-time. That is to say, "
"in any case the dimension length of ``-1`` cannot be inferred in compile-"
"time, an error will be thrown."
msgstr ""

#: of tvm.relax.op.binary.right_shift:1
msgid ""
"Bitwise Shift Right :param x1: The input tensor to be shifted. :type x1: "
"relax.Expr :param x2: The number of positions to shift. :type x2: "
"relax.Expr"
msgstr ""

#: of tvm.relax.op.unary.round:1
msgid "Rounds each element of the input data to nearest integer."
msgstr ""

#: of tvm.relax.op.unary.rsqrt:1
msgid "Compute element-wise reciprocal square root of the input data."
msgstr ""

#: of tvm.relax.op.unary.rsqrt:3
msgid "1/sqrt(x)"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:1
msgid ""
"ONNX style scatter elements. This operation updates its value in `data` "
"to values specified by `updates` at specific index positions specified by"
" `indices`. For example, in 2D tensor, the update corresponding to the "
"[i][j] entry is performed as below:"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:11
msgid ""
"When the `reduction` is set to some reduction function `f`, the update "
"corresponding to [i][j] entry is performed as below:"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:19
msgid "Where `f` is update, add, mul, mean, max, min."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:23
#: tvm.relax.op.manipulate.scatter_nd:5
msgid "The index positions to update in `data`."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:25
#: tvm.relax.op.manipulate.scatter_nd:7
msgid "Values to replace to."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:27
msgid "Axis to scatter on."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:29
msgid ""
"Type of reduction to apply: update, add, mul, mean, max, min. It is "
"\"update\" by default."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_elements:33
msgid ""
"**result** -- The result has the same size as data, and the same shape as"
" data"
msgstr ""

#: of tvm.relax.op.manipulate.scatter_nd:1
msgid "Scatter updates into an array according to indices."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_nd:3
msgid "The input data to be updated."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_nd:9
msgid ""
"Type of reduction to apply: update, add, mul, max, min. It is \"update\" "
"by default."
msgstr ""

#: of tvm.relax.op.manipulate.scatter_nd:13
msgid "**result** -- The result has the same shape as data."
msgstr ""

#: of tvm.relax.op.base.shape_of:1
msgid "Get shape of a tensor."
msgstr ""

#: of tvm.relax.op.base.shape_of:3
msgid "The input Expr."
msgstr ""

#: of tvm.relax.op.base.shape_of:6
msgid "**result** -- A relax Call, which gets the shape of the input"
msgstr ""

#: of tvm.relax.op.base.shape_to_tensor:1
msgid "Convert shape to tensor expr. :param expr: The input Expr :type expr: Expr"
msgstr ""

#: of tvm.relax.op.base.shape_to_tensor:5
msgid ""
"**result** -- A relax Call, which transforms the shape values to the "
"tensor"
msgstr ""

#: of tvm.relax.op.unary.sigmoid:1
msgid "Compute element-wise sigmoid of the input data."
msgstr ""

#: of tvm.relax.op.unary.sign:1
msgid ""
"Returns an indication of the sign of a number for each element of the "
"input data."
msgstr ""

#: of tvm.relax.op.unary.sin:1
msgid "Compute element-wise sin of the input data."
msgstr ""

#: of tvm.relax.op.unary.sinh:1
msgid "Compute element-wise sinh of the input data."
msgstr ""

#: of tvm.relax.op.manipulate.slice_scatter:1
msgid "Embeds the values of the src tensor into input at the given dimension."
msgstr ""

#: of tvm.relax.op.manipulate.slice_scatter:3
msgid "The input tensor to be updated."
msgstr ""

#: of tvm.relax.op.manipulate.slice_scatter:5
msgid "The tensor to embed into input."
msgstr ""

#: of tvm.relax.op.manipulate.slice_scatter:7
msgid "The dimension to insert the slice into."
msgstr ""

#: of tvm.relax.op.manipulate.slice_scatter:9
msgid "The start index of where to insert the slice."
msgstr ""

#: of tvm.relax.op.manipulate.slice_scatter:10
msgid "The end index of where to insert the slice."
msgstr ""

#: of tvm.relax.op.manipulate.slice_scatter:11
msgid "The how many elements to skip in."
msgstr ""

#: of tvm.relax.op.manipulate.slice_scatter:13
msgid "**result** -- The computed result tensor with the same shape as `data`."
msgstr ""

#: of tvm.relax.op.sorting.sort:1
msgid ""
"Performs sorting along the given axis and returns an array in sorted "
"order."
msgstr ""

#: of tvm.relax.op.sorting.sort:6
msgid ""
"Axis along which to sort the input tensor. By default the last axis of "
"the input is used."
msgstr ""

#: of tvm.relax.op.sorting.sort:12
msgid "**out** -- Sorted tensor."
msgstr ""

#: of tvm.relax.op.manipulate.split:1
msgid "Split input tensor along axis by sections or indices."
msgstr ""

#: of tvm.relax.op.manipulate.split:3
msgid ""
"If indices_or_sections is an integer, the input will be divided equally "
"along given axis (if possible). Last section will be smaller if the "
"tensor size along the given dimension is not divisible by the integer."
msgstr ""

#: of tvm.relax.op.manipulate.split:7
msgid ""
"If indices_or_sections is a tuple of mixture of int or PrimExpr, the "
"entries indicate the indices where along axis the array is split."
msgstr ""

#: of tvm.relax.op.manipulate.split:10
msgid "The tensor to be split."
msgstr ""

#: of tvm.relax.op.manipulate.split:12
msgid "Indices or sections to split into. Accepts an int or a list."
msgstr ""

#: of tvm.relax.op.manipulate.split:14
msgid "The axis over which to split."
msgstr ""

#: of tvm.relax.op.unary.sqrt:1
msgid "Compute element-wise square root of the input data."
msgstr ""

#: of tvm.relax.op.unary.square:1
msgid "Squares each element of the input data."
msgstr ""

#: of tvm.relax.op.manipulate.squeeze:1
msgid "Squeeze axes in the array."
msgstr ""

#: of tvm.relax.op.manipulate.squeeze:5
msgid ""
"The set of axes to remove. If axis = None, remove all axis of dimensions "
"1. If any specified axis has dimension that does not equal 1, it is an "
"error."
msgstr ""

#: of tvm.relax.op.manipulate.squeeze:10
msgid "**result** -- The squeezed result."
msgstr ""

#: of tvm.relax.op.manipulate.stack:1
msgid "Stack the input tensors along a new axis."
msgstr ""

#: of tvm.relax.op.manipulate.stack:3
msgid ""
"An Expr in Tuple type, containing the tensors to be stacked, or a list of"
" Tensors. All input tensors must have the same shape."
msgstr ""

#: of tvm.relax.op.manipulate.stack:6
msgid ""
"The axis in the resulting tensor along which the input tensors will be "
"stacked. Negative values wrap around. Default is 0."
msgstr ""

#: of tvm.relax.op.manipulate.stack:10
msgid ""
"**result** -- The stacked tensor with an additional dimension compared to"
" the input tensors."
msgstr ""

#: of tvm.relax.op.statistical.std:1
msgid "Computes the standard deviation of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.std:5
msgid ""
"Axis or axes along which a standard deviation is performed. The default, "
"axis=None, will compute the std of all elements of the input tensor. "
"Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.index.strided_slice:1
msgid "Strided slice of a tensor."
msgstr ""

#: of tvm.relax.op.index.strided_slice:5
msgid "Axes along which slicing is applied."
msgstr ""

#: of tvm.relax.op.index.strided_slice:15
#: tvm.relax.op.op_attrs.StridedSliceAttrs.assume_inbound:1
msgid ""
"Whether to assume the indices are in bound. If it is set to false, out of"
" bound indices will be clipped to the bound."
msgstr ""

#: of tvm.relax.op.index.strided_slice:24
msgid ""
"strided_slice require the input `begin`, `end` and `strides` to have the "
"same length as `axes`."
msgstr ""

#: of tvm.relax.op.binary.subtract:1
msgid "Subtraction with numpy-style broadcasting."
msgstr ""

#: of tvm.relax.op.statistical.sum:1
msgid "Computes the sum of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.sum:5
msgid ""
"Axis or axes along which a sum is performed. The default, axis=None, will"
" sum all of the elements of the input tensor. Negative indexing is "
"supported."
msgstr ""

#: of tvm.relax.op.index.take:1
msgid ""
"Take elements from a tensor along an axis. Its semantic is mostly similar"
" to `numpy.take` "
"(https://numpy.org/doc/stable/reference/generated/numpy.take.html), which"
" can cover `torch.take` "
"(https://pytorch.org/docs/stable/generated/torch.take.html) and "
"`onnx.gather` "
"(https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Gather-13)."
msgstr ""

#: of tvm.relax.op.index.take:7
msgid "The source tensor."
msgstr ""

#: of tvm.relax.op.index.take:9
msgid "The indices of the values to extract."
msgstr ""

#: of tvm.relax.op.index.take:11
msgid ""
"The axis over which to select values. If it is none, the input tensor is "
"required to be one-dimensional."
msgstr ""

#: of tvm.relax.op.index.take:14
msgid ""
"Specifies how out-of-bounds indices will behave. - fast (default): extra "
"indices lead to seg fault (user must make sure indices are in-bound) - "
"nan: produce NaNs for out-of-bounds indices - wrap: wrap around the "
"indices - clip: clip to the range"
msgstr ""

#: of tvm.relax.op.index.take:21
msgid "**ret** -- The taken result."
msgstr ""

#: of tvm.relax.op.unary.tan:1
msgid "Compute element-wise tan of the input data."
msgstr ""

#: of tvm.relax.op.unary.tanh:1
msgid "Compute element-wise tanh of the input data."
msgstr ""

#: of tvm.relax.op.base.tensor_to_shape:1
msgid "Convert tensor to shape expr. :param expr: The input Expr :type expr: Expr"
msgstr ""

#: of tvm.relax.op.base.tensor_to_shape:5
msgid ""
"**result** -- A relax Call, which transforms the tensor values to the "
"shape"
msgstr ""

#: of tvm.relax.op.manipulate.tile:1
msgid "Construct an array by repeating data the number of times given by repeats."
msgstr ""

#: of tvm.relax.op.manipulate.tile:3
msgid ""
"If repeats has length l, and data has dimension d, the result will have "
"dimension of max(l, d)."
msgstr ""

#: of tvm.relax.op.manipulate.tile:5
msgid ""
"If d < l, data is promoted to be l-dimensional by prepending new axes. So"
" a shape (3,) Tensor is promoted to (1, 3) for 2-D replication, or shape "
"(1, 1, 3) for 3-D replication. If this is not the desired behavior, "
"promote data to d-dimensions manually before calling this function."
msgstr ""

#: of tvm.relax.op.manipulate.tile:9
msgid ""
"If d > l, reps is promoted to length d by pre-pending 1's to it. Thus for"
" a data of shape (2, 3, 4, 5), a reps of (2, 2) is treated as (1, 1, 2, "
"2)."
msgstr ""

#: of tvm.relax.op.manipulate.tile:14 tvm.relax.op.op_attrs.TileAttrs.repeats:1
msgid "The number of repetitions of data along each axis."
msgstr ""

#: of tvm.relax.op.base.to_vdevice:1
msgid ""
"Copy data to the destination device. This operator helps data "
"transferring between difference devices for heterogeneous execution."
msgstr ""

#: of tvm.relax.op.base.to_vdevice:7
#: tvm.relax.op.op_attrs.ToVDeviceAttrs.dst_vdevice:1
msgid "The destination device where the data is copied to."
msgstr ""

#: of tvm.relax.op.base.to_vdevice:10
msgid "**result** -- The copied result."
msgstr ""

#: of tvm.relax.op.sorting.topk:1
msgid "Get the top k elements in an input tensor along the given axis."
msgstr ""

#: of tvm.relax.op.sorting.topk:3
msgid ""
"ret_type specifies the return type, can be one of (\"both\", \"values\", "
"\"indices\")."
msgstr ""

#: of tvm.relax.op.sorting.topk:7
msgid "Number of top elements to select. Return all elements if k < 1."
msgstr ""

#: of tvm.relax.op.sorting.topk:11
msgid ""
"The return type [both, values, indices]. \"both\": return both top k data"
" and indices. \"values\": return top k data only. \"indices\": return top"
" k indices only."
msgstr ""

#: of tvm.relax.op.sorting.topk:16
msgid ""
"Whether to return largest or smallest elements. The k smallest elements "
"are returned if largest is False."
msgstr ""

#: of tvm.relax.op.sorting.topk:19
msgid "The data type of the indices output."
msgstr ""

#: of tvm.relax.op.sorting.topk:22
msgid "**out** -- The computed result."
msgstr ""

#: of tvm.relax.op.create.tril:1
msgid "Return the lower triangular part of a matrix or a batch of matrices."
msgstr ""

#: of tvm.relax.op.create.tril:3
msgid ""
"The tensor that tril will be applied to. It is required to have at least "
"two dimensions."
msgstr ""

#: of tvm.relax.op.create.tril:6
msgid ""
"The index indicating the diagonal above which to zero elements. If k = 0,"
" the diagonal is the main diagonal. If k < 0, the diagonal is below the "
"main diagonal. If k > 0, the diagonal is above the main diagonal."
msgstr ""

#: of tvm.relax.op.create.triu:1
msgid "Return the upper triangular part of a matrix or a batch of matrices."
msgstr ""

#: of tvm.relax.op.create.triu:3
msgid ""
"The tensor that triu will be applied to. It is required to have at least "
"two dimensions."
msgstr ""

#: of tvm.relax.op.create.triu:6
msgid ""
"The index indicating the diagonal below which to zero elements. If k = 0,"
" the diagonal is the main diagonal. If k < 0, the diagonal is below the "
"main diagonal. If k > 0, the diagonal is above the main diagonal."
msgstr ""

#: of tvm.relax.op.unary.trunc:1
msgid "Take trunc of input data. :param x: The input data :type x: relax.Expr"
msgstr ""

#: of tvm.relax.op.set.unique:1
msgid ""
"Find the unique elements in a given tensor. In addition, it optionally "
"returns - the indices of the input tensor that give the unique values; - "
"the indices of the unique tensor that reconstruct the input tensor; - the"
" number of times each unique value comes up in the input tensor."
msgstr ""

#: of tvm.relax.op.set.unique:9
msgid ""
"Whether to sort the unique elements in ascending order before returning "
"as output."
msgstr ""

#: of tvm.relax.op.set.unique:12
msgid ""
"Whether to return an additional tensor with indices for where elements in"
" the unique tensor come from the original input."
msgstr ""

#: of tvm.relax.op.set.unique:15
msgid ""
"Whether to return an additional tensor with indices for where elements in"
" the original input ended up in the returned unique list."
msgstr ""

#: of tvm.relax.op.set.unique:18
msgid ""
"Whether to return an additional tensor with counts of each unique "
"elements."
msgstr ""

#: of tvm.relax.op.set.unique:20
msgid ""
"The dimension to apply unique. If not specified, the unique values of the"
" flattened input are returned."
msgstr ""

#: of tvm.relax.op.set.unique:24
msgid "**ret** -- The created relax call with"
msgstr ""

#: of tvm.relax.op.statistical.variance:1
msgid "Computes the variance of tensor elements over given axes."
msgstr ""

#: of tvm.relax.op.statistical.variance:5
msgid ""
"Axis or axes along which a variance operation is performed. The default, "
"axis=None, will compute the variance of all elements in the input tensor."
" Negative indexing is supported."
msgstr ""

#: of tvm.relax.op.search.where:1
msgid ""
"Selecting elements from either the input tensors depending on the value "
"of the condition."
msgstr ""

#: of tvm.relax.op.search.where:4
msgid ""
"For a given position, return the corresponding value in `x1` if "
"`condition` is True, and return the corresponding value in `x2` "
"otherwise."
msgstr ""

#: of tvm.relax.op.search.where:7
msgid ""
"When True, yield `x1`; otherwise, yield `x2`. Must be broadcasting "
"compatible with `x1` and `x2`. Must have boolean dtype."
msgstr ""

#: of tvm.relax.op.search.where:11
msgid ""
"The first input tensor. Must be broadcasting compatible with `condition` "
"and `x2`."
msgstr ""

#: of tvm.relax.op.search.where:14
msgid ""
"The second input tensor. Must be broadcasting compatible with `condition`"
" and `x1`."
msgstr ""

#: of tvm.relax.op.datatype.wrap_param:1
msgid ""
"Cast input tensor which is model param to data type if the dtype of the "
"input data is not the same as the given dtype. :param data: The input "
"data to the operator. :type data: relax.Expr :param dtype: The target "
"data type :type dtype: Union[str, DataType]"
msgstr ""

#: of tvm.relax.op.create.zeros:1
msgid "Construct a tensor of all zeros, with the input shape and dtype."
msgstr ""

#: of tvm.relax.op.create.zeros_like:1
msgid "Construct a tensor with all zeros, with shape of the input tensor shape."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:28
msgid "tvm.relax.op.nn"
msgstr ""

#: of tvm.relax.op.nn:1
msgid "Neural network related operators."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:1
msgid "1D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation "
"across each window represented by W."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:7
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, in_channels, width)`, to produce an output Tensor "
"with shape (batch_size, in_channels, output_width)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:12
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:12
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:12
msgid ""
"The pooling kernel and stride sizes are automatically chosen for desired "
"output sizes."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:15
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:15
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:15
msgid "For output_size:"
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output width."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size) for any input (NCW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:24
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:27
msgid ""
"Output height and width. If not specified, it will be the same as the "
"input height and width. If specified, it is required to have length "
"either 1 or 2."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:21
#: tvm.relax.op.nn.nn.adaptive_avg_pool1d:28
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:31
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:31 tvm.relax.op.nn.nn.avg_pool1d:29
#: tvm.relax.op.nn.nn.avg_pool2d:37 tvm.relax.op.nn.nn.avg_pool3d:30
#: tvm.relax.op.nn.nn.conv1d:41 tvm.relax.op.nn.nn.conv1d_transpose:29
#: tvm.relax.op.nn.nn.conv2d:41 tvm.relax.op.nn.nn.conv2d_transpose:39
#: tvm.relax.op.nn.nn.conv3d:43 tvm.relax.op.nn.nn.max_pool1d:29
#: tvm.relax.op.nn.nn.max_pool2d:36 tvm.relax.op.nn.nn.max_pool3d:30
msgid "Layout of the input."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool1d:30
#: tvm.relax.op.nn.nn.adaptive_avg_pool2d:33
#: tvm.relax.op.nn.nn.adaptive_avg_pool3d:33 tvm.relax.op.nn.nn.avg_pool1d:31
#: tvm.relax.op.nn.nn.avg_pool2d:39 tvm.relax.op.nn.nn.avg_pool3d:32
#: tvm.relax.op.nn.nn.conv1d:45 tvm.relax.op.nn.nn.conv1d_transpose:33
#: tvm.relax.op.nn.nn.conv2d:45 tvm.relax.op.nn.nn.conv2d_transpose:43
#: tvm.relax.op.nn.nn.conv3d:47 tvm.relax.op.nn.nn.max_pool1d:31
#: tvm.relax.op.nn.nn.max_pool2d:38 tvm.relax.op.nn.nn.max_pool3d:32
msgid "Layout of the output. If not specified, it is the same as data_layout"
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:1
msgid "2D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D average value calculation "
"across each window represented by WxH."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with shape (batch_size, in_channels, output_height, output_width)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:16
msgid ""
"If this argument is not provided, input height and width will be used as "
"output height and width."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size x output_size) for any input (NCHW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool2d:22
msgid ""
"If a tuple of integers (height, width) are provided for output_size, the "
"output size is (N x C x height x width) for any input (NCHW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:1
msgid "3D adaptive average pooling operator. This operator is experimental."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation "
"across each window represented by WxH."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, in_channels, depth, height, width)`, to produce an "
"output Tensor with shape (batch_size, in_channels, output_depth, "
"output_height, output_width)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:16
msgid ""
"If this argument is not provided, input depth, height and width will be "
"used as output depth, height and width."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:19
msgid ""
"If a single integer is provided for output_size, the output size is (N x "
"C x output_size x output_size x output_size) for any input (NCDHW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:22
msgid ""
"If a tuple of integers (depth, height, width) are provided for "
"output_size, the output size is (N x C x depth x height x width) for any "
"input (NCDHW)."
msgstr ""

#: of tvm.relax.op.nn.nn.adaptive_avg_pool3d:27
msgid ""
"Output height and width. If not specified, it will be the same as the "
"input height and width. If specified, it is required to have length "
"either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:1 tvm.relax.op.nn.nn.attention_bias:1
msgid "Computes fused multi head attention."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:3 tvm.relax.op.nn.nn.attention_bias:7
msgid "All input tensors are of 4-D tensors with BSNH layout."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:5 tvm.relax.op.nn.nn.attention_bias:9
#, python-brace-format
msgid ""
"FMA(Q, K, V) = \\text{Softmax}(Q @ K^T) @ V\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:9 tvm.relax.op.nn.nn.attention_bias:13
msgid "The input tensor is required to have float16 dtype"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:11 tvm.relax.op.nn.nn.attention_bias:15
msgid ""
"The input query to the operator. The layout of the input query should be "
"(batch_size, seq_len, num_head, head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:14 tvm.relax.op.nn.nn.attention_bias:18
msgid ""
"The input key to the operator. The layout of the input key should be "
"(batch_size, seq_len_kv, num_head, head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:17 tvm.relax.op.nn.nn.attention_bias:21
msgid ""
"The input value to the operator. The layout of the input value should be "
"(batch_size, seq_len_kv, num_head, head_dim_v)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:20 tvm.relax.op.nn.nn.attention_bias:24
msgid ""
"The optional attention bias to the operator. The layout of the attention "
"bias should be a 4-D tensor ending with seq_len_kv, and broadcastable to "
"(batch_size, num_head, seq_len, seq_len_kv)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:24 tvm.relax.op.nn.nn.attention_bias:28
#: tvm.relax.op.nn.nn.attention_var_len:28
msgid ""
"The scale value to be applied to the attention score, by default 1 / "
"sqrt(head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:26 tvm.relax.op.nn.nn.attention_bias:30
#: tvm.relax.op.nn.nn.attention_var_len:30
msgid ""
"The optional causal mask, i.e. 'TopLeft' and 'BottomRight'. For "
"'TopLeft', the mask matrix is as `np.tril(*, k=0)`, while for "
"'BottomRight', the mask matrix is as `np.tril(*, k=abs(seq_len - "
"seq_len_kv))` For example, with seq_len = 4, seq_len_kv = 2, mask for "
"'TopLeft':  .. code:: python      [[1, 0],     [1, 1],     [1, 1],     "
"[1, 1]]  mask for 'BottomRight':  .. code:: python      [[1, 1],     [1, "
"1],     [1, 1],     [1, 1]]  with seq_len = 2, seq_len_kv = 4, mask for "
"'TopLeft':  .. code:: python      [[1, 0, 0, 0],     [1, 1, 0, 0]]  mask "
"for 'BottomRight':  .. code:: python      [[1, 1, 1, 0],     [1, 1, 1, "
"1]]"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:26 tvm.relax.op.nn.nn.attention_bias:30
#: tvm.relax.op.nn.nn.attention_var_len:30
msgid ""
"The optional causal mask, i.e. 'TopLeft' and 'BottomRight'. For "
"'TopLeft', the mask matrix is as `np.tril(*, k=0)`, while for "
"'BottomRight', the mask matrix is as `np.tril(*, k=abs(seq_len - "
"seq_len_kv))` For example, with seq_len = 4, seq_len_kv = 2, mask for "
"'TopLeft':"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:39 tvm.relax.op.nn.nn.attention:56
#: tvm.relax.op.nn.nn.attention_bias:43 tvm.relax.op.nn.nn.attention_bias:60
#: tvm.relax.op.nn.nn.attention_var_len:43
#: tvm.relax.op.nn.nn.attention_var_len:60
msgid "mask for 'BottomRight':"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:48 tvm.relax.op.nn.nn.attention_bias:52
#: tvm.relax.op.nn.nn.attention_var_len:52
msgid "with seq_len = 2, seq_len_kv = 4, mask for 'TopLeft':"
msgstr ""

#: of tvm.relax.op.nn.nn.attention:63 tvm.relax.op.nn.nn.attention_bias:67
#: tvm.relax.op.nn.nn.attention_var_len:67
#: tvm.relax.op.op_attrs.AttentionAttrs.window_size:1
msgid "The size of the window for sliding-window attention."
msgstr ""

#: of tvm.relax.op.nn.nn.attention:66 tvm.relax.op.nn.nn.attention_bias:70
msgid ""
"**result** -- The computed result. The layout of the output should be "
"(batch_size, seq_len, num_head, head_dim_v)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_bias:3
msgid ""
"IRModule.script() transforms attention op to attention_bias which is "
"incompatible with TVMScript Parser. The function makes TVMScript's print "
"compatible with TVMScript's parser."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:1
msgid ""
"Computes fused multi head attention over batched sequences of variable "
"lengths."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:3
msgid ""
"Given concatenated inputs and sequence lengths information, this operator"
" computes attention for all sequences more efficiently than calling the "
"normal attention operator for each sequence individually."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:7
msgid ""
"The input queries concatenated along the second axis. Its shape must be "
"(1, total_seq_len, num_head, head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:10
msgid ""
"The input keys concatenated along the second axis. Its shape must be (1, "
"total_seq_len_kv, num_head, head_dim)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:13
msgid ""
"The input values concatenated along the second axis. Its shape must be "
"(1, total_seq_len_kv, num_head, head_dim_v)."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:16
msgid ""
"The cumsum of query sequence lengths, prepended with 0. Its dtype must be"
" int32. For example, if the lengths of the sequences that are batched are"
" [2, 5, 3], this tensor has values [0, 2, 7, 10]."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:20
msgid ""
"The cumsum of key sequence lengths, prepended with 0. By default it is "
"the same as seqstart_q."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:23
msgid "The maximum query sequence length in the batch. It must be int32."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:25
msgid ""
"The maximum key sequence length in the batch. It must be int32. By "
"default it is the same as max_seqlen_q."
msgstr ""

#: of tvm.relax.op.nn.nn.attention_var_len:70
msgid ""
"**result** -- The computed result with shape `(1, total_seq_len, "
"num_head, head_dim_v)`."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:1
msgid "1D average pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:3
msgid ""
"This operator takes data as input and does 1D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:6
msgid ""
"In the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, channels, width)`, to produce an output Tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:10 tvm.relax.op.nn.nn.avg_pool3d:11
#: tvm.relax.op.nn.nn.max_pool1d:10 tvm.relax.op.nn.nn.max_pool3d:11
msgid ""
"The ceil_mode is used to take ceil or floor while computing out shape. "
"count_include_pad indicates including or excluding padded input values in"
" computation. This operator accepts data layout specification."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:16
msgid "The size of window for pooling. It is required to have length is 1."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:18
msgid "The strides of pooling. It is required to have length is 1."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:20 tvm.relax.op.nn.nn.max_pool1d:20
msgid "The padding for pooling. It is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:22
msgid "The dilation of pooling. It is required to have length is 1."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:24 tvm.relax.op.nn.nn.avg_pool2d:32
#: tvm.relax.op.nn.nn.avg_pool3d:25 tvm.relax.op.nn.nn.max_pool1d:24
#: tvm.relax.op.nn.nn.max_pool2d:31 tvm.relax.op.nn.nn.max_pool3d:25
#: tvm.relax.op.op_attrs.Pool1DAttrs.ceil_mode:1
#: tvm.relax.op.op_attrs.Pool2DAttrs.ceil_mode:1
#: tvm.relax.op.op_attrs.Pool3DAttrs.ceil_mode:1
msgid ""
"A boolean indicating if use ceil or floor to compute the output shape. By"
" using ceil, every element in the input tensor will be covered by a "
"sliding window."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool1d:27 tvm.relax.op.nn.nn.avg_pool2d:35
#: tvm.relax.op.nn.nn.avg_pool3d:28 tvm.relax.op.nn.nn.max_pool1d:27
#: tvm.relax.op.nn.nn.max_pool2d:34 tvm.relax.op.nn.nn.max_pool3d:28
msgid "To include padding to compute the average."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:1 tvm.relax.op.nn.nn.avg_pool3d:1
msgid "2D average pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:3
msgid ""
"This operator takes data as input and does 2D avarage value calculation "
"with in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:6 tvm.relax.op.nn.nn.max_pool2d:6
msgid ""
"In the default case, where the data_layout is `NCHW` a data Tensor with "
"shape `(batch_size, in_channels, height, width)`, to produce an output "
"Tensor with the following rule:"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:10 tvm.relax.op.nn.nn.max_pool2d:10
msgid "with data of shape (b, c, h, w) and pool_size (kh, kw)"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:12
#, python-brace-format
msgid ""
"\\mbox{out}(b, c, y, x)  = \\frac{1}{kh * kw} \\sum_{m=0, \\ldots, kh-1}\n"
"    \\sum_{n=0, \\ldots, kw-1}\n"
"    \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x +"
" n)"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:18 tvm.relax.op.nn.nn.max_pool2d:17
msgid ""
"Padding is applied to data before the computation. ceil_mode is used to "
"take ceil or floor while computing out shape. This operator accepts data "
"layout specification."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:24 tvm.relax.op.nn.nn.max_pool2d:23
msgid ""
"The size of window for pooling. It is required to have length either 1 or"
" 2."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:26 tvm.relax.op.nn.nn.max_pool2d:25
msgid "The strides of pooling. It is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:28 tvm.relax.op.nn.nn.max_pool2d:27
msgid "The padding for pooling. It is required to have length either 1, 2 or 4."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool2d:30 tvm.relax.op.nn.nn.max_pool2d:29
msgid "The dilation of pooling. It is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:3
msgid ""
"This operator takes data as input and does 3D average value calculation "
"with in pool_size sized window by striding defined by stride"
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:7 tvm.relax.op.nn.nn.max_pool3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` a data Tensor with "
"shape `(batch_size, channels, depth, height, width)`, to produce an "
"output Tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:17 tvm.relax.op.nn.nn.max_pool3d:17
msgid ""
"The size of window for pooling. It is required to have length either 1 or"
" 3."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:19 tvm.relax.op.nn.nn.max_pool3d:19
msgid "The strides of pooling. It is required to have length either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:21 tvm.relax.op.nn.nn.max_pool3d:21
msgid "The padding for pooling. It is required to have length either 1, 3 or 6."
msgstr ""

#: of tvm.relax.op.nn.nn.avg_pool3d:23 tvm.relax.op.nn.nn.max_pool3d:23
msgid "The dilation of pooling. It is required to have length either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:1
msgid "Batch normalization layer (Ioffe and Szegedy, 2014)."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:3
msgid ""
"Normalizes the input at each batch, i.e. applies a transformation that "
"maintains the mean activation close to 0 and the activation standard "
"deviation close to 1."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:7
msgid ""
"data\\_mean[i] = mean(data[:,i,:,...]) \\\\\n"
"data\\_var[i] = var(data[:,i,:,...])"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:12
msgid "Both *mean* and *var* returns a scalar by treating the input as a vector."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:14
msgid ""
"Then compute the normalized output, which has the same shape as input, as"
" following:"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:16
msgid ""
"out[:,i,:,...] = \\frac{data[:,i,:,...] - "
"data\\_mean[i]}{\\sqrt{data\\_var[i]+\\epsilon}}\n"
"    * gamma[i] + beta[i]"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:21
msgid ""
"Assume the input has size *k* on axis 1, then both ``gamma`` and ``beta``"
" have shape *(k,)*."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:24
msgid ""
"Besides the inputs and the outputs, this operator accepts two auxiliary "
"states, ``moving_mean`` and ``moving_var``, which are *k*-length vectors."
" They are global statistics for the whole dataset, which are updated by"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:33
msgid ""
"The parameter ``axis`` specifies which axis of the input shape denotes "
"the 'channel' (separately normalized groups).  The default is 1. "
"Specifying -1 sets the channel axis to be the last item in the input "
"shape."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:39
msgid "This operator has two modes:"
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:41
msgid "Training mode."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:42
msgid "Use the mean and var computed from THIS batch to normalize."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:43
msgid "Update and then return the running mean and running var."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:45
msgid "Inference mode."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:46
msgid "Use the running_mean and running_var parameters to normalize."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:47
msgid ""
"Do not update the running mean and running var. Just return the original "
"value."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:49
msgid ""
"In the legalization stage, this operator will be legalized to the "
"training mode by default."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:51
msgid ""
"You can use tvm.relax.transform.DecomposeOpsForInference to decompose the"
" operator, so it executes the inference mode computation. Similarly, use "
"tvm.relax.transform.DecomposeOpsForTraining to execute the training mode "
"computation."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:57 tvm.relax.op.nn.nn.group_norm:8
#: tvm.relax.op.nn.nn.instance_norm:5 tvm.relax.op.nn.nn.layer_norm:21
msgid "The gamma scale factor."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:59 tvm.relax.op.nn.nn.group_norm:10
#: tvm.relax.op.nn.nn.instance_norm:7 tvm.relax.op.nn.nn.layer_norm:23
msgid "The beta offset factor."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:61
msgid "Running mean of input."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:63
msgid "Running variance of input."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:65
#: tvm.relax.op.op_attrs.BatchNormAttrs.axis:1
msgid "The axis along which the normalization is applied."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:67 tvm.relax.op.nn.nn.group_norm:18
#: tvm.relax.op.nn.nn.instance_norm:11 tvm.relax.op.nn.nn.layer_norm:27
msgid "Small float added to variance to avoid dividing by zero."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:69 tvm.relax.op.nn.nn.group_norm:20
#: tvm.relax.op.nn.nn.instance_norm:13 tvm.relax.op.nn.nn.layer_norm:29
#: tvm.relax.op.op_attrs.BatchNormAttrs.center:1
#: tvm.relax.op.op_attrs.GroupNormAttrs.center:1
#: tvm.relax.op.op_attrs.InstanceNormAttrs.center:1
#: tvm.relax.op.op_attrs.LayerNormAttrs.center:1
msgid "Indicating if the beta offset will be added to the normalized tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:71 tvm.relax.op.nn.nn.group_norm:22
#: tvm.relax.op.nn.nn.instance_norm:15 tvm.relax.op.nn.nn.layer_norm:31
#: tvm.relax.op.op_attrs.BatchNormAttrs.scale:1
#: tvm.relax.op.op_attrs.GroupNormAttrs.scale:1
#: tvm.relax.op.op_attrs.InstanceNormAttrs.scale:1
#: tvm.relax.op.op_attrs.LayerNormAttrs.scale:1
msgid "Indicating if the gamma scale will be multiplied."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:73
#: tvm.relax.op.op_attrs.BatchNormAttrs.momentum:1
msgid "The value used for the moving_mean and moving_var update."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:75
msgid ""
"A boolean value to indicate whether training or in eval mode. By default."
"   relax batch_norm is training mode. To transform it to inference mode,"
"   can use DecomposeOpsForInference."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:76
msgid "A boolean value to indicate whether training or in eval mode. By default."
msgstr ""

#: of tvm.relax.op.nn.nn.batch_norm:77
msgid ""
"relax batch_norm is training mode. To transform it to inference mode, can"
" use DecomposeOpsForInference."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:1
msgid "1D convolution."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:3
msgid ""
"This operator takes the weight as the 1D convolution kernel and convolves"
" it with data to produce an output."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:7
msgid ""
"In the default case, where the data_layout is `NCW` and kernel_layout is "
"`OIW`, conv1d takes in a data Tensor with shape `(batch_size, "
"in_channels, width)`, and a weight Tensor with shape `(channels, "
"in_channels, kernel_w)`, where `kernel_w` is the length of the `W` kernel"
" dimension, to produce an output Tensor with the following rule:"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:14
#, python-brace-format
msgid ""
"\\mbox{out}[b, c, x] = \\sum_{dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides} * x + dx] *\n"
"   \\mbox{weight}[c, k, dx]"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:20
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCW` for data and `OIW` for weight), perform the computation, "
"then convert to the out_layout."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:28 tvm.relax.op.nn.nn.conv1d_transpose:14
#: tvm.relax.op.nn.nn.conv2d:28 tvm.relax.op.nn.nn.conv2d_transpose:24
#: tvm.relax.op.nn.nn.conv3d:30
msgid "The weight expressions."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:30 tvm.relax.op.nn.nn.conv1d_transpose:16
msgid "The strides of convolution. It is required to have length 1."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:32 tvm.relax.op.nn.nn.conv1d_transpose:18
msgid ""
"The padding of convolution on both sides of inputs before convolution. It"
" is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:35
msgid ""
"Specifies the dilation rate to be used for dilated convolution. It is "
"required to have length 1."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:38 tvm.relax.op.nn.nn.conv1d_transpose:26
#: tvm.relax.op.nn.nn.conv2d:38 tvm.relax.op.nn.nn.conv2d_transpose:36
#: tvm.relax.op.nn.nn.conv3d:40 tvm.relax.op.op_attrs.Conv1DAttrs.groups:1
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.groups:1
#: tvm.relax.op.op_attrs.Conv2DAttrs.groups:1
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.groups:1
#: tvm.relax.op.op_attrs.Conv3DAttrs.groups:1
msgid ""
"Number of groups to split the input into for grouped convolution. The "
"number of input and output channels should be divisible by the number of "
"groups."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:43 tvm.relax.op.nn.nn.conv1d_transpose:31
#: tvm.relax.op.nn.nn.conv2d:43 tvm.relax.op.nn.nn.conv2d_transpose:41
#: tvm.relax.op.nn.nn.conv3d:45
msgid "Layout of the weight."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d:47
msgid "Specifies the output data type for mixed precision conv1d."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:1
msgid "1D transposed convolution operator."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:3
msgid "This operator can be seen as the gradient operator of conv1d."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:5
#, python-format
msgid ""
"The output shape can be explained in the simple case when `data_layout =="
" \"NCW\"` and `kernel_layout == \"IOW\"`. Suppose `data` has shape `(N, "
"in_channel, in_w)`, `weight` has shape `(in_channel, out_channel, "
"weight_w)`, we need to assure that `in_channel % groups == 0`. The shape "
"of the output will be `(N, out_channel * groups, out_w)`, where"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:10
msgid ""
"`out_w = ((in_w - 1) * strides[0] + weight_w - 2 * padding[0] + "
"output_padding[0])`"
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:21
#: tvm.relax.op.nn.nn.conv2d_transpose:31
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.output_padding:1
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.output_padding:1
msgid "Used to disambiguate the output shape."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:23
msgid ""
"Specifies the dilation rate to be used for dilated convolution. It is "
"required to have length either 1."
msgstr ""

#: of tvm.relax.op.nn.nn.conv1d_transpose:35 tvm.relax.op.nn.nn.conv2d:47
#: tvm.relax.op.nn.nn.conv2d_transpose:45 tvm.relax.op.nn.nn.conv3d:49
msgid "Specifies the output data type for mixed precision conv2d."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:1
msgid "2D convolution."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:3 tvm.relax.op.nn.nn.conv3d:3
msgid ""
"This operator takes the weight as the convolution kernel and convolves it"
" with data to produce an output."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:7
msgid ""
"In the default case, where the data_layout is `NCHW` and kernel_layout is"
" `OIHW`, conv2d takes in a data Tensor with shape `(batch_size, "
"in_channels, height, width)`, and a weight Tensor with shape `(channels, "
"in_channels, kernel_h, kernel_w)`, where `kernel_h` and `kernel_w` is the"
" lengths of the `H` and `W` kernel dimensions, to produce an output "
"Tensor with the following rule:"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:14
#, python-brace-format
msgid ""
"\\mbox{out}[b, c, y, x] = \\sum_{dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * y  + dy, \\mbox{strides}[1] * "
"x + dx] *\n"
"   \\mbox{weight}[c, k, dy, dx]"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:20
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCHW` for data and `OIHW` for weight), perform the computation, "
"then convert to the out_layout."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:30 tvm.relax.op.nn.nn.conv2d_transpose:26
msgid "The strides of convolution. It is required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:32 tvm.relax.op.nn.nn.conv2d_transpose:28
msgid ""
"The padding of convolution on both sides of inputs before convolution. It"
" is required to have length either 1, 2 or 4."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d:35 tvm.relax.op.nn.nn.conv2d_transpose:33
msgid ""
"Specifies the dilation rate to be used for dilated convolution. It is "
"required to have length either 1 or 2."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:1
msgid "Two dimensional transposed convolution operator."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:3
msgid ""
"This operator is intended to be the gradient operator of conv2d. That "
"means, if"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:5
msgid "`out = conv2d(data, weight, strides, padding, dilation)`,"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:7
msgid "The gradient w.r.t. data can be calculated as follows:"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:9
msgid ""
"`data_grad = conv2d_transpose(out_grad, weight, strides, padding, "
"output_padding, dilation)`,"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:11
msgid "where `output_padding` is a parameter used to determine the output shape."
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:13
#, python-format
msgid ""
"The output shape can be explained in the simple case when `data_layout =="
" \"NCHW\"` and `kernel_layout == \"IOHW\"`. Suppose `data` has shape `(N,"
" in_channel, in_h, in_w)`, `weight` has shape `(in_channel, out_channel, "
"weight_h, weight_w)`, we need to assure that `in_channel % groups == 0`. "
"The shape of the output will be `(N, out_channel * groups, out_h, "
"out_w)`, where"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:19
msgid ""
"`out_h = ((in_h - 1) * strides[0] + weight_h - 2 * padding[0] + "
"output_padding[0])`"
msgstr ""

#: of tvm.relax.op.nn.nn.conv2d_transpose:20
msgid ""
"`out_w = ((in_w - 1) * strides[1] + weight_w - 2 * padding[1] + "
"output_padding[1])`"
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:1
msgid "3D convolution."
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:7
msgid ""
"In the default case, where the data_layout is `NCDHW` and kernel_layout "
"is `OIDHW`, conv3d takes in a data Tensor with shape `(batch_size, "
"in_channels, depth, height, width)`, and a weight Tensor with shape "
"`(channels, in_channels, kernel_d, kernel_h, kernel_w)`, where "
"`kernel_d`, `kernel_h`, and `kernel_w` are the lengths of the `D`, `H`, "
"and `W` kernel dimensions, to produce an output Tensor with the following"
" rule:"
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:14
#, python-brace-format
msgid ""
"\\mbox{out}[b, c, z, y, x] = \\sum_{dz, dy, dx, k}\n"
"   \\mbox{data}[b, k, \\mbox{strides}[0] * z + dz,\n"
"   \\mbox{strides}[1] * y  + dy,\n"
"   \\mbox{strides}[2] * x + dx] *\n"
"   \\mbox{weight}[c, k, dz, dy, dx]"
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:22
msgid ""
"Padding and dilation are applied to data and weight respectively before "
"the computation. This operator accepts data layout specification. "
"Semantically, the operator will convert the layout to the canonical "
"layout (`NCDHW` for data and `OIDHW` for weight), perform the "
"computation, then convert to the out_layout."
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:32
msgid "The strides of convolution. It is required to have length either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:34
msgid ""
"The padding of convolution on both sides of inputs before convolution. It"
" is required to have length either 1, 3 or 6."
msgstr ""

#: of tvm.relax.op.nn.nn.conv3d:37
msgid ""
"Specifies the dilation rate to be used for dilated convolution. It is "
"required to have length either 1 or 3."
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:1
msgid "CrossEntropy with logits between the predictions and labels."
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:3
msgid ""
"The shape of predictions and labels must be the same. And when ndim >= 2,"
" the first dimension is regarded as the batch_size N. In this case the "
"computed result will divide by N to perform a mean reduction."
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:7
#, python-brace-format
msgid ""
"\\text{cross\\_entropy\\_with\\_logits}(x_i, y_i) = \\frac{\\sum_i -x_i "
"\\cdot y_i}{N}"
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:11
msgid "The predictions."
msgstr ""

#: of tvm.relax.op.nn.nn.cross_entropy_with_logits:13
msgid "The labels (the ground truth values)."
msgstr ""

#: of tvm.relax.op.nn.nn.dropout:1
msgid "Applies the dropout operation to the input tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.dropout:3
msgid ""
"During training, each element of the input is set to zero with "
"probability ``p``. The whole array is scaled by ``1/(1-p)`` to keep the "
"expected sum of the input unchanged."
msgstr ""

#: of tvm.relax.op.nn.nn.dropout:9
msgid "The probability for an element to be reset to 0."
msgstr ""

#: of tvm.relax.op.nn.nn.dropout:12
msgid ""
"**result** -- The result of dropout, which is a tuple of two tensors. The"
" first one is the original tensor and the second one is a mask tensor "
"(1.0 where element not dropped, 0.0 where dropped)"
msgstr ""

#: of tvm.relax.op.nn.nn.gelu:1
msgid "Gaussian Error Linear Units function"
msgstr ""

#: of tvm.relax.op.nn.nn.gelu:3
#, python-brace-format
msgid ""
"\\text{GeLU}(x) = 0.5 * x * (1 + \\text{erf}(x * 0.5**0.5))\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.gelu:6
msgid "where :math:`erf` is the Gauss Error function."
msgstr ""

#: of tvm.relax.op.nn.nn.gelu_tanh:1
msgid "Gaussian Error Linear Units function with tanh approximation"
msgstr ""

#: of tvm.relax.op.nn.nn.gelu_tanh:3
#, python-brace-format
msgid ""
"\\text{GELU}(x) = 0.5 * x * (1 + \\text{Tanh}(\\sqrt(2 / \\pi) * (x + "
"0.044715 * x^3)))\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:1
msgid ""
"Group normalization (Yuxin Wu and et al., 2016). Applies group "
"normalization to the n-dimensional input array. This operator takes an "
"n-dimensional input array. First separate the input array into groups "
"along the channel axis. Then apply layer normalization to each group."
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:6
msgid "Input to which group_norm will be applied."
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:12
msgid "Number of groups to separate the channels into."
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:14
msgid "The index of the channel axis in the input data."
msgstr ""

#: of tvm.relax.op.nn.nn.group_norm:16
msgid ""
"The axes that along which the normalization is applied (excluding the "
"group axis)"
msgstr ""

#: of tvm.relax.op.nn.nn.instance_norm:1
msgid "Instance normalization"
msgstr ""

#: of tvm.relax.op.nn.nn.instance_norm:3
msgid "Input to which instance_norm will be applied."
msgstr ""

#: of tvm.relax.op.nn.nn.instance_norm:9 tvm.relax.op.nn.nn.layer_norm:25
#: tvm.relax.op.nn.nn.rms_norm:14
#: tvm.relax.op.op_attrs.InstanceNormAttrs.axes:1
#: tvm.relax.op.op_attrs.LayerNormAttrs.axes:1
#: tvm.relax.op.op_attrs.RMSNormAttrs.axes:1
msgid "The axes that along which the normalization is applied."
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:1
msgid ""
"Layer normalization (Lei Ba and et al., 2016). Applies layer "
"normalization to the n-dimensional input array. This operator takes an "
"n-dimensional input array and normalizes the input using the given axis:"
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:6
msgid ""
"out = \\frac{data - mean(data, axis)}{\\sqrt{var(data, axis)+\\epsilon}}\n"
"    * gamma + beta"
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:11
msgid ""
"Unlike batch normalization, the mean and var are computed along the "
"channel dimension."
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:13
msgid ""
"Assume the input has size k on axis 1, then both gamma and beta have "
"shape (k,)."
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:17 tvm.relax.op.nn.nn.log_softmax:8
msgid "This operator can be optimized away for inference."
msgstr ""

#: of tvm.relax.op.nn.nn.layer_norm:19
msgid "Input to which layer_norm will be applied."
msgstr ""

#: of tvm.relax.op.nn.nn.leakyrelu:1 tvm.relax.op.nn.nn.relu:1
msgid "Rectified linear unit."
msgstr ""

#: of tvm.relax.op.nn.nn.leakyrelu:3
#, python-brace-format
msgid ""
"text{LeakyReLU, negative_slope}(x) = max(x, 0) + negative_slope * min(x, "
"0)\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.leakyrelu:8
msgid ""
"Controls the angle of the negative slope, used for nagative inputs. "
"Default value is 0.01"
msgstr ""

#: of tvm.relax.op.nn.nn.log_softmax:1
msgid "Computes log softmax."
msgstr ""

#: of tvm.relax.op.nn.nn.log_softmax:3
#, python-brace-format
msgid ""
"\\text{log\\_softmax}(x_i) = \\log\\left( \\frac{\\exp(x_i)}{\\sum_j "
"\\exp(x_j)}\\right)"
msgstr ""

#: of tvm.relax.op.nn.nn.log_softmax:12
msgid ""
"The axis to sum over when computing log softmax. If not specified, it is "
"by default the last axis of the input tensor. Supports negative indexing."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:1
msgid "1D maximum pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:3
msgid ""
"This operator takes data as input and does 1D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:6
msgid ""
"IIn the default case, where the data_layout is `NCW` a data Tensor with "
"shape `(batch_size, channels, width)`, to produce an output Tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:16
msgid "The size of window for pooling. It is required to have length either 1."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:18
msgid "The strides of pooling. It is required to have length either 1."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool1d:22
msgid "The dilation of pooling. It is required to have length either 1."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool2d:1
msgid "2D maximum pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool2d:3
msgid ""
"This operator takes data as input and does 2D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool2d:12
#, python-brace-format
msgid ""
"\\mbox{out}(b, c, y, x)  = \\max_{m=0, \\ldots, kh-1} \\max_{n=0, "
"\\ldots, kw-1}\n"
"     \\mbox{data}(b, c, \\mbox{stride}[0] * y + m, \\mbox{stride}[1] * x "
"+ n)"
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool3d:1
msgid "3D maximum pooling operator."
msgstr ""

#: of tvm.relax.op.nn.nn.max_pool3d:3
msgid ""
"This operator takes data as input and does 3D max value calculation with "
"in pool_size sized window by striding defined by stride."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:1
msgid "Negative log likelihood loss."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:3
msgid ""
"`output[n, i_1, i_2, ..., i_k] = -p * w`, where - `p = predictions[n, t, "
"i_1, i_2, i_k]`, - `t = targets[n, i_1, i_2, ..., i_k]`, - `w = "
"weights[t] if t != ignore_index else 0`"
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:8
msgid "result = reduction(output)"
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:10
msgid ""
"The predictions. Should be a `(k+2)-D` Tensor with shape `(N, C, d_1, "
"d_2, ..., d_k)` where C is the number of target classes."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:13
msgid ""
"The target value of each prediction. Should be a `(k+1)-D` Tensor with "
"shape `(N, d_1, d_2, ..., d_k)`. Must be of int dtype."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:16
msgid ""
"The weight of each target value. Should be a `1-D` Tensor with shape "
"`(C,)`. If not specified, it is treated as if having all ones."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:19
msgid ""
"The reduction method to apply to the output. Possible values are "
"\"mean\", \"sum\" and \"none\"."
msgstr ""

#: of tvm.relax.op.nn.nn.nll_loss:22
#: tvm.relax.op.op_attrs.NLLLossAttrs.ignore_index:1
msgid "The target value to ignore."
msgstr ""

#: of tvm.relax.op.nn.nn.pad:1
msgid "Padding"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:3
msgid ""
"This operator takes in a tensor and pads each axis by the specified "
"widths using the specified value."
msgstr ""

#: of tvm.relax.op.nn.nn.pad:6
msgid "The input data to the operator"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:8
msgid ""
"Number of values padded to the edges of each axis, in the format of "
"((before_1, after_1), ..., (before_N, after_N))"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:11
msgid ""
"'constant', 'reflect', 'replicate', 'circular' 'constant' pads with "
"constant value pad_value 'reflect' pads by mirroring values excluding the"
" edge 'replicate' pads by repeating the edge values. 'circular' pads by "
"looping values from the other side Default is 'constant'"
msgstr ""

#: of tvm.relax.op.nn.nn.pad:18
msgid "The value used for padding. Default is 0."
msgstr ""

#: of tvm.relax.op.nn.nn.pixel_shuffle:1
msgid "Pixel Shuffle Operator"
msgstr ""

#: of tvm.relax.op.nn.nn.pixel_shuffle:3
msgid ""
"This operator performs the pixel shuffle operation on the input tensor, "
"which is often used for efficient sub-pixel convolution in image super-"
"resolution tasks. It rearranges elements in a tensor of shape (N, C × "
"r^2, H, W) to a tensor of shape (N, C, H × r, W × r), where `r` is the "
"upscale factor."
msgstr ""

#: of tvm.relax.op.nn.nn.pixel_shuffle:9
msgid ""
"The input tensor to the pixel shuffle operator. It must have 4 dimensions"
" with the format (N, C * r^2, H, W), where `r` is the upscale factor."
msgstr ""

#: of tvm.relax.op.nn.nn.pixel_shuffle:12
msgid ""
"The upscaling factor `r`. It determines how much to increase the spatial "
"resolution (height and width) of the input tensor."
msgstr ""

#: of tvm.relax.op.nn.nn.pixel_shuffle:16
msgid "**result** -- The transformed tensor with shape (N, C, H * r, W * r)."
msgstr ""

#: of tvm.relax.op.nn.nn.pixel_shuffle:21
msgid ""
"If the input tensor has shape (1, 8, 10, 15) and `upscale_factor` is 2, "
"the resulting tensor will have shape (1, 2, 20, 30)."
msgstr ""

#: of tvm.relax.op.nn.nn.prelu:1
msgid "Parametric Rectified Linear Unit (PReLU)."
msgstr ""

#: of tvm.relax.op.nn.nn.prelu:3
#, python-brace-format
msgid ""
"PReLU(x) = x \\text{ if } x > 0 \\text{ else } \\alpha * x\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.prelu:8
msgid "The learnable slope tensor, applied channel-wise."
msgstr ""

#: of tvm.relax.op.nn.nn.prelu:10
msgid ""
"The axis along which the `alpha` values are applied Default is 1 "
"(assuming NCHW format)."
msgstr ""

#: of tvm.relax.op.nn.nn.relu:3
#, python-brace-format
msgid ""
"\\text{ReLU}(x) = \\max(x, 0)\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.relu6:1
msgid "ReLU6 activation function."
msgstr ""

#: of tvm.relax.op.nn.nn.relu6:3
#, python-brace-format
msgid ""
"\\text{ReLU6}(x) = \\min(\\max(x, 0), 6)\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:1
msgid ""
"Root mean square normalization (Biao Zhang and et al., 2019). Applies "
"root mean square normalization to the n-dimensional input array. This "
"operator takes an n-dimensional input array and normalizes the input "
"using the given axis:"
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:6
msgid "out = \\frac{data}{\\sqrt{mean(data, axis)+\\epsilon}} * weight"
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:10
msgid "Input to which rms_norm will be applied."
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:12
msgid "The scale factor."
msgstr ""

#: of tvm.relax.op.nn.nn.rms_norm:16
msgid "Small float added to square mean to avoid dividing by zero."
msgstr ""

#: of tvm.relax.op.nn.nn.selu:1
msgid "Scaled Exponential Linear Unit (SELU)."
msgstr ""

#: of tvm.relax.op.nn.nn.selu:3
#, python-brace-format
msgid ""
"\\text{SELU}(x) = \\lambda \\begin{cases}\n"
"    x & \\text{if } x > 0 \\\\\n"
"    \\alpha (e^x - 1) & \\text{if } x \\leq 0\n"
"\\end{cases}\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.selu:9
msgid ""
"where :math:`\\lambda \\approx 1.0507` and :math:`\\alpha \\approx "
"1.6733`."
msgstr ""

#: of tvm.relax.op.nn.nn.silu:1
msgid "Sigmoid Linear Unit function"
msgstr ""

#: of tvm.relax.op.nn.nn.silu:3
#, python-brace-format
msgid ""
"\\text{SiLU}(x) = x * \\text{sigmoid}(x)\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.softmax:1
msgid "Computes softmax."
msgstr ""

#: of tvm.relax.op.nn.nn.softmax:3
#, python-brace-format
msgid ""
"\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.softmax:7
msgid ""
"The axis to sum over when computing softmax. If not specified, it is by "
"default the last axis of the input tensor. Supports negative indexing."
msgstr ""

#: of tvm.relax.op.nn.nn.softplus:1
msgid "Softplus activation function."
msgstr ""

#: of tvm.relax.op.nn.nn.softplus:3
#, python-brace-format
msgid ""
"\\text{Softplus}(x) = \\frac{1}{\\beta} \\log(1 + e^{\\beta x})\n"
"\n"
msgstr ""

#: of tvm.relax.op.nn.nn.softplus:7
msgid "Controls the smoothness of the transition. Default is 1.0."
msgstr ""

#: of tvm.relax.op.nn.nn.softplus:9
msgid ""
"The value beyond which the function is approximated as linear to avoid "
"numerical instability. Default is 20.0."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:34
msgid "tvm.relax.op.builtin"
msgstr ""

#: of tvm.relax.op.builtin:1
msgid "Relax builtin operators."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:1
msgid ""
"Construct a Call to allocate a tensor with specific shape, dtype, "
"runtime_device_index."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:3
#: tvm.relax.op.memory.memory.alloc_tensor:7
msgid "The shape of the tensor to be allocated."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:5
#: tvm.relax.op.memory.memory.alloc_tensor:9
msgid "The datatype of the tensor to be allocated."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:7
msgid ""
"The device index indicating on which device the tensor is to be allocated"
" at runtime. Index -1 is reserved for the host device."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:10
#: tvm.relax.op.memory.memory.alloc_storage:9
msgid "The storage scope to allocate the storage to."
msgstr ""

#: of tvm.relax.op.builtin.builtin.alloc_tensor:13
#: tvm.relax.op.memory.memory.alloc_tensor:12
msgid "**result** -- A relax Call, which gets the allocated tensor."
msgstr ""

#: of tvm.relax.op.builtin.builtin.stop_lift_params:1
msgid ""
"An indicator that the consumers of input tensor should not be lifted to "
"transform_params function"
msgstr ""

#: of tvm.relax.op.builtin.builtin.stop_lift_params:7
msgid "**result** -- The result tensor that is the same as input tensor"
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:40
msgid "tvm.relax.op.ccl"
msgstr ""

#: of tvm.relax.op.ccl:1
msgid "CCL related operators."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:1
msgid "AllGather operator"
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:5
msgid "The number of workers to gather data from."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:7
msgid "Whether the gather operation performs globally or in group as default."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allgather:10
msgid "**result** -- The result of allgather."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allreduce:1
msgid "Allreduce operator"
msgstr ""

#: of tvm.relax.op.ccl.ccl.allreduce:5
msgid ""
"The type of reduction operation to be applied to the input data. Now "
"\"sum\", \"prod\", \"min\", \"max\" and \"avg\" are supported."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allreduce:8
msgid "Whether the reduction operation performs globally or in group as default."
msgstr ""

#: of tvm.relax.op.ccl.ccl.allreduce:11
msgid "**result** -- The result of allreduce."
msgstr ""

#: of tvm.relax.op.ccl.ccl.broadcast_from_worker0:1
msgid "Broadcast data from worker-0 to all other workers."
msgstr ""

#: of tvm.relax.op.ccl.ccl.broadcast_from_worker0:3
msgid "The tensor to be broadcast."
msgstr ""

#: of tvm.relax.op.ccl.ccl.broadcast_from_worker0:6
msgid ""
"**result** -- The same tensor, which has been broadcast to all other "
"workers."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:1
msgid ""
"Perform a scatter operation from worker-0, chunking the given buffer into"
" equal parts."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:3
msgid ""
"The buffer to be divided into equal parts and sent to each worker "
"accordingly."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:5
msgid ""
"The number of workers, i.e. the number of parts the given buffer should "
"be chunked into."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:7
msgid "The dimension of the tensor to be scattered. Default is 0."
msgstr ""

#: of tvm.relax.op.ccl.ccl.scatter_from_worker0:10
msgid "**result** -- Chunked Tensor received by different workers."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:46
msgid "tvm.relax.op.distributed"
msgstr ""

#: of tvm.relax.op.distributed:1
msgid "Operators serving for distributed Relax."
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:1
msgid "Annotate sharding plan for tensor"
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:5
msgid "The device mesh of the sharding plan"
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:7
msgid "The placement of the sharding plan"
msgstr ""

#: of tvm.relax.op.distributed.distributed.annotate_sharding:10
msgid "**result** -- The tensor unmodified."
msgstr ""

#: of tvm.relax.op.distributed.distributed.call_tir_local_view:1
msgid ""
"Call a tir.prim_func and return the output. The prim_func should be a "
"worker-local function that is actually executed on each worker, instead "
"of the unpartitioned function. The output of this operator is DTensor or "
"a tuple of DTensors."
msgstr ""

#: of tvm.relax.op.distributed.distributed.call_tir_local_view:9
msgid ""
"The structure info of the call_tir output. It should be a single or a "
"list of DTensorStructInfo. Each one denotes the structure info of a "
"returned tensor."
msgstr ""

#: of tvm.relax.op.distributed.distributed.call_tir_local_view:16
msgid "**ret** -- A call node for the call_tir_local_view operator."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute:1
msgid "Redistribute tensor"
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute:5
msgid "The device mesh after redistribution"
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute:7
msgid "The placement after redistribution"
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute:10
msgid "**result** -- The tensor after redistribution."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:1
msgid "Slice tensor into several parts along one axis,"
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:2
msgid ""
"and each worker takes one part. input.struct_info.shape[axis] % "
"num_workers == 0 is required. Each worker must have an identical copy of "
"the input. This is a specialized version of redistribute op."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:7
msgid "The buffer to be sliced into equal parts."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:9
msgid ""
"The number of workers, i.e. the number of parts the given buffer should "
"be sliced into."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:11
msgid "The axis of the tensor to be sliced."
msgstr ""

#: of tvm.relax.op.distributed.distributed.redistribute_replica_to_shard:14
msgid "**result** -- Sliced Tensor kept by each device."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:52
msgid "tvm.relax.op.grad"
msgstr ""

#: of tvm.relax.op.grad:1
msgid "Operators serving for finding gradient of relax operators."
msgstr ""

#: of tvm.relax.op.grad.grad.avg_pool2d_backward:1
msgid ""
"Backward operator of relax.nn.avg_pool2d. All parameters except "
"output_grad is the same as relax.nn.avg_pool2d. Returns the gradient "
"w.r.t. data."
msgstr ""

#: of tvm.relax.op.grad.grad.avg_pool2d_backward:4
msgid "The gradient w.r.t. the result of avg_pool2d."
msgstr ""

#: of tvm.relax.op.grad.grad.avg_pool2d_backward:7
#: tvm.relax.op.grad.grad.max_pool2d_backward:7
msgid "**result** -- The gradient w.r.t. data."
msgstr ""

#: of tvm.relax.op.grad.grad.end_checkpoint:1
msgid "Mark the end of checkpoint stage. See tvm.relax.op.grad.start_checkpoint."
msgstr ""

#: of tvm.relax.op.grad.grad.end_checkpoint:3
msgid "The output of the checkpoint stage."
msgstr ""

#: of tvm.relax.op.grad.grad.end_checkpoint:6
#: tvm.relax.op.grad.grad.start_checkpoint:27
msgid "**result** -- The same tensor as the input."
msgstr ""

#: of tvm.relax.op.grad.grad.max_pool2d_backward:1
msgid ""
"Backward operator of relax.nn.max_pool2d. All parameters except "
"output_grad is the same as relax.nn.max_pool2d. Returns the gradient "
"w.r.t. data."
msgstr ""

#: of tvm.relax.op.grad.grad.max_pool2d_backward:4
msgid "The gradient w.r.t. the result of max_pool2d."
msgstr ""

#: of tvm.relax.op.grad.grad.nll_loss_backward:1
msgid ""
"Backward operator of relax.nn.nll_loss. All parameters except output_grad"
" is the same as relax.nn.nll_loss. Returns the gradient w.r.t. "
"predictions."
msgstr ""

#: of tvm.relax.op.grad.grad.nll_loss_backward:4
msgid "The gradient w.r.t. the result of nll_loss."
msgstr ""

#: of tvm.relax.op.grad.grad.nll_loss_backward:7
msgid "**result** -- The gradient w.r.t. predictions."
msgstr ""

#: of tvm.relax.op.grad.grad.no_grad:1
msgid "No gradient dummy operator w.r.t. the input."
msgstr ""

#: of tvm.relax.op.grad.grad.no_grad:3
msgid "The corresponding input tensor."
msgstr ""

#: of tvm.relax.op.grad.grad.no_grad:6
msgid "**result** -- The no-gradient representation w.r.t. input."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:1
msgid ""
"Mark the start of the checkpoint stage. The computation between "
"start_checkpoint and end_checkpoint will be marked as the checkpoint "
"stage."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:4
msgid ""
"Rather than storing all intermediate activations of the entire "
"computation graph for computing backward, the checkpointed stage does not"
" save intermediate activations, and instead recomputes them in backward "
"process."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:8
msgid ""
"For instance, ``` a = relax.Var(\"a\", relax.TensorStructInfo((2, 2), "
"\"float32\")) b = relax.Var(\"b\", relax.TensorStructInfo((2, 2), "
"\"float32\")) c = a * 2 d = b * 2 c_cp = start_checkpoint(c) d_cp = "
"start_checkpoint(d) e = c_cp + d_cp e_out = end_checkpoint(e) ``` Then "
"`e` will be recomputed in the backward stage."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:21
msgid ""
"See tvm.relax.transform.Gradient, tvm.relax.testing.nn.checkpoint, "
"tvm.relax.op.grad.end_checkpoint for more information."
msgstr ""

#: of tvm.relax.op.grad.grad.start_checkpoint:24
msgid "The tensor marking the input of the checkpoint stage."
msgstr ""

#: of tvm.relax.op.grad.grad.take_backward:1
msgid ""
"Backward operator of relax.take. All parameters except output_grad is the"
" same as relax.take. Returns the gradient w.r.t. x."
msgstr ""

#: of tvm.relax.op.grad.grad.take_backward:4
msgid "The gradient w.r.t. the result of take."
msgstr ""

#: of tvm.relax.op.grad.grad.take_backward:7
msgid "**result** -- The gradient w.r.t. x."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:58
msgid "tvm.relax.op.image"
msgstr ""

#: of tvm.relax.op.image:1
msgid "Image operators."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:1
msgid "Image resize2d operator."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:3
msgid ""
"This operator takes data as input and does 2D scaling to the given scale "
"factor. In the default case, where the data_layout is `NCHW` with data of"
" shape (n, c, h, w) out will have a shape (n, c, size[0], size[1])"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:8
msgid ""
"method indicates the algorithm to be used while calculating the out value"
" and method can be one of (\"linear\", \"nearest_neighbor\", \"cubic\")"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:13
msgid ""
"The out size to which the image will be resized. If specified as a list, "
"it is required to have length either 1 or 2. If specified as an Expr, it "
"is required to have ndim 2."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:17
msgid ""
"The region of interest for cropping the input image. Expected to be of "
"size 4, and format [start_h, start_w, end_h, end_w]. Only used if "
"coordinate_transformation_mode is tf_crop_and_resize."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:23
msgid "Scale method to used [nearest_neighbor, linear, cubic]."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:25
msgid ""
"Describes how to transform the coordinate in the resized tensor to the "
"coordinate in the original tensor. Definitions can be found in "
"topi/image/resize.py. [half_pixel, align_corners, asymmetric, "
"pytorch_half_pixel, tf_half_pixel_for_nn, and tf_crop_and_resize]."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:31
msgid ""
"indicates how to find the \"nearest\" pixel in nearest_neighbor method "
"[round, floor, ceil]"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:34
msgid "Spline Coefficient for bicubic interpolation"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:36
#: tvm.relax.op.op_attrs.Resize2DAttrs.cubic_exclude:1
msgid "Flag to exclude exterior of the image during bicubic interpolation"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:38
msgid "Fill value to use when roi is outside of the image"
msgstr ""

#: of tvm.relax.op.image.image.resize2d:40
#: tvm.relax.op.op_attrs.Resize2DAttrs.out_dtype:1
msgid ""
"The dtype of the output tensor. It it is not specified, the output will "
"have the same dtype as input if not specified."
msgstr ""

#: of tvm.relax.op.image.image.resize2d:44
msgid "**result** -- The resized result."
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:64
msgid "tvm.relax.op.memory"
msgstr ""

#: of tvm.relax.op.memory:1
msgid "Relax memory primitives."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:1
msgid ""
"Construct a Call to allocate a storage with specific size, "
"virtual_device_index, storage_scope and dtype."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:4
msgid "The size of the storage to be allocated."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:6
msgid ""
"The virtual device index indicating on which device the storage is to be "
"allocated. Index -1 is reserved for the host device."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:11
msgid "The datatype of the storage to be allocated."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_storage:14
msgid "**result** -- A relax Call, which gets the allocated storage."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_tensor:1
msgid ""
"Construct a Call to allocate a tensor on a certain storage starting from "
"the given offset."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_tensor:3
msgid "The storage to allocate the tensor to."
msgstr ""

#: of tvm.relax.op.memory.memory.alloc_tensor:5
msgid "The storage offset to allocate the tensor."
msgstr ""

#: of tvm.relax.op.memory.view.ensure_zero_offset:1
msgid "Ensure the tensor has elem_offset == 0. A copy will be made if necessary."
msgstr ""

#: of tvm.relax.op.memory.view.ensure_zero_offset:3
msgid "The input tensor"
msgstr ""

#: of tvm.relax.op.memory.view.ensure_zero_offset:7
msgid "The tensor with elem_offset == 0"
msgstr ""

#: of tvm.relax.op.memory.memory.kill_storage:1
msgid "Construct a Call to kill a storage."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_storage:3
msgid "The storage to be killed."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_storage:6
msgid "**result** -- A relax Call to kill a storage."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_tensor:1
msgid "Construct a Call to kill a tensor."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_tensor:3
msgid "The tensor to be killed."
msgstr ""

#: of tvm.relax.op.memory.memory.kill_tensor:6
msgid "**result** -- A relax Call to kill a tensor."
msgstr ""

#: of tvm.relax.op.memory.view.view:1
msgid "Provide a view into an existing tensor"
msgstr ""

#: of tvm.relax.op.memory.view.view:3
msgid ""
"The view may have a different shape, may be a different datatype, and may"
" start at an offset relative to the source array."
msgstr ""

#: of tvm.relax.op.memory.view.view:6
msgid ""
"Regardless of which combination of these options are used, the view may "
"never access memory that was not accessible through the input `data` "
"array.  This restriction applies even if the `data` array is itself a "
"view into a shared backing array."
msgstr ""

#: of tvm.relax.op.memory.view.view:13
msgid ""
"The target shape.  Should be a `relax.ShapeExpr`, or a collection that "
"can be converted to a `relax.ShapeExpr`."
msgstr ""

#: of tvm.relax.op.memory.view.view:16
msgid ""
"The target datatype.  Should be a `relax.ShapeExpr`, or a collection that"
" can be converted to a `relax.ShapeExpr`."
msgstr ""

#: of tvm.relax.op.memory.view.view:19
msgid ""
"The offset of the output Tensor, relative to the byte offset of `data`.  "
"If `None`, the offset of the view is the same as the offset of `data`."
msgstr ""

#: of tvm.relax.op.memory.view.view:24
msgid "**result** -- The tensor view"
msgstr ""

#: ../../doc/docs/reference/api/python/relax/op.rst:70
msgid "tvm.relax.op.op_attrs"
msgstr ""

#: of tvm.relax.op.op_attrs:1
msgid "The attributes node used for Relax operators"
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool1DAttrs:1
msgid "Attributes for 1d adaptive pool operator"
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool1DAttrs.layout:1
msgid ""
"Dimension ordering of input data. Can be 'NCW', 'NWC', etc.'N', 'C', 'W' "
"stands for batch, channel and widthdimensions respectively. Pooling is "
"applied on the'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool1DAttrs.out_layout:1
msgid ""
"Dimension ordering of output data. Can be 'NCW', 'NWC', etc.'N', 'C', 'W'"
" stands for batch, channel and widthdimensions respectively. Pooling is "
"applied on the'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool1DAttrs.output_size:1
msgid "Output width."
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool2DAttrs:1
msgid "Attributes for 2d adaptive pool operator"
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool2DAttrs.layout:1
#: tvm.relax.op.op_attrs.Pool2DAttrs.layout:1
msgid ""
"Dimension ordering of input data. Can be 'NCHW', 'NHWC', etc.'N', 'C', "
"'H', 'W' stands for batch, channel, height, and widthdimensions "
"respectively. Pooling is applied on the 'H' and'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool2DAttrs.out_layout:1
#: tvm.relax.op.op_attrs.Pool2DAttrs.out_layout:1
msgid ""
"Dimension ordering of output data. Can be 'NCHW', 'NHWC', etc.'N', 'C', "
"'H', 'W' stands for batch, channel, height, and widthdimensions "
"respectively. Pooling is applied on the 'H' and'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool2DAttrs.output_size:1
msgid "Output height and width."
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool3DAttrs:1
msgid "Attributes for 3d adaptive pool operator"
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool3DAttrs.layout:1
msgid ""
"Dimension ordering of input data. Can be 'NCDHW', 'NDHWC', etc.'N', 'C', "
"'D', 'H', 'W' stands for batch, channel, depth, height, and "
"widthdimensions respectively. Pooling is applied on 'D', 'H' and'W' "
"dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool3DAttrs.out_layout:1
msgid ""
"Dimension ordering of output data. Can be 'NCDHW', 'NDHWC', etc.'N', 'C',"
" 'D', 'H', 'W' stands for batch, channel, depth, height, and "
"widthdimensions respectively. Pooling is applied on 'D', 'H' and'W' "
"dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.AdaptivePool3DAttrs.output_size:1
msgid "Output depth, height and width."
msgstr ""

#: of tvm.relax.op.op_attrs.AllGatherAttrs:1
msgid "Attributes used in allgather operator"
msgstr ""

#: of tvm.relax.op.op_attrs.AllGatherAttrs.in_group:1
msgid ""
"Whether the allgather operation performs in group or globally or in group"
" as default."
msgstr ""

#: of tvm.relax.op.op_attrs.AllGatherAttrs.num_workers:1
#: tvm.relax.op.op_attrs.ScatterCollectiveAttrs.num_workers:1
msgid ""
"The number of workers, also the number of parts the given buffer should "
"be chunked into."
msgstr ""

#: of tvm.relax.op.op_attrs.AllReduceAttrs:1
msgid "Attributes used in allreduce operator"
msgstr ""

#: of tvm.relax.op.op_attrs.AllReduceAttrs.in_group:1
msgid ""
"Whether the reduction operation performs in group or globally or in group"
" as default."
msgstr ""

#: of tvm.relax.op.op_attrs.AllReduceAttrs.op_type:1
msgid ""
"The type of reduction operation to be applied to the input data. Now only"
" sum is supported."
msgstr ""

#: of tvm.relax.op.op_attrs.ArgmaxArgminAttrs:1
msgid "Attributes for argmax/argmin operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ArgmaxArgminAttrs.axis:1
msgid "The axis along which to perform the argmin/argmax."
msgstr ""

#: of tvm.relax.op.op_attrs.ArgmaxArgminAttrs.keepdims:1
msgid ""
"If this is set to `True`, the reduced axis is left in the result as "
"dimension with size one."
msgstr ""

#: of tvm.relax.op.op_attrs.ArgsortAttrs:1
msgid "Attributes for argsort operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ArgsortAttrs.axis:1
msgid ""
"Axis along which the argsort is computed.The default the last axis is "
"used."
msgstr ""

#: of tvm.relax.op.op_attrs.ArgsortAttrs.descending:1
msgid ""
"Whether to argsort in descending order.If it is not specified, it "
"defaults to the ascending order."
msgstr ""

#: of tvm.relax.op.op_attrs.ArgsortAttrs.dtype:1
msgid "DType of the output indices."
msgstr ""

#: of tvm.relax.op.op_attrs.AstypeAttrs:1
msgid "Attributes used in astype operator"
msgstr ""

#: of tvm.relax.op.op_attrs.AstypeAttrs.dtype:1
#: tvm.relax.op.op_attrs.WrapParamAttrs.dtype:1
msgid "Target data type"
msgstr ""

#: of tvm.relax.op.op_attrs.AttentionAttrs:1
msgid "Attributes used in attention operator"
msgstr ""

#: of tvm.relax.op.op_attrs.AttentionAttrs.causal_mask:1
msgid "The type of the causal mask, i.e. 'TopLeft' and 'BottomRight'."
msgstr ""

#: of tvm.relax.op.op_attrs.AttentionAttrs.scale:1
msgid ""
"The custom scale applied before the softmax. The default value is 1 / "
"sqrt(head_dim)."
msgstr ""

#: of tvm.relax.op.op_attrs.BatchNormAttrs:1
msgid "Attributes used in batch_norm operator"
msgstr ""

#: of tvm.relax.op.op_attrs.BatchNormAttrs.epsilon:1
#: tvm.relax.op.op_attrs.GroupNormAttrs.epsilon:1
#: tvm.relax.op.op_attrs.InstanceNormAttrs.epsilon:1
#: tvm.relax.op.op_attrs.LayerNormAttrs.epsilon:1
#: tvm.relax.op.op_attrs.RMSNormAttrs.epsilon:1
msgid "Small float added to variance to avoid dividing by zero"
msgstr ""

#: of tvm.relax.op.op_attrs.BatchNormAttrs.training:1
msgid "Whether we are training (i.e., not in eval mode)."
msgstr ""

#: of tvm.relax.op.op_attrs.CallInplacePackedAttrs:1
msgid "Attributes used in call_inplace_packed operator"
msgstr ""

#: of tvm.relax.op.op_attrs.CallTIRInplaceAttrs:1
msgid "Attributes used in call_tir_inplace operator"
msgstr ""

#: of tvm.relax.op.op_attrs.CallTIRWithGradAttrs:1
msgid "Attributes used in call_tir_with_grad operator"
msgstr ""

#: of tvm.relax.op.op_attrs.CallTIRWithGradAttrs.te_grad_kwargs:1
msgid "The keyword arguments passed to the te gradient function."
msgstr ""

#: of tvm.relax.op.op_attrs.CallTIRWithGradAttrs.te_grad_name:1
msgid ""
"The name of the te gradient function associated with this "
"call_tir_with_grad node."
msgstr ""

#: of tvm.relax.op.op_attrs.ConcatAttrs:1 tvm.relax.op.op_attrs.StackAttrs:1
msgid "Attributes for concat operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ConcatAttrs.axis:1
msgid ""
"The axis at which the input arrays are concatenated.Should lie in range "
"`[-ndim, ndim)`."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs:1
msgid "Attributes for nn.conv1d"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.data_layout:1
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.data_layout:1
msgid ""
"Dimension ordering of input data. Can be 'NCW', 'NWC', etc.'N', 'C', 'W' "
"stands for batch, channel, widthdimensions respectively. Convolution is "
"applied on the 'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.dilation:1
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.dilation:1
#: tvm.relax.op.op_attrs.Conv2DAttrs.dilation:1
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.dilation:1
#: tvm.relax.op.op_attrs.Conv3DAttrs.dilation:1
msgid "Specifies the dilation rate to use for dilated convolution."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.kernel_layout:1
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.kernel_layout:1
msgid ""
"Dimension ordering of weight. Can be 'OIW', 'IOW', etc.'O', 'I', 'W' "
"stands for num_filter, input_channel, and widthdimensions respectively."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.out_dtype:1
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.out_dtype:1
#: tvm.relax.op.op_attrs.Conv2DAttrs.out_dtype:1
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.out_dtype:1
#: tvm.relax.op.op_attrs.Conv3DAttrs.out_dtype:1
msgid "Output data type, set to explicit type under mixed precision setting"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.out_layout:1
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.out_layout:1
msgid ""
"Dimension ordering of output. Can be 'NCW', 'NWC', etc.'N', 'C', 'W' "
"stands for batch, channel, and widthdimensions respectively. Default to "
"be same as input layout."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.padding:1
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.padding:1
#: tvm.relax.op.op_attrs.Pool1DAttrs.padding:1
msgid "padding width in the order of (left, right)"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.padding
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.padding
#: tvm.relax.op.op_attrs.Conv2DAttrs.padding
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.padding
#: tvm.relax.op.op_attrs.Conv3DAttrs.padding
#: tvm.relax.op.op_attrs.Pool1DAttrs.padding
#: tvm.relax.op.op_attrs.Pool2DAttrs.padding
#: tvm.relax.op.op_attrs.Pool3DAttrs.padding
msgid "type"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.padding:3
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.padding:3
#: tvm.relax.op.op_attrs.Conv2DAttrs.padding:3
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.padding:3
#: tvm.relax.op.op_attrs.Conv3DAttrs.padding:3
#: tvm.relax.op.op_attrs.Pool1DAttrs.padding:3
#: tvm.relax.op.op_attrs.Pool2DAttrs.padding:3
#: tvm.relax.op.op_attrs.Pool3DAttrs.padding:3
msgid ""
"If padding is non-zero, then the input is implicitly zero-paddedPadding "
"support both symmetric and asymmetric asone int"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.padding:5
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.padding:5
msgid "same padding used on both sidestwo int"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DAttrs.strides:1
#: tvm.relax.op.op_attrs.Conv1DTransposeAttrs.strides:1
#: tvm.relax.op.op_attrs.Conv2DAttrs.strides:1
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.strides:1
#: tvm.relax.op.op_attrs.Conv3DAttrs.strides:1
#: tvm.relax.op.op_attrs.Pool1DAttrs.strides:1
#: tvm.relax.op.op_attrs.Pool2DAttrs.strides:1
#: tvm.relax.op.op_attrs.Pool3DAttrs.strides:1
msgid "Specifies the strides of the convolution."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv1DTransposeAttrs:1
msgid "Attributes for nn.conv1d_transpose"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv2DAttrs:1
msgid "Attributes for nn.conv2d"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv2DAttrs.data_layout:1
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.data_layout:1
msgid ""
"Dimension ordering of input data. Can be 'NCHW', 'NHWC', etc.'N', 'C', "
"'H', 'W' stands for batch, channel, height, and widthdimensions "
"respectively. Convolution is applied on the 'H' and'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv2DAttrs.kernel_layout:1
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.kernel_layout:1
msgid ""
"Dimension ordering of weight. Can be 'OIHW', 'OIHW16o16i', etc.'O', 'I', "
"'H', 'W' stands for num_filter, input_channel, height, and "
"widthdimensions respectively."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv2DAttrs.out_layout:1
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.out_layout:1
msgid ""
"Dimension ordering of output. Can be 'NCHW', 'NHWC', etc.'N', 'C', 'H', "
"'W' stands for batch, channel, height, and widthdimensions respectively. "
"Default to be same as input layout."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv2DAttrs.padding:1
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.padding:1
#: tvm.relax.op.op_attrs.Pool2DAttrs.padding:1
msgid ""
"bottom, right will use same padding as top, leftfour int : padding width "
"in the order of (top, left, bottom, right)"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv2DAttrs.padding:5
#: tvm.relax.op.op_attrs.Conv2DTransposeAttrs.padding:5
#: tvm.relax.op.op_attrs.Conv3DAttrs.padding:5
#: tvm.relax.op.op_attrs.Pool1DAttrs.padding:5
#: tvm.relax.op.op_attrs.Pool2DAttrs.padding:5
msgid "same padding used on all sidestwo int"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv2DTransposeAttrs:1
msgid "Attributes for nn.conv2d_transpose"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv3DAttrs:1
msgid "Attributes for nn.conv3d"
msgstr ""

#: of tvm.relax.op.op_attrs.Conv3DAttrs.data_layout:1
msgid ""
"Dimension ordering of input data. Can be 'NCDHW', 'NDHWC', etc.'N', 'C', "
"'D', 'H', 'W' stands for batch, channel, depth, height, and "
"widthdimensions respectively. Convolution is applied on the 'D', 'H', "
"and'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv3DAttrs.kernel_layout:1
msgid ""
"Dimension ordering of weight. Can be 'OIDHW', 'OIDHW16o16i', etc.'O', "
"'I', 'D', 'H', 'W' stands for num_filter, input_channel, depth, height, "
"and widthdimensions respectively."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv3DAttrs.out_layout:1
msgid ""
"Dimension ordering of output. Can be 'NCDHW', 'NDHWC', etc.'N', 'C', 'D',"
" 'H', 'W' stands for batch, channel, depth, height, and widthdimensions "
"respectively. Default to be same as input layout."
msgstr ""

#: of tvm.relax.op.op_attrs.Conv3DAttrs.padding:1
msgid ""
"bottom, right will use same padding as top, leftfour int : padding width "
"in the order of (forward, back, top, left, bottom, right)"
msgstr ""

#: of tvm.relax.op.op_attrs.DropoutAttrs:1
msgid "Attributes for dropout operator"
msgstr ""

#: of tvm.relax.op.op_attrs.DropoutAttrs.rate:1
msgid "Fraction of the input that gets dropped out during training time"
msgstr ""

#: of tvm.relax.op.op_attrs.EinsumAttrs:1
msgid "Attributes for einsum operator"
msgstr ""

#: of tvm.relax.op.op_attrs.EinsumAttrs.subscripts:1
msgid "The einsum expression string"
msgstr ""

#: of tvm.relax.op.op_attrs.ExpandDimsAttrs:1
msgid "Attributes for expand_dims operator"
msgstr ""

#: of tvm.relax.op.op_attrs.FlipAttrs:1
msgid "Attributes for flip operator"
msgstr ""

#: of tvm.relax.op.op_attrs.FlipAttrs.axis:1
msgid "The axis along which to flip over."
msgstr ""

#: of tvm.relax.op.op_attrs.GatherElementsAttrs:1
msgid "Attributes for gather_elements operator"
msgstr ""

#: of tvm.relax.op.op_attrs.GatherElementsAttrs.axis:1
msgid "The axis along which to index."
msgstr ""

#: of tvm.relax.op.op_attrs.GatherNDAttrs:1
msgid "Attributes for gather_nd operator"
msgstr ""

#: of tvm.relax.op.op_attrs.GatherNDAttrs.batch_dims:1
msgid "The number of batch dims."
msgstr ""

#: of tvm.relax.op.op_attrs.GroupNormAttrs:1
msgid "Attributes used in group_norm operator"
msgstr ""

#: of tvm.relax.op.op_attrs.GroupNormAttrs.axes:1
msgid ""
"The axes that along which the normalization is applied (excluding the "
"channel axis)."
msgstr ""

#: of tvm.relax.op.op_attrs.GroupNormAttrs.channel_axis:1
#: tvm.relax.op.op_attrs.InstanceNormAttrs.channel_axis:1
msgid "The axis that represents the channel."
msgstr ""

#: of tvm.relax.op.op_attrs.GroupNormAttrs.num_groups:1
msgid "The number of groups to separate the channels into."
msgstr ""

#: of tvm.relax.op.op_attrs.HintOnDeviceAttrs:1
msgid "Attributes used in hint_on_device operator"
msgstr ""

#: of tvm.relax.op.op_attrs.HintOnDeviceAttrs.device_type:1
msgid "The device type where the data is supposed to be executed."
msgstr ""

#: of tvm.relax.op.op_attrs.HintOnDeviceAttrs.index:1
msgid "The device id."
msgstr ""

#: of tvm.relax.op.op_attrs.IndexPutAttrs:1
msgid "Attributes for index_put operator"
msgstr ""

#: of tvm.relax.op.op_attrs.IndexPutAttrs.accumulate:1
msgid ""
"Whether to accumulate (add) values rather than replace. If true, performs"
" tensor[indices] += values, otherwise performs tensor[indices] = values."
msgstr ""

#: of tvm.relax.op.op_attrs.InitAttrs:1
msgid ""
"Attributes used in full/full_like, ones/ones_like, and zeros/zeros_like "
"operator"
msgstr ""

#: of tvm.relax.op.op_attrs.InstanceNormAttrs:1
msgid "Attributes used in instance_norm operator"
msgstr ""

#: of tvm.relax.op.op_attrs.LayerNormAttrs:1
msgid "Attributes used in layer_norm operator"
msgstr ""

#: of tvm.relax.op.op_attrs.LayoutTransformAttrs:1
msgid "Attributes used in layout_transform operator"
msgstr ""

#: of tvm.relax.op.op_attrs.LayoutTransformAttrs.axis_separators:1
msgid "The separators between input axes when generating flat output axes"
msgstr ""

#: of tvm.relax.op.op_attrs.LayoutTransformAttrs.index_map:1
msgid "The layout transformation to apply."
msgstr ""

#: of tvm.relax.op.op_attrs.LayoutTransformAttrs.input_axis_separators:1
msgid "The separators between axes to regenerate output"
msgstr ""

#: of tvm.relax.op.op_attrs.LayoutTransformAttrs.pad_value:1
msgid ""
"The specific value to be used to pad if the layout transform would result"
" in implicit padding. If not specified, the compiler is free to choose "
"any value."
msgstr ""

#: of tvm.relax.op.op_attrs.LeakyReluAttrs:1
msgid "Attributes used in leaky_relu operator"
msgstr ""

#: of tvm.relax.op.op_attrs.LeakyReluAttrs.alpha:1
msgid "The slope of the negative part."
msgstr ""

#: of tvm.relax.op.op_attrs.MatmulAttrs:1
msgid "Attributes for matmul operator"
msgstr ""

#: of tvm.relax.op.op_attrs.MatmulAttrs.out_dtype:1
msgid "The data type of the output tensor"
msgstr ""

#: of tvm.relax.op.op_attrs.MeshgridAttrs:1
msgid "Attributes for meshgrid operator"
msgstr ""

#: of tvm.relax.op.op_attrs.MeshgridAttrs.indexing:1
msgid "Specifies how the grid dimensions are ordered."
msgstr ""

#: of tvm.relax.op.op_attrs.MultinomialFromUniformAttrs:1
msgid "Attributes for multinomial_from_uniform operator"
msgstr ""

#: of tvm.relax.op.op_attrs.MultinomialFromUniformAttrs.dtype:1
#: tvm.relax.op.op_attrs.TopKAttrs.dtype:1
msgid "Data type of the output indices."
msgstr ""

#: of tvm.relax.op.op_attrs.NLLLossAttrs:1
msgid "Attributes used in nll_loss operator"
msgstr ""

#: of tvm.relax.op.op_attrs.NLLLossAttrs.reduction:1
msgid ""
"The reduction method to apply to the output. Can be'none', 'mean' or "
"'sum'."
msgstr ""

#: of tvm.relax.op.op_attrs.OneHotAttrs:1
msgid "Attributes for one_hot operator"
msgstr ""

#: of tvm.relax.op.op_attrs.OneHotAttrs.axis:1
msgid "Axis to fill."
msgstr ""

#: of tvm.relax.op.op_attrs.OneHotAttrs.depth:1
msgid "Depth of the one hot dimension."
msgstr ""

#: of tvm.relax.op.op_attrs.PReluAttrs:1
msgid "Attributes used in prelu operator"
msgstr ""

#: of tvm.relax.op.op_attrs.PReluAttrs.axis:1
msgid "The axis along which the alpha values are applied."
msgstr ""

#: of tvm.relax.op.op_attrs.PadAttrs:1
msgid "Attributes used in pad operator"
msgstr ""

#: of tvm.relax.op.op_attrs.PadAttrs.pad_mode:1
msgid ""
"Padding type to use. \"constant\" pads with constant_value, \"edge\" pads"
" using the edge values of the input array, \"reflect\" pads by reflecting"
" values with respect to the edges."
msgstr ""

#: of tvm.relax.op.op_attrs.PadAttrs.pad_value:1
msgid "The value to fill in padded area with"
msgstr ""

#: of tvm.relax.op.op_attrs.PadAttrs.pad_width:1
msgid ""
"Number of values padded to the edges of each axis, in the format of "
"(before_1, after_1, ..., before_N, after_N)"
msgstr ""

#: of tvm.relax.op.op_attrs.PermuteDimsAttrs:1
msgid "Attributes for permute_dims operator"
msgstr ""

#: of tvm.relax.op.op_attrs.PermuteDimsAttrs.axes:1
msgid "The target axes order, reverse order if not specified."
msgstr ""

#: of tvm.relax.op.op_attrs.PixelShuffleAttrs:1
msgid "Attributes used in pixel_shuffle operator"
msgstr ""

#: of tvm.relax.op.op_attrs.PixelShuffleAttrs.upscale_factor:1
msgid "Scale factor for spatial upsampling."
msgstr ""

#: of tvm.relax.op.op_attrs.Pool1DAttrs:1
msgid "Attributes for nn.max_pool1d and nn.avg_pool1d"
msgstr ""

#: of tvm.relax.op.op_attrs.Pool1DAttrs.count_include_pad:1
#: tvm.relax.op.op_attrs.Pool2DAttrs.count_include_pad:1
#: tvm.relax.op.op_attrs.Pool3DAttrs.count_include_pad:1
msgid "When true, will include padding to compute the average"
msgstr ""

#: of tvm.relax.op.op_attrs.Pool1DAttrs.dilation:1
#: tvm.relax.op.op_attrs.Pool2DAttrs.dilation:1
#: tvm.relax.op.op_attrs.Pool3DAttrs.dilation:1
msgid "Specifies the dilation of the convolution."
msgstr ""

#: of tvm.relax.op.op_attrs.Pool1DAttrs.layout:1
msgid ""
"Dimension ordering of input data. Can be 'NCW', 'NWC', etc.'N', 'C', 'W' "
"stands for batch, channel, and widthdimensions respectively. Pooling is "
"applied on the 'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.Pool1DAttrs.out_layout:1
msgid ""
"Dimension ordering of output data. Can be 'NCW', 'NWC', etc.'N', 'C', 'W'"
" stands for batch, channel, and widthdimensions respectively. Pooling is "
"applied on the 'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.Pool1DAttrs.pool_size:1
#: tvm.relax.op.op_attrs.Pool2DAttrs.pool_size:1
#: tvm.relax.op.op_attrs.Pool3DAttrs.pool_size:1
msgid "Size of the pooling windows."
msgstr ""

#: of tvm.relax.op.op_attrs.Pool2DAttrs:1
msgid "Attributes for nn.max_pool2d"
msgstr ""

#: of tvm.relax.op.op_attrs.Pool3DAttrs:1
msgid "Attributes for nn.max_pool3d and nn.avg_pool3d"
msgstr ""

#: of tvm.relax.op.op_attrs.Pool3DAttrs.layout:1
msgid ""
"Dimension ordering of input data. Can be 'NCDHW', 'NDHWC', etc.'N', 'C', "
"'D', 'H', 'W' stands for batch, channel, depth, height, and "
"widthdimensions respectively. Pooling is applied on the 'D', 'H' and'W' "
"dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.Pool3DAttrs.out_layout:1
msgid ""
"Dimension ordering of output data. Can be 'NCDHW', 'NDHWC', etc.'N', 'C',"
" 'D', 'H', 'W' stands for batch, channel, depth, height, and "
"widthdimensions respectively. Pooling is applied on the 'D', 'H' and'W' "
"dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.Pool3DAttrs.padding:1
msgid ""
"back, bottom, right will use same padding as front, top, leftfour int : "
"padding width in the order of (front, top, left, back, bottom, right)"
msgstr ""

#: of tvm.relax.op.op_attrs.Pool3DAttrs.padding:5
msgid "same padding used on all sidesthree int"
msgstr ""

#: of tvm.relax.op.op_attrs.QuantizeAttrs:1
msgid "Attributes used in quantize/dequantize operators"
msgstr ""

#: of tvm.relax.op.op_attrs.QuantizeAttrs.axis:1
msgid ""
"The output channel axis for channel wise quantization/dequantization. "
"Default value is -1, which corresponds to the last axis."
msgstr ""

#: of tvm.relax.op.op_attrs.QuantizeAttrs.out_dtype:1
msgid "Output data type."
msgstr ""

#: of tvm.relax.op.op_attrs.RMSNormAttrs:1
msgid "Attributes used in rms_norm operator"
msgstr ""

#: of tvm.relax.op.op_attrs.RepeatAttrs:1
msgid "Attributes for repeat operator"
msgstr ""

#: of tvm.relax.op.op_attrs.Resize2DAttrs:1
msgid "Attributes used in image resize2d operator"
msgstr ""

#: of tvm.relax.op.op_attrs.Resize2DAttrs.coordinate_transformation_mode:1
msgid ""
"Describes how to transform the coordinate in the resized tensorto the "
"coordinate in the original tensor.Refer to the ONNX Resize operator "
"specification for detailsAvailable options are half_pixel, align_corners "
"and asymmetric"
msgstr ""

#: of tvm.relax.op.op_attrs.Resize2DAttrs.cubic_alpha:1
msgid "Spline Coefficient for Bicubic Interpolation"
msgstr ""

#: of tvm.relax.op.op_attrs.Resize2DAttrs.extrapolation_value:1
msgid "Value to return when roi is outside of the image"
msgstr ""

#: of tvm.relax.op.op_attrs.Resize2DAttrs.layout:1
msgid ""
"Dimension ordering of input data. Can be 'NCHW', 'NHWC', etc.'N', 'C', "
"'H', 'W' stands for batch, channel, height, and widthdimensions "
"respectively. Resize is applied on the 'H' and'W' dimensions."
msgstr ""

#: of tvm.relax.op.op_attrs.Resize2DAttrs.method:1
msgid ""
"Specify the mode to use for scaling.nearest_neighbor -  Nearest "
"Neighborlinear - Bilinear Interpolationcubic - Bicubic Interpolation"
msgstr ""

#: of tvm.relax.op.op_attrs.Resize2DAttrs.roi:1
msgid "Region of Interest for coordinate transformation mode 'tf_crop_and_resize'"
msgstr ""

#: of tvm.relax.op.op_attrs.Resize2DAttrs.rounding_method:1
msgid ""
"indicates how to find the \"nearest\" pixel in nearest_neighbor "
"methodAvailable options are round, floor, and ceil."
msgstr ""

#: of tvm.relax.op.op_attrs.ScanopAttrs:1
msgid "Attributes for scan operators"
msgstr ""

#: of tvm.relax.op.op_attrs.ScanopAttrs.axis:1
msgid ""
"The axis along which to perform the scan computation.The default (None) "
"is to compute over the flattened array."
msgstr ""

#: of tvm.relax.op.op_attrs.ScanopAttrs.dtype:1
msgid ""
"The output data type.If dtype is not specified, it defaults to the dtype "
"of input data."
msgstr ""

#: of tvm.relax.op.op_attrs.ScanopAttrs.exclusive:1
msgid "The first element is not included"
msgstr ""

#: of tvm.relax.op.op_attrs.ScatterCollectiveAttrs:1
msgid "Attributes used in scatter collective operators"
msgstr ""

#: of tvm.relax.op.op_attrs.ScatterCollectiveAttrs.axis:1
msgid ""
"The axis of the tensor to be scattered. The tensor will be chunked along "
"this axis."
msgstr ""

#: of tvm.relax.op.op_attrs.ScatterElementsAttrs:1
msgid "Attributes for scatter_elements operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ScatterElementsAttrs.axis:1
#: tvm.relax.op.op_attrs.TakeAttrs.axis:1
msgid "The axis over which to select values."
msgstr ""

#: of tvm.relax.op.op_attrs.ScatterElementsAttrs.reduction:1
msgid ""
"Reduction mode of the scatter elements, either \"update\", \"add\", "
"\"mul\", \"mean\", \"min\" or \"max\"."
msgstr ""

#: of tvm.relax.op.op_attrs.ScatterNDAttrs:1
msgid "Attributes for scatter_nd operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ScatterNDAttrs.reduction:1
msgid ""
"Accumulation mode of the ScatterND, either \"update\", \"add\", \"mul\", "
"\"min\" or \"max\"."
msgstr ""

#: of tvm.relax.op.op_attrs.SliceScatterAttrs:1
msgid "Attributes for slice_scatter operator"
msgstr ""

#: of tvm.relax.op.op_attrs.SliceScatterAttrs.axis:1
msgid "the dimension to insert the slice into"
msgstr ""

#: of tvm.relax.op.op_attrs.SoftmaxAttrs:1
msgid "Attributes for nn.softmax"
msgstr ""

#: of tvm.relax.op.op_attrs.SoftmaxAttrs.axis:1
msgid "The axis to sum over when computing softmax."
msgstr ""

#: of tvm.relax.op.op_attrs.SoftplusAttrs:1
msgid "Attributes used in softplus operator"
msgstr ""

#: of tvm.relax.op.op_attrs.SoftplusAttrs.beta:1
msgid "Scaling factor controlling the sharpness of the Softplus transition."
msgstr ""

#: of tvm.relax.op.op_attrs.SoftplusAttrs.threshold:1
msgid ""
"Value determining when to use linear approximation for numerical "
"stability."
msgstr ""

#: of tvm.relax.op.op_attrs.SortAttrs:1
msgid "Attributes for sort operator"
msgstr ""

#: of tvm.relax.op.op_attrs.SortAttrs.axis:1
msgid "Axis along which the sort is computed.The default the last axis is used."
msgstr ""

#: of tvm.relax.op.op_attrs.SortAttrs.descending:1
msgid ""
"Whether to sort in descending order.If it is not specified, it defaults "
"to the ascending order."
msgstr ""

#: of tvm.relax.op.op_attrs.SplitAttrs:1
msgid "Attributes used in split operator"
msgstr ""

#: of tvm.relax.op.op_attrs.SplitAttrs.axis:1
msgid "The axis to be splitted"
msgstr ""

#: of tvm.relax.op.op_attrs.SplitAttrs.indices_or_sections:1
msgid "The input array of indices or the number of split sections."
msgstr ""

#: of tvm.relax.op.op_attrs.SqueezeAttrs:1
msgid "Attributes for squeeze operator"
msgstr ""

#: of tvm.relax.op.op_attrs.SqueezeAttrs.axis:1
msgid ""
"The axis to squeeze in the input tensor.If `axis = None`, all axis of "
"dimension 1 get squeezed;Else, the dimension in axes get squeezed.It is "
"an error if an axis does not has dimension 1."
msgstr ""

#: of tvm.relax.op.op_attrs.StackAttrs.axis:1
msgid ""
"The axis along which to stack the input tensors. The axis will be "
"inserted at this position in the output, so it must be in range [-ndim-1,"
" ndim] where ndim is the number of dimensions of the input tensors."
msgstr ""

#: of tvm.relax.op.op_attrs.StatisticalAttrs:1
msgid "Attributes used in statistical operator"
msgstr ""

#: of tvm.relax.op.op_attrs.StatisticalAttrs.axis:1
msgid "The axis or axes along which to perform the reduction."
msgstr ""

#: of tvm.relax.op.op_attrs.StatisticalAttrs.keepdims:1
msgid ""
"If this is set to `True`, the reduced axes are left in the result as "
"dimension with size one."
msgstr ""

#: of tvm.relax.op.op_attrs.StridedSliceAttrs:1
msgid "Attributes used in strided_slice operator"
msgstr ""

#: of tvm.relax.op.op_attrs.TakeAttrs:1
msgid "Attributes used in take operator"
msgstr ""

#: of tvm.relax.op.op_attrs.TakeAttrs.mode:1
msgid "The mode for handling out-of-bounds indices."
msgstr ""

#: of tvm.relax.op.op_attrs.TileAttrs:1
msgid "Attributes for tile operator"
msgstr ""

#: of tvm.relax.op.op_attrs.ToVDeviceAttrs:1
msgid "Attributes used in to_vdevice operator"
msgstr ""

#: of tvm.relax.op.op_attrs.TopKAttrs:1
msgid "Attributes for topk operators"
msgstr ""

#: of tvm.relax.op.op_attrs.TopKAttrs.axis:1
msgid "Axis along which to sort the input tensor."
msgstr ""

#: of tvm.relax.op.op_attrs.TopKAttrs.k:1
msgid "Number of top elements to select"
msgstr ""

#: of tvm.relax.op.op_attrs.TopKAttrs.largest:1
msgid ""
"Whether to return largest or smallest elements.By default, return the "
"largest k elements."
msgstr ""

#: of tvm.relax.op.op_attrs.TopKAttrs.ret_type:1
msgid ""
"The return type [both, values, indices].both - return both top k data and"
" indices.values - return top k data only.indices - return top k indices "
"only."
msgstr ""

#: of tvm.relax.op.op_attrs.TriluAttrs:1
msgid "Attributes used in tril and triu operator"
msgstr ""

#: of tvm.relax.op.op_attrs.TriluAttrs.k:1
msgid ""
"The number of diagonals above or below the main diagonal to exclude or "
"include."
msgstr ""

#: of tvm.relax.op.op_attrs.WrapParamAttrs:1
msgid "Attributes used in wrap_param operator"
msgstr ""

#~ msgid "The value used for padding"
#~ msgstr ""

#~ msgid ""
#~ "'constant' pads with constant_value pad_value"
#~ " 'edge' pads using the edge values"
#~ " of the input array 'reflect' pads"
#~ " by reflecting values with respect to"
#~ " the edge"
#~ msgstr ""

#~ msgid "Relax core operators."
#~ msgstr ""

#~ msgid "Compute element-wise absolute value of the input data."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "The input data"
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "**result** -- The computed result."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "Compute element-wise arc cos of the input data."
#~ msgstr ""

#~ msgid "The input tensor is required to have float dtype"
#~ msgstr ""

#~ msgid "Compute element-wise arc cosh of the input data."
#~ msgstr ""

#~ msgid "Addition with numpy-style broadcasting."
#~ msgstr ""

#~ msgid "The first input tensor."
#~ msgstr ""

#~ msgid "The second input tensor."
#~ msgstr ""

#~ msgid "示例"
#~ msgstr ""

#~ msgid "Construct a tensor with evenly spaced elements."
#~ msgstr ""

#~ msgid "The start of the interval."
#~ msgstr ""

#~ msgid ""
#~ "The end of the interval. If not"
#~ " given, it will be set to "
#~ "start, and start will be set to"
#~ " 0."
#~ msgstr ""

#~ msgid "The step size."
#~ msgstr ""

#~ msgid "The data type of the created tensor."
#~ msgstr ""

#~ msgid "**result** -- The result tensor."
#~ msgstr ""

#~ msgid "Computes the argmax of tensor elements over given axis."
#~ msgstr ""

#~ msgid "The input data tensor"
#~ msgstr ""

#~ msgid ""
#~ "Axis along which an argmax operation "
#~ "is performed. The default, axis=None, "
#~ "will compute the argmax of all "
#~ "elements in the input tensor. Negative"
#~ " indexing is supported."
#~ msgstr ""

#~ msgid ""
#~ "If this is set to True, the "
#~ "axis being reduced is left in the"
#~ " result as dimensions with size one."
#~ " With this option, the result will"
#~ " broadcast correctly against the input "
#~ "tensor."
#~ msgstr ""

#~ msgid "Computes the argmin of tensor elements over given axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which an argmin operation "
#~ "is performed. The default, axis=None, "
#~ "will compute the argmin of all "
#~ "elements in the input tensor. Negative"
#~ " indexing is supported."
#~ msgstr ""

#~ msgid ""
#~ "Performs sorting along the given axis"
#~ " and returns an array of indices "
#~ "having same shape as an input "
#~ "array that index data in sorted "
#~ "order."
#~ msgstr ""

#~ msgid "The input data tensor."
#~ msgstr ""

#~ msgid "Axis long which to sort the input tensor."
#~ msgstr ""

#~ msgid "Whether to sort in descending order, the default is False"
#~ msgstr ""

#~ msgid "The data type of the output indices."
#~ msgstr ""

#~ msgid "**out** -- Tensor with same shape as data."
#~ msgstr ""

#~ msgid "Compute element-wise arc sin of the input data."
#~ msgstr ""

#~ msgid "Compute element-wise arc sinh of the input data."
#~ msgstr ""

#~ msgid ""
#~ "Create a call to Relax's assert_op "
#~ "operation (`assert` is reserved in "
#~ "Python, so the name must be "
#~ "distinct)."
#~ msgstr ""

#~ msgid "The assertion condition."
#~ msgstr ""

#~ msgid "Format arguments for the error message if the condition fails."
#~ msgstr ""

#~ msgid "The format string or StringImm for the error message."
#~ msgstr ""

#~ msgid "**result** -- A Call to the Relax assert operation."
#~ msgstr ""

#~ msgid "Cast input tensor to the given data type."
#~ msgstr ""

#~ msgid "The input data to the operator."
#~ msgstr ""

#~ msgid "The target data type"
#~ msgstr ""

#~ msgid "**result** -- The casted result."
#~ msgstr ""

#~ msgid "Compute element-wise arc tan of the input data."
#~ msgstr ""

#~ msgid "Compute element-wise arc tanh of the input data."
#~ msgstr ""

#~ msgid ""
#~ "Bitwise AND :param x1: The first "
#~ "input tensor. :type x1: relax.Expr "
#~ ":param x2: The second input tensor. "
#~ ":type x2: relax.Expr"
#~ msgstr ""

#~ msgid "Compute bitwise NOT of the input data."
#~ msgstr ""

#~ msgid ""
#~ "Bitwise OR :param x1: The first "
#~ "input tensor. :type x1: relax.Expr "
#~ ":param x2: The second input tensor. "
#~ ":type x2: relax.Expr"
#~ msgstr ""

#~ msgid ""
#~ "Bitwise XOR :param x1: The first "
#~ "input tensor. :type x1: relax.Expr "
#~ ":param x2: The second input tensor. "
#~ ":type x2: relax.Expr"
#~ msgstr ""

#~ msgid "Broadcasts a tensor to a specified shape."
#~ msgstr ""

#~ msgid "The target shape."
#~ msgstr ""

#~ msgid "**result** -- The broadcasted tensor."
#~ msgstr ""

#~ msgid "Call a builtin function func."
#~ msgstr ""

#~ msgid "The builtin function to be called."
#~ msgstr ""

#~ msgid "The input arguments."
#~ msgstr ""

#~ msgid "The struct info arguments to the call node."
#~ msgstr ""

#~ msgid "**ret** -- The created call node."
#~ msgstr ""

#~ msgid "Call a destination-passing-style packed function and return the output."
#~ msgstr ""

#~ msgid ""
#~ "Note: The called function is assumed "
#~ "to be _pure_ (other than modifying "
#~ "the designated output arguments). If the"
#~ " function _does_ result in other side"
#~ " effects, then the compiler may end"
#~ " up removing, reordering, or repeating "
#~ "those effects--no guarantees can be "
#~ "made."
#~ msgstr ""

#~ msgid "The destination-passing-style function, can be ExternFunc."
#~ msgstr ""

#~ msgid ""
#~ "The structure info of the "
#~ "call_dps_packed output. It should be a"
#~ " single or a list of "
#~ "TensorStructInfo. Each one denotes the "
#~ "structure info of a returned tensor."
#~ msgstr ""

#~ msgid "**ret** -- A call node for the call_dps_packed operator."
#~ msgstr ""

#~ msgid ""
#~ "Construct a call to a packed "
#~ "function that consumes some of its "
#~ "arguments \"in-place\" and returns the"
#~ " mutated arguments (aliased), but should"
#~ " be considered to be otherwise pure."
#~ " The `inplace_indices` argument indicates "
#~ "which of the outputs are mutated "
#~ "arguments."
#~ msgstr ""

#~ msgid ""
#~ "The resulting call will have the "
#~ "same semantics as calling the packed "
#~ "function directly."
#~ msgstr ""

#~ msgid ""
#~ "Note: This should be used for "
#~ "cases when the user knows that "
#~ "calling the packed function with these"
#~ " arguments will **in reality** not "
#~ "cause any other side effects. If "
#~ "it is used for a call that "
#~ "**does** result in other side effects,"
#~ " then the compiler may end up "
#~ "removing, reordering, or repeating that "
#~ "call, with no guarantees made about "
#~ "any side effects from the callee."
#~ msgstr ""

#~ msgid ""
#~ "Warning: This operator as treated as "
#~ "pure by the type system even "
#~ "though it *is* performing side effects"
#~ " (mutating some arguments). It is "
#~ "therefore incumbent upon the user to "
#~ "ensure that it is being used "
#~ "safely (viz., that mutated arguments are"
#~ " not live after the mutation, that"
#~ " they do not alias values live "
#~ "after the mutation)."
#~ msgstr ""

#~ msgid "The name (global symbol) for a PackedFunc or an ExternFunc node."
#~ msgstr ""

#~ msgid "The arguments for the PackedFunc."
#~ msgstr ""

#~ msgid ""
#~ "Specify which arguments should be used"
#~ " for in-place computations. If "
#~ "`inplace_indices` is a single integer, "
#~ "it will be made into a singleton"
#~ " list. Suppose `inplace_indices[i] = j`,"
#~ " where `j >= 0`. Then the `i`th"
#~ " output will be an alias of "
#~ "`args[j]`. If `inplace_indices[i] = -1`, "
#~ "then the `i`th output will be a"
#~ " freshly allocated tensor. At least "
#~ "one member of `inplace_indices` must not"
#~ " be -1."
#~ msgstr ""

#~ msgid ""
#~ "The list of structure info arguments "
#~ "(giving the structural info for the "
#~ "returned value)."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- A Relax call, "
#~ "corresponding to `call_pure_packed(ExternFunc(func), "
#~ "args, DictAttrs(kwargs), sinfo_args)`"
#~ msgstr ""

#~ msgid ""
#~ "Construct a call to a packed "
#~ "function that should be treated as "
#~ "pure, even though packed calls are "
#~ "normally not treated as pure."
#~ msgstr ""

#~ msgid ""
#~ "Note: This should be used for "
#~ "cases when the user knows that "
#~ "calling the packed function with these"
#~ " arguments will **in reality** not "
#~ "cause any side effects. If it is"
#~ " used for a call that **does** "
#~ "result in side effects, then the "
#~ "compiler may end up removing, "
#~ "reordering, or repeating that call, with"
#~ " no guarantees made about any side"
#~ " effects from the callee."
#~ msgstr ""

#~ msgid "Call a tir.prim_func and return the output."
#~ msgstr ""

#~ msgid "The GlobalVar referring to a tir PrimFunc."
#~ msgstr ""

#~ msgid ""
#~ "The structure info of the call_tir "
#~ "output. It should be a single or"
#~ " a list of TensorStructInfo. Each one"
#~ " denotes the structure info of a "
#~ "returned tensor."
#~ msgstr ""

#~ msgid ""
#~ "ShapeExpr representing a tuple of "
#~ "integers to unpack when calling func."
#~ " Is null if not used"
#~ msgstr ""

#~ msgid "**ret** -- A call node for the call_tir operator."
#~ msgstr ""

#~ msgid ""
#~ "Call a TIR PrimFunc and return the"
#~ " result, doing the specified computations"
#~ " in-place (based on the "
#~ "`inplace_indices` argument; outputs will alias"
#~ " the inputs selected by in-place "
#~ "indices)."
#~ msgstr ""

#~ msgid ""
#~ "Warning: This operator is considered "
#~ "pure by the type system but "
#~ "actually mutates the arguments specified "
#~ "by `inplace_indices`. This operator should "
#~ "not be used directly, but rather "
#~ "should be inserted by passes that "
#~ "have checked whether it is safe to"
#~ " perform operations in-place (i.e., "
#~ "none of the arguments specified as "
#~ "an output is aliased or is live"
#~ " after calling call_tir_inplace)."
#~ msgstr ""

#~ msgid "Direct calls to this operator should be done for testing purposes only."
#~ msgstr ""

#~ msgid "The GlobalVar referring to a TIR PrimFunc."
#~ msgstr ""

#~ msgid ""
#~ "The structure info of the "
#~ "call_tir_inplace output. It should be a"
#~ " single `TensorStructInfo` or a list "
#~ "of `TensorStructInfo`. Each one denotes "
#~ "the structure info of a returned "
#~ "tensor. If a list of `TensorStructInfo`"
#~ " is given, the result will be a"
#~ " tuple of `TensorStructInfo`."
#~ msgstr ""

#~ msgid ""
#~ "Call a tir.prim_func and return the "
#~ "output. This intrinsic will bind a "
#~ "te gradient function (refered by "
#~ "te_grad_name) to the call_tir_with_grad node."
#~ " The te gradient function will be "
#~ "called by the Gradient pass."
#~ msgstr ""

#~ msgid ""
#~ "The structure info of the "
#~ "call_tir_with_grad output. It should be "
#~ "a single or a list of "
#~ "TensorStructInfo. Each one denotes the "
#~ "structure info of a returned tensor."
#~ msgstr ""

#~ msgid ""
#~ "The registered name of the te "
#~ "gradient function associated with the "
#~ "call_tir_with_grad node. Must be provided "
#~ "as a keyword argument."
#~ msgstr ""

#~ msgid ""
#~ "The keyword arguments passed to the "
#~ "te gradient function. Optionally provided "
#~ "as a keyword argument. Default: {}."
#~ msgstr ""

#~ msgid "**ret** -- A call node for the call_tir_with_grad operator."
#~ msgstr ""

#~ msgid "Take ceil of input data."
#~ msgstr ""

#~ msgid "Clips tensor values to a specified min and max."
#~ msgstr ""

#~ msgid "The minimum value"
#~ msgstr ""

#~ msgid "The maximum value"
#~ msgstr ""

#~ msgid "Return a summation of data to the shape of collapse_target."
#~ msgstr ""

#~ msgid "For details, please see relax.op.collapse_sum_to."
#~ msgstr ""

#~ msgid "The input tensor."
#~ msgstr ""

#~ msgid "The tensor whose shape is the shape to collapse to."
#~ msgstr ""

#~ msgid "**result** -- The result tensor after summation."
#~ msgstr ""

#~ msgid "Return a summation of data to the given shape."
#~ msgstr ""

#~ msgid ""
#~ "collapse_sum_to is intended as the "
#~ "backward operator of tvm.relax.op.broadcast_to "
#~ "and other broadcast operators in the "
#~ "automatic differentiation process."
#~ msgstr ""

#~ msgid ""
#~ "We expect that data is the result"
#~ " of broadcasting some tensor of the"
#~ " given shape in some broadcast "
#~ "operation. Thus the given `shape` and"
#~ " `data.shape` must follow broadcast rules."
#~ msgstr ""

#~ msgid ""
#~ "During computation, all axes of "
#~ "`data.shape` and `shape` are checked "
#~ "from right to left. For an axis,"
#~ " if it follows these rules, `data`"
#~ " will be summed over this axis: "
#~ "- the axis exists in `data.shape` "
#~ "but not in `shape`, or - the "
#~ "axis exists in `data.shape` and equals"
#~ " to 1 in `shape`."
#~ msgstr ""

#~ msgid "The shape to collapse to."
#~ msgstr ""

#~ msgid "**result** -- The result tensor of the given shape after summation."
#~ msgstr ""

#~ msgid "Concatenate the input tensors along the given axis."
#~ msgstr ""

#~ msgid ""
#~ "An Expr in Tuple type, containing "
#~ "the tensors to be concatenated, or "
#~ "a list of Tensors."
#~ msgstr ""

#~ msgid ""
#~ "The axis along which the tensors "
#~ "are concatenated. If `axis` is `None`,"
#~ " the input tensor is required to "
#~ "be flattened before concatenation."
#~ msgstr ""

#~ msgid "**result** -- The concatenated tensor."
#~ msgstr ""

#~ msgid "Compute element-wise cos of the input data."
#~ msgstr ""

#~ msgid "Compute element-wise cosh of the input data."
#~ msgstr ""

#~ msgid ""
#~ "Numpy style cumprod op. Return the "
#~ "cumulative product of the elements along"
#~ " a given axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which the cumulative product"
#~ " is computed. The default (None) is"
#~ " to compute the cumprod over the "
#~ "flattened array."
#~ msgstr ""

#~ msgid ""
#~ "Type of the returned array and of"
#~ " the accumulator in which the "
#~ "elements are computed. If dtype is "
#~ "not specified, it defaults to the "
#~ "dtype of data."
#~ msgstr ""

#~ msgid ""
#~ "If false (default), all elements are "
#~ "included in the product.  If true, "
#~ "the first element is excluded from "
#~ "the product."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The result has the "
#~ "same size as data, and the same"
#~ " shape as data if axis is not"
#~ " None. If axis is None, the "
#~ "result is a 1-d array."
#~ msgstr ""

#~ msgid ""
#~ "Numpy style cumsum op. Return the "
#~ "cumulative inclusive sum of the elements"
#~ " along a given axis."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which the cumulative sum "
#~ "is computed. The default (None) is "
#~ "to compute the cumsum over the "
#~ "flattened array."
#~ msgstr ""

#~ msgid ""
#~ "Type of the returned array and of"
#~ " the accumulator in which the "
#~ "elements are summed. If dtype is "
#~ "not specified, it defaults to the "
#~ "dtype of data."
#~ msgstr ""

#~ msgid ""
#~ "If false (default), all elements are "
#~ "included in the sum.  If true, the"
#~ " first element is excluded from the"
#~ " sum."
#~ msgstr ""

#~ msgid ""
#~ "Dequantize op This operator takes input"
#~ " and produces dequantized output. The "
#~ "input tensor can be of any shape."
#~ " The output shape is the same "
#~ "as input shape."
#~ msgstr ""

#~ msgid ""
#~ "output = clamp(scale * (input_tensor -"
#~ " zero_point), out_dtype::min, out_dtype::max)"
#~ msgstr ""

#~ msgid "The input tensor to be dequantized."
#~ msgstr ""

#~ msgid "The input scale."
#~ msgstr ""

#~ msgid "The input zero_point."
#~ msgstr ""

#~ msgid ""
#~ "The channel axis for dequantization. "
#~ "Default value is -1 which corresponds"
#~ " to the last axis."
#~ msgstr ""

#~ msgid "The data type of the output tensor."
#~ msgstr ""

#~ msgid "Division with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ""
#~ "Dynamic strided slice of a tensor. "
#~ "`begin`, `end`, `strides` can be "
#~ "computed at runtime."
#~ msgstr ""

#~ msgid "The source tensor to be sliced."
#~ msgstr ""

#~ msgid "The indices to begin with in the slicing, inclusive."
#~ msgstr ""

#~ msgid "The indices indicating end of the slice, exclusive."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the stride values, it can "
#~ "be negative in that case, the "
#~ "input tensor will be reversed in "
#~ "that particular axis. If not specified,"
#~ " it by default is an list of"
#~ " ones of the same length as "
#~ "`axes`."
#~ msgstr ""

#~ msgid "**ret** -- The sliced result."
#~ msgstr ""

#~ msgid ""
#~ "dyn_strided_slice require the input `begin`,"
#~ " `end` and `strides` to have the "
#~ "same length as rank of `data` "
#~ "tensor."
#~ msgstr ""

#~ msgid "Evaluates the Einstein summation convention on data"
#~ msgstr ""

#~ msgid "A list of expression."
#~ msgstr ""

#~ msgid "The einsum expression string."
#~ msgstr ""

#~ msgid "**result** -- The output from the einsum op."
#~ msgstr ""

#~ msgid "Broadcasted element-wise test for (lhs == rhs)."
#~ msgstr ""

#~ msgid "Computes the error function of the input."
#~ msgstr ""

#~ msgid "**result** -- Computed error function for each element."
#~ msgstr ""

#~ msgid ""
#~ "Elementwise fused multiply-add operator "
#~ "Returns elementwise result of :math:`x1 "
#~ "* x2 + x3`"
#~ msgstr ""

#~ msgid "The left hand operand of the multiplication"
#~ msgstr ""

#~ msgid "The right hand operand of the multiplication"
#~ msgstr ""

#~ msgid "The operand of the addition"
#~ msgstr ""

#~ msgid "Compute element-wise exp of data."
#~ msgstr ""

#~ msgid "Insert new axes at the positions given by `axis`."
#~ msgstr ""

#~ msgid ""
#~ "The axes at which the input array"
#~ " are expanded. All values are "
#~ "required to lie in range `[-data.ndim"
#~ " - 1, data.ndim]`, with the "
#~ "convention of negative indexing."
#~ msgstr ""

#~ msgid "**result** -- The transformed result."
#~ msgstr ""

#~ msgid "Construct a 2-D tensor with ones on the diagonal and zeros elsewhere."
#~ msgstr ""

#~ msgid "Number of rows in the output."
#~ msgstr ""

#~ msgid "Number of columns in the output. If None, defaults to n."
#~ msgstr ""

#~ msgid ""
#~ "Index of the diagonal: 0 (the "
#~ "default) refers to the main diagonal,"
#~ " a positive value refers to an "
#~ "upper diagonal, and a negative value "
#~ "to a lower diagonal."
#~ msgstr ""

#~ msgid ""
#~ "Return a 2-D tensor with ones on"
#~ " the diagonal and zeros elsewhere, "
#~ "with the same shape as the input"
#~ " tensor."
#~ msgstr ""

#~ msgid ""
#~ "The input tensor, which provides the "
#~ "shape, and dtype when the `dtype` "
#~ "field is not specified."
#~ msgstr ""

#~ msgid ""
#~ "The data type of the created "
#~ "tensor. If dtype is not given, it"
#~ " will by default use the dtype "
#~ "of the input tensor."
#~ msgstr ""

#~ msgid "Flatten all the tensor dimensions into one."
#~ msgstr ""

#~ msgid "**result** -- The flattened result."
#~ msgstr ""

#~ msgid ""
#~ "Reverses the order of elements along "
#~ "given axis while preserving array shape."
#~ msgstr ""

#~ msgid "axis to flip on"
#~ msgstr ""

#~ msgid "**ret** -- The computed result."
#~ msgstr ""

#~ msgid "Take floor of input data."
#~ msgstr ""

#~ msgid "Floor division with numpy-style broadcasting."
#~ msgstr ""

#~ msgid "Floor modulo with numpy-style broadcasting."
#~ msgstr ""

#~ msgid "Fill array with scalar value."
#~ msgstr ""

#~ msgid "The shape of the created tensor."
#~ msgstr ""

#~ msgid "The value to fill. Must be a scalar tensor."
#~ msgstr ""

#~ msgid ""
#~ "The data type of the created "
#~ "tensor. If dtype is not given, it"
#~ " will by default use the dtype "
#~ "of fill_value."
#~ msgstr ""

#~ msgid ""
#~ "Construct a tensor such that - its"
#~ " shape is the same as the input"
#~ " data tensor's shape, - its value "
#~ "is filled with the input scalar "
#~ "fill value."
#~ msgstr ""

#~ msgid ""
#~ "Gather elements from data according to"
#~ " indices along the specified axis."
#~ msgstr ""

#~ msgid "The indices tensor, must have integer type."
#~ msgstr ""

#~ msgid "The axis along which to index. Default is 0."
#~ msgstr ""

#~ msgid "Update data at positions defined by indices with values in updates."
#~ msgstr ""

#~ msgid "The number of batch dimensions. Default is 0."
#~ msgstr ""

#~ msgid "Broadcasted element-wise test for (lhs > rhs)."
#~ msgstr ""

#~ msgid "Broadcasted element-wise test for (lhs >= rhs)."
#~ msgstr ""

#~ msgid ""
#~ "It provides a hint specifying the "
#~ "device on which the input data "
#~ "should be executed. This hint is "
#~ "utilized by RealizeVDevice to propagate "
#~ "the virtual device.\""
#~ msgstr ""

#~ msgid "The tensor to be copied."
#~ msgstr ""

#~ msgid "The destination device where the data is supposed to be executed."
#~ msgstr ""

#~ msgid "**result** -- The result."
#~ msgstr ""

#~ msgid "Invoke a closure."
#~ msgstr ""

#~ msgid "The VMClosure object."
#~ msgstr ""

#~ msgid "The structure info arguments of the CallNode"
#~ msgstr ""

#~ msgid "**ret** -- A call to `invoke_closure`."
#~ msgstr ""

#~ msgid "Invoke a closure and indicate to the compiler that it is pure."
#~ msgstr ""

#~ msgid ""
#~ "Note: This should be used for "
#~ "cases when the user knows that "
#~ "calling the closure with these arguments"
#~ " will **in reality** not cause any"
#~ " side effects. If it is used "
#~ "for a call that _does_ result in"
#~ " side effects, then the compiler may"
#~ " end up removing, reordering, or "
#~ "repeating that call, with no guarantees"
#~ " made about any side effects from "
#~ "the callee."
#~ msgstr ""

#~ msgid "**ret** -- A call to `invoke_pure_closure`."
#~ msgstr ""

#~ msgid "Check if input value is finite."
#~ msgstr ""

#~ msgid "Check if input value is infinite."
#~ msgstr ""

#~ msgid "Check if input value is Nan."
#~ msgstr ""

#~ msgid "Modifies the layout of a tensor."
#~ msgstr ""

#~ msgid "The input tensor to the operator."
#~ msgstr ""

#~ msgid "The transformation to apply."
#~ msgstr ""

#~ msgid ""
#~ "The value used for padding if the"
#~ " transformation results in implicit "
#~ "padding. If not specified, any value "
#~ "can be used."
#~ msgstr ""

#~ msgid "The axis_separators for index_map to create non flat buffers."
#~ msgstr ""

#~ msgid "**result** -- The transformed tensor."
#~ msgstr ""

#~ msgid ""
#~ "Bitwise Shift Left :param x1: The "
#~ "input tensor to be shifted. :type "
#~ "x1: relax.Expr :param x2: The number "
#~ "of positions to shift. :type x2: "
#~ "relax.Expr"
#~ msgstr ""

#~ msgid "Broadcasted element-wise test for (lhs < rhs)."
#~ msgstr ""

#~ msgid "Broadcasted element-wise test for (lhs <= rhs)."
#~ msgstr ""

#~ msgid "Applies a linear transformation to the incoming data: y = xA^T + b"
#~ msgstr ""

#~ msgid "The input data."
#~ msgstr ""

#~ msgid "The weight tensor."
#~ msgstr ""

#~ msgid "The bias tensor."
#~ msgstr ""

#~ msgid ""
#~ "The data type of the matmul "
#~ "result. When it is not specified, "
#~ "the output dtype will be the same"
#~ " as input dtype."
#~ msgstr ""

#~ msgid "备注"
#~ msgstr ""

#~ msgid ""
#~ "Relax does not regard the Linear "
#~ "Op as a primitive Op, while "
#~ "combine the transpose, matmul and add"
#~ " op to implement it."
#~ msgstr ""

#~ msgid "Compute element-wise natural logarithm of the input data."
#~ msgstr ""

#~ msgid ""
#~ "Logical AND :param x1: The first "
#~ "input tensor. :type x1: relax.Expr "
#~ ":param x2: The second input tensor. "
#~ ":type x2: relax.Expr"
#~ msgstr ""

#~ msgid "Compute logical NOT of the input data."
#~ msgstr ""

#~ msgid ""
#~ "Logical OR :param x1: The first "
#~ "input tensor. :type x1: relax.Expr "
#~ ":param x2: The second input tensor. "
#~ ":type x2: relax.Expr"
#~ msgstr ""

#~ msgid ""
#~ "Logical XOR :param x1: The first "
#~ "input tensor. :type x1: relax.Expr "
#~ ":param x2: The second input tensor. "
#~ ":type x2: relax.Expr"
#~ msgstr ""

#~ msgid "Create a closure with free variables and return the closure."
#~ msgstr ""

#~ msgid "The closure, can be ExternFunc or PrimFunc."
#~ msgstr ""

#~ msgid "**ret** -- The VMClosure."
#~ msgstr ""

#~ msgid ""
#~ "Fill a tensor by a specified value"
#~ " in places defined by a mask. "
#~ ":param x: The input data to the"
#~ " operator. :type x: relax.Expr :param "
#~ "mask: The mask. :type mask: relax.Expr"
#~ " :param value: The value to set "
#~ "in the input tensor. :type value: "
#~ "relax.Expr"
#~ msgstr ""

#~ msgid "**result** -- The filled tensor."
#~ msgstr ""

#~ msgid ""
#~ "General matrix multiplication of two "
#~ "tensors, with broadcasting on batched "
#~ "dimensions."
#~ msgstr ""

#~ msgid ""
#~ "The semantics and output shape deduction"
#~ " rule is specified as https://data-"
#~ "apis.org/array-"
#~ "api/latest/API_specification/generated/array_api.matmul.html."
#~ msgstr ""

#~ msgid "Computes the max of tensor elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a max "
#~ "operation is performed. The default, "
#~ "axis=None, will compute the max of "
#~ "all elements in the input tensor. "
#~ "Negative indexing is supported."
#~ msgstr ""

#~ msgid ""
#~ "If this is set to True, the "
#~ "axes which are reduced are left in"
#~ " the result as dimensions with size"
#~ " one. With this option, the result"
#~ " will broadcast correctly against the "
#~ "input tensor."
#~ msgstr ""

#~ msgid "Element-wise maximum"
#~ msgstr ""

#~ msgid "Computes the mean of tensor elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a mean"
#~ " operation is performed. The default, "
#~ "axis=None, will compute the mean of "
#~ "all elements in the input tensor. "
#~ "Negative indexing is supported."
#~ msgstr ""

#~ msgid "Computes the min of tensor elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a min "
#~ "operation is performed. The default, "
#~ "axis=None, will compute the min of "
#~ "all elements in the input tensor. "
#~ "Negative indexing is supported."
#~ msgstr ""

#~ msgid "Element-wise minimum"
#~ msgstr ""

#~ msgid "Modulo with numpy-style broadcasting."
#~ msgstr ""

#~ msgid ""
#~ "Returns a tensor where each row "
#~ "contains the index sampled from the "
#~ "multinomial probability distribution located "
#~ "in the corresponding row of tensor "
#~ "prob."
#~ msgstr ""

#~ msgid ""
#~ "For better cpu performance, use "
#~ "'vm.builtin.multinomial_from_uniform'. For accurate "
#~ "results, ensure probabilities are between "
#~ "0 and 1 and sum to 1."
#~ msgstr ""

#~ msgid ""
#~ "A 2-D tensor of shape (batch, "
#~ "vocab_size) representing probability distributions."
#~ " Each row is a distribution across"
#~ " vocabulary for a batch, where: "
#~ "Values range from [0, 1], indicating "
#~ "the probability of each vocabulary item."
#~ " The sum of values in each row"
#~ " is 1, forming a valid distribution."
#~ msgstr ""

#~ msgid ""
#~ "The uniformly sampled 2-D tensor with"
#~ " the shape (n, 1). Values range "
#~ "from 0 to 1, indicating probabilities"
#~ " sampled uniformly."
#~ msgstr ""

#~ msgid ""
#~ "The 2-D tensor with the shape [n,"
#~ " 1], which indicates the specific "
#~ "probability distribution to sample from. "
#~ "The value of sample_indices[i] determines "
#~ "that the ith token should be "
#~ "sampled from the sample_indices[i]th "
#~ "probability distribution. For instance, if "
#~ "there are 3 distinct probability "
#~ "distributions and the requirement is to"
#~ " sample 2, 3, and 4 tokens from"
#~ " each, then sample_indices would be "
#~ "[0, 0, 1, 1, 1, 2, 2, 2, "
#~ "2]."
#~ msgstr ""

#~ msgid "**result** -- The computed tensor with shape (n, 1)."
#~ msgstr ""

#~ msgid "Multiplication with numpy-style broadcasting."
#~ msgstr ""

#~ msgid "Compute element-wise negative of the input data."
#~ msgstr ""

#~ msgid "**result** -- The computed result"
#~ msgstr ""

#~ msgid "Find the indices of elements of a tensor that are non-zero."
#~ msgstr ""

#~ msgid "**result** -- A 2-D tensor containing indices of non-zero elements."
#~ msgstr ""

#~ msgid "This function is equivalent to `onnx.nonzero`."
#~ msgstr ""

#~ msgid "Broadcasted element-wise test for (lhs != rhs)."
#~ msgstr ""

#~ msgid "Create a call node that represents a null value object."
#~ msgstr ""

#~ msgid "Returns a one-hot tensor."
#~ msgstr ""

#~ msgid "The indices to set to `on_value`."
#~ msgstr ""

#~ msgid "The value to fill at `indices`."
#~ msgstr ""

#~ msgid "The value to fill at other locations."
#~ msgstr ""

#~ msgid "The depth of the one-hot dimension."
#~ msgstr ""

#~ msgid "The axis to fill. Default is -1 which adds a new dimension at the end."
#~ msgstr ""

#~ msgid "Construct a tensor of all ones, with the input shape and dtype."
#~ msgstr ""

#~ msgid "Construct a tensor with all ones, with shape of the input tensor shape."
#~ msgstr ""

#~ msgid "Permutes the dimensions of an array."
#~ msgstr ""

#~ msgid ""
#~ "The target axes order. If not "
#~ "specified, permute_dims will reverse the "
#~ "order of all axes."
#~ msgstr ""

#~ msgid "**result** -- The transposed result."
#~ msgstr ""

#~ msgid "Power with numpy-style broadcasting."
#~ msgstr ""

#~ msgid "Print op to print the values"
#~ msgstr ""

#~ msgid "The values to print."
#~ msgstr ""

#~ msgid "The format string or StringImm."
#~ msgstr ""

#~ msgid "**result** -- A relax Call, which will print the value during runtime."
#~ msgstr ""

#~ msgid "Computes the product of tensor elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a product"
#~ " is performed. The default, axis=None, "
#~ "will compute the product of all "
#~ "elements of the input tensor. Negative"
#~ " indexing is supported."
#~ msgstr ""

#~ msgid ""
#~ "Quantize op This operator takes input"
#~ " and produces quantized output. The "
#~ "input tensor can be of any shape."
#~ " The output shape is the same "
#~ "as input shape."
#~ msgstr ""

#~ msgid ""
#~ "Q_output = clamp((round(input_tensor/scale) + "
#~ "zero_point), out_dtype::min, out_dtype::max)"
#~ msgstr ""

#~ msgid "The input tensor to be quantized."
#~ msgstr ""

#~ msgid "The output scale."
#~ msgstr ""

#~ msgid "The output zero_point."
#~ msgstr ""

#~ msgid ""
#~ "The channel axis for quantization. "
#~ "Default value is -1 which corresponds"
#~ " to the last axis."
#~ msgstr ""

#~ msgid "Register operator gradient function for a relax operator."
#~ msgstr ""

#~ msgid "The name of the op."
#~ msgstr ""

#~ msgid "-> partials: List[Expr] The gradient function being used."
#~ msgstr ""

#~ msgid "The priority level"
#~ msgstr ""

#~ msgid "Repeats elements of an array."
#~ msgstr ""

#~ msgid "The number of repetitions."
#~ msgstr ""

#~ msgid ""
#~ "The axis along which to repeat "
#~ "values. The negative numbers are "
#~ "interpreted counting from the backward. "
#~ "By default, use the flattened input "
#~ "array, and return a flat output "
#~ "array."
#~ msgstr ""

#~ msgid "Reshape the input array."
#~ msgstr ""

#~ msgid ""
#~ "``-1`` infers the dimension of the "
#~ "output shape by using the remainder "
#~ "of the input dimensions keeping the "
#~ "size of the new array same as "
#~ "that of the input array. At most"
#~ " one dimension of shape can be "
#~ "-1."
#~ msgstr ""

#~ msgid "The new shape. Should be compatible with the original shape."
#~ msgstr ""

#~ msgid "**result** -- The reshaped result."
#~ msgstr ""

#~ msgid ""
#~ "The ``-1`` inference is only performed"
#~ " at compile-time. That is to "
#~ "say, in any case the dimension "
#~ "length of ``-1`` cannot be inferred "
#~ "in compile-time, an error will be"
#~ " thrown."
#~ msgstr ""

#~ msgid ""
#~ "Bitwise Shift Right :param x1: The "
#~ "input tensor to be shifted. :type "
#~ "x1: relax.Expr :param x2: The number "
#~ "of positions to shift. :type x2: "
#~ "relax.Expr"
#~ msgstr ""

#~ msgid "Rounds each element of the input data to nearest integer."
#~ msgstr ""

#~ msgid "Compute element-wise reciprocal square root of the input data."
#~ msgstr ""

#~ msgid "1/sqrt(x)"
#~ msgstr ""

#~ msgid ""
#~ "ONNX style scatter elements. This "
#~ "operation updates its value in `data`"
#~ " to values specified by `updates` at"
#~ " specific index positions specified by "
#~ "`indices`. For example, in 2D tensor,"
#~ " the update corresponding to the "
#~ "[i][j] entry is performed as below:"
#~ msgstr ""

#~ msgid ""
#~ "When the `reduction` is set to "
#~ "some reduction function `f`, the update"
#~ " corresponding to [i][j] entry is "
#~ "performed as below:"
#~ msgstr ""

#~ msgid "Where `f` is update, add, mul, mean, max, min."
#~ msgstr ""

#~ msgid "The index positions to update in `data`."
#~ msgstr ""

#~ msgid "Values to replace to."
#~ msgstr ""

#~ msgid "Axis to scatter on."
#~ msgstr ""

#~ msgid ""
#~ "Type of reduction to apply: update, "
#~ "add, mul, mean, max, min. It is"
#~ " \"update\" by default."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The result has the "
#~ "same size as data, and the same"
#~ " shape as data"
#~ msgstr ""

#~ msgid "Scatter updates into an array according to indices."
#~ msgstr ""

#~ msgid "The input data to be updated."
#~ msgstr ""

#~ msgid ""
#~ "Type of reduction to apply: update, "
#~ "add, mul, max, min. It is "
#~ "\"update\" by default."
#~ msgstr ""

#~ msgid "**result** -- The result has the same shape as data."
#~ msgstr ""

#~ msgid "Get shape of a tensor."
#~ msgstr ""

#~ msgid "The input Expr."
#~ msgstr ""

#~ msgid "**result** -- A relax Call, which gets the shape of the input"
#~ msgstr ""

#~ msgid ""
#~ "Convert shape to tensor expr. :param "
#~ "expr: The input Expr :type expr: "
#~ "Expr"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- A relax Call, which "
#~ "transforms the shape values to the "
#~ "tensor"
#~ msgstr ""

#~ msgid "Compute element-wise sigmoid of the input data."
#~ msgstr ""

#~ msgid ""
#~ "Returns an indication of the sign "
#~ "of a number for each element of"
#~ " the input data."
#~ msgstr ""

#~ msgid "Compute element-wise sin of the input data."
#~ msgstr ""

#~ msgid "Compute element-wise sinh of the input data."
#~ msgstr ""

#~ msgid ""
#~ "Performs sorting along the given axis"
#~ " and returns an array in sorted "
#~ "order."
#~ msgstr ""

#~ msgid ""
#~ "Axis along which to sort the input"
#~ " tensor. By default the last axis "
#~ "of the input is used."
#~ msgstr ""

#~ msgid "**out** -- Sorted tensor."
#~ msgstr ""

#~ msgid "Split input tensor along axis by sections or indices."
#~ msgstr ""

#~ msgid ""
#~ "If indices_or_sections is an integer, "
#~ "the input will be divided equally "
#~ "along given axis (if possible). Last "
#~ "section will be smaller if the "
#~ "tensor size along the given dimension"
#~ " is not divisible by the integer."
#~ msgstr ""

#~ msgid ""
#~ "If indices_or_sections is a tuple of "
#~ "mixture of int or PrimExpr, the "
#~ "entries indicate the indices where along"
#~ " axis the array is split."
#~ msgstr ""

#~ msgid "The tensor to be split."
#~ msgstr ""

#~ msgid "Indices or sections to split into. Accepts an int or a list."
#~ msgstr ""

#~ msgid "The axis over which to split."
#~ msgstr ""

#~ msgid "Compute element-wise square root of the input data."
#~ msgstr ""

#~ msgid "Squares each element of the input data."
#~ msgstr ""

#~ msgid "Squeeze axes in the array."
#~ msgstr ""

#~ msgid ""
#~ "The set of axes to remove. If "
#~ "axis = None, remove all axis of"
#~ " dimensions 1. If any specified axis"
#~ " has dimension that does not equal"
#~ " 1, it is an error."
#~ msgstr ""

#~ msgid "**result** -- The squeezed result."
#~ msgstr ""

#~ msgid "Computes the standard deviation of tensor elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a "
#~ "standard deviation is performed. The "
#~ "default, axis=None, will compute the std"
#~ " of all elements of the input "
#~ "tensor. Negative indexing is supported."
#~ msgstr ""

#~ msgid "Strided slice of a tensor."
#~ msgstr ""

#~ msgid "Axes along which slicing is applied."
#~ msgstr ""

#~ msgid ""
#~ "Whether to assume the indices are "
#~ "in bound. If it is set to "
#~ "false, out of bound indices will "
#~ "be clipped to the bound."
#~ msgstr ""

#~ msgid ""
#~ "strided_slice require the input `begin`, "
#~ "`end` and `strides` to have the "
#~ "same length as `axes`."
#~ msgstr ""

#~ msgid "Subtraction with numpy-style broadcasting."
#~ msgstr ""

#~ msgid "Computes the sum of tensor elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a sum "
#~ "is performed. The default, axis=None, "
#~ "will sum all of the elements of"
#~ " the input tensor. Negative indexing "
#~ "is supported."
#~ msgstr ""

#~ msgid ""
#~ "Take elements from a tensor along "
#~ "an axis. Its semantic is mostly "
#~ "similar to `numpy.take` "
#~ "(https://numpy.org/doc/stable/reference/generated/numpy.take.html),"
#~ " which can cover `torch.take` "
#~ "(https://pytorch.org/docs/stable/generated/torch.take.html) and"
#~ " `onnx.gather` "
#~ "(https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Gather-13)."
#~ msgstr ""

#~ msgid "The source tensor."
#~ msgstr ""

#~ msgid "The indices of the values to extract."
#~ msgstr ""

#~ msgid ""
#~ "The axis over which to select "
#~ "values. If it is none, the input"
#~ " tensor is required to be one-"
#~ "dimensional."
#~ msgstr ""

#~ msgid "**ret** -- The taken result."
#~ msgstr ""

#~ msgid "Compute element-wise tan of the input data."
#~ msgstr ""

#~ msgid "Compute element-wise tanh of the input data."
#~ msgstr ""

#~ msgid ""
#~ "Convert tensor to shape expr. :param "
#~ "expr: The input Expr :type expr: "
#~ "Expr"
#~ msgstr ""

#~ msgid ""
#~ "**result** -- A relax Call, which "
#~ "transforms the tensor values to the "
#~ "shape"
#~ msgstr ""

#~ msgid ""
#~ "Construct an array by repeating data "
#~ "the number of times given by "
#~ "repeats."
#~ msgstr ""

#~ msgid ""
#~ "If repeats has length l, and data"
#~ " has dimension d, the result will "
#~ "have dimension of max(l, d)."
#~ msgstr ""

#~ msgid ""
#~ "If d < l, data is promoted "
#~ "to be l-dimensional by prepending new"
#~ " axes. So a shape (3,) Tensor "
#~ "is promoted to (1, 3) for 2-D "
#~ "replication, or shape (1, 1, 3) "
#~ "for 3-D replication. If this is "
#~ "not the desired behavior, promote data"
#~ " to d-dimensions manually before calling"
#~ " this function."
#~ msgstr ""

#~ msgid ""
#~ "If d > l, reps is promoted "
#~ "to length d by pre-pending 1's "
#~ "to it. Thus for a data of "
#~ "shape (2, 3, 4, 5), a reps "
#~ "of (2, 2) is treated as (1, "
#~ "1, 2, 2)."
#~ msgstr ""

#~ msgid "The number of repetitions of data along each axis."
#~ msgstr ""

#~ msgid ""
#~ "Copy data to the destination device. "
#~ "This operator helps data transferring "
#~ "between difference devices for heterogeneous"
#~ " execution."
#~ msgstr ""

#~ msgid "The destination device where the data is copied to."
#~ msgstr ""

#~ msgid "**result** -- The copied result."
#~ msgstr ""

#~ msgid "Get the top k elements in an input tensor along the given axis."
#~ msgstr ""

#~ msgid ""
#~ "ret_type specifies the return type, can"
#~ " be one of (\"both\", \"values\", "
#~ "\"indices\")."
#~ msgstr ""

#~ msgid "Number of top elements to select. Return all elements if k < 1."
#~ msgstr ""

#~ msgid ""
#~ "The return type [both, values, indices]."
#~ " \"both\": return both top k data "
#~ "and indices. \"values\": return top k"
#~ " data only. \"indices\": return top k"
#~ " indices only."
#~ msgstr ""

#~ msgid ""
#~ "Whether to return largest or smallest"
#~ " elements. The k smallest elements "
#~ "are returned if largest is False."
#~ msgstr ""

#~ msgid "The data type of the indices output."
#~ msgstr ""

#~ msgid "**out** -- The computed result."
#~ msgstr ""

#~ msgid "Return the lower triangular part of a matrix or a batch of matrices."
#~ msgstr ""

#~ msgid ""
#~ "The tensor that tril will be "
#~ "applied to. It is required to have"
#~ " at least two dimensions."
#~ msgstr ""

#~ msgid ""
#~ "The index indicating the diagonal above"
#~ " which to zero elements. If k ="
#~ " 0, the diagonal is the main "
#~ "diagonal. If k < 0, the diagonal"
#~ " is below the main diagonal. If "
#~ "k > 0, the diagonal is above "
#~ "the main diagonal."
#~ msgstr ""

#~ msgid "**ret** -- The result tensor."
#~ msgstr ""

#~ msgid "Return the upper triangular part of a matrix or a batch of matrices."
#~ msgstr ""

#~ msgid ""
#~ "The tensor that triu will be "
#~ "applied to. It is required to have"
#~ " at least two dimensions."
#~ msgstr ""

#~ msgid ""
#~ "The index indicating the diagonal below"
#~ " which to zero elements. If k ="
#~ " 0, the diagonal is the main "
#~ "diagonal. If k < 0, the diagonal"
#~ " is below the main diagonal. If "
#~ "k > 0, the diagonal is above "
#~ "the main diagonal."
#~ msgstr ""

#~ msgid ""
#~ "Find the unique elements in a "
#~ "given tensor. In addition, it optionally"
#~ " returns - the indices of the "
#~ "input tensor that give the unique "
#~ "values; - the indices of the "
#~ "unique tensor that reconstruct the input"
#~ " tensor; - the number of times "
#~ "each unique value comes up in the"
#~ " input tensor."
#~ msgstr ""

#~ msgid ""
#~ "Whether to sort the unique elements "
#~ "in ascending order before returning as"
#~ " output."
#~ msgstr ""

#~ msgid ""
#~ "Whether to return an additional tensor"
#~ " with indices for where elements in"
#~ " the unique tensor come from the "
#~ "original input."
#~ msgstr ""

#~ msgid ""
#~ "Whether to return an additional tensor"
#~ " with indices for where elements in"
#~ " the original input ended up in "
#~ "the returned unique list."
#~ msgstr ""

#~ msgid ""
#~ "Whether to return an additional tensor"
#~ " with counts of each unique elements."
#~ msgstr ""

#~ msgid ""
#~ "The dimension to apply unique. If "
#~ "not specified, the unique values of "
#~ "the flattened input are returned."
#~ msgstr ""

#~ msgid "**ret** -- The created relax call with"
#~ msgstr ""

#~ msgid "Computes the variance of tensor elements over given axes."
#~ msgstr ""

#~ msgid ""
#~ "Axis or axes along which a "
#~ "variance operation is performed. The "
#~ "default, axis=None, will compute the "
#~ "variance of all elements in the "
#~ "input tensor. Negative indexing is "
#~ "supported."
#~ msgstr ""

#~ msgid ""
#~ "Selecting elements from either the input"
#~ " tensors depending on the value of"
#~ " the condition."
#~ msgstr ""

#~ msgid ""
#~ "For a given position, return the "
#~ "corresponding value in `x1` if "
#~ "`condition` is True, and return the "
#~ "corresponding value in `x2` otherwise."
#~ msgstr ""

#~ msgid ""
#~ "When True, yield `x1`; otherwise, yield"
#~ " `x2`. Must be broadcasting compatible "
#~ "with `x1` and `x2`. Must have "
#~ "boolean dtype."
#~ msgstr ""

#~ msgid ""
#~ "The first input tensor. Must be "
#~ "broadcasting compatible with `condition` and"
#~ " `x2`."
#~ msgstr ""

#~ msgid ""
#~ "The second input tensor. Must be "
#~ "broadcasting compatible with `condition` and"
#~ " `x1`."
#~ msgstr ""

#~ msgid ""
#~ "Cast input tensor which is model "
#~ "param to data type if the dtype"
#~ " of the input data is not the"
#~ " same as the given dtype. :param "
#~ "data: The input data to the "
#~ "operator. :type data: relax.Expr :param "
#~ "dtype: The target data type :type "
#~ "dtype: Union[str, DataType]"
#~ msgstr ""

#~ msgid "Construct a tensor of all zeros, with the input shape and dtype."
#~ msgstr ""

#~ msgid ""
#~ "Construct a tensor with all zeros, "
#~ "with shape of the input tensor "
#~ "shape."
#~ msgstr ""

#~ msgid "Neural network related operators."
#~ msgstr ""

#~ msgid "1D adaptive average pooling operator. This operator is experimental."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 1D average value calculation"
#~ " across each window represented by W."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCW` a data Tensor "
#~ "with shape `(batch_size, in_channels, width)`,"
#~ " to produce an output Tensor with "
#~ "shape (batch_size, in_channels, output_width)."
#~ msgstr ""

#~ msgid ""
#~ "The pooling kernel and stride sizes "
#~ "are automatically chosen for desired "
#~ "output sizes."
#~ msgstr ""

#~ msgid "For output_size:"
#~ msgstr ""

#~ msgid ""
#~ "If this argument is not provided, "
#~ "input height and width will be "
#~ "used as output width."
#~ msgstr ""

#~ msgid ""
#~ "If a single integer is provided "
#~ "for output_size, the output size is "
#~ "(N x C x output_size) for any "
#~ "input (NCW)."
#~ msgstr ""

#~ msgid ""
#~ "Output height and width. If not "
#~ "specified, it will be the same as"
#~ " the input height and width. If "
#~ "specified, it is required to have "
#~ "length either 1 or 2."
#~ msgstr ""

#~ msgid "Layout of the input."
#~ msgstr ""

#~ msgid "Layout of the output. If not specified, it is the same as data_layout"
#~ msgstr ""

#~ msgid "2D adaptive average pooling operator. This operator is experimental."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 2D average value calculation"
#~ " across each window represented by "
#~ "WxH."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCHW` a data Tensor "
#~ "with shape `(batch_size, in_channels, height,"
#~ " width)`, to produce an output Tensor"
#~ " with shape (batch_size, in_channels, "
#~ "output_height, output_width)."
#~ msgstr ""

#~ msgid ""
#~ "If this argument is not provided, "
#~ "input height and width will be "
#~ "used as output height and width."
#~ msgstr ""

#~ msgid ""
#~ "If a single integer is provided "
#~ "for output_size, the output size is "
#~ "(N x C x output_size x "
#~ "output_size) for any input (NCHW)."
#~ msgstr ""

#~ msgid ""
#~ "If a tuple of integers (height, "
#~ "width) are provided for output_size, the"
#~ " output size is (N x C x "
#~ "height x width) for any input "
#~ "(NCHW)."
#~ msgstr ""

#~ msgid "3D adaptive average pooling operator. This operator is experimental."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 3D average value calculation"
#~ " across each window represented by "
#~ "WxH."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCDHW` a data Tensor "
#~ "with shape `(batch_size, in_channels, depth,"
#~ " height, width)`, to produce an "
#~ "output Tensor with shape (batch_size, "
#~ "in_channels, output_depth, output_height, "
#~ "output_width)."
#~ msgstr ""

#~ msgid ""
#~ "If this argument is not provided, "
#~ "input depth, height and width will "
#~ "be used as output depth, height "
#~ "and width."
#~ msgstr ""

#~ msgid ""
#~ "If a single integer is provided "
#~ "for output_size, the output size is "
#~ "(N x C x output_size x output_size"
#~ " x output_size) for any input "
#~ "(NCDHW)."
#~ msgstr ""

#~ msgid ""
#~ "If a tuple of integers (depth, "
#~ "height, width) are provided for "
#~ "output_size, the output size is (N "
#~ "x C x depth x height x "
#~ "width) for any input (NCDHW)."
#~ msgstr ""

#~ msgid ""
#~ "Output height and width. If not "
#~ "specified, it will be the same as"
#~ " the input height and width. If "
#~ "specified, it is required to have "
#~ "length either 1 or 3."
#~ msgstr ""

#~ msgid "Computes fused multi head attention."
#~ msgstr ""

#~ msgid "All input tensors are of 4-D tensors with BSNH layout."
#~ msgstr ""

#~ msgid ""
#~ "FMA(Q, K, V) = \\text{Softmax}(Q @ K^T) @ V\n"
#~ "\n"
#~ msgstr ""

#~ msgid "The input tensor is required to have float16 dtype"
#~ msgstr ""

#~ msgid ""
#~ "The input query to the operator. "
#~ "The layout of the input query "
#~ "should be (batch_size, seq_len, num_head, "
#~ "head_dim)."
#~ msgstr ""

#~ msgid ""
#~ "The input key to the operator. The"
#~ " layout of the input key should "
#~ "be (batch_size, seq_len_kv, num_head, "
#~ "head_dim)."
#~ msgstr ""

#~ msgid ""
#~ "The input value to the operator. "
#~ "The layout of the input value "
#~ "should be (batch_size, seq_len_kv, num_head,"
#~ " head_dim_v)."
#~ msgstr ""

#~ msgid ""
#~ "The optional attention bias to the "
#~ "operator. The layout of the attention"
#~ " bias should be a 4-D tensor "
#~ "ending with seq_len_kv, and broadcastable "
#~ "to (batch_size, num_head, seq_len, "
#~ "seq_len_kv)."
#~ msgstr ""

#~ msgid ""
#~ "The scale value to be applied to"
#~ " the attention score, by default 1"
#~ " / sqrt(head_dim)."
#~ msgstr ""

#~ msgid ""
#~ "The optional causal mask, i.e. 'TopLeft'"
#~ " and 'BottomRight'. For 'TopLeft', the "
#~ "mask matrix is as `np.tril(*, k=0)`, "
#~ "while for 'BottomRight', the mask matrix"
#~ " is as `np.tril(*, k=abs(seq_len - "
#~ "seq_len_kv))` For example, with seq_len "
#~ "= 4, seq_len_kv = 2, mask for "
#~ "'TopLeft':  .. code:: python      [[1, "
#~ "0],     [1, 1],     [1, 1],     [1, "
#~ "1]]  mask for 'BottomRight':  .. code::"
#~ " python      [[1, 1],     [1, 1],     "
#~ "[1, 1],     [1, 1]]  with seq_len "
#~ "= 2, seq_len_kv = 4, mask for "
#~ "'TopLeft':  .. code:: python      [[1, "
#~ "0, 0, 0],     [1, 1, 0, 0]]  "
#~ "mask for 'BottomRight':  .. code:: "
#~ "python      [[1, 1, 1, 0],     [1, "
#~ "1, 1, 1]]"
#~ msgstr ""

#~ msgid ""
#~ "The optional causal mask, i.e. 'TopLeft'"
#~ " and 'BottomRight'. For 'TopLeft', the "
#~ "mask matrix is as `np.tril(*, k=0)`, "
#~ "while for 'BottomRight', the mask matrix"
#~ " is as `np.tril(*, k=abs(seq_len - "
#~ "seq_len_kv))` For example, with seq_len "
#~ "= 4, seq_len_kv = 2, mask for "
#~ "'TopLeft':"
#~ msgstr ""

#~ msgid "mask for 'BottomRight':"
#~ msgstr ""

#~ msgid "with seq_len = 2, seq_len_kv = 4, mask for 'TopLeft':"
#~ msgstr ""

#~ msgid "The size of the window for sliding-window attention."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The computed result. The"
#~ " layout of the output should be "
#~ "(batch_size, seq_len, num_head, head_dim_v)."
#~ msgstr ""

#~ msgid ""
#~ "Computes fused multi head attention over"
#~ " batched sequences of variable lengths."
#~ msgstr ""

#~ msgid ""
#~ "Given concatenated inputs and sequence "
#~ "lengths information, this operator computes"
#~ " attention for all sequences more "
#~ "efficiently than calling the normal "
#~ "attention operator for each sequence "
#~ "individually."
#~ msgstr ""

#~ msgid ""
#~ "The input queries concatenated along the"
#~ " second axis. Its shape must be "
#~ "(1, total_seq_len, num_head, head_dim)."
#~ msgstr ""

#~ msgid ""
#~ "The input keys concatenated along the"
#~ " second axis. Its shape must be "
#~ "(1, total_seq_len_kv, num_head, head_dim)."
#~ msgstr ""

#~ msgid ""
#~ "The input values concatenated along the"
#~ " second axis. Its shape must be "
#~ "(1, total_seq_len_kv, num_head, head_dim_v)."
#~ msgstr ""

#~ msgid ""
#~ "The cumsum of query sequence lengths,"
#~ " prepended with 0. Its dtype must "
#~ "be int32. For example, if the "
#~ "lengths of the sequences that are "
#~ "batched are [2, 5, 3], this tensor"
#~ " has values [0, 2, 7, 10]."
#~ msgstr ""

#~ msgid ""
#~ "The cumsum of key sequence lengths, "
#~ "prepended with 0. By default it is"
#~ " the same as seqstart_q."
#~ msgstr ""

#~ msgid "The maximum query sequence length in the batch. It must be int32."
#~ msgstr ""

#~ msgid ""
#~ "The maximum key sequence length in "
#~ "the batch. It must be int32. By"
#~ " default it is the same as "
#~ "max_seqlen_q."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The computed result with"
#~ " shape `(1, total_seq_len, num_head, "
#~ "head_dim_v)`."
#~ msgstr ""

#~ msgid "1D average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 1D average value calculation"
#~ " with in pool_size sized window by"
#~ " striding defined by stride"
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCW` a data Tensor "
#~ "with shape `(batch_size, channels, width)`,"
#~ " to produce an output Tensor."
#~ msgstr ""

#~ msgid ""
#~ "The ceil_mode is used to take ceil"
#~ " or floor while computing out shape."
#~ " count_include_pad indicates including or "
#~ "excluding padded input values in "
#~ "computation. This operator accepts data "
#~ "layout specification."
#~ msgstr ""

#~ msgid "The size of window for pooling. It is required to have length is 1."
#~ msgstr ""

#~ msgid "The strides of pooling. It is required to have length is 1."
#~ msgstr ""

#~ msgid "The padding for pooling. It is required to have length either 1 or 2."
#~ msgstr ""

#~ msgid "The dilation of pooling. It is required to have length is 1."
#~ msgstr ""

#~ msgid ""
#~ "A boolean indicating if use ceil "
#~ "or floor to compute the output "
#~ "shape. By using ceil, every element "
#~ "in the input tensor will be "
#~ "covered by a sliding window."
#~ msgstr ""

#~ msgid "To include padding to compute the average."
#~ msgstr ""

#~ msgid "2D average pooling operator."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 2D avarage value calculation"
#~ " with in pool_size sized window by"
#~ " striding defined by stride."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCHW` a data Tensor "
#~ "with shape `(batch_size, in_channels, height,"
#~ " width)`, to produce an output Tensor"
#~ " with the following rule:"
#~ msgstr ""

#~ msgid "with data of shape (b, c, h, w) and pool_size (kh, kw)"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}(b, c, y, x)  = \\frac{1}{kh"
#~ " * kw} \\sum_{m=0, \\ldots, kh-1}\n"
#~ "    \\sum_{n=0, \\ldots, kw-1}\n"
#~ "    \\mbox{data}(b, c, \\mbox{stride}[0] * "
#~ "y + m, \\mbox{stride}[1] * x + "
#~ "n)"
#~ msgstr ""

#~ msgid ""
#~ "Padding is applied to data before "
#~ "the computation. ceil_mode is used to"
#~ " take ceil or floor while computing"
#~ " out shape. This operator accepts "
#~ "data layout specification."
#~ msgstr ""

#~ msgid ""
#~ "The size of window for pooling. It"
#~ " is required to have length either"
#~ " 1 or 2."
#~ msgstr ""

#~ msgid "The strides of pooling. It is required to have length either 1 or 2."
#~ msgstr ""

#~ msgid ""
#~ "The padding for pooling. It is "
#~ "required to have length either 1, "
#~ "2 or 4."
#~ msgstr ""

#~ msgid "The dilation of pooling. It is required to have length either 1 or 2."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 3D average value calculation"
#~ " with in pool_size sized window by"
#~ " striding defined by stride"
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCDHW` a data Tensor "
#~ "with shape `(batch_size, channels, depth, "
#~ "height, width)`, to produce an output"
#~ " Tensor."
#~ msgstr ""

#~ msgid ""
#~ "The size of window for pooling. It"
#~ " is required to have length either"
#~ " 1 or 3."
#~ msgstr ""

#~ msgid "The strides of pooling. It is required to have length either 1 or 3."
#~ msgstr ""

#~ msgid ""
#~ "The padding for pooling. It is "
#~ "required to have length either 1, "
#~ "3 or 6."
#~ msgstr ""

#~ msgid "The dilation of pooling. It is required to have length either 1 or 3."
#~ msgstr ""

#~ msgid "Batch normalization layer (Ioffe and Szegedy, 2014)."
#~ msgstr ""

#~ msgid ""
#~ "Normalizes the input at each batch, "
#~ "i.e. applies a transformation that "
#~ "maintains the mean activation close to"
#~ " 0 and the activation standard "
#~ "deviation close to 1."
#~ msgstr ""

#~ msgid ""
#~ "data\\_mean[i] = mean(data[:,i,:,...]) \\\\\n"
#~ "data\\_var[i] = var(data[:,i,:,...])"
#~ msgstr ""

#~ msgid ""
#~ "Both *mean* and *var* returns a "
#~ "scalar by treating the input as a"
#~ " vector."
#~ msgstr ""

#~ msgid ""
#~ "Then compute the normalized output, "
#~ "which has the same shape as input,"
#~ " as following:"
#~ msgstr ""

#~ msgid ""
#~ "out[:,i,:,...] = \\frac{data[:,i,:,...] - "
#~ "data\\_mean[i]}{\\sqrt{data\\_var[i]+\\epsilon}}\n"
#~ "    * gamma[i] + beta[i]"
#~ msgstr ""

#~ msgid ""
#~ "Assume the input has size *k* on"
#~ " axis 1, then both ``gamma`` and "
#~ "``beta`` have shape *(k,)*."
#~ msgstr ""

#~ msgid ""
#~ "Besides the inputs and the outputs, "
#~ "this operator accepts two auxiliary "
#~ "states, ``moving_mean`` and ``moving_var``, "
#~ "which are *k*-length vectors. They are"
#~ " global statistics for the whole "
#~ "dataset, which are updated by"
#~ msgstr ""

#~ msgid ""
#~ "The parameter ``axis`` specifies which "
#~ "axis of the input shape denotes "
#~ "the 'channel' (separately normalized groups)."
#~ "  The default is 1. Specifying -1 "
#~ "sets the channel axis to be the"
#~ " last item in the input shape."
#~ msgstr ""

#~ msgid "This operator has two modes:"
#~ msgstr ""

#~ msgid "Training mode."
#~ msgstr ""

#~ msgid "Use the mean and var computed from THIS batch to normalize."
#~ msgstr ""

#~ msgid "Update and then return the running mean and running var."
#~ msgstr ""

#~ msgid "Inference mode."
#~ msgstr ""

#~ msgid "Use the running_mean and running_var parameters to normalize."
#~ msgstr ""

#~ msgid ""
#~ "Do not update the running mean and"
#~ " running var. Just return the "
#~ "original value."
#~ msgstr ""

#~ msgid ""
#~ "In the legalization stage, this operator"
#~ " will be legalized to the training"
#~ " mode by default."
#~ msgstr ""

#~ msgid ""
#~ "You can use "
#~ "tvm.relax.transform.DecomposeOpsForInference to decompose"
#~ " the operator, so it executes the "
#~ "inference mode computation. Similarly, use "
#~ "tvm.relax.transform.DecomposeOpsForTraining to execute "
#~ "the training mode computation."
#~ msgstr ""

#~ msgid "The gamma scale factor."
#~ msgstr ""

#~ msgid "The beta offset factor."
#~ msgstr ""

#~ msgid "Running mean of input."
#~ msgstr ""

#~ msgid "Running variance of input."
#~ msgstr ""

#~ msgid "The axis along which the normalization is applied."
#~ msgstr ""

#~ msgid "Small float added to variance to avoid dividing by zero."
#~ msgstr ""

#~ msgid "Indicating if the beta offset will be added to the normalized tensor."
#~ msgstr ""

#~ msgid "Indicating if the gamma scale will be multiplied."
#~ msgstr ""

#~ msgid "The value used for the moving_mean and moving_var update."
#~ msgstr ""

#~ msgid ""
#~ "A boolean value to indicate whether "
#~ "training or in eval mode. By "
#~ "default.   relax batch_norm is training "
#~ "mode. To transform it to inference "
#~ "mode,   can use DecomposeOpsForInference."
#~ msgstr ""

#~ msgid ""
#~ "A boolean value to indicate whether "
#~ "training or in eval mode. By "
#~ "default."
#~ msgstr ""

#~ msgid ""
#~ "relax batch_norm is training mode. To"
#~ " transform it to inference mode, can"
#~ " use DecomposeOpsForInference."
#~ msgstr ""

#~ msgid "1D convolution."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes the weight as "
#~ "the 1D convolution kernel and convolves"
#~ " it with data to produce an "
#~ "output."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCW` and kernel_layout "
#~ "is `OIW`, conv1d takes in a data"
#~ " Tensor with shape `(batch_size, "
#~ "in_channels, width)`, and a weight "
#~ "Tensor with shape `(channels, in_channels, "
#~ "kernel_w)`, where `kernel_w` is the "
#~ "length of the `W` kernel dimension, "
#~ "to produce an output Tensor with "
#~ "the following rule:"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}[b, c, x] = \\sum_{dx, k}\n"
#~ "   \\mbox{data}[b, k, \\mbox{strides} * x + dx] *\n"
#~ "   \\mbox{weight}[c, k, dx]"
#~ msgstr ""

#~ msgid ""
#~ "Padding and dilation are applied to "
#~ "data and weight respectively before the"
#~ " computation. This operator accepts data"
#~ " layout specification. Semantically, the "
#~ "operator will convert the layout to "
#~ "the canonical layout (`NCW` for data "
#~ "and `OIW` for weight), perform the "
#~ "computation, then convert to the "
#~ "out_layout."
#~ msgstr ""

#~ msgid "The weight expressions."
#~ msgstr ""

#~ msgid "The strides of convolution. It is required to have length 1."
#~ msgstr ""

#~ msgid ""
#~ "The padding of convolution on both "
#~ "sides of inputs before convolution. It"
#~ " is required to have length either"
#~ " 1 or 2."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the dilation rate to be "
#~ "used for dilated convolution. It is "
#~ "required to have length 1."
#~ msgstr ""

#~ msgid ""
#~ "Number of groups to split the "
#~ "input into for grouped convolution. The"
#~ " number of input and output channels"
#~ " should be divisible by the number"
#~ " of groups."
#~ msgstr ""

#~ msgid "Layout of the weight."
#~ msgstr ""

#~ msgid "Specifies the output data type for mixed precision conv1d."
#~ msgstr ""

#~ msgid "1D transposed convolution operator."
#~ msgstr ""

#~ msgid "This operator can be seen as the gradient operator of conv1d."
#~ msgstr ""

#~ msgid ""
#~ "The output shape can be explained "
#~ "in the simple case when `data_layout "
#~ "== \"NCW\"` and `kernel_layout == "
#~ "\"IOW\"`. Suppose `data` has shape `(N,"
#~ " in_channel, in_w)`, `weight` has shape "
#~ "`(in_channel, out_channel, weight_w)`, we need"
#~ " to assure that `in_channel % groups"
#~ " == 0`. The shape of the output"
#~ " will be `(N, out_channel * groups,"
#~ " out_w)`, where"
#~ msgstr ""

#~ msgid ""
#~ "`out_w = ((in_w - 1) * strides[0]"
#~ " + weight_w - 2 * padding[0] +"
#~ " output_padding[0])`"
#~ msgstr ""

#~ msgid "Used to disambiguate the output shape."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the dilation rate to be "
#~ "used for dilated convolution. It is "
#~ "required to have length either 1."
#~ msgstr ""

#~ msgid "Specifies the output data type for mixed precision conv2d."
#~ msgstr ""

#~ msgid "2D convolution."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes the weight as "
#~ "the convolution kernel and convolves it"
#~ " with data to produce an output."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCHW` and kernel_layout "
#~ "is `OIHW`, conv2d takes in a data"
#~ " Tensor with shape `(batch_size, "
#~ "in_channels, height, width)`, and a "
#~ "weight Tensor with shape `(channels, "
#~ "in_channels, kernel_h, kernel_w)`, where "
#~ "`kernel_h` and `kernel_w` is the lengths"
#~ " of the `H` and `W` kernel "
#~ "dimensions, to produce an output Tensor"
#~ " with the following rule:"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}[b, c, y, x] = \\sum_{dy, dx, k}\n"
#~ "   \\mbox{data}[b, k, \\mbox{strides}[0] * "
#~ "y  + dy, \\mbox{strides}[1] * x +"
#~ " dx] *\n"
#~ "   \\mbox{weight}[c, k, dy, dx]"
#~ msgstr ""

#~ msgid ""
#~ "Padding and dilation are applied to "
#~ "data and weight respectively before the"
#~ " computation. This operator accepts data"
#~ " layout specification. Semantically, the "
#~ "operator will convert the layout to "
#~ "the canonical layout (`NCHW` for data"
#~ " and `OIHW` for weight), perform the"
#~ " computation, then convert to the "
#~ "out_layout."
#~ msgstr ""

#~ msgid ""
#~ "The strides of convolution. It is "
#~ "required to have length either 1 "
#~ "or 2."
#~ msgstr ""

#~ msgid ""
#~ "The padding of convolution on both "
#~ "sides of inputs before convolution. It"
#~ " is required to have length either"
#~ " 1, 2 or 4."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the dilation rate to be "
#~ "used for dilated convolution. It is "
#~ "required to have length either 1 "
#~ "or 2."
#~ msgstr ""

#~ msgid "Two dimensional transposed convolution operator."
#~ msgstr ""

#~ msgid ""
#~ "This operator is intended to be "
#~ "the gradient operator of conv2d. That"
#~ " means, if"
#~ msgstr ""

#~ msgid "`out = conv2d(data, weight, strides, padding, dilation)`,"
#~ msgstr ""

#~ msgid "The gradient w.r.t. data can be calculated as follows:"
#~ msgstr ""

#~ msgid ""
#~ "`data_grad = conv2d_transpose(out_grad, weight, "
#~ "strides, padding, output_padding, dilation)`,"
#~ msgstr ""

#~ msgid ""
#~ "where `output_padding` is a parameter "
#~ "used to determine the output shape."
#~ msgstr ""

#~ msgid ""
#~ "The output shape can be explained "
#~ "in the simple case when `data_layout "
#~ "== \"NCHW\"` and `kernel_layout == "
#~ "\"IOHW\"`. Suppose `data` has shape `(N,"
#~ " in_channel, in_h, in_w)`, `weight` has "
#~ "shape `(in_channel, out_channel, weight_h, "
#~ "weight_w)`, we need to assure that "
#~ "`in_channel % groups == 0`. The "
#~ "shape of the output will be `(N,"
#~ " out_channel * groups, out_h, out_w)`, "
#~ "where"
#~ msgstr ""

#~ msgid ""
#~ "`out_h = ((in_h - 1) * strides[0]"
#~ " + weight_h - 2 * padding[0] +"
#~ " output_padding[0])`"
#~ msgstr ""

#~ msgid ""
#~ "`out_w = ((in_w - 1) * strides[1]"
#~ " + weight_w - 2 * padding[1] +"
#~ " output_padding[1])`"
#~ msgstr ""

#~ msgid "3D convolution."
#~ msgstr ""

#~ msgid ""
#~ "In the default case, where the "
#~ "data_layout is `NCDHW` and kernel_layout "
#~ "is `OIDHW`, conv3d takes in a data"
#~ " Tensor with shape `(batch_size, "
#~ "in_channels, depth, height, width)`, and "
#~ "a weight Tensor with shape `(channels,"
#~ " in_channels, kernel_d, kernel_h, kernel_w)`, "
#~ "where `kernel_d`, `kernel_h`, and `kernel_w`"
#~ " are the lengths of the `D`, "
#~ "`H`, and `W` kernel dimensions, to "
#~ "produce an output Tensor with the "
#~ "following rule:"
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}[b, c, z, y, x] = \\sum_{dz, dy, dx, k}\n"
#~ "   \\mbox{data}[b, k, \\mbox{strides}[0] * z + dz,\n"
#~ "   \\mbox{strides}[1] * y  + dy,\n"
#~ "   \\mbox{strides}[2] * x + dx] *\n"
#~ "   \\mbox{weight}[c, k, dz, dy, dx]"
#~ msgstr ""

#~ msgid ""
#~ "Padding and dilation are applied to "
#~ "data and weight respectively before the"
#~ " computation. This operator accepts data"
#~ " layout specification. Semantically, the "
#~ "operator will convert the layout to "
#~ "the canonical layout (`NCDHW` for data"
#~ " and `OIDHW` for weight), perform the"
#~ " computation, then convert to the "
#~ "out_layout."
#~ msgstr ""

#~ msgid ""
#~ "The strides of convolution. It is "
#~ "required to have length either 1 "
#~ "or 3."
#~ msgstr ""

#~ msgid ""
#~ "The padding of convolution on both "
#~ "sides of inputs before convolution. It"
#~ " is required to have length either"
#~ " 1, 3 or 6."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the dilation rate to be "
#~ "used for dilated convolution. It is "
#~ "required to have length either 1 "
#~ "or 3."
#~ msgstr ""

#~ msgid "CrossEntropy with logits between the predictions and labels."
#~ msgstr ""

#~ msgid ""
#~ "The shape of predictions and labels "
#~ "must be the same. And when ndim"
#~ " >= 2, the first dimension is "
#~ "regarded as the batch_size N. In "
#~ "this case the computed result will "
#~ "divide by N to perform a mean "
#~ "reduction."
#~ msgstr ""

#~ msgid ""
#~ "\\text{cross\\_entropy\\_with\\_logits}(x_i, y_i) = "
#~ "\\frac{\\sum_i -x_i \\cdot y_i}{N}"
#~ msgstr ""

#~ msgid "The predictions."
#~ msgstr ""

#~ msgid "The labels (the ground truth values)."
#~ msgstr ""

#~ msgid "Applies the dropout operation to the input tensor."
#~ msgstr ""

#~ msgid ""
#~ "During training, each element of the "
#~ "input is set to zero with "
#~ "probability ``p``. The whole array is"
#~ " scaled by ``1/(1-p)`` to keep the"
#~ " expected sum of the input unchanged."
#~ msgstr ""

#~ msgid "The probability for an element to be reset to 0."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The result of dropout, "
#~ "which is a tuple of two tensors."
#~ " The first one is the original "
#~ "tensor and the second one is a "
#~ "mask tensor (1.0 where element not "
#~ "dropped, 0.0 where dropped)"
#~ msgstr ""

#~ msgid "Gaussian Error Linear Units function"
#~ msgstr ""

#~ msgid ""
#~ "\\text{GeLU}(x) = 0.5 * x * (1 + \\text{erf}(x * 0.5**0.5))\n"
#~ "\n"
#~ msgstr ""

#~ msgid "where :math:`erf` is the Gauss Error function."
#~ msgstr ""

#~ msgid "Gaussian Error Linear Units function with tanh approximation"
#~ msgstr ""

#~ msgid ""
#~ "\\text{GELU}(x) = 0.5 * x * (1 "
#~ "+ \\text{Tanh}(\\sqrt(2 / \\pi) * (x "
#~ "+ 0.044715 * x^3)))\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "Group normalization (Yuxin Wu and et "
#~ "al., 2016). Applies group normalization "
#~ "to the n-dimensional input array. This"
#~ " operator takes an n-dimensional input "
#~ "array. First separate the input array"
#~ " into groups along the channel axis."
#~ " Then apply layer normalization to "
#~ "each group."
#~ msgstr ""

#~ msgid "Input to which group_norm will be applied."
#~ msgstr ""

#~ msgid "Number of groups to separate the channels into."
#~ msgstr ""

#~ msgid "The index of the channel axis in the input data."
#~ msgstr ""

#~ msgid ""
#~ "The axes that along which the "
#~ "normalization is applied (excluding the "
#~ "group axis)"
#~ msgstr ""

#~ msgid ""
#~ "Layer normalization (Lei Ba and et "
#~ "al., 2016). Applies layer normalization "
#~ "to the n-dimensional input array. This"
#~ " operator takes an n-dimensional input "
#~ "array and normalizes the input using "
#~ "the given axis:"
#~ msgstr ""

#~ msgid ""
#~ "out = \\frac{data - mean(data, "
#~ "axis)}{\\sqrt{var(data, axis)+\\epsilon}}\n"
#~ "    * gamma + beta"
#~ msgstr ""

#~ msgid ""
#~ "Unlike batch normalization, the mean and"
#~ " var are computed along the channel"
#~ " dimension."
#~ msgstr ""

#~ msgid ""
#~ "Assume the input has size k on "
#~ "axis 1, then both gamma and beta"
#~ " have shape (k,)."
#~ msgstr ""

#~ msgid "This operator can be optimized away for inference."
#~ msgstr ""

#~ msgid "Input to which layer_norm will be applied."
#~ msgstr ""

#~ msgid "The axes that along which the normalization is applied."
#~ msgstr ""

#~ msgid "Rectified linear unit."
#~ msgstr ""

#~ msgid ""
#~ "text{LeakyReLU, negative_slope}(x) = max(x, 0)"
#~ " + negative_slope * min(x, 0)\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "Controls the angle of the negative "
#~ "slope, used for nagative inputs. Default"
#~ " value is 0.01"
#~ msgstr ""

#~ msgid "Computes log softmax."
#~ msgstr ""

#~ msgid ""
#~ "\\text{log\\_softmax}(x_i) = \\log\\left( "
#~ "\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\right)"
#~ msgstr ""

#~ msgid ""
#~ "The axis to sum over when "
#~ "computing log softmax. If not specified,"
#~ " it is by default the last axis"
#~ " of the input tensor. Supports "
#~ "negative indexing."
#~ msgstr ""

#~ msgid "1D maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 1D max value calculation "
#~ "with in pool_size sized window by "
#~ "striding defined by stride."
#~ msgstr ""

#~ msgid ""
#~ "IIn the default case, where the "
#~ "data_layout is `NCW` a data Tensor "
#~ "with shape `(batch_size, channels, width)`,"
#~ " to produce an output Tensor."
#~ msgstr ""

#~ msgid "The size of window for pooling. It is required to have length either 1."
#~ msgstr ""

#~ msgid "The strides of pooling. It is required to have length either 1."
#~ msgstr ""

#~ msgid "The dilation of pooling. It is required to have length either 1."
#~ msgstr ""

#~ msgid "2D maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 2D max value calculation "
#~ "with in pool_size sized window by "
#~ "striding defined by stride."
#~ msgstr ""

#~ msgid ""
#~ "\\mbox{out}(b, c, y, x)  = \\max_{m=0,"
#~ " \\ldots, kh-1} \\max_{n=0, \\ldots, kw-1}"
#~ "\n"
#~ "     \\mbox{data}(b, c, \\mbox{stride}[0] *"
#~ " y + m, \\mbox{stride}[1] * x +"
#~ " n)"
#~ msgstr ""

#~ msgid "3D maximum pooling operator."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 3D max value calculation "
#~ "with in pool_size sized window by "
#~ "striding defined by stride."
#~ msgstr ""

#~ msgid "Negative log likelihood loss."
#~ msgstr ""

#~ msgid ""
#~ "`output[n, i_1, i_2, ..., i_k] = "
#~ "-p * w`, where - `p = "
#~ "predictions[n, t, i_1, i_2, i_k]`, - "
#~ "`t = targets[n, i_1, i_2, ..., "
#~ "i_k]`, - `w = weights[t] if t "
#~ "!= ignore_index else 0`"
#~ msgstr ""

#~ msgid "result = reduction(output)"
#~ msgstr ""

#~ msgid ""
#~ "The predictions. Should be a `(k+2)-D`"
#~ " Tensor with shape `(N, C, d_1, "
#~ "d_2, ..., d_k)` where C is the "
#~ "number of target classes."
#~ msgstr ""

#~ msgid ""
#~ "The target value of each prediction. "
#~ "Should be a `(k+1)-D` Tensor with "
#~ "shape `(N, d_1, d_2, ..., d_k)`. "
#~ "Must be of int dtype."
#~ msgstr ""

#~ msgid ""
#~ "The weight of each target value. "
#~ "Should be a `1-D` Tensor with "
#~ "shape `(C,)`. If not specified, it "
#~ "is treated as if having all ones."
#~ msgstr ""

#~ msgid ""
#~ "The reduction method to apply to "
#~ "the output. Possible values are "
#~ "\"mean\", \"sum\" and \"none\"."
#~ msgstr ""

#~ msgid "The target value to ignore."
#~ msgstr ""

#~ msgid "Padding"
#~ msgstr ""

#~ msgid ""
#~ "This operator takes in a tensor "
#~ "and pads each axis by the "
#~ "specified widths using the specified "
#~ "value."
#~ msgstr ""

#~ msgid "The input data to the operator"
#~ msgstr ""

#~ msgid ""
#~ "Number of values padded to the "
#~ "edges of each axis, in the format"
#~ " of ((before_1, after_1), ..., (before_N,"
#~ " after_N))"
#~ msgstr ""

#~ msgid ""
#~ "'constant', 'edge', or 'reflect' 'constant'"
#~ " pads with constant_value pad_value 'edge'"
#~ " pads using the edge values of "
#~ "the input array 'reflect' pads by "
#~ "reflecting values with respect to the"
#~ " edge Default is 'constant'"
#~ msgstr ""

#~ msgid "The value used for padding. Default is 0."
#~ msgstr ""

#~ msgid ""
#~ "\\text{ReLU}(x) = \\max(x, 0)\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "Root mean square normalization (Biao "
#~ "Zhang and et al., 2019). Applies "
#~ "root mean square normalization to the"
#~ " n-dimensional input array. This operator"
#~ " takes an n-dimensional input array "
#~ "and normalizes the input using the "
#~ "given axis:"
#~ msgstr ""

#~ msgid "out = \\frac{data}{\\sqrt{mean(data, axis)+\\epsilon}} * weight + bias"
#~ msgstr ""

#~ msgid "Input to which rms_norm will be applied."
#~ msgstr ""

#~ msgid "The scale factor."
#~ msgstr ""

#~ msgid "The offset factor."
#~ msgstr ""

#~ msgid "Small float added to square mean to avoid dividing by zero."
#~ msgstr ""

#~ msgid "Scaled Exponential Linear Unit (SELU)."
#~ msgstr ""

#~ msgid ""
#~ "\\text{SELU}(x) = \\lambda \\begin{cases}\n"
#~ "    x & \\text{if } x > 0 \\\\\n"
#~ "    \\alpha (e^x - 1) & \\text{if } x \\leq 0\n"
#~ "\\end{cases}\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "where :math:`\\lambda \\approx 1.0507` and "
#~ ":math:`\\alpha \\approx 1.6733`."
#~ msgstr ""

#~ msgid "Sigmoid Linear Unit function"
#~ msgstr ""

#~ msgid ""
#~ "\\text{SiLU}(x) = x * \\text{sigmoid}(x)\n"
#~ "\n"
#~ msgstr ""

#~ msgid "Computes softmax."
#~ msgstr ""

#~ msgid ""
#~ "\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n"
#~ "\n"
#~ msgstr ""

#~ msgid ""
#~ "The axis to sum over when "
#~ "computing softmax. If not specified, it"
#~ " is by default the last axis of"
#~ " the input tensor. Supports negative "
#~ "indexing."
#~ msgstr ""

#~ msgid "Relax builtin operators."
#~ msgstr ""

#~ msgid ""
#~ "Construct a Call to allocate a "
#~ "tensor with specific shape, dtype, "
#~ "runtime_device_index."
#~ msgstr ""

#~ msgid "The shape of the tensor to be allocated."
#~ msgstr ""

#~ msgid "The datatype of the tensor to be allocated."
#~ msgstr ""

#~ msgid ""
#~ "The device index indicating on which "
#~ "device the tensor is to be "
#~ "allocated at runtime. Index -1 is "
#~ "reserved for the host device."
#~ msgstr ""

#~ msgid "The storage scope to allocate the storage to."
#~ msgstr ""

#~ msgid "**result** -- A relax Call, which gets the allocated tensor."
#~ msgstr ""

#~ msgid ""
#~ "An indicator that the consumers of "
#~ "input tensor should not be lifted "
#~ "to transform_params function"
#~ msgstr ""

#~ msgid "**result** -- The result tensor that is the same as input tensor"
#~ msgstr ""

#~ msgid "CCL related operators."
#~ msgstr ""

#~ msgid "AllGather operator"
#~ msgstr ""

#~ msgid "The number of workers to gather data from."
#~ msgstr ""

#~ msgid "Whether the gather operation performs globally or in group as default."
#~ msgstr ""

#~ msgid "**result** -- The result of allgather."
#~ msgstr ""

#~ msgid "Allreduce operator"
#~ msgstr ""

#~ msgid ""
#~ "The type of reduction operation to "
#~ "be applied to the input data. Now"
#~ " \"sum\", \"prod\", \"min\", \"max\" and"
#~ " \"avg\" are supported."
#~ msgstr ""

#~ msgid ""
#~ "Whether the reduction operation performs "
#~ "globally or in group as default."
#~ msgstr ""

#~ msgid "**result** -- The result of allreduce."
#~ msgstr ""

#~ msgid "Broadcast data from worker-0 to all other workers."
#~ msgstr ""

#~ msgid "The tensor to be broadcast."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The same tensor, which "
#~ "has been broadcast to all other "
#~ "workers."
#~ msgstr ""

#~ msgid ""
#~ "Perform a scatter operation from "
#~ "worker-0, chunking the given buffer into"
#~ " equal parts."
#~ msgstr ""

#~ msgid ""
#~ "The buffer to be divided into "
#~ "equal parts and sent to each "
#~ "worker accordingly."
#~ msgstr ""

#~ msgid ""
#~ "The number of workers, i.e. the "
#~ "number of parts the given buffer "
#~ "should be chunked into."
#~ msgstr ""

#~ msgid "The dimension of the tensor to be scattered. Default is 0."
#~ msgstr ""

#~ msgid "**result** -- Chunked Tensor received by different workers."
#~ msgstr ""

#~ msgid "Operators serving for distributed Relax."
#~ msgstr ""

#~ msgid "Annotate sharding plan for tensor"
#~ msgstr ""

#~ msgid "The device mesh of the sharding plan"
#~ msgstr ""

#~ msgid "The placement of the sharding plan"
#~ msgstr ""

#~ msgid "**result** -- The tensor unmodified."
#~ msgstr ""

#~ msgid ""
#~ "Call a tir.prim_func and return the "
#~ "output. The prim_func should be a "
#~ "worker-local function that is actually "
#~ "executed on each worker, instead of "
#~ "the unpartitioned function. The output "
#~ "of this operator is DTensor or a"
#~ " tuple of DTensors."
#~ msgstr ""

#~ msgid ""
#~ "The structure info of the call_tir "
#~ "output. It should be a single or"
#~ " a list of DTensorStructInfo. Each "
#~ "one denotes the structure info of "
#~ "a returned tensor."
#~ msgstr ""

#~ msgid "**ret** -- A call node for the call_tir_local_view operator."
#~ msgstr ""

#~ msgid "Redistribute tensor"
#~ msgstr ""

#~ msgid "The device mesh after redistribution"
#~ msgstr ""

#~ msgid "The placement after redistribution"
#~ msgstr ""

#~ msgid "**result** -- The tensor after redistribution."
#~ msgstr ""

#~ msgid "Slice tensor into several parts along one axis,"
#~ msgstr ""

#~ msgid ""
#~ "and each worker takes one part. "
#~ "input.struct_info.shape[axis] % num_workers == "
#~ "0 is required. Each worker must "
#~ "have an identical copy of the "
#~ "input. This is a specialized version "
#~ "of redistribute op."
#~ msgstr ""

#~ msgid "The buffer to be sliced into equal parts."
#~ msgstr ""

#~ msgid ""
#~ "The number of workers, i.e. the "
#~ "number of parts the given buffer "
#~ "should be sliced into."
#~ msgstr ""

#~ msgid "The axis of the tensor to be sliced."
#~ msgstr ""

#~ msgid "**result** -- Sliced Tensor kept by each device."
#~ msgstr ""

#~ msgid "Operators serving for finding gradient of relax operators."
#~ msgstr ""

#~ msgid ""
#~ "Backward operator of relax.nn.avg_pool2d. All"
#~ " parameters except output_grad is the "
#~ "same as relax.nn.avg_pool2d. Returns the "
#~ "gradient w.r.t. data."
#~ msgstr ""

#~ msgid "The gradient w.r.t. the result of avg_pool2d."
#~ msgstr ""

#~ msgid "**result** -- The gradient w.r.t. data."
#~ msgstr ""

#~ msgid ""
#~ "Mark the end of checkpoint stage. "
#~ "See tvm.relax.op.grad.start_checkpoint."
#~ msgstr ""

#~ msgid "The output of the checkpoint stage."
#~ msgstr ""

#~ msgid "**result** -- The same tensor as the input."
#~ msgstr ""

#~ msgid ""
#~ "Backward operator of relax.nn.max_pool2d. All"
#~ " parameters except output_grad is the "
#~ "same as relax.nn.max_pool2d. Returns the "
#~ "gradient w.r.t. data."
#~ msgstr ""

#~ msgid "The gradient w.r.t. the result of max_pool2d."
#~ msgstr ""

#~ msgid ""
#~ "Backward operator of relax.nn.nll_loss. All"
#~ " parameters except output_grad is the "
#~ "same as relax.nn.nll_loss. Returns the "
#~ "gradient w.r.t. predictions."
#~ msgstr ""

#~ msgid "The gradient w.r.t. the result of nll_loss."
#~ msgstr ""

#~ msgid "**result** -- The gradient w.r.t. predictions."
#~ msgstr ""

#~ msgid "No gradient dummy operator w.r.t. the input."
#~ msgstr ""

#~ msgid "The corresponding input tensor."
#~ msgstr ""

#~ msgid "**result** -- The no-gradient representation w.r.t. input."
#~ msgstr ""

#~ msgid ""
#~ "Mark the start of the checkpoint "
#~ "stage. The computation between "
#~ "start_checkpoint and end_checkpoint will be"
#~ " marked as the checkpoint stage."
#~ msgstr ""

#~ msgid ""
#~ "Rather than storing all intermediate "
#~ "activations of the entire computation "
#~ "graph for computing backward, the "
#~ "checkpointed stage does not save "
#~ "intermediate activations, and instead "
#~ "recomputes them in backward process."
#~ msgstr ""

#~ msgid ""
#~ "For instance, ``` a = relax.Var(\"a\","
#~ " relax.TensorStructInfo((2, 2), \"float32\")) b"
#~ " = relax.Var(\"b\", relax.TensorStructInfo((2, "
#~ "2), \"float32\")) c = a * 2 "
#~ "d = b * 2 c_cp = "
#~ "start_checkpoint(c) d_cp = start_checkpoint(d) "
#~ "e = c_cp + d_cp e_out = "
#~ "end_checkpoint(e) ``` Then `e` will be"
#~ " recomputed in the backward stage."
#~ msgstr ""

#~ msgid ""
#~ "See tvm.relax.transform.Gradient, "
#~ "tvm.relax.testing.nn.checkpoint, "
#~ "tvm.relax.op.grad.end_checkpoint for more "
#~ "information."
#~ msgstr ""

#~ msgid "The tensor marking the input of the checkpoint stage."
#~ msgstr ""

#~ msgid ""
#~ "Backward operator of relax.take. All "
#~ "parameters except output_grad is the "
#~ "same as relax.take. Returns the gradient"
#~ " w.r.t. x."
#~ msgstr ""

#~ msgid "The gradient w.r.t. the result of take."
#~ msgstr ""

#~ msgid "**result** -- The gradient w.r.t. x."
#~ msgstr ""

#~ msgid "Image operators."
#~ msgstr ""

#~ msgid "Image resize2d operator."
#~ msgstr ""

#~ msgid ""
#~ "This operator takes data as input "
#~ "and does 2D scaling to the given"
#~ " scale factor. In the default case,"
#~ " where the data_layout is `NCHW` with"
#~ " data of shape (n, c, h, w) "
#~ "out will have a shape (n, c, "
#~ "size[0], size[1])"
#~ msgstr ""

#~ msgid ""
#~ "method indicates the algorithm to be "
#~ "used while calculating the out value "
#~ "and method can be one of "
#~ "(\"linear\", \"nearest_neighbor\", \"cubic\")"
#~ msgstr ""

#~ msgid ""
#~ "The out size to which the image"
#~ " will be resized. If specified as "
#~ "a list, it is required to have "
#~ "length either 1 or 2. If specified"
#~ " as an Expr, it is required to"
#~ " have ndim 2."
#~ msgstr ""

#~ msgid ""
#~ "The region of interest for cropping "
#~ "the input image. Expected to be of"
#~ " size 4, and format [start_h, "
#~ "start_w, end_h, end_w]. Only used if "
#~ "coordinate_transformation_mode is tf_crop_and_resize."
#~ msgstr ""

#~ msgid "Scale method to used [nearest_neighbor, linear, cubic]."
#~ msgstr ""

#~ msgid ""
#~ "Describes how to transform the "
#~ "coordinate in the resized tensor to "
#~ "the coordinate in the original tensor."
#~ " Definitions can be found in "
#~ "topi/image/resize.py. [half_pixel, align_corners, "
#~ "asymmetric, pytorch_half_pixel, tf_half_pixel_for_nn, "
#~ "and tf_crop_and_resize]."
#~ msgstr ""

#~ msgid ""
#~ "indicates how to find the \"nearest\""
#~ " pixel in nearest_neighbor method [round,"
#~ " floor, ceil]"
#~ msgstr ""

#~ msgid "Spline Coefficient for bicubic interpolation"
#~ msgstr ""

#~ msgid "Flag to exclude exterior of the image during bicubic interpolation"
#~ msgstr ""

#~ msgid "Fill value to use when roi is outside of the image"
#~ msgstr ""

#~ msgid ""
#~ "The dtype of the output tensor. It"
#~ " it is not specified, the output "
#~ "will have the same dtype as input"
#~ " if not specified."
#~ msgstr ""

#~ msgid "**result** -- The resized result."
#~ msgstr ""

#~ msgid "Relax memory primitives."
#~ msgstr ""

#~ msgid ""
#~ "Construct a Call to allocate a "
#~ "storage with specific size, "
#~ "virtual_device_index, storage_scope and dtype."
#~ msgstr ""

#~ msgid "The size of the storage to be allocated."
#~ msgstr ""

#~ msgid ""
#~ "The virtual device index indicating on"
#~ " which device the storage is to "
#~ "be allocated. Index -1 is reserved "
#~ "for the host device."
#~ msgstr ""

#~ msgid "The datatype of the storage to be allocated."
#~ msgstr ""

#~ msgid "**result** -- A relax Call, which gets the allocated storage."
#~ msgstr ""

#~ msgid ""
#~ "Construct a Call to allocate a "
#~ "tensor on a certain storage starting "
#~ "from the given offset."
#~ msgstr ""

#~ msgid "The storage to allocate the tensor to."
#~ msgstr ""

#~ msgid "The storage offset to allocate the tensor."
#~ msgstr ""

#~ msgid ""
#~ "Ensure the tensor has elem_offset == "
#~ "0. A copy will be made if "
#~ "necessary."
#~ msgstr ""

#~ msgid "The input tensor"
#~ msgstr ""

#~ msgid "The tensor with elem_offset == 0"
#~ msgstr ""

#~ msgid "Construct a Call to kill a storage."
#~ msgstr ""

#~ msgid "The storage to be killed."
#~ msgstr ""

#~ msgid "**result** -- A relax Call to kill a storage."
#~ msgstr ""

#~ msgid "Construct a Call to kill a tensor."
#~ msgstr ""

#~ msgid "The tensor to be killed."
#~ msgstr ""

#~ msgid "**result** -- A relax Call to kill a tensor."
#~ msgstr ""

#~ msgid "Provide a view into an existing tensor"
#~ msgstr ""

#~ msgid ""
#~ "The view may have a different "
#~ "shape, may be a different datatype, "
#~ "and may start at an offset "
#~ "relative to the source array."
#~ msgstr ""

#~ msgid ""
#~ "Regardless of which combination of these"
#~ " options are used, the view may "
#~ "never access memory that was not "
#~ "accessible through the input `data` "
#~ "array.  This restriction applies even if"
#~ " the `data` array is itself a "
#~ "view into a shared backing array."
#~ msgstr ""

#~ msgid ""
#~ "The target shape.  Should be a "
#~ "`relax.ShapeExpr`, or a collection that "
#~ "can be converted to a `relax.ShapeExpr`."
#~ msgstr ""

#~ msgid ""
#~ "The target datatype.  Should be a "
#~ "`relax.ShapeExpr`, or a collection that "
#~ "can be converted to a `relax.ShapeExpr`."
#~ msgstr ""

#~ msgid ""
#~ "The offset of the output NDArray, "
#~ "relative to the byte offset of "
#~ "`data`.  If `None`, the offset of "
#~ "the view is the same as the "
#~ "offset of `data`."
#~ msgstr ""

#~ msgid "**result** -- The tensor view"
#~ msgstr ""

#~ msgid "The attributes node used for Relax operators"
#~ msgstr ""

#~ msgid "Attributes for 2d adaptive pool operator"
#~ msgstr ""

#~ msgid "Attributes for argmax/argmin operator"
#~ msgstr ""

#~ msgid "Attributes for argsort operator"
#~ msgstr ""

#~ msgid "Attributes used in astype operator"
#~ msgstr ""

#~ msgid "Attributes used in batch_norm operator"
#~ msgstr ""

#~ msgid "Attributes used in call_tir_with_grad operator"
#~ msgstr ""

#~ msgid "Attributes for concat operator"
#~ msgstr ""

#~ msgid "Attributes for nn.conv2d"
#~ msgstr ""

#~ msgid "Attributes for nn.conv2d_transpose"
#~ msgstr ""

#~ msgid "Attributes for nn.conv3d"
#~ msgstr ""

#~ msgid "Attributes for dropout operator"
#~ msgstr ""

#~ msgid "Attributes for einsum operator"
#~ msgstr ""

#~ msgid "Attributes for expand_dims operator"
#~ msgstr ""

#~ msgid "Attributes for flip operator"
#~ msgstr ""

#~ msgid ""
#~ "Attributes used in full/full_like, "
#~ "ones/ones_like, and zeros/zeros_like operator"
#~ msgstr ""

#~ msgid "Attributes used in layer_norm operator"
#~ msgstr ""

#~ msgid "Attributes used in layout_transform operator"
#~ msgstr ""

#~ msgid "Attributes for matmul operator"
#~ msgstr ""

#~ msgid "Attributes for permute_dims operator"
#~ msgstr ""

#~ msgid "Attributes for nn.max_pool2d"
#~ msgstr ""

#~ msgid "Attributes for repeat operator"
#~ msgstr ""

#~ msgid "Attributes used in image resize2d operator"
#~ msgstr ""

#~ msgid "Attributes for scan operators"
#~ msgstr ""

#~ msgid "Attributes for nn.softmax"
#~ msgstr ""

#~ msgid "Attributes for sort operator"
#~ msgstr ""

#~ msgid "Attributes used in split operator"
#~ msgstr ""

#~ msgid "Attributes for squeeze operator"
#~ msgstr ""

#~ msgid "Attributes used in statistical operator"
#~ msgstr ""

#~ msgid "Attributes used in strided_slice operator"
#~ msgstr ""

#~ msgid "Attributes used in take operator"
#~ msgstr ""

#~ msgid "Attributes for tile operator"
#~ msgstr ""

#~ msgid "Attributes for topk operators"
#~ msgstr ""

#~ msgid "Attributes used in tril and triu operator"
#~ msgstr ""

