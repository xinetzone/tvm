# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-08-07 14:09+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../doc/docs/reference/api/python/te.rst:19
msgid "tvm.te"
msgstr ""

#: of tvm.te:1
msgid "Namespace for Tensor Expression Language"
msgstr ""

#: of tvm.te:1
msgid "**Classes:**"
msgstr ""

#: of tvm.te:1:<autosummary>:1
msgid ":py:obj:`ComputeOp <tvm.te.ComputeOp>`\\"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1 tvm.te:1:<autosummary>:1
msgid "Scalar operation."
msgstr ""

#: of tvm.te:1:<autosummary>:1
msgid ":py:obj:`ExternOp <tvm.te.ExternOp>`\\"
msgstr ""

#: of tvm.te.tensor.ExternOp:1 tvm.te:1:<autosummary>:1
msgid "External operation."
msgstr ""

#: of tvm.te:1:<autosummary>:1
msgid ":py:obj:`PlaceholderOp <tvm.te.PlaceholderOp>`\\"
msgstr ""

#: of tvm.te.tensor.PlaceholderOp:1 tvm.te:1:<autosummary>:1
msgid "Placeholder operation."
msgstr ""

#: of tvm.te:1:<autosummary>:1
msgid ":py:obj:`ScanOp <tvm.te.ScanOp>`\\"
msgstr ""

#: of tvm.te.tensor.ScanOp:1 tvm.te:1:<autosummary>:1
msgid "Scan operation."
msgstr ""

#: of tvm.te:1:<autosummary>:1
msgid ":py:obj:`Tensor <tvm.te.Tensor>`\\"
msgstr ""

#: of tvm.te.tensor.Tensor:1 tvm.te:1:<autosummary>:1
msgid "Tensor object, to construct, see function.Tensor"
msgstr ""

#: of tvm.te:1:<autosummary>:1
msgid ":py:obj:`TensorSlice <tvm.te.TensorSlice>`\\ \\(tensor\\, indices\\)"
msgstr ""

#: of tvm.te.tensor.TensorSlice:1 tvm.te:1:<autosummary>:1
msgid "Auxiliary data structure for enable slicing syntax from tensor."
msgstr ""

#: of tvm.te:1
msgid "**Functions:**"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`abs <tvm.te.abs>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.abs:1
msgid "Get absolute value of the input element-wise."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`acos <tvm.te.acos>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.acos:1
#: tvm.tir.op.acosh:1
msgid "Take acos of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`acosh <tvm.te.acosh>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`add <tvm.te.add>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.generic.add:1
msgid "Generic add operator."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`all <tvm.te.all>`\\ \\(\\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.all:1
msgid "Create a new expression of the intersection of all conditions in the"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`any <tvm.te.any>`\\ \\(\\*args\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.any:1
msgid "Create a new experssion of the union of all conditions in the arguments"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`asin <tvm.te.asin>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.asin:1
msgid "Take asin of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`asinh <tvm.te.asinh>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.asinh:1
msgid "Take asinh of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`atan <tvm.te.atan>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.atan:1
msgid "Take atan of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`atanh <tvm.te.atanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.atanh:1
msgid "Take atanh of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`ceil <tvm.te.ceil>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.ceil:1
msgid "Take ceil of float input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`comm_reducer <tvm.te.comm_reducer>`\\ \\(fcombine\\, "
"fidentity\\[\\, name\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.comm_reducer:1
msgid "Create a commutative reducer for reduction."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`compute <tvm.te.compute>`\\ \\(shape\\, fcompute\\[\\, name\\, "
"tag\\, attrs\\, ...\\]\\)"
msgstr ""

#: of tvm.te.operation.compute:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Construct a new tensor by computing over the shape domain."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`const <tvm.te.const>`\\ \\(value\\[\\, dtype\\, span\\]\\)"
msgstr ""

#: of tvm.te.operation.const:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Create a new constant with specified value and dtype"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`cos <tvm.te.cos>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.cos:1
msgid "Take cos of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`cosh <tvm.te.cosh>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.cosh:1
msgid "Take cosh of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`create_prim_func <tvm.te.create_prim_func>`\\ \\(ops\\[\\, "
"index\\_dtype\\_override\\]\\)"
msgstr ""

#: of tvm.te.operation.create_prim_func:1
#: tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Create a TensorIR PrimFunc from tensor expression"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`div <tvm.te.div>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.div:1
msgid "Compute a / b as in C/C++ semantics."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`erf <tvm.te.erf>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.erf:1
msgid "Take gauss error function of the input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`exp <tvm.te.exp>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.exp:1
msgid "Take exponential of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`extern <tvm.te.extern>`\\ \\(shape\\, inputs\\, fcompute\\[\\, "
"name\\, ...\\]\\)"
msgstr ""

#: of tvm.te.operation.extern:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Compute several tensors via an extern function."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`extern_primfunc <tvm.te.extern_primfunc>`\\ "
"\\(input\\_tensors\\, primfunc\\, ...\\)"
msgstr ""

#: of tvm.te.operation.extern_primfunc:1
#: tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Compute tensors via a schedulable TIR PrimFunc"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`floor <tvm.te.floor>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.floor:1
msgid "Take floor of float input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`floordiv <tvm.te.floordiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.floordiv:1
msgid "Compute the floordiv of two expressions."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`floormod <tvm.te.floormod>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.floormod:1
msgid "Compute the floormod of two expressions."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`fmod <tvm.te.fmod>`\\ \\(x\\, y\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.fmod:1
msgid "Return the remainder of x divided by y with the same sign as x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`if_then_else <tvm.te.if_then_else>`\\ \\(cond\\, t\\, f\\[\\, "
"span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.if_then_else:1
msgid "Conditional selection expression."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`indexdiv <tvm.te.indexdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.indexdiv:1
msgid "Compute floor(a / b) where a and b are non-negative."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`indexmod <tvm.te.indexmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Compute the remainder of indexdiv."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`isfinite <tvm.te.isfinite>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.isfinite:1
msgid "Check if input value is finite."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`isinf <tvm.te.isinf>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.isinf:1
msgid "Check if input value is infinite."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`isnan <tvm.te.isnan>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.isnan:1
msgid "Check if input value is Nan."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`log <tvm.te.log>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.log:1
msgid "Take log of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`log10 <tvm.te.log10>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.log10:1
msgid "Take log10 of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`log2 <tvm.te.log2>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.log2:1
msgid "Take log2 of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`logaddexp <tvm.te.logaddexp>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.logaddexp:1
msgid "Compute the logaddexp of two expressions."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`max <tvm.te.max>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
#: tvm.tir.op.comm_reducer.<locals>.reducer:1
msgid "Create a max expression over axis."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`max_value <tvm.te.max_value>`\\ \\(dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.max_value:1
msgid "maximum value of dtype"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`min <tvm.te.min>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
#: tvm.tir.op.comm_reducer.<locals>.reducer:1
msgid "Create a min expression over axis."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`min_value <tvm.te.min_value>`\\ \\(dtype\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.min_value:1
msgid "minimum value of dtype"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`multiply <tvm.te.multiply>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.generic.multiply:1
msgid "Generic multiply operator."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`nearbyint <tvm.te.nearbyint>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.round:1
msgid "Round elements of the array to the nearest integer."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`placeholder <tvm.te.placeholder>`\\ \\(shape\\[\\, dtype\\, "
"name\\]\\)"
msgstr ""

#: of tvm.te.operation.placeholder:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Construct an empty tensor object."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`popcount <tvm.te.popcount>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.popcount:1
msgid "Count the number of set bits in input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`power <tvm.te.power>`\\ \\(x\\, y\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.power:1
msgid "x power y"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`reduce_axis <tvm.te.reduce_axis>`\\ \\(dom\\[\\, name\\, "
"thread\\_tag\\, span\\]\\)"
msgstr ""

#: of tvm.te.operation.reduce_axis:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Create a new IterVar for reduction."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`round <tvm.te.round>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`rsqrt <tvm.te.rsqrt>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.rsqrt:1
msgid "Take reciprocal of square root of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`scan <tvm.te.scan>`\\ \\(init\\, update\\, "
"state\\_placeholder\\[\\, ...\\]\\)"
msgstr ""

#: of tvm.te.operation.scan:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Construct new tensors by scanning over axis."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`sigmoid <tvm.te.sigmoid>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.sigmoid:1
msgid "Quick function to get sigmoid"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`sin <tvm.te.sin>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.sin:1
msgid "Take sin of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`sinh <tvm.te.sinh>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.sinh:1
msgid "Take sinh of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`size_var <tvm.te.size_var>`\\ \\(\\[name\\, dtype\\, span\\]\\)"
msgstr ""

#: of tvm.te.operation.size_var:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
"Create a new variable represents a tensor shape size, which is non-"
"negative."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`sqrt <tvm.te.sqrt>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.sqrt:1
msgid "Take square root of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`subtract <tvm.te.subtract>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.generic.subtract:1
msgid "Generic subtract operator."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`sum <tvm.te.sum>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
#: tvm.tir.op.comm_reducer.<locals>.reducer:1
msgid "Create a sum expression over axis."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`tag_scope <tvm.te.tag_scope>`\\ \\(tag\\)"
msgstr ""

#: of tvm.te.tag.tag_scope:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "The operator tag scope."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`tan <tvm.te.tan>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.tan:1
msgid "Take tan of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`tanh <tvm.te.tanh>`\\ \\(x\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.tanh:1
msgid "Take hyperbolic tanh of input x."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ""
":py:obj:`thread_axis <tvm.te.thread_axis>`\\ \\(\\[dom\\, tag\\, name\\, "
"span\\]\\)"
msgstr ""

#: of tvm.te.operation.thread_axis:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Create a new IterVar to represent thread index."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`trace <tvm.te.trace>`\\ \\(args\\[\\, trace\\_action\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.trace:1
msgid "Trace tensor data at the runtime."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`trunc <tvm.te.trunc>`\\ \\(x\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.trunc:1
msgid "Get truncated value of the input."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`truncdiv <tvm.te.truncdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.truncdiv:1
msgid "Compute the truncdiv of two expressions."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`truncmod <tvm.te.truncmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1 tvm.tir.op.truncmod:1
msgid "Compute the truncmod of two expressions."
msgstr ""

#: of tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid ":py:obj:`var <tvm.te.var>`\\ \\(\\[name\\, dtype\\, span\\]\\)"
msgstr ""

#: of tvm.te.operation.var:1 tvm.te.tensor.ComputeOp:1:<autosummary>:1
msgid "Create a new variable with specified name and dtype"
msgstr ""

#: of tvm.te.tensor.ScanOp:1 tvm.te.tensor.Tensor:1 tvm.te.tensor.TensorSlice:1
msgid "**Attributes:**"
msgstr ""

#: of tvm.te.ScanOp.scan_axis:1:<autosummary>:1
msgid ":py:obj:`scan_axis <tvm.te.ScanOp.scan_axis>`\\"
msgstr ""

#: of tvm.te.ScanOp.scan_axis:1 tvm.te.ScanOp.scan_axis:1:<autosummary>:1
msgid "Represent the scan axis, only defined when it is a ScanOp"
msgstr ""

#: of tvm.te.Tensor.ndim:1:<autosummary>:1
msgid ":py:obj:`ndim <tvm.te.Tensor.ndim>`\\"
msgstr ""

#: of tvm.te.Tensor.ndim:1 tvm.te.Tensor.ndim:1:<autosummary>:1
msgid "Dimension of the tensor."
msgstr ""

#: of tvm.te.tensor.TensorSlice:1
msgid "**Methods:**"
msgstr ""

#: of tvm.te.tensor.TensorSlice:1:<autosummary>:1
msgid ":py:obj:`asobject <tvm.te.TensorSlice.asobject>`\\ \\(\\)"
msgstr ""

#: of tvm.te.tensor.TensorSlice.asobject:1
#: tvm.te.tensor.TensorSlice:1:<autosummary>:1
msgid "Convert slice to object."
msgstr ""

#: of tvm.te.tensor.TensorSlice.asobject:1:<autosummary>:1
msgid ":py:obj:`dtype <tvm.te.TensorSlice.dtype>`\\"
msgstr ""

#: of tvm.te.TensorSlice.dtype:1
#: tvm.te.tensor.TensorSlice.asobject:1:<autosummary>:1
msgid "Data content of the tensor."
msgstr ""

#: ../../doc/docs/reference/api/python/te.rst
msgid "参数"
msgstr ""

#: of tvm.tir.op.abs:3 tvm.tir.op.acos:3 tvm.tir.op.acosh:3 tvm.tir.op.asin:3
#: tvm.tir.op.asinh:3 tvm.tir.op.atan:3 tvm.tir.op.atanh:3 tvm.tir.op.ceil:3
#: tvm.tir.op.cos:3 tvm.tir.op.cosh:3 tvm.tir.op.erf:3 tvm.tir.op.exp:3
#: tvm.tir.op.floor:3 tvm.tir.op.fmod:3 tvm.tir.op.fmod:5 tvm.tir.op.isfinite:3
#: tvm.tir.op.isinf:3 tvm.tir.op.isnan:3 tvm.tir.op.log:3 tvm.tir.op.log10:3
#: tvm.tir.op.log2:3 tvm.tir.op.nearbyint:10 tvm.tir.op.popcount:3
#: tvm.tir.op.power:3 tvm.tir.op.round:3 tvm.tir.op.rsqrt:3
#: tvm.tir.op.sigmoid:3 tvm.tir.op.sin:3 tvm.tir.op.sinh:3 tvm.tir.op.sqrt:3
#: tvm.tir.op.tan:3 tvm.tir.op.tanh:3 tvm.tir.op.trunc:6
msgid "Input argument."
msgstr ""

#: of tvm.tir.op.abs:5 tvm.tir.op.all:6 tvm.tir.op.any:5 tvm.tir.op.ceil:5
#: tvm.tir.op.floor:5 tvm.tir.op.isfinite:5 tvm.tir.op.isinf:5
#: tvm.tir.op.isnan:5 tvm.tir.op.max_value:5 tvm.tir.op.min_value:5
#: tvm.tir.op.nearbyint:12 tvm.tir.op.power:7 tvm.tir.op.round:5
#: tvm.tir.op.trunc:8
msgid "The location of this operator in the source code."
msgstr ""

#: ../../doc/docs/reference/api/python/te.rst
msgid "返回"
msgstr ""

#: of tvm.tir.op.abs:8 tvm.tir.op.acos:6 tvm.tir.op.acosh:6 tvm.tir.op.asin:6
#: tvm.tir.op.asinh:6 tvm.tir.op.atan:6 tvm.tir.op.atanh:6 tvm.tir.op.ceil:8
#: tvm.tir.op.cos:6 tvm.tir.op.cosh:6 tvm.tir.op.erf:6 tvm.tir.op.exp:6
#: tvm.tir.op.floor:8 tvm.tir.op.isfinite:8 tvm.tir.op.isinf:8
#: tvm.tir.op.isnan:8 tvm.tir.op.log:6 tvm.tir.op.log10:6 tvm.tir.op.log2:6
#: tvm.tir.op.nearbyint:15 tvm.tir.op.popcount:6 tvm.tir.op.round:8
#: tvm.tir.op.rsqrt:6 tvm.tir.op.sigmoid:6 tvm.tir.op.sin:6 tvm.tir.op.sinh:6
#: tvm.tir.op.sqrt:6 tvm.tir.op.tan:6 tvm.tir.op.tanh:6 tvm.tir.op.trunc:11
msgid "**y** -- The result."
msgstr ""

#: ../../doc/docs/reference/api/python/te.rst
msgid "返回类型"
msgstr ""

#: of tvm.tir.generic.add:3 tvm.tir.generic.multiply:3
#: tvm.tir.generic.subtract:3
msgid "The left operand."
msgstr ""

#: of tvm.tir.generic.add:5 tvm.tir.generic.multiply:5
#: tvm.tir.generic.subtract:5
msgid "The right operand."
msgstr ""

#: of tvm.tir.generic.add:7 tvm.tir.generic.multiply:7
#: tvm.tir.generic.subtract:7 tvm.tir.op.div:7 tvm.tir.op.floordiv:7
#: tvm.tir.op.floormod:7 tvm.tir.op.if_then_else:9 tvm.tir.op.indexdiv:7
#: tvm.tir.op.indexmod:7 tvm.tir.op.logaddexp:7 tvm.tir.op.truncdiv:7
#: tvm.tir.op.truncmod:7
msgid "The location of this operator in the source."
msgstr ""

#: of tvm.tir.generic.add:10
msgid "**op** -- The result Expr of add operaton."
msgstr ""

#: of tvm.tir.op.all:2
msgid "arguments"
msgstr ""

#: of tvm.tir.op.all:4 tvm.tir.op.any:3
msgid "List of symbolic boolean expressions"
msgstr ""

#: of tvm.tir.op.all:9 tvm.tir.op.any:8
msgid "**expr** -- Expression"
msgstr ""

#: of tvm.tir.op.comm_reducer:3
msgid "A binary function which takes two Expr as input to return a Expr."
msgstr ""

#: of tvm.tir.op.comm_reducer:5
msgid "A function which takes a type string as input to return a const Expr."
msgstr ""

#: of tvm.tir.op.comm_reducer:8
msgid ""
"**reducer** -- A function which creates a reduce expression over axis. "
"There are two ways to use it:  1. accept (expr, axis, where) to produce "
"an Reduce Expr on    specified axis; 2. simply use it with multiple "
"Exprs."
msgstr ""

#: of tvm.tir.op.comm_reducer:8
msgid ""
"**reducer** -- A function which creates a reduce expression over axis. "
"There are two ways to use it:"
msgstr ""

#: of tvm.tir.op.comm_reducer:11
msgid "accept (expr, axis, where) to produce an Reduce Expr on specified axis;"
msgstr ""

#: of tvm.tir.op.comm_reducer:13
msgid "simply use it with multiple Exprs."
msgstr ""

#: of tvm.te.operation.create_prim_func:7 tvm.te.operation.extern:40
#: tvm.te.operation.extern_primfunc:12 tvm.te.operation.scan:23
#: tvm.te.tag.tag_scope:11 tvm.tir.op.comm_reducer:17
#: tvm.tir.op.comm_reducer.<locals>.reducer:14
msgid "示例"
msgstr ""

#: of tvm.te.operation.compute:3
msgid "The compute rule is result[axis] = fcompute(axis)"
msgstr ""

#: of tvm.te.operation.compute:5 tvm.te.operation.placeholder:3
msgid "The shape of the tensor"
msgstr ""

#: of tvm.te.operation.compute:7
msgid "Specifies the input source expression"
msgstr ""

#: of tvm.te.operation.compute:9 tvm.te.operation.extern:20
#: tvm.te.operation.placeholder:7 tvm.te.operation.scan:12
msgid "The name hint of the tensor"
msgstr ""

#: of tvm.te.operation.compute:11
msgid "Additional tag information about the compute."
msgstr ""

#: of tvm.te.operation.compute:13 tvm.te.operation.extern:34
#: tvm.te.operation.scan:16
msgid "The additional auxiliary attributes about the compute."
msgstr ""

#: of tvm.te.operation.compute:15
msgid ""
"The names to use for each of the varargs. If not supplied, the varargs "
"will be called i1, i2, ..."
msgstr ""

#: of tvm.te.operation.compute:19 tvm.te.operation.placeholder:10
msgid "**tensor** -- The created tensor"
msgstr ""

#: of tvm.te.operation.const:3
msgid "The constant value."
msgstr ""

#: of tvm.te.operation.const:5 tvm.te.operation.size_var:5
#: tvm.te.operation.var:5
msgid "The data type"
msgstr ""

#: of tvm.te.operation.const:7 tvm.te.operation.reduce_axis:9
#: tvm.te.operation.size_var:7 tvm.te.operation.thread_axis:10
#: tvm.te.operation.var:7
msgid "The location of this variable in the source."
msgstr ""

#: of tvm.te.operation.const:10
msgid "**const** -- The result constant expr."
msgstr ""

#: of tvm.te.operation.create_prim_func:3
#: tvm.tir.op.comm_reducer.<locals>.reducer:3
msgid "The source expression."
msgstr ""

#: of tvm.te.operation.create_prim_func:8
msgid "We define a matmul kernel using following code:"
msgstr ""

#: of tvm.te.operation.create_prim_func:24
msgid ""
"If we want to use TensorIR schedule to do transformations on such kernel,"
" we need to use `create_prim_func([A, B, C])` to create a schedulable "
"PrimFunc. The generated function looks like:"
msgstr ""

#: of tvm.te.operation.create_prim_func:43
msgid "**func** -- The created function."
msgstr ""

#: of tvm.tir.op.div:3 tvm.tir.op.indexdiv:3 tvm.tir.op.indexmod:3
msgid "The left hand operand, known to be non-negative."
msgstr ""

#: of tvm.tir.op.div:5 tvm.tir.op.indexdiv:5 tvm.tir.op.indexmod:5
msgid "The right hand operand, known to be non-negative."
msgstr ""

#: of tvm.tir.op.div:10 tvm.tir.op.floordiv:10 tvm.tir.op.floormod:10
#: tvm.tir.op.indexdiv:10 tvm.tir.op.indexmod:10 tvm.tir.op.logaddexp:10
#: tvm.tir.op.truncdiv:10 tvm.tir.op.truncmod:10
msgid "**res** -- The result expression."
msgstr ""

#: of tvm.tir.op.div:13
msgid "When operands are integers, returns truncdiv(a, b, span)."
msgstr ""

#: of tvm.te.operation.extern:3
msgid "The shape of the outputs."
msgstr ""

#: of tvm.te.operation.extern:5
msgid "The inputs"
msgstr ""

#: of tvm.te.operation.extern:7
msgid ""
"Specifies the IR statement to do the computation. See the following note "
"for function signature of fcompute  .. note::      **Parameters**       -"
" **ins** (list of :any:`tvm.tir.Buffer`) - Placeholder for each inputs"
"      - **outs** (list of :any:`tvm.tir.Buffer`) - Placeholder for each "
"outputs       **Returns**       - **stmt** (:any:`tvm.tir.Stmt`) - The "
"statement that carries out array computation."
msgstr ""

#: of tvm.te.operation.extern:7
msgid ""
"Specifies the IR statement to do the computation. See the following note "
"for function signature of fcompute"
msgstr ""

#: of tvm.te.operation.extern:11
msgid "**Parameters**"
msgstr ""

#: of tvm.te.operation.extern:13
msgid "**ins** (list of :any:`tvm.tir.Buffer`) - Placeholder for each inputs"
msgstr ""

#: of tvm.te.operation.extern:14
msgid "**outs** (list of :any:`tvm.tir.Buffer`) - Placeholder for each outputs"
msgstr ""

#: of tvm.te.operation.extern:16
msgid "**Returns**"
msgstr ""

#: of tvm.te.operation.extern:18
msgid ""
"**stmt** (:any:`tvm.tir.Stmt`) - The statement that carries out array "
"computation."
msgstr ""

#: of tvm.te.operation.extern:22
msgid "The data types of outputs, by default dtype will be same as inputs."
msgstr ""

#: of tvm.te.operation.extern:25
msgid "Input buffers."
msgstr ""

#: of tvm.te.operation.extern:27
msgid "Output buffers."
msgstr ""

#: of tvm.te.operation.extern:30
msgid "tag: str, optional"
msgstr ""

#: of tvm.te.operation.extern:31 tvm.te.operation.scan:14
msgid "Additonal tag information about the compute."
msgstr ""

#: of tvm.te.operation.extern:33
msgid "attrs: dict, optional"
msgstr ""

#: of tvm.te.operation.extern:36 tvm.te.operation.scan:19
msgid ""
"**tensor** -- The created tensor or tuple of tensors contains multiple "
"outputs."
msgstr ""

#: of tvm.te.operation.extern:41
msgid ""
"In the code below, C is generated by calling external PackedFunc "
"`tvm.contrib.cblas.matmul`"
msgstr ""

#: of tvm.te.operation.extern_primfunc:3
msgid "Input tensors that map to the corresponding primfunc input params."
msgstr ""

#: of tvm.te.operation.extern_primfunc:5
msgid "The TIR PrimFunc"
msgstr ""

#: of tvm.te.operation.extern_primfunc:8
msgid ""
"**tensor** -- The created tensor or tuple of tensors if it contains "
"multiple outputs."
msgstr ""

#: of tvm.te.operation.extern_primfunc:13
msgid ""
"In the code below, a TVMScript defined TIR PrimFunc is inlined into a TE "
"ExternOp. Applying te.create_prim_func on this"
msgstr ""

#: of tvm.tir.op.floordiv:3 tvm.tir.op.floormod:3 tvm.tir.op.logaddexp:3
#: tvm.tir.op.truncdiv:3 tvm.tir.op.truncmod:3
msgid "The left hand operand"
msgstr ""

#: of tvm.tir.op.floordiv:5 tvm.tir.op.floormod:5 tvm.tir.op.logaddexp:5
#: tvm.tir.op.truncdiv:5 tvm.tir.op.truncmod:5
msgid "The right hand operand"
msgstr ""

#: of tvm.tir.op.fmod:8 tvm.tir.op.power:10
msgid "**z** -- The result."
msgstr ""

#: of tvm.tir.op.if_then_else:3
msgid "The condition"
msgstr ""

#: of tvm.tir.op.if_then_else:5
msgid "The result expression if cond is true."
msgstr ""

#: of tvm.tir.op.if_then_else:7
msgid "The result expression if cond is false."
msgstr ""

#: of tvm.tir.op.if_then_else:12
msgid "**result** -- The result of conditional expression."
msgstr ""

#: of tvm.tir.op.if_then_else:17
msgid ""
"Unlike Select, if_then_else will not execute the branch that does not "
"satisfy the condition. You can use it to guard against out of bound "
"access. Unlike Select, if_then_else cannot be vectorized if some lanes in"
" the vector have different conditions."
msgstr ""

#: of tvm.tir.op.indexdiv:15 tvm.tir.op.indexmod:15
msgid ""
"Use this function to split non-negative indices. This function may take "
"advantage of operands' non-negativeness."
msgstr ""

#: of tvm.tir.op.indexmod:1
msgid "Compute the remainder of indexdiv. a and b are non-negative."
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:5
msgid "The reduction IterVar axis"
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:7
msgid "Filtering predicate of the reduction."
msgstr ""

#: of tvm.tir.op.comm_reducer.<locals>.reducer:10
msgid "**value** -- The result value."
msgstr ""

#: of tvm.tir.op.max_value:3 tvm.tir.op.min_value:3
msgid "The data type."
msgstr ""

#: of tvm.tir.op.max_value:8
msgid "**value** -- The maximum value of dtype."
msgstr ""

#: of tvm.tir.op.min_value:8
msgid "**value** -- The minimum value of dtype."
msgstr ""

#: of tvm.tir.generic.multiply:10
msgid "**op** -- The result Expr of multiply operaton."
msgstr ""

#: of tvm.tir.op.nearbyint:1
msgid ""
"Round elements of the array to the nearest integer. This intrinsic uses "
"llvm.nearbyint instead of llvm.round which is faster but will results "
"different from te.round. Notably nearbyint rounds according to the "
"rounding mode, whereas te.round (llvm.round) ignores that. For "
"differences between the two see: "
"https://en.cppreference.com/w/cpp/numeric/math/round "
"https://en.cppreference.com/w/cpp/numeric/math/nearbyint"
msgstr ""

#: of tvm.te.operation.placeholder:5
msgid "The data type of the tensor"
msgstr ""

#: of tvm.tir.op.power:5
msgid "The exponent"
msgstr ""

#: of tvm.te.operation.reduce_axis:3
msgid "The domain of iteration."
msgstr ""

#: of tvm.te.operation.reduce_axis:5
msgid "The name of the variable."
msgstr ""

#: of tvm.te.operation.reduce_axis:7
msgid "The name of the thread_tag."
msgstr ""

#: of tvm.te.operation.reduce_axis:12
msgid "**axis** -- An iteration variable representing the value."
msgstr ""

#: of tvm.te.operation.scan:3
msgid "The initial condition of first init.shape[0] timestamps"
msgstr ""

#: of tvm.te.operation.scan:5
msgid "The update rule of the scan given by symbolic tensor."
msgstr ""

#: of tvm.te.operation.scan:7
msgid "The placeholder variables used by update."
msgstr ""

#: of tvm.te.operation.scan:9
msgid ""
"The list of inputs to the scan. This is not required, but can be useful "
"for the compiler to detect scan body faster."
msgstr ""

#: of tvm.te.operation.size_var:3 tvm.te.operation.var:3
msgid "The name"
msgstr ""

#: of tvm.te.operation.size_var:10
msgid "**var** -- The result symbolic shape variable."
msgstr ""

#: of tvm.tir.generic.subtract:10
msgid "**op** -- The result Expr of subtract operaton."
msgstr ""

#: of tvm.te.tag.tag_scope:3
msgid "The tag name."
msgstr ""

#: of tvm.te.tag.tag_scope:6
msgid ""
"**tag_scope** -- The tag scope object, which can be used as decorator or "
"context manger."
msgstr ""

#: of tvm.te.operation.thread_axis:3
msgid ""
"The domain of iteration When str is passed, dom is set to None and str is"
" used as tag"
msgstr ""

#: of tvm.te.operation.thread_axis:6
msgid "The thread tag"
msgstr ""

#: of tvm.te.operation.thread_axis:8
msgid "The name of the var."
msgstr ""

#: of tvm.te.operation.thread_axis:13
msgid "**axis** -- The thread itervar."
msgstr ""

#: of tvm.tir.op.trace:3
msgid ""
"The trace function allows to trace specific tensor at the runtime. The "
"tracing value should come as last argument. The trace action should be "
"specified, by default tvm.default_trace_action is used."
msgstr ""

#: of tvm.tir.op.trace:8
msgid "Positional arguments."
msgstr ""

#: of tvm.tir.op.trace:10
msgid "The name of the trace action."
msgstr ""

#: of tvm.tir.op.trace:13
msgid "**call** -- The call expression."
msgstr ""

#: of tvm.tir.op.trace:18
msgid ":py:obj:`tvm.tir.call_packed`"
msgstr ""

#: of tvm.tir.op.trace:19
msgid "Creates packed function."
msgstr ""

#: of tvm.tir.op.trunc:3
msgid ""
"The truncated value of the scalar x is the nearest integer i which is "
"closer to zero than x is."
msgstr ""

#: of tvm.tir.op.truncdiv:13 tvm.tir.op.truncmod:13
msgid "This is the default integer division behavior in C."
msgstr ""

#: of tvm.te.operation.var:10
msgid "**var** -- The result symbolic variable."
msgstr ""

#~ msgid ":py:obj:`ComputeOp <tvm.te.ComputeOp>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`ExternOp <tvm.te.ExternOp>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`HybridOp <tvm.te.HybridOp>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Hybrid operation."
#~ msgstr ""

#~ msgid ":py:obj:`PlaceholderOp <tvm.te.PlaceholderOp>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`ScanOp <tvm.te.ScanOp>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`Schedule <tvm.te.Schedule>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Schedule for all the stages."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SpecializedCondition <tvm.te.SpecializedCondition>`\\"
#~ " \\(conditions\\)"
#~ msgstr ""

#~ msgid "Specialized condition to enable op specialization."
#~ msgstr ""

#~ msgid ":py:obj:`Stage <tvm.te.Stage>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "A Stage represents schedule for one operation."
#~ msgstr ""

#~ msgid ":py:obj:`Tensor <tvm.te.Tensor>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ":py:obj:`TensorComputeOp <tvm.te.TensorComputeOp>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Tensor operation."
#~ msgstr ""

#~ msgid ":py:obj:`create_schedule <tvm.te.create_schedule>`\\ \\(ops\\)"
#~ msgstr ""

#~ msgid "Create a schedule for list of ops"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`decl_tensor_intrin <tvm.te.decl_tensor_intrin>`\\ "
#~ "\\(op\\, fcompute\\[\\, name\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Declare a tensor intrinsic function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`gradient <tvm.te.gradient>`\\ \\(output\\, "
#~ "inputs\\[\\, head\\]\\)"
#~ msgstr ""

#~ msgid "Perform reverse-mode automatic differentiation."
#~ msgstr ""

#~ msgid ":py:obj:`axis <tvm.te.HybridOp.axis>`\\"
#~ msgstr ""

#~ msgid "Represent the IterVar axis, also defined when it is a HybridOp"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cache_read <tvm.te.Schedule.cache_read>`\\ "
#~ "\\(tensor\\, scope\\, readers\\)"
#~ msgstr ""

#~ msgid "Create a cache read of original tensor for readers."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`cache_write <tvm.te.Schedule.cache_write>`\\ "
#~ "\\(tensor\\, scope\\)"
#~ msgstr ""

#~ msgid "Create a cache write of original tensor, before storing into tensor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`create_group <tvm.te.Schedule.create_group>`\\ "
#~ "\\(outputs\\, inputs\\[\\, include\\_inputs\\]\\)"
#~ msgstr ""

#~ msgid "Create stage group by giving output and input boundary."
#~ msgstr ""

#~ msgid ":py:obj:`normalize <tvm.te.Schedule.normalize>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Build a normalized schedule from the current schedule."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`rfactor <tvm.te.Schedule.rfactor>`\\ \\(tensor\\,"
#~ " axis\\[\\, factor\\_axis\\]\\)"
#~ msgstr ""

#~ msgid "Factor a reduction axis in tensor's schedule to be an explicit axis."
#~ msgstr ""

#~ msgid ""
#~ "This will mutate the body of the"
#~ " readers. A new cache stage will "
#~ "be created for the tensor. Call "
#~ "this before doing any split/fuse "
#~ "schedule."
#~ msgstr ""

#~ msgid "The tensor to be cached."
#~ msgstr ""

#~ msgid "The scope of cached"
#~ msgstr ""

#~ msgid "The readers to read the cache."
#~ msgstr ""

#~ msgid "**cache** -- The created cache tensor."
#~ msgstr ""

#~ msgid ""
#~ "This will mutate the body of the"
#~ " tensor. A new cache stage will "
#~ "created before feed into the tensor."
#~ msgstr ""

#~ msgid ""
#~ "This function can be used to "
#~ "support data layout transformation. If "
#~ "there is a split/fuse/reorder on the "
#~ "data parallel axis of tensor before "
#~ "cache_write is called. The intermediate "
#~ "cache stores the data in the "
#~ "layout as the iteration order of "
#~ "leave axis. The data will be "
#~ "transformed back to the original layout"
#~ " in the original tensor. User can "
#~ "further call compute_inline to inline "
#~ "the original layout and keep the "
#~ "data stored in the transformed layout."
#~ msgstr ""

#~ msgid ""
#~ "The tensors to be feed to. All "
#~ "the tensors must be produced by "
#~ "one computeOp"
#~ msgstr ""

#~ msgid ""
#~ "The operators between outputs and inputs"
#~ " are placed as member of group. "
#~ "outputs are include in the group, "
#~ "while inputs are not included."
#~ msgstr ""

#~ msgid "The outputs of the group."
#~ msgstr ""

#~ msgid "The inputs of the group."
#~ msgstr ""

#~ msgid ""
#~ "Whether include input operations in the"
#~ " group if they are used by "
#~ "outputs."
#~ msgstr ""

#~ msgid ""
#~ "**group** -- A virtual stage represents"
#~ " the group, user can use compute_at"
#~ " to move the attachment point of "
#~ "the group."
#~ msgstr ""

#~ msgid ""
#~ "Insert necessary rebase to make certain"
#~ " iter var to start from 0. This"
#~ " is needed before bound inference and"
#~ " followup step."
#~ msgstr ""

#~ msgid "**sch** -- The normalized schedule."
#~ msgstr ""

#~ msgid ""
#~ "This will create a new stage that"
#~ " generated the new tensor with axis"
#~ " as the first dimension. The tensor's"
#~ " body will be rewritten as a "
#~ "reduction over the factored tensor."
#~ msgstr ""

#~ msgid "The tensor to be factored."
#~ msgstr ""

#~ msgid "The reduction axis in the schedule to be factored."
#~ msgstr ""

#~ msgid "The position where the new axis is placed."
#~ msgstr ""

#~ msgid "**tfactor** -- The created factored tensor."
#~ msgstr ""

#~ msgid ":py:obj:`current <tvm.te.SpecializedCondition.current>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Returns the current specialized condition"
#~ msgstr ""

#~ msgid ":py:obj:`bind <tvm.te.Stage.bind>`\\ \\(ivar\\, thread\\_ivar\\)"
#~ msgstr ""

#~ msgid "Bind ivar to thread index thread_ivar"
#~ msgstr ""

#~ msgid ":py:obj:`compute_at <tvm.te.Stage.compute_at>`\\ \\(parent\\, scope\\)"
#~ msgstr ""

#~ msgid "Attach the stage at parent's scope"
#~ msgstr ""

#~ msgid ":py:obj:`compute_inline <tvm.te.Stage.compute_inline>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Mark stage as inline"
#~ msgstr ""

#~ msgid ":py:obj:`compute_root <tvm.te.Stage.compute_root>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Attach the stage at parent, and mark it as root"
#~ msgstr ""

#~ msgid ":py:obj:`double_buffer <tvm.te.Stage.double_buffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Compute the current stage via double buffering."
#~ msgstr ""

#~ msgid ":py:obj:`env_threads <tvm.te.Stage.env_threads>`\\ \\(threads\\)"
#~ msgstr ""

#~ msgid "Mark threads to be launched at the outer scope of composed op."
#~ msgstr ""

#~ msgid ":py:obj:`fuse <tvm.te.Stage.fuse>`\\ \\(\\*args\\)"
#~ msgstr ""

#~ msgid ""
#~ "Fuse multiple consecutive iteration variables"
#~ " into a single iteration variable."
#~ msgstr ""

#~ msgid ":py:obj:`parallel <tvm.te.Stage.parallel>`\\ \\(var\\)"
#~ msgstr ""

#~ msgid "Parallelize the iteration."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`pragma <tvm.te.Stage.pragma>`\\ \\(var\\, "
#~ "pragma\\_type\\[\\, pragma\\_value\\]\\)"
#~ msgstr ""

#~ msgid "Annotate the iteration with pragma"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`prefetch <tvm.te.Stage.prefetch>`\\ \\(tensor\\,"
#~ " var\\, offset\\)"
#~ msgstr ""

#~ msgid "Prefetch the specified variable"
#~ msgstr ""

#~ msgid ":py:obj:`reorder <tvm.te.Stage.reorder>`\\ \\(\\*args\\)"
#~ msgstr ""

#~ msgid "reorder the arguments in the specified order."
#~ msgstr ""

#~ msgid ":py:obj:`rolling_buffer <tvm.te.Stage.rolling_buffer>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Compute the current stage via rolling buffering."
#~ msgstr ""

#~ msgid ":py:obj:`set_scope <tvm.te.Stage.set_scope>`\\ \\(scope\\)"
#~ msgstr ""

#~ msgid "Set the thread scope of this stage"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`set_store_predicate "
#~ "<tvm.te.Stage.set_store_predicate>`\\ \\(predicate\\)"
#~ msgstr ""

#~ msgid "Set predicate under which store to the array can be performed."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`split <tvm.te.Stage.split>`\\ \\(parent\\[\\, "
#~ "factor\\, nparts\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Split the stage either by factor providing outer scope, or both"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`storage_align <tvm.te.Stage.storage_align>`\\ "
#~ "\\(axis\\, factor\\, offset\\)"
#~ msgstr ""

#~ msgid "Set alignment requirement for specific axis"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tensorize <tvm.te.Stage.tensorize>`\\ \\(var\\,"
#~ " tensor\\_intrin\\)"
#~ msgstr ""

#~ msgid "Tensorize the computation enclosed by var with tensor_intrin"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tile <tvm.te.Stage.tile>`\\ \\(x\\_parent\\, "
#~ "y\\_parent\\, x\\_factor\\, y\\_factor\\)"
#~ msgstr ""

#~ msgid "Perform tiling on two dimensions"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`transform_layout <tvm.te.Stage.transform_layout>`\\ "
#~ "\\(mapping\\_function\\)"
#~ msgstr ""

#~ msgid "Defines the layout transformation for the current stage's tensor."
#~ msgstr ""

#~ msgid ":py:obj:`unroll <tvm.te.Stage.unroll>`\\ \\(var\\)"
#~ msgstr ""

#~ msgid "Unroll the iteration."
#~ msgstr ""

#~ msgid ":py:obj:`vectorize <tvm.te.Stage.vectorize>`\\ \\(var\\)"
#~ msgstr ""

#~ msgid "Vectorize the iteration."
#~ msgstr ""

#~ msgid "The iteration to be binded to thread."
#~ msgstr ""

#~ msgid "The thread to be binded."
#~ msgstr ""

#~ msgid "The parent stage"
#~ msgstr ""

#~ msgid "The loop scope t be attached to."
#~ msgstr ""

#~ msgid ""
#~ "This can only be applied to "
#~ "intermediate stage. This will double the"
#~ " storage cost of the current stage."
#~ " Can be useful to hide load "
#~ "latency."
#~ msgstr ""

#~ msgid "The threads to be launched."
#~ msgstr ""

#~ msgid ""
#~ "fused = fuse(...fuse(fuse(args[0], args[1]), "
#~ "args[2]),..., args[-1]) The order is "
#~ "from outer to inner."
#~ msgstr ""

#~ msgid "Itervars that proceeds each other"
#~ msgstr ""

#~ msgid "**fused** -- The fused variable of iteration."
#~ msgstr ""

#~ msgid "The iteration to be parallelized."
#~ msgstr ""

#~ msgid ""
#~ "This will translate to a pragma_scope"
#~ " surrounding the corresponding loop "
#~ "generated. Useful to support experimental "
#~ "features and extensions."
#~ msgstr ""

#~ msgid "The iteration to be anotated"
#~ msgstr ""

#~ msgid "The pragma string to be annotated"
#~ msgstr ""

#~ msgid "The pragma value to pass along the pragma"
#~ msgstr ""

#~ msgid ""
#~ "Most pragmas are advanced/experimental "
#~ "features and may subject to change. "
#~ "List of supported pragmas:"
#~ msgstr ""

#~ msgid "**debug_skip_region**"
#~ msgstr ""

#~ msgid ""
#~ "Force skip the region marked by "
#~ "the axis and turn it into no-"
#~ "op. This is useful for debug "
#~ "purposes."
#~ msgstr ""

#~ msgid "**parallel_launch_point**"
#~ msgstr ""

#~ msgid ""
#~ "Specify to launch parallel threads "
#~ "outside the specified iteration loop. By"
#~ " default the threads launch at the"
#~ " point of parallel construct. This "
#~ "pragma moves the launching point to "
#~ "even outer scope. The threads are "
#~ "launched once and reused across multiple"
#~ " parallel constructs as BSP style "
#~ "program."
#~ msgstr ""

#~ msgid "**parallel_barrier_when_finish**"
#~ msgstr ""

#~ msgid ""
#~ "Insert a synchronization barrier between "
#~ "working threads after the specified loop"
#~ " iteration finishes."
#~ msgstr ""

#~ msgid "**parallel_stride_pattern**"
#~ msgstr ""

#~ msgid ""
#~ "Hint parallel loop to execute in "
#~ "strided pattern. :code:`for (int i = "
#~ "task_id; i < end; i += num_task)`"
#~ msgstr ""

#~ msgid "The tensor to be prefetched"
#~ msgstr ""

#~ msgid "The loop point at which the prefetching is applied"
#~ msgstr ""

#~ msgid "The number of iterations to be prefetched before actual execution"
#~ msgstr ""

#~ msgid "The order to be ordered"
#~ msgstr ""

#~ msgid ""
#~ "This can only be applied to "
#~ "intermediate stage. This will change the"
#~ " storage cost of the current stage."
#~ msgstr ""

#~ msgid "The thread scope of this stage"
#~ msgstr ""

#~ msgid ""
#~ "Use this when there are duplicated "
#~ "threads doing the same store and "
#~ "we only need one of them to "
#~ "do the store."
#~ msgstr ""

#~ msgid "The guard condition fo store."
#~ msgstr ""

#~ msgid "The parent iter var."
#~ msgstr ""

#~ msgid "The splitting factor"
#~ msgstr ""

#~ msgid "The number of outer parts."
#~ msgstr ""

#~ msgid ""
#~ "If enabled, don't create a predicate "
#~ "for guarding the loop. This can be"
#~ " useful when splitting with scalable "
#~ "factors that the schedule writer knows"
#~ " are divisible by the loop bound."
#~ "  Warning: enabling this feature may "
#~ "result in incorrect code generation if"
#~ " not used carefully."
#~ msgstr ""

#~ msgid ""
#~ "If enabled, don't create a predicate "
#~ "for guarding the loop. This can be"
#~ " useful when splitting with scalable "
#~ "factors that the schedule writer knows"
#~ " are divisible by the loop bound."
#~ msgstr ""

#~ msgid ""
#~ "Warning: enabling this feature may "
#~ "result in incorrect code generation if"
#~ " not used carefully."
#~ msgstr ""

#~ msgid ""
#~ "* **outer** (*IterVar*) -- The outer "
#~ "variable of iteration. * **inner** "
#~ "(*IterVar*) -- The inner variable of "
#~ "iteration."
#~ msgstr ""

#~ msgid "**outer** (*IterVar*) -- The outer variable of iteration."
#~ msgstr ""

#~ msgid "**inner** (*IterVar*) -- The inner variable of iteration."
#~ msgstr ""

#~ msgid ""
#~ "This ensures that stride[axis] == k "
#~ "* factor + offset for some k. "
#~ "This is useful to set memory "
#~ "layout to for more friendly memory "
#~ "access pattern. For example, we can "
#~ "set alignment to be factor=2, offset=1"
#~ " to avoid bank conflict for thread"
#~ " access on higher dimension in GPU"
#~ " shared memory."
#~ msgstr ""

#~ msgid "The axis dimension to be aligned."
#~ msgstr ""

#~ msgid "The factor in alignment specification."
#~ msgstr ""

#~ msgid "The offset in the alignment specification."
#~ msgstr ""

#~ msgid "The iteration boundary of tensorization."
#~ msgstr ""

#~ msgid "The tensor intrinsic used for computation."
#~ msgstr ""

#~ msgid ""
#~ "The final loop order from outmost "
#~ "to inner most are [x_outer, y_outer, "
#~ "x_inner, y_inner]"
#~ msgstr ""

#~ msgid "The original x dimension"
#~ msgstr ""

#~ msgid "The original y dimension"
#~ msgstr ""

#~ msgid "The stride factor on x axis"
#~ msgstr ""

#~ msgid "The stride factor on y axis"
#~ msgstr ""

#~ msgid ""
#~ "* **x_outer** (*IterVar*) -- Outer axis"
#~ " of x dimension * **y_outer** "
#~ "(*IterVar*) -- Outer axis of y "
#~ "dimension * **x_inner** (*IterVar*) -- "
#~ "Inner axis of x dimension * "
#~ "**p_y_inner** (*IterVar*) -- Inner axis "
#~ "of y dimension"
#~ msgstr ""

#~ msgid "**x_outer** (*IterVar*) -- Outer axis of x dimension"
#~ msgstr ""

#~ msgid "**y_outer** (*IterVar*) -- Outer axis of y dimension"
#~ msgstr ""

#~ msgid "**x_inner** (*IterVar*) -- Inner axis of x dimension"
#~ msgstr ""

#~ msgid "**p_y_inner** (*IterVar*) -- Inner axis of y dimension"
#~ msgstr ""

#~ msgid ""
#~ "The map from initial_indices to "
#~ "final_indices must be an invertible "
#~ "affine transformation.  This method may "
#~ "be called more than once for a "
#~ "given tensor, in which case each "
#~ "transformation is applied sequentially."
#~ msgstr ""

#~ msgid ""
#~ "If the stage is a ComputeOp, then"
#~ " the iteration order of the compute"
#~ " stage is rewritten to be a "
#~ "row-major traversal of the tensor, "
#~ "and the new loop iteration variables "
#~ "are returned. For all other stages, "
#~ "the loop iteration order is unmodified,"
#~ " and the return value is None."
#~ msgstr ""

#~ msgid ""
#~ "A callable that accepts N arguments "
#~ "of type tvm.tir.Var, and outputs a "
#~ "list of PrimExpr.  The input arguments"
#~ " represent the location of a value"
#~ " in the current stage's tensor, using"
#~ " the pre-transformation layout.  The "
#~ "return value of the function gives "
#~ "the location of that value in the"
#~ " current stage's tensor, using the "
#~ "post-transformation layout."
#~ msgstr ""

#~ msgid ""
#~ "**new_iter_vars** -- If the stage is "
#~ "a ComputeOp, then the return will "
#~ "be the updated loop iteration variables"
#~ " over the data array, in the "
#~ "same order as the output values "
#~ "from the `mapping_function`.  Otherwise, the"
#~ " return value is None."
#~ msgstr ""

#~ msgid ""
#~ "**new_iter_vars** -- If the stage is "
#~ "a ComputeOp, then the return will "
#~ "be the updated loop iteration variables"
#~ " over the data array, in the "
#~ "same order as the output values "
#~ "from the `mapping_function`."
#~ msgstr ""

#~ msgid "Otherwise, the return value is None."
#~ msgstr ""

#~ msgid "The iteration to be unrolled."
#~ msgstr ""

#~ msgid "The iteration to be vectorize"
#~ msgstr ""

#~ msgid "**sch** -- The created schedule."
#~ msgstr ""

#~ msgid "The symbolic description of the intrinsic operation"
#~ msgstr ""

#~ msgid ""
#~ "Specifies the IR statement to do "
#~ "the computation. See the following note"
#~ " for function signature of fcompute  "
#~ ".. note::      **Parameters**       - **ins**"
#~ " (list of :any:`tvm.tir.Buffer`) - "
#~ "Placeholder for each inputs      - "
#~ "**outs** (list of :any:`tvm.tir.Buffer`) - "
#~ "Placeholder for each outputs       **Returns**"
#~ "       - **stmt** (:any:`tvm.tir.Stmt`, or "
#~ "tuple of three stmts)      - If a"
#~ " single stmt is returned, it "
#~ "represents the body      - If tuple "
#~ "of three stmts are returned they "
#~ "corresponds to body,        reduce_init, "
#~ "reduce_update"
#~ msgstr ""

#~ msgid "**stmt** (:any:`tvm.tir.Stmt`, or tuple of three stmts)"
#~ msgstr ""

#~ msgid "If a single stmt is returned, it represents the body"
#~ msgstr ""

#~ msgid ""
#~ "If tuple of three stmts are "
#~ "returned they corresponds to body, "
#~ "reduce_init, reduce_update"
#~ msgstr ""

#~ msgid "The name of the intrinsic."
#~ msgstr ""

#~ msgid ""
#~ "Dictionary that maps the Tensor to "
#~ "Buffer which specified the data layout"
#~ " requirement of the function. By "
#~ "default, a new compact buffer is "
#~ "created for each tensor in the "
#~ "argument."
#~ msgstr ""

#~ msgid "as scalar_inputs when the tensor intrinsic is called."
#~ msgstr ""

#~ msgid "Dictionary of buffer arguments to be passed when constructing a buffer."
#~ msgstr ""

#~ msgid "**intrin** -- A TensorIntrin that can be used in tensorize schedule."
#~ msgstr ""

#~ msgid "The tensor to differentiate."
#~ msgstr ""

#~ msgid "The list of input tensors to be differentiated wrt."
#~ msgstr ""

#~ msgid ""
#~ "The adjoint of the output, in "
#~ "other words, some tensor, by which "
#~ "the Jacobians will be multiplied. Its"
#~ " shape must be of the form "
#~ "`prefix + output.shape`. If `None` is"
#~ " passed, the identity tensor of shape"
#~ " `output.shape + output.shape` will be "
#~ "used."
#~ msgstr ""

#~ msgid "**tensors** -- The result gradient, in the same order as the inputs"
#~ msgstr ""

#~ msgid "Hybrid Programming APIs of TVM Python Package."
#~ msgstr ""

#~ msgid ""
#~ "This package maps a subset of "
#~ "python to HalideIR so that: 1. "
#~ "Users can write some preliminary "
#~ "versions of the computation patterns "
#~ "have not been supported yet and "
#~ "verify it across the real execution "
#~ "and python semantic emulation. 2. So "
#~ "far, it is a text format dedicated"
#~ " to HalideIR Phase 0. Refer tvm.lower"
#~ " for more details. A larger ambition"
#~ " of this module is to support "
#~ "all levels of HalideIR."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`HybridModule <tvm.te.hybrid.HybridModule>`\\ "
#~ "\\(\\[src\\, name\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "The usage of Hybrid Module is very"
#~ " similar to conventional TVM module, "
#~ "but conventional TVM module requires a"
#~ " function body which is already fully"
#~ " lowered."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`build <tvm.te.hybrid.build>`\\ \\(sch\\, "
#~ "inputs\\, outputs\\[\\, name\\]\\)"
#~ msgstr ""

#~ msgid "Dump the current schedule to hybrid module"
#~ msgstr ""

#~ msgid ":py:obj:`decorate <tvm.te.hybrid.decorate>`\\ \\(func\\, fwrapped\\)"
#~ msgstr ""

#~ msgid "A wrapper call of decorator package, differs to call time"
#~ msgstr ""

#~ msgid ":py:obj:`script <tvm.te.hybrid.script>`\\ \\(pyfunc\\)"
#~ msgstr ""

#~ msgid "Decorate a python function as hybrid script."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`source_to_op <tvm.te.hybrid.source_to_op>`\\ "
#~ "\\(src\\, args\\, symbols\\, closure\\_vars\\)"
#~ msgstr ""

#~ msgid "Another level of wrapper"
#~ msgstr ""

#~ msgid ""
#~ "The usage of Hybrid Module is very"
#~ " similar to conventional TVM module, "
#~ "but conventional TVM module requires a"
#~ " function body which is already fully"
#~ " lowered. This contradicts to the "
#~ "fact that Hybrid Module is originally"
#~ " a text format for Phase 0 "
#~ "HalideIR. Thus, a totally separated "
#~ "module is defined."
#~ msgstr ""

#~ msgid ":py:obj:`load <tvm.te.hybrid.HybridModule.load>`\\ \\(path\\)"
#~ msgstr ""

#~ msgid "Load the module from a python file"
#~ msgstr ""

#~ msgid "Path to the given python file"
#~ msgstr ""

#~ msgid "The schedule to be dumped"
#~ msgstr ""

#~ msgid "The inputs of the function body"
#~ msgstr ""

#~ msgid "The outputs of the function body"
#~ msgstr ""

#~ msgid ""
#~ "**module** -- The built results is "
#~ "wrapped in a HybridModule. The usage "
#~ "of HybridModule is roughly the same "
#~ "as normal TVM-built modules."
#~ msgstr ""

#~ msgid "The original function"
#~ msgstr ""

#~ msgid "The wrapped function"
#~ msgstr ""

#~ msgid ""
#~ "The hybrid function support emulation "
#~ "mode and parsing to the internal "
#~ "language IR."
#~ msgstr ""

#~ msgid "**hybrid_func** -- A decorated hybrid script function."
#~ msgstr ""

#~ msgid ""
#~ "If an ast.node, then directly lower "
#~ "it. If a str, then parse it "
#~ "to ast and lower it."
#~ msgstr ""

#~ msgid ""
#~ "The argument lists to the function. "
#~ "It is NOT encouraged to write a"
#~ " function without arguments. It is "
#~ "NOT encouraged to write a function "
#~ "with side effect."
#~ msgstr ""

#~ msgid "The symbol list of the global context of the function."
#~ msgstr ""

#~ msgid "A dict of external name reference captured by this function."
#~ msgstr ""

#~ msgid "**res** -- The result of output tensors of the formed OpNode."
#~ msgstr ""

#~ msgid "Namespace for Tensor Expression Language"
#~ msgstr ""

#~ msgid "**Classes:**"
#~ msgstr ""

#~ msgid ":py:obj:`ComputeOp <tvm.te.ComputeOp>`\\"
#~ msgstr ""

#~ msgid "Scalar operation."
#~ msgstr ""

#~ msgid ":py:obj:`ExternOp <tvm.te.ExternOp>`\\"
#~ msgstr ""

#~ msgid "External operation."
#~ msgstr ""

#~ msgid ":py:obj:`PlaceholderOp <tvm.te.PlaceholderOp>`\\"
#~ msgstr ""

#~ msgid "Placeholder operation."
#~ msgstr ""

#~ msgid ":py:obj:`ScanOp <tvm.te.ScanOp>`\\"
#~ msgstr ""

#~ msgid "Scan operation."
#~ msgstr ""

#~ msgid ":py:obj:`Tensor <tvm.te.Tensor>`\\"
#~ msgstr ""

#~ msgid "Tensor object, to construct, see function.Tensor"
#~ msgstr ""

#~ msgid ":py:obj:`TensorSlice <tvm.te.TensorSlice>`\\ \\(tensor\\, indices\\)"
#~ msgstr ""

#~ msgid "Auxiliary data structure for enable slicing syntax from tensor."
#~ msgstr ""

#~ msgid "**Functions:**"
#~ msgstr ""

#~ msgid ":py:obj:`abs <tvm.te.abs>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Get absolute value of the input element-wise."
#~ msgstr ""

#~ msgid ":py:obj:`acos <tvm.te.acos>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take acos of input x."
#~ msgstr ""

#~ msgid ":py:obj:`acosh <tvm.te.acosh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid ":py:obj:`add <tvm.te.add>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Generic add operator."
#~ msgstr ""

#~ msgid ":py:obj:`all <tvm.te.all>`\\ \\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new expression of the intersection of all conditions in the"
#~ msgstr ""

#~ msgid ":py:obj:`any <tvm.te.any>`\\ \\(\\*args\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new experssion of the union of all conditions in the arguments"
#~ msgstr ""

#~ msgid ":py:obj:`asin <tvm.te.asin>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take asin of input x."
#~ msgstr ""

#~ msgid ":py:obj:`asinh <tvm.te.asinh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take asinh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`atan <tvm.te.atan>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take atan of input x."
#~ msgstr ""

#~ msgid ":py:obj:`atanh <tvm.te.atanh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take atanh of input x."
#~ msgstr ""

#~ msgid ":py:obj:`ceil <tvm.te.ceil>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take ceil of float input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`comm_reducer <tvm.te.comm_reducer>`\\ "
#~ "\\(fcombine\\, fidentity\\[\\, name\\]\\)"
#~ msgstr ""

#~ msgid "Create a commutative reducer for reduction."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`compute <tvm.te.compute>`\\ \\(shape\\, "
#~ "fcompute\\[\\, name\\, tag\\, attrs\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Construct a new tensor by computing over the shape domain."
#~ msgstr ""

#~ msgid ":py:obj:`const <tvm.te.const>`\\ \\(value\\[\\, dtype\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new constant with specified value and dtype"
#~ msgstr ""

#~ msgid ":py:obj:`cos <tvm.te.cos>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take cos of input x."
#~ msgstr ""

#~ msgid ":py:obj:`cosh <tvm.te.cosh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take cosh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`create_prim_func <tvm.te.create_prim_func>`\\ "
#~ "\\(ops\\[\\, index\\_dtype\\_override\\]\\)"
#~ msgstr ""

#~ msgid "Create a TensorIR PrimFunc from tensor expression"
#~ msgstr ""

#~ msgid ":py:obj:`div <tvm.te.div>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute a / b as in C/C++ semantics."
#~ msgstr ""

#~ msgid ":py:obj:`erf <tvm.te.erf>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take gauss error function of the input x."
#~ msgstr ""

#~ msgid ":py:obj:`exp <tvm.te.exp>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take exponential of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`extern <tvm.te.extern>`\\ \\(shape\\, "
#~ "inputs\\, fcompute\\[\\, name\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Compute several tensors via an extern function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`extern_primfunc <tvm.te.extern_primfunc>`\\ "
#~ "\\(input\\_tensors\\, primfunc\\, ...\\)"
#~ msgstr ""

#~ msgid "Compute tensors via a schedulable TIR PrimFunc"
#~ msgstr ""

#~ msgid ":py:obj:`floor <tvm.te.floor>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Take floor of float input x."
#~ msgstr ""

#~ msgid ":py:obj:`floordiv <tvm.te.floordiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the floordiv of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`floormod <tvm.te.floormod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the floormod of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`fmod <tvm.te.fmod>`\\ \\(x\\, y\\)"
#~ msgstr ""

#~ msgid "Return the remainder of x divided by y with the same sign as x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`if_then_else <tvm.te.if_then_else>`\\ \\(cond\\,"
#~ " t\\, f\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Conditional selection expression."
#~ msgstr ""

#~ msgid ":py:obj:`indexdiv <tvm.te.indexdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute floor(a / b) where a and b are non-negative."
#~ msgstr ""

#~ msgid ":py:obj:`indexmod <tvm.te.indexmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the remainder of indexdiv."
#~ msgstr ""

#~ msgid ":py:obj:`isfinite <tvm.te.isfinite>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is finite."
#~ msgstr ""

#~ msgid ":py:obj:`isinf <tvm.te.isinf>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is infinite."
#~ msgstr ""

#~ msgid ":py:obj:`isnan <tvm.te.isnan>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Check if input value is Nan."
#~ msgstr ""

#~ msgid ":py:obj:`log <tvm.te.log>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log of input x."
#~ msgstr ""

#~ msgid ":py:obj:`log10 <tvm.te.log10>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log10 of input x."
#~ msgstr ""

#~ msgid ":py:obj:`log2 <tvm.te.log2>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take log2 of input x."
#~ msgstr ""

#~ msgid ":py:obj:`max <tvm.te.max>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
#~ msgstr ""

#~ msgid "Create a max expression over axis."
#~ msgstr ""

#~ msgid ":py:obj:`max_value <tvm.te.max_value>`\\ \\(dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "maximum value of dtype"
#~ msgstr ""

#~ msgid ":py:obj:`min <tvm.te.min>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
#~ msgstr ""

#~ msgid "Create a min expression over axis."
#~ msgstr ""

#~ msgid ":py:obj:`min_value <tvm.te.min_value>`\\ \\(dtype\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "minimum value of dtype"
#~ msgstr ""

#~ msgid ":py:obj:`multiply <tvm.te.multiply>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Generic multiply operator."
#~ msgstr ""

#~ msgid ":py:obj:`nearbyint <tvm.te.nearbyint>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Round elements of the array to the nearest integer."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`placeholder <tvm.te.placeholder>`\\ "
#~ "\\(shape\\[\\, dtype\\, name\\]\\)"
#~ msgstr ""

#~ msgid "Construct an empty tensor object."
#~ msgstr ""

#~ msgid ":py:obj:`popcount <tvm.te.popcount>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Count the number of set bits in input x."
#~ msgstr ""

#~ msgid ":py:obj:`power <tvm.te.power>`\\ \\(x\\, y\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "x power y"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`reduce_axis <tvm.te.reduce_axis>`\\ \\(dom\\[\\,"
#~ " name\\, thread\\_tag\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new IterVar for reduction."
#~ msgstr ""

#~ msgid ":py:obj:`round <tvm.te.round>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid ":py:obj:`rsqrt <tvm.te.rsqrt>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take reciprocal of square root of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`scan <tvm.te.scan>`\\ \\(init\\, update\\,"
#~ " state\\_placeholder\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Construct new tensors by scanning over axis."
#~ msgstr ""

#~ msgid ":py:obj:`sigmoid <tvm.te.sigmoid>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Quick function to get sigmoid"
#~ msgstr ""

#~ msgid ":py:obj:`sin <tvm.te.sin>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take sin of input x."
#~ msgstr ""

#~ msgid ":py:obj:`sinh <tvm.te.sinh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take sinh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`size_var <tvm.te.size_var>`\\ \\(\\[name\\, "
#~ "dtype\\, span\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Create a new variable represents a "
#~ "tensor shape size, which is non-"
#~ "negative."
#~ msgstr ""

#~ msgid ":py:obj:`sqrt <tvm.te.sqrt>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take square root of input x."
#~ msgstr ""

#~ msgid ":py:obj:`subtract <tvm.te.subtract>`\\ \\(lhs\\, rhs\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Generic subtract operator."
#~ msgstr ""

#~ msgid ":py:obj:`sum <tvm.te.sum>`\\ \\(expr\\, axis\\[\\, where\\, init\\]\\)"
#~ msgstr ""

#~ msgid "Create a sum expression over axis."
#~ msgstr ""

#~ msgid ":py:obj:`tag_scope <tvm.te.tag_scope>`\\ \\(tag\\)"
#~ msgstr ""

#~ msgid "The operator tag scope."
#~ msgstr ""

#~ msgid ":py:obj:`tan <tvm.te.tan>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take tan of input x."
#~ msgstr ""

#~ msgid ":py:obj:`tanh <tvm.te.tanh>`\\ \\(x\\)"
#~ msgstr ""

#~ msgid "Take hyperbolic tanh of input x."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`thread_axis <tvm.te.thread_axis>`\\ \\(\\[dom\\,"
#~ " tag\\, name\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new IterVar to represent thread index."
#~ msgstr ""

#~ msgid ":py:obj:`trace <tvm.te.trace>`\\ \\(args\\[\\, trace\\_action\\]\\)"
#~ msgstr ""

#~ msgid "Trace tensor data at the runtime."
#~ msgstr ""

#~ msgid ":py:obj:`trunc <tvm.te.trunc>`\\ \\(x\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Get truncated value of the input."
#~ msgstr ""

#~ msgid ":py:obj:`truncdiv <tvm.te.truncdiv>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the truncdiv of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`truncmod <tvm.te.truncmod>`\\ \\(a\\, b\\[\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Compute the truncmod of two expressions."
#~ msgstr ""

#~ msgid ":py:obj:`var <tvm.te.var>`\\ \\(\\[name\\, dtype\\, span\\]\\)"
#~ msgstr ""

#~ msgid "Create a new variable with specified name and dtype"
#~ msgstr ""

#~ msgid "**Attributes:**"
#~ msgstr ""

#~ msgid ":py:obj:`scan_axis <tvm.te.ScanOp.scan_axis>`\\"
#~ msgstr ""

#~ msgid "Represent the scan axis, only defined when it is a ScanOp"
#~ msgstr ""

#~ msgid ":py:obj:`axis <tvm.te.Tensor.axis>`\\"
#~ msgstr ""

#~ msgid "Axis of the tensor."
#~ msgstr ""

#~ msgid ":py:obj:`ndim <tvm.te.Tensor.ndim>`\\"
#~ msgstr ""

#~ msgid "Dimension of the tensor."
#~ msgstr ""

#~ msgid ":py:obj:`op <tvm.te.Tensor.op>`\\"
#~ msgstr ""

#~ msgid "The corressponding :py:class:`Operation`."
#~ msgstr ""

#~ msgid ":py:obj:`shape <tvm.te.Tensor.shape>`\\"
#~ msgstr ""

#~ msgid "The output shape of the tensor."
#~ msgstr ""

#~ msgid ":py:obj:`value_index <tvm.te.Tensor.value_index>`\\"
#~ msgstr ""

#~ msgid "The output value index the tensor corresponds to."
#~ msgstr ""

#~ msgid "**Methods:**"
#~ msgstr ""

#~ msgid ":py:obj:`asobject <tvm.te.TensorSlice.asobject>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Convert slice to object."
#~ msgstr ""

#~ msgid ":py:obj:`dtype <tvm.te.TensorSlice.dtype>`\\"
#~ msgstr ""

#~ msgid "Data content of the tensor."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "Input argument."
#~ msgstr ""

#~ msgid "The location of this operator in the source code."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "**y** -- The result."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "The left operand."
#~ msgstr ""

#~ msgid "The right operand."
#~ msgstr ""

#~ msgid "The location of this operator in the source."
#~ msgstr ""

#~ msgid "**op** -- The result Expr of add operaton."
#~ msgstr ""

#~ msgid "arguments"
#~ msgstr ""

#~ msgid "List of symbolic boolean expressions"
#~ msgstr ""

#~ msgid "**expr** -- Expression"
#~ msgstr ""

#~ msgid "A binary function which takes two Expr as input to return a Expr."
#~ msgstr ""

#~ msgid "A function which takes a type string as input to return a const Expr."
#~ msgstr ""

#~ msgid ""
#~ "**reducer** -- A function which creates"
#~ " a reduce expression over axis. There"
#~ " are two ways to use it:  1."
#~ " accept (expr, axis, where) to "
#~ "produce an Reduce Expr on    specified"
#~ " axis; 2. simply use it with "
#~ "multiple Exprs."
#~ msgstr ""

#~ msgid ""
#~ "**reducer** -- A function which creates"
#~ " a reduce expression over axis. There"
#~ " are two ways to use it:"
#~ msgstr ""

#~ msgid "accept (expr, axis, where) to produce an Reduce Expr on specified axis;"
#~ msgstr ""

#~ msgid "simply use it with multiple Exprs."
#~ msgstr ""

#~ msgid "示例"
#~ msgstr ""

#~ msgid "The compute rule is result[axis] = fcompute(axis)"
#~ msgstr ""

#~ msgid "The shape of the tensor"
#~ msgstr ""

#~ msgid "Specifies the input source expression"
#~ msgstr ""

#~ msgid "The name hint of the tensor"
#~ msgstr ""

#~ msgid "Additional tag information about the compute."
#~ msgstr ""

#~ msgid "The additional auxiliary attributes about the compute."
#~ msgstr ""

#~ msgid ""
#~ "The names to use for each of "
#~ "the varargs. If not supplied, the "
#~ "varargs will be called i1, i2, ..."
#~ msgstr ""

#~ msgid "**tensor** -- The created tensor"
#~ msgstr ""

#~ msgid "The constant value."
#~ msgstr ""

#~ msgid "The data type"
#~ msgstr ""

#~ msgid "The location of this variable in the source."
#~ msgstr ""

#~ msgid "**const** -- The result constant expr."
#~ msgstr ""

#~ msgid "The source expression."
#~ msgstr ""

#~ msgid "We define a matmul kernel using following code:"
#~ msgstr ""

#~ msgid ""
#~ "If we want to use TensorIR "
#~ "schedule to do transformations on such"
#~ " kernel, we need to use "
#~ "`create_prim_func([A, B, C])` to create "
#~ "a schedulable PrimFunc. The generated "
#~ "function looks like:"
#~ msgstr ""

#~ msgid "**func** -- The created function."
#~ msgstr ""

#~ msgid "The left hand operand, known to be non-negative."
#~ msgstr ""

#~ msgid "The right hand operand, known to be non-negative."
#~ msgstr ""

#~ msgid "**res** -- The result expression."
#~ msgstr ""

#~ msgid "When operands are integers, returns truncdiv(a, b, span)."
#~ msgstr ""

#~ msgid "The shape of the outputs."
#~ msgstr ""

#~ msgid "The inputs"
#~ msgstr ""

#~ msgid ""
#~ "Specifies the IR statement to do "
#~ "the computation. See the following note"
#~ " for function signature of fcompute  "
#~ ".. note::      **Parameters**       - **ins**"
#~ " (list of :any:`tvm.tir.Buffer`) - "
#~ "Placeholder for each inputs      - "
#~ "**outs** (list of :any:`tvm.tir.Buffer`) - "
#~ "Placeholder for each outputs       **Returns**"
#~ "       - **stmt** (:any:`tvm.tir.Stmt`) - "
#~ "The statement that carries out array "
#~ "computation."
#~ msgstr ""

#~ msgid ""
#~ "Specifies the IR statement to do "
#~ "the computation. See the following note"
#~ " for function signature of fcompute"
#~ msgstr ""

#~ msgid "**Parameters**"
#~ msgstr ""

#~ msgid "**ins** (list of :any:`tvm.tir.Buffer`) - Placeholder for each inputs"
#~ msgstr ""

#~ msgid "**outs** (list of :any:`tvm.tir.Buffer`) - Placeholder for each outputs"
#~ msgstr ""

#~ msgid "**Returns**"
#~ msgstr ""

#~ msgid ""
#~ "**stmt** (:any:`tvm.tir.Stmt`) - The statement"
#~ " that carries out array computation."
#~ msgstr ""

#~ msgid "The data types of outputs, by default dtype will be same as inputs."
#~ msgstr ""

#~ msgid "Input buffers."
#~ msgstr ""

#~ msgid "Output buffers."
#~ msgstr ""

#~ msgid "tag: str, optional"
#~ msgstr ""

#~ msgid "Additonal tag information about the compute."
#~ msgstr ""

#~ msgid "attrs: dict, optional"
#~ msgstr ""

#~ msgid ""
#~ "**tensor** -- The created tensor or "
#~ "tuple of tensors contains multiple "
#~ "outputs."
#~ msgstr ""

#~ msgid ""
#~ "In the code below, C is generated"
#~ " by calling external PackedFunc "
#~ "`tvm.contrib.cblas.matmul`"
#~ msgstr ""

#~ msgid "Input tensors that map to the corresponding primfunc input params."
#~ msgstr ""

#~ msgid "The TIR PrimFunc"
#~ msgstr ""

#~ msgid ""
#~ "**tensor** -- The created tensor or "
#~ "tuple of tensors if it contains "
#~ "multiple outputs."
#~ msgstr ""

#~ msgid ""
#~ "In the code below, a TVMScript "
#~ "defined TIR PrimFunc is inlined into "
#~ "a TE ExternOp. Applying te.create_prim_func"
#~ " on this"
#~ msgstr ""

#~ msgid "The left hand operand"
#~ msgstr ""

#~ msgid "The right hand operand"
#~ msgstr ""

#~ msgid "**z** -- The result."
#~ msgstr ""

#~ msgid "The condition"
#~ msgstr ""

#~ msgid "The result expression if cond is true."
#~ msgstr ""

#~ msgid "The result expression if cond is false."
#~ msgstr ""

#~ msgid "**result** -- The result of conditional expression."
#~ msgstr ""

#~ msgid ""
#~ "Unlike Select, if_then_else will not "
#~ "execute the branch that does not "
#~ "satisfy the condition. You can use "
#~ "it to guard against out of bound"
#~ " access. Unlike Select, if_then_else cannot"
#~ " be vectorized if some lanes in "
#~ "the vector have different conditions."
#~ msgstr ""

#~ msgid ""
#~ "Use this function to split non-"
#~ "negative indices. This function may take"
#~ " advantage of operands' non-negativeness."
#~ msgstr ""

#~ msgid "Compute the remainder of indexdiv. a and b are non-negative."
#~ msgstr ""

#~ msgid "The reduction IterVar axis"
#~ msgstr ""

#~ msgid "Filtering predicate of the reduction."
#~ msgstr ""

#~ msgid "**value** -- The result value."
#~ msgstr ""

#~ msgid "The data type."
#~ msgstr ""

#~ msgid "**value** -- The maximum value of dtype."
#~ msgstr ""

#~ msgid "**value** -- The minimum value of dtype."
#~ msgstr ""

#~ msgid "**op** -- The result Expr of multiply operaton."
#~ msgstr ""

#~ msgid ""
#~ "Round elements of the array to the"
#~ " nearest integer. This intrinsic uses "
#~ "llvm.nearbyint instead of llvm.round which "
#~ "is faster but will results different "
#~ "from te.round. Notably nearbyint rounds "
#~ "according to the rounding mode, whereas"
#~ " te.round (llvm.round) ignores that. For"
#~ " differences between the two see: "
#~ "https://en.cppreference.com/w/cpp/numeric/math/round "
#~ "https://en.cppreference.com/w/cpp/numeric/math/nearbyint"
#~ msgstr ""

#~ msgid "The data type of the tensor"
#~ msgstr ""

#~ msgid "The exponent"
#~ msgstr ""

#~ msgid "The domain of iteration."
#~ msgstr ""

#~ msgid "The name of the variable."
#~ msgstr ""

#~ msgid "The name of the thread_tag."
#~ msgstr ""

#~ msgid "**axis** -- An iteration variable representing the value."
#~ msgstr ""

#~ msgid "The initial condition of first init.shape[0] timestamps"
#~ msgstr ""

#~ msgid "The update rule of the scan given by symbolic tensor."
#~ msgstr ""

#~ msgid "The placeholder variables used by update."
#~ msgstr ""

#~ msgid ""
#~ "The list of inputs to the scan."
#~ " This is not required, but can "
#~ "be useful for the compiler to "
#~ "detect scan body faster."
#~ msgstr ""

#~ msgid "The name"
#~ msgstr ""

#~ msgid "**var** -- The result symbolic shape variable."
#~ msgstr ""

#~ msgid "**op** -- The result Expr of subtract operaton."
#~ msgstr ""

#~ msgid "The tag name."
#~ msgstr ""

#~ msgid ""
#~ "**tag_scope** -- The tag scope object,"
#~ " which can be used as decorator "
#~ "or context manger."
#~ msgstr ""

#~ msgid ""
#~ "The domain of iteration When str "
#~ "is passed, dom is set to None "
#~ "and str is used as tag"
#~ msgstr ""

#~ msgid "The thread tag"
#~ msgstr ""

#~ msgid "The name of the var."
#~ msgstr ""

#~ msgid "**axis** -- The thread itervar."
#~ msgstr ""

#~ msgid ""
#~ "The trace function allows to trace "
#~ "specific tensor at the runtime. The "
#~ "tracing value should come as last "
#~ "argument. The trace action should be "
#~ "specified, by default tvm.default_trace_action "
#~ "is used."
#~ msgstr ""

#~ msgid "Positional arguments."
#~ msgstr ""

#~ msgid "The name of the trace action."
#~ msgstr ""

#~ msgid "**call** -- The call expression."
#~ msgstr ""

#~ msgid ":obj:`tvm.tir.call_packed`"
#~ msgstr ""

#~ msgid "Creates packed function."
#~ msgstr ""

#~ msgid ""
#~ "The truncated value of the scalar "
#~ "x is the nearest integer i which"
#~ " is closer to zero than x is."
#~ msgstr ""

#~ msgid "This is the default integer division behavior in C."
#~ msgstr ""

#~ msgid "**var** -- The result symbolic variable."
#~ msgstr ""

#~ msgid "tvm.te.hybrid"
#~ msgstr ""

