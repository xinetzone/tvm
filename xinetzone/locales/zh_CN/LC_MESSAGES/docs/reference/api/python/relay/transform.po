# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-31 18:33+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../docs/reference/api/python/relay/transform.rst:19
msgid "tvm.relay.transform"
msgstr ""

#~ msgid ""
#~ ":py:obj:`FunctionPass "
#~ "<tvm.relay.transform.tvm.relay.transform.FunctionPass>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FunctionPass "
#~ "<tvm.relay.transform.tvm.relay.transform.FunctionPass>`\\"
#~ msgstr ""

#~ msgid "The Relay IR namespace containing transformations."
#~ msgstr ""

#~ msgid "**Functions:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AlterOpLayout "
#~ "<tvm.relay.transform.tvm.relay.transform.AlterOpLayout>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Alternate the layouts of operators or"
#~ " replace primitive operators with other "
#~ "expressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AnnotateSpans "
#~ "<tvm.relay.transform.tvm.relay.transform.AnnotateSpans>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Annotate a program with span information"
#~ " by first generating its textual "
#~ "representation and then parsing it back"
#~ " into a Relay AST annotated with "
#~ "span information."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AnnotateTarget "
#~ "<tvm.relay.transform.tvm.relay.transform.AnnotateTarget>`\\ "
#~ "\\(targets\\[\\, include\\_non\\_call\\_ops\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Annotate ops in an experession with "
#~ "a provied compiler/target and then use"
#~ " it for codegen."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BackwardFoldScaleAxis "
#~ "<tvm.relay.transform.tvm.relay.transform.BackwardFoldScaleAxis>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Backward fold axis scaling into weights of conv2d/dense."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BatchingOps "
#~ "<tvm.relay.transform.tvm.relay.transform.BatchingOps>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Batching parallel operators into one for Conv2D, Dense and BatchMatmul."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CanonicalizeCast "
#~ "<tvm.relay.transform.tvm.relay.transform.CanonicalizeCast>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Canonicalize cast expressions to make operator fusion more efficient."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CanonicalizeOps "
#~ "<tvm.relay.transform.tvm.relay.transform.CanonicalizeOps>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Canonicalize special operators to basic operators."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CombineParallelBatchMatmul "
#~ "<tvm.relay.transform.tvm.relay.transform.CombineParallelBatchMatmul>`\\"
#~ " \\(\\[min\\_num\\_branches\\]\\)"
#~ msgstr ""

#~ msgid "Combine multiple batch matmul operators into one."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CombineParallelConv2D "
#~ "<tvm.relay.transform.tvm.relay.transform.CombineParallelConv2D>`\\ "
#~ "\\(\\[min\\_num\\_branches\\]\\)"
#~ msgstr ""

#~ msgid "Combine multiple conv2d operators into one."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CombineParallelDense "
#~ "<tvm.relay.transform.tvm.relay.transform.CombineParallelDense>`\\ "
#~ "\\(\\[min\\_num\\_branches\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Combine multiple dense operators into one."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Conv2dToSparse "
#~ "<tvm.relay.transform.tvm.relay.transform.Conv2dToSparse>`\\ "
#~ "\\(weight\\_name\\, weight\\_shape\\, ...\\)"
#~ msgstr ""

#~ msgid "Rewrite qualified ```nn.conv2d operation``` to ```nn.sparse_conv2d```"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Conv2dToSparse2 "
#~ "<tvm.relay.transform.tvm.relay.transform.Conv2dToSparse2>`\\ "
#~ "\\(layout\\, kernel\\_size\\, ...\\)"
#~ msgstr ""

#~ msgid "Rewrite freezed ```nn.conv2d``` operation to ```nn.sparse_conv2d```"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ConvertLayout "
#~ "<tvm.relay.transform.tvm.relay.transform.ConvertLayout>`\\ "
#~ "\\(desired\\_layouts\\)"
#~ msgstr ""

#~ msgid ""
#~ "Given a dest layout, this pass "
#~ "transforms the expr such that most "
#~ "of the ops input data layout is"
#~ " changed to the dest layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DeadCodeElimination "
#~ "<tvm.relay.transform.tvm.relay.transform.DeadCodeElimination>`\\ "
#~ "\\(\\[inline\\_once\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Remove expressions that do not have any users (dead code)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Defunctionalization "
#~ "<tvm.relay.transform.tvm.relay.transform.Defunctionalization>`\\ "
#~ "\\(func\\, mod\\)"
#~ msgstr ""

#~ msgid ""
#~ "Performs defunctionalization on func, "
#~ "transforming func from a higher-order"
#~ " program to a first-order program."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DefuseOps "
#~ "<tvm.relay.transform.tvm.relay.transform.DefuseOps>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "The inverse operation of FuseOps."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DenseToSparse "
#~ "<tvm.relay.transform.tvm.relay.transform.DenseToSparse>`\\ "
#~ "\\(weight\\_name\\, weight\\_shape\\)"
#~ msgstr ""

#~ msgid ""
#~ "Rewrite qualified ```nn.dense operation``` to"
#~ " ```nn.sparse_dense``` This pass is used"
#~ " in ```data_dep_optimization.bsr_dense``` Parameters"
#~ " of this pass is generated by "
#~ "```analysis.sparse_dense.process_params```"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DynamicToStatic "
#~ "<tvm.relay.transform.tvm.relay.transform.DynamicToStatic>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "If possible, convert tvm.relay.dynamic* ops to static versions"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`EliminateCommonSubexpr "
#~ "<tvm.relay.transform.tvm.relay.transform.EliminateCommonSubexpr>`\\"
#~ " \\(\\[fskip\\]\\)"
#~ msgstr ""

#~ msgid "Eliminate common subexpressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`EtaExpand "
#~ "<tvm.relay.transform.tvm.relay.transform.EtaExpand>`\\ "
#~ "\\(\\[expand\\_constructor\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Add abstraction over a constructor or"
#~ " global variable bound to a function"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FakeQuantizationToInteger "
#~ "<tvm.relay.transform.tvm.relay.transform.FakeQuantizationToInteger>`\\"
#~ " \\(\\[hard\\_fail\\]\\)"
#~ msgstr ""

#~ msgid "Find regions of the graph of the form"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FastMath "
#~ "<tvm.relay.transform.tvm.relay.transform.FastMath>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Converts the expensive non linear "
#~ "functions to their fast but approximate"
#~ " counterparts."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FirstOrderGradient "
#~ "<tvm.relay.transform.tvm.relay.transform.FirstOrderGradient>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Transforms all global functions in the"
#~ " module to return the original "
#~ "result, paired with the gradients of "
#~ "the inputs."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FoldConstant "
#~ "<tvm.relay.transform.tvm.relay.transform.FoldConstant>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Fold the constant expressions in a Relay program."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FoldConstantExpr "
#~ "<tvm.relay.transform.tvm.relay.transform.FoldConstantExpr>`\\ "
#~ "\\(expr\\, mod\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FoldExplicitPadding "
#~ "<tvm.relay.transform.tvm.relay.transform.FoldExplicitPadding>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "FoldExplicitPadding finds explict padding "
#~ "before an op that can support "
#~ "implicit padding and fuses them."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FoldScaleAxis "
#~ "<tvm.relay.transform.tvm.relay.transform.FoldScaleAxis>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Fold the scaling of axis into weights of conv2d/dense."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ForwardFoldScaleAxis "
#~ "<tvm.relay.transform.tvm.relay.transform.ForwardFoldScaleAxis>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FuseOps "
#~ "<tvm.relay.transform.tvm.relay.transform.FuseOps>`\\ "
#~ "\\(\\[fuse\\_opt\\_level\\]\\)"
#~ msgstr ""

#~ msgid "Fuse operators in an expr to a larger operator according to some rules."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InferType "
#~ "<tvm.relay.transform.tvm.relay.transform.InferType>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Infer the type of an expr."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InferTypeLocal "
#~ "<tvm.relay.transform.tvm.relay.transform.InferTypeLocal>`\\ "
#~ "\\(expr\\)"
#~ msgstr ""

#~ msgid "Infer the type of a single expr, reusing type information to do so."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Inline "
#~ "<tvm.relay.transform.tvm.relay.transform.Inline>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Perform inlining on the given Relay IR module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LambdaLift "
#~ "<tvm.relay.transform.tvm.relay.transform.LambdaLift>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lift the closure to global function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LazyGradientInit "
#~ "<tvm.relay.transform.tvm.relay.transform.LazyGradientInit>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Reduces memory usage of gradient tensors"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Legalize "
#~ "<tvm.relay.transform.tvm.relay.transform.Legalize>`\\ "
#~ "\\(\\[legalize\\_map\\_attr\\_name\\]\\)"
#~ msgstr ""

#~ msgid "Legalizes an expression with another expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ManifestLifetimes "
#~ "<tvm.relay.transform.tvm.relay.transform.ManifestLifetimes>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Manifest the lifetimes of variables "
#~ "after allocations have been manifested, "
#~ "by inserting kill operations once "
#~ "variables become dead."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MergeCompilerRegions "
#~ "<tvm.relay.transform.tvm.relay.transform.MergeCompilerRegions>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Merge together compiler regions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MergeComposite "
#~ "<tvm.relay.transform.tvm.relay.transform.MergeComposite>`\\ "
#~ "\\(pattern\\_table\\)"
#~ msgstr ""

#~ msgid "Merge multiple operators into a single composite relay function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PartialEvaluate "
#~ "<tvm.relay.transform.tvm.relay.transform.PartialEvaluate>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Evaluate the static fragment of the code."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PartitionGraph "
#~ "<tvm.relay.transform.tvm.relay.transform.PartitionGraph>`\\ "
#~ "\\(\\[mod\\_name\\, bind\\_constants\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Partition a Relay program into regions"
#~ " that can be executed on different"
#~ " backends."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PlanDevices "
#~ "<tvm.relay.transform.tvm.relay.transform.PlanDevices>`\\ "
#~ "\\(config\\)"
#~ msgstr ""

#~ msgid ""
#~ "Uses existing \"on_device\" and "
#~ "\"device_copy\" calls to infer the "
#~ "virtual device on which every Relay "
#~ "sub-expression should run and the "
#~ "result stored."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RemoveUnusedFunctions "
#~ "<tvm.relay.transform.tvm.relay.transform.RemoveUnusedFunctions>`\\ "
#~ "\\(\\[entry\\_functions\\]\\)"
#~ msgstr ""

#~ msgid "Remove unused global relay functions in a relay module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SimplifyExpr "
#~ "<tvm.relay.transform.tvm.relay.transform.SimplifyExpr>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Simplify the Relay expression, including merging consecutive reshapes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SimplifyFCTranspose "
#~ "<tvm.relay.transform.tvm.relay.transform.SimplifyFCTranspose>`\\ "
#~ "\\(target\\_weight\\_name\\)"
#~ msgstr ""

#~ msgid ""
#~ "Rewrite ```y = nn.dense(x, transpose(w, "
#~ "[1, 0]))``` to ```y = nn.dense(x, "
#~ "wt)``` This pass is used in "
#~ "```data_dep_optimization.simplify_fc_transpose```"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SimplifyInference "
#~ "<tvm.relay.transform.tvm.relay.transform.SimplifyInference>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Simplify the data-flow graph for inference phase."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SplitArgs "
#~ "<tvm.relay.transform.tvm.relay.transform.SplitArgs>`\\ "
#~ "\\(max\\_function\\_args\\)"
#~ msgstr ""

#~ msgid "Split function with huge number of arguments to smaller pieces."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToANormalForm "
#~ "<tvm.relay.transform.tvm.relay.transform.ToANormalForm>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Turn Graph Normal Form expression into A Normal Form Expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToANormalFormExpr "
#~ "<tvm.relay.transform.tvm.relay.transform.ToANormalFormExpr>`\\ "
#~ "\\(e\\)"
#~ msgstr ""

#~ msgid "ToANormalForm, but on expression level."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToBasicBlockNormalForm "
#~ "<tvm.relay.transform.tvm.relay.transform.ToBasicBlockNormalForm>`\\"
#~ " \\(\\)"
#~ msgstr ""

#~ msgid "Turn an expression to Basic Block Normal Form."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToCPS "
#~ "<tvm.relay.transform.tvm.relay.transform.ToCPS>`\\ \\(expr\\[\\,"
#~ " mod\\]\\)"
#~ msgstr ""

#~ msgid "Turn expression into continuation passing style(CPS)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToGraphNormalForm "
#~ "<tvm.relay.transform.tvm.relay.transform.ToGraphNormalForm>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Turn a Relay program in A Normal Form into Graph Normal Form"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToMixedPrecision "
#~ "<tvm.relay.transform.tvm.relay.transform.ToMixedPrecision>`\\ "
#~ "\\(\\[mixed\\_precision\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Automatic mixed precision rewriter."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`build_config "
#~ "<tvm.relay.transform.tvm.relay.transform.build_config>`\\ "
#~ "\\(\\[opt\\_level\\, required\\_pass\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Configure the build behavior by setting config variables."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`function_pass "
#~ "<tvm.relay.transform.tvm.relay.transform.function_pass>`\\ "
#~ "\\(\\[pass\\_func\\, opt\\_level\\, name\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Decorate a function pass."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`gradient "
#~ "<tvm.relay.transform.tvm.relay.transform.gradient>`\\ "
#~ "\\(expr\\[\\, mod\\, mode\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Transform the input function, returning "
#~ "a function that calculate the original"
#~ " result, paired with gradient of the"
#~ " input."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`recast "
#~ "<tvm.relay.transform.tvm.relay.transform.recast>`\\ \\(expr\\,"
#~ " dtype\\, out\\_dtype\\[\\, ops\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Convert the types of operations in a graph to a new value."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`to_cps "
#~ "<tvm.relay.transform.tvm.relay.transform.to_cps>`\\ "
#~ "\\(func\\[\\, mod\\]\\)"
#~ msgstr ""

#~ msgid "Turn expression into CPS expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`un_cps "
#~ "<tvm.relay.transform.tvm.relay.transform.un_cps>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid "Turn an cps function into a Function without the continuation argument."
#~ msgstr ""

#~ msgid "**Classes:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ChangeBatch "
#~ "<tvm.relay.transform.tvm.relay.transform.ChangeBatch>`\\ "
#~ "\\(data\\[\\, batch\\_size\\]\\)"
#~ msgstr ""

#~ msgid "Change the batch size."
#~ msgstr ""

#~ msgid "A pass that works on each tvm.relay.Function in a module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LayoutConfig "
#~ "<tvm.relay.transform.tvm.relay.transform.LayoutConfig>`\\ "
#~ "\\(\\[skip\\_layers\\]\\)"
#~ msgstr ""

#~ msgid "A structure for customizing the ConvertLayout pass."
#~ msgstr ""

#~ msgid ""
#~ "Alternate the layouts of operators or"
#~ " replace primitive operators with other "
#~ "expressions. This pass can be used "
#~ "for computing convolution in custom "
#~ "layouts or other general weight pre-"
#~ "transformation."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that alters the layout of operators."
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "**ret** -- The registered AnnotateSpans pass."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid "The list of target compilers used for codegen."
#~ msgstr ""

#~ msgid ""
#~ "If True then non-call ops also "
#~ "will be annotated with targets If "
#~ "False then non-call ops will not"
#~ " be processed"
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The annotated pass that "
#~ "wrapps ops with subgraph_start and "
#~ "subgraph_end."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to backward fold expressions."
#~ msgstr ""

#~ msgid ""
#~ "It is recommended to call "
#~ "backward_fold_scale_axis before using "
#~ "forward_fold_scale_axis as backward folding "
#~ "targets the common conv->bn pattern."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The sequential pass which "
#~ "apply batching for different operator "
#~ "types."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that canonicalizes cast expression."
#~ msgstr ""

#~ msgid ""
#~ "Canonicalize special operators to basic "
#~ "operators. This can simplify followed "
#~ "analysis, e.g. expanding bias_add to "
#~ "expand_dims and broadcast_add."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass performing the canonicalization."
#~ msgstr ""

#~ msgid ""
#~ "A dictionary of all the params to"
#~ " change. The keys are all params, "
#~ "and the values are which dimension "
#~ "hold the batch."
#~ msgstr ""

#~ msgid "The batch size to change to."
#~ msgstr ""

#~ msgid "**pass** -- The pass."
#~ msgstr ""

#~ msgid "Combine multiple batch matmul operators into one. For example:"
#~ msgstr ""

#~ msgid "Would become:"
#~ msgstr ""

#~ msgid ""
#~ "The minimum number of required parallel"
#~ " branches for performing this optimization."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that combines parallel dense operators."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that combines parallel conv2d operators."
#~ msgstr ""

#~ msgid "Combine multiple dense operators into one. For example:"
#~ msgstr ""

#~ msgid "or (if to_batch=False)"
#~ msgstr ""

#~ msgid ""
#~ "If True, combine parallel dense ops "
#~ "into batch_matmul op. If False, combine"
#~ " parallel dense ops into dense op."
#~ msgstr ""

#~ msgid "Names of weights which qualified sparse contrains"
#~ msgstr ""

#~ msgid "Weights shape in BSR format."
#~ msgstr ""

#~ msgid "layout of data"
#~ msgstr ""

#~ msgid "**ret** -- The registered DenseToSparse pass."
#~ msgstr ""

#~ msgid "kernel size of conv2d"
#~ msgstr ""

#~ msgid ""
#~ "Given a dest layout, this pass "
#~ "transforms the expr such that most "
#~ "of the ops input data layout is"
#~ " changed to the dest layout. In "
#~ "ideal situation, there are only 2 "
#~ "layout transforms, one at the start "
#~ "and one at the end."
#~ msgstr ""

#~ msgid ""
#~ "This pass is not a part of "
#~ "relay.build and is expected to be "
#~ "called between framework-relay parser "
#~ "and relay.build call. This is very "
#~ "helpful for hardware backends that "
#~ "support/prefer only type of data layout."
#~ msgstr ""

#~ msgid "RFC - https://discuss.tvm.apache.org/t/layout-conversion-pass/4009"
#~ msgstr ""

#~ msgid ""
#~ "This pass uses most of the "
#~ "AlterOpLayout and InferCorrectLayout infrastructure."
#~ " We can define new layouts for "
#~ "conv2d ops for now. Most of the"
#~ " other operators try to adapt to "
#~ "their input layout using the "
#~ "InferCorrectLayout infrastructure."
#~ msgstr ""

#~ msgid ""
#~ "Specify a mapping of operator names "
#~ "to a list of layouts to convert"
#~ " to, in the order defined by "
#~ "the operator. An example for nn.conv2d"
#~ " could be: {\"nn.conv2d\", [\"NHWC\", "
#~ "\"OHWI]}, where the first item in "
#~ "the list specifies the data layout "
#~ "and the second specifies the kernel "
#~ "layout."
#~ msgstr ""

#~ msgid "Whether to inline a binding that is referenced exactly once."
#~ msgstr ""

#~ msgid "Whether to ignore possible side-effects in let-bound expressions."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "eliminates the dead code in a "
#~ "Relay program."
#~ msgstr ""

#~ msgid ""
#~ "At each call site, the function is"
#~ " cloned and type parameters are "
#~ "substituted in. Function arguments are "
#~ "encoded as datatypes and additional "
#~ "apply functions are used for "
#~ "application."
#~ msgstr ""

#~ msgid ""
#~ "The input function, which should not "
#~ "be polymorphic or be higher-order. "
#~ "This is because all types must be"
#~ " known and we can't encode function"
#~ " arguments to the program itself."
#~ msgstr ""

#~ msgid ""
#~ "The IRModule containing function and "
#~ "type definitions, which is also mutated"
#~ " during this pass."
#~ msgstr ""

#~ msgid "**expr** -- The output function."
#~ msgstr ""

#~ msgid ""
#~ "The inverse operation of FuseOps. It "
#~ "transforms a fused program returned by"
#~ " FuseOps into the program before "
#~ "FuseOps. (i.e., x == DefuseOps(FuseOps(x)))"
#~ msgstr ""

#~ msgid "**ret** -- The registered pass for operator defusion."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass for dynamic->static conversion."
#~ msgstr ""

#~ msgid ""
#~ "The callback function that decides "
#~ "whether an expression should be skipped."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that eliminates common subexpressions."
#~ msgstr ""

#~ msgid "Whether to expand constructors."
#~ msgstr ""

#~ msgid "Whether to expand global variables."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that eta expands an expression."
#~ msgstr ""

#~ msgid ""
#~ "where ``q == qnn.quantize`` and ``dq "
#~ "= qnn.dequantize`` and rewrite them into"
#~ " integer versions of ``op1`` and "
#~ "``op2``"
#~ msgstr ""

#~ msgid ""
#~ "Rules for rewriting indivdual ops are"
#~ " in fake_quantization_to_integer.py"
#~ msgstr ""

#~ msgid ""
#~ "How do deal with errors during "
#~ "graph rewriting. If true, raise an "
#~ "error. If false, skip rewriting the "
#~ "subgraph."
#~ msgstr ""

#~ msgid "**ret** -- The registered SimplifyExpr pass."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to perform fast math operations."
#~ msgstr ""

#~ msgid ""
#~ "Transforms all global functions in the"
#~ " module to return the original "
#~ "result, paired with the gradients of "
#~ "the inputs. This pass transforms each"
#~ " global function independently and does "
#~ "not support interprocedural AD. Additionally,"
#~ " this pass does not support any "
#~ "control-flow or references, and should"
#~ " only be used on pure data-flow"
#~ " graphs."
#~ msgstr ""

#~ msgid "**ret** -- The registered FirstOrderGradient pass."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass for constant folding."
#~ msgstr ""

#~ msgid ""
#~ "Fold the constant expressions in a "
#~ "Relay program. :param expr: The "
#~ "expression to fold :type expr: Expr "
#~ ":param mod: The module the expr "
#~ "lives in (for global calls) :type "
#~ "mod: IRModule"
#~ msgstr ""

#~ msgid "**new_expr** -- The expr after Constant Folding"
#~ msgstr ""

#~ msgid "**ret** -- The registered ImplicitPadding pass."
#~ msgstr ""

#~ msgid ""
#~ "Fold the scaling of axis into "
#~ "weights of conv2d/dense. This pass will"
#~ " invoke both forward and backward "
#~ "scale folding."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to fold expressions."
#~ msgstr ""

#~ msgid ""
#~ "Internally, we will call "
#~ "backward_fold_scale_axis before using "
#~ "forward_fold_scale_axis as backward folding "
#~ "targets the common conv->bn pattern."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to forward fold expressions."
#~ msgstr ""

#~ msgid ""
#~ "It is recommended to call "
#~ "backward_fold_scale_axis before using "
#~ "forward_fold_scale_axis, as backward folding "
#~ "targets the common conv->bn pattern."
#~ msgstr ""

#~ msgid ""
#~ "A pass that works on each "
#~ "tvm.relay.Function in a module. A "
#~ "function pass class should be created"
#~ " through `function_pass`."
#~ msgstr ""

#~ msgid ""
#~ "The level of fuse optimization. -1 "
#~ "indicates that the level will be "
#~ "inferred from pass context."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass for operator fusion."
#~ msgstr ""

#~ msgid "**ret** -- The registered type inference pass."
#~ msgstr ""

#~ msgid ""
#~ "This populates the checked_type field in"
#~ " expr. We assume existing type "
#~ "information in the graph is correct!"
#~ msgstr ""

#~ msgid "The expression we want to know the type of"
#~ msgstr ""

#~ msgid "**type** -- The type of the expression"
#~ msgstr ""

#~ msgid ""
#~ "Perform inlining on the given Relay "
#~ "IR module. The global functions that "
#~ "are marked as `inline` should be "
#~ "always inlined. A cost model will "
#~ "be needed in the future to decide"
#~ " if it is profitable to inline "
#~ "the function."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "performs inlining for a Relay IR "
#~ "module."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that lifts the lambda function."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- A pass which delays "
#~ "and/or reduces memory allocation, by "
#~ "lazily allocating 0 or one filled "
#~ "tensors."
#~ msgstr ""

#~ msgid ""
#~ "Legalizes an expression with another "
#~ "expression. This pass can be used "
#~ "to replace an expr with another "
#~ "expr for target dependent optimizations. "
#~ "For example, one expr, though "
#~ "semnatically equivalent to the other, "
#~ "can have better performance on a "
#~ "target. This pass can be used to"
#~ " legalize the expr in a target-"
#~ "dependent manner."
#~ msgstr ""

#~ msgid "The Op's attr name which corresponds to the legalize rule function."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that rewrites an expr."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that merges compiler regions."
#~ msgstr ""

#~ msgid ""
#~ "A list of (pattern_name, pattern, check)"
#~ " tuples. The order of the patterns"
#~ " in the list will determine the "
#~ "order of priority in which they "
#~ "are matched. 'check' is a function "
#~ "to check whether an extracted pattern"
#~ " matches. It can be implemented by"
#~ " pattern writer but if not specified"
#~ " it will always return True."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "merges operators into a single composite"
#~ " relay function."
#~ msgstr ""

#~ msgid ""
#~ "This transformation could be either "
#~ "`Module -> Module` or `Expr -> "
#~ "Expr`. It will directly transform the"
#~ " input expression to a new one "
#~ "if the target expression is provided."
#~ " Otherwise, it will rely on the "
#~ "pass manager to carry out "
#~ "transformation."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "performs partial evaluation on an "
#~ "expression."
#~ msgstr ""

#~ msgid ""
#~ "Controls the prefix of the name of"
#~ " each partitioned subraph. If `mod_name`"
#~ " is None, then `tvmgen_` prefix is"
#~ " used. Otherwise, `tvmgen_mod_name_` prefix "
#~ "is used."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not to bind constants "
#~ "in partitioned subgraphs. Note that the"
#~ " codegen needs to maintain the bound"
#~ " constants; Otherwise the constants will"
#~ " be maintained by the metadata "
#~ "module. So it is recommended for "
#~ "C-source based codegens to set "
#~ "bind_constants=False to avoid embedding large"
#~ " constants in a C source file."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that partitions the Relay program."
#~ msgstr ""

#~ msgid ""
#~ "Uses existing \"on_device\" and "
#~ "\"device_copy\" calls to infer the "
#~ "virtual device on which every Relay "
#~ "sub-expression should run and the "
#~ "result stored. Captures the result of"
#~ " that analysis using new \"on_device\" "
#~ "and \"device_copy\" calls. Sub-expressions "
#~ "which are not otherwise constrained are"
#~ " assigned to the default primitive "
#~ "virtual device describe by config. "
#~ "However data and computations which must"
#~ " be hosted on a CPU (such as"
#~ " shapes and shape functions) use the"
#~ " host virtual device of the config."
#~ msgstr ""

#~ msgid ""
#~ "The compilation configuration, specifying "
#~ "available targets and default devices."
#~ msgstr ""

#~ msgid "**ret** -- The pass."
#~ msgstr ""

#~ msgid "The set of entry functions to start from."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to remove unused functions."
#~ msgstr ""

#~ msgid ""
#~ "Names of weights which qualified ```y"
#~ " = nn.dense(x, transpose(w, [1, 0]))``` "
#~ "This parameter is generated by "
#~ "```analysis.search_fc_transpose``` function"
#~ msgstr ""

#~ msgid "**ret** -- The registered SimplifyFCTranspose pass."
#~ msgstr ""

#~ msgid ""
#~ "Simplify the data-flow graph for "
#~ "inference phase. An simplified expression "
#~ "which is semantically equal to the "
#~ "input expression will be returned."
#~ msgstr ""

#~ msgid ""
#~ "Note that batch norms will only be"
#~ " simplified if their result is "
#~ "indexed at tuple index 0."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to perform operator simplification."
#~ msgstr ""

#~ msgid ""
#~ "Turn Graph Normal Form expression into"
#~ " A Normal Form Expression. The scope"
#~ " of the root expression is the "
#~ "global scope. The scope of any non"
#~ " root expression is the least common"
#~ " ancestor of all it's scope. Values"
#~ " are ordered by post-DFS order "
#~ "in each scope."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "transforms an expression into A Normal"
#~ " Form."
#~ msgstr ""

#~ msgid "The graph expression."
#~ msgstr ""

#~ msgid "**ret** -- The transformed expresion."
#~ msgstr ""

#~ msgid ""
#~ "Turn an expression to Basic Block "
#~ "Normal Form. We define a block as"
#~ " a group of expressions implied by"
#~ " the scope structure. Each graph node"
#~ " can only belong to a single "
#~ "block. For any value that is being"
#~ " used in multiple blocks, it has "
#~ "to be referred by a Var which "
#~ "is defined in a block, whose scope"
#~ " is the least common ancestor of "
#~ "blocks this value is used."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "transforms an expression into Basic "
#~ "Block Normal Form."
#~ msgstr ""

#~ msgid "Every intermediate compute will be passed to a continuation."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The registered pass that"
#~ " transforms an expression into CPS."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "transforms an expression into Graph "
#~ "Normal Form."
#~ msgstr ""

#~ msgid ""
#~ "Automatic mixed precision rewriter. Rewrite"
#~ " an FP32 relay graph into a "
#~ "version where as many operations as "
#~ "possible are in the target "
#~ "mixed_precision_type."
#~ msgstr ""

#~ msgid "The target datatype to transform operations in the graph to use."
#~ msgstr ""

#~ msgid ""
#~ "Determines how to handle ops not "
#~ "registered with FTVMMixedPrecisionConversionType   "
#~ "0: Does not allow any missing ops."
#~ " Will throw errors when encountering "
#~ "any.   1: Allow missing ops but "
#~ "emit warnings.   2: Allow missing ops"
#~ " and silently ignore them."
#~ msgstr ""

#~ msgid ""
#~ "Determines how to handle ops not "
#~ "registered with FTVMMixedPrecisionConversionType"
#~ msgstr ""

#~ msgid ""
#~ "0: Does not allow any missing ops."
#~ " Will throw errors when encountering "
#~ "any. 1: Allow missing ops but emit"
#~ " warnings. 2: Allow missing ops and"
#~ " silently ignore them."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass."
#~ msgstr ""

#~ msgid ""
#~ "Configure the build behavior by setting"
#~ " config variables. This function will "
#~ "be deprecated in TVM v0.7. Instead, "
#~ "we should directly use "
#~ "tvm.transform.PassContext."
#~ msgstr ""

#~ msgid ""
#~ "Optimization level. The optimization pass "
#~ "name and level are as the "
#~ "following:  .. code-block:: python      "
#~ "OPT_PASS_LEVEL = {         \"SimplifyInference\":"
#~ " 0,         \"OpFusion\": 1,         "
#~ "\"FoldConstant\": 2,         \"FoldScaleAxis\": 3,"
#~ "         \"AlterOpLayout\": 3,         "
#~ "\"CanonicalizeOps\": 3,         \"CanonicalizeCast\": "
#~ "3,         \"EliminateCommonSubexpr\": 3,         "
#~ "\"CombineParallelConv2D\": 4,         "
#~ "\"CombineParallelDense\": 4,         "
#~ "\"CombineParallelBatchMatmul\": 4,         \"FastMath\":"
#~ " 4     }"
#~ msgstr ""

#~ msgid ""
#~ "Optimization level. The optimization pass "
#~ "name and level are as the "
#~ "following:"
#~ msgstr ""

#~ msgid "Optimization passes that are required regardless of optimization level."
#~ msgstr ""

#~ msgid "Optimization passes to be disabled during optimization."
#~ msgstr ""

#~ msgid "A tracing function for debugging or introspection."
#~ msgstr ""

#~ msgid "**pass_context** -- The pass context for optimizations."
#~ msgstr ""

#~ msgid ""
#~ "This function returns a callback when"
#~ " pass_func is provided. Otherwise, it "
#~ "returns the created function pass using"
#~ " the given optimization function."
#~ msgstr ""

#~ msgid "The transformation function or class."
#~ msgstr ""

#~ msgid "The optimization level of this module pass."
#~ msgstr ""

#~ msgid ""
#~ "The name of the function pass. The"
#~ " name could be empty. In this "
#~ "case, the name of the optimization "
#~ "function will be used as the pass"
#~ " name."
#~ msgstr ""

#~ msgid "The list of passes that the module pass is dependent on."
#~ msgstr ""

#~ msgid ""
#~ "**create_function_pass** -- A decorator will"
#~ " be returned if pass_func is not "
#~ "provided, otherwise return the decorated "
#~ "result. The returned decorator has two"
#~ " behaviors depending on the input: A"
#~ " new FunctionPass will be returned "
#~ "when we decorate a pass function. "
#~ "A new FunctionPass class will be "
#~ "returned when we decorate a class "
#~ "type."
#~ msgstr ""

#~ msgid "实际案例"
#~ msgstr ""

#~ msgid "The following code block decorates a function pass class."
#~ msgstr ""

#~ msgid ""
#~ "The following code creates a function"
#~ " pass by decorating a user defined"
#~ " transform function."
#~ msgstr ""

#~ msgid "The input expression, which is a Function or a GlobalVar."
#~ msgstr ""

#~ msgid ""
#~ "The mode of the automatic "
#~ "differentiation algorithm. 'first_order' only "
#~ "works on first order code, but "
#~ "will not produce reference nor closure."
#~ " 'higher_order' works on all code "
#~ "using reference and closure."
#~ msgstr ""

#~ msgid "**expr** -- The transformed expression."
#~ msgstr ""

#~ msgid ""
#~ "Convert the types of operations in "
#~ "a graph to a new value. Note "
#~ "that this is primarily useful for "
#~ "testing performance of individual operations"
#~ " at the new datatype. In a real"
#~ " setting, this pass will almost "
#~ "certainly do a poor job converting "
#~ "from one datatype to another as it"
#~ " just applies hard casting. For "
#~ "example, when recasting from float to"
#~ " integer, many small values will "
#~ "simply be set to 0. Although this"
#~ " will allow autotuning and benchmarking "
#~ "to produce proper timings at the "
#~ "new data type, the output of the"
#~ " model will of course be heavily "
#~ "impacted."
#~ msgstr ""

#~ msgid "The original function that will have its type changed."
#~ msgstr ""

#~ msgid "The target type to cast to."
#~ msgstr ""

#~ msgid "The output type to cast to."
#~ msgstr ""

#~ msgid ""
#~ "A list of operations that should "
#~ "have their type changed, others will "
#~ "be left as is."
#~ msgstr ""

#~ msgid ""
#~ "A list of integers indicating operations"
#~ " that should not have their type "
#~ "changed, counted starting with the first"
#~ " valid operation encountered. Negative "
#~ "indices are allowed and indicate "
#~ "starting at the last layer."
#~ msgstr ""

#~ msgid "**output_expr** -- The graph after recasting to the specified datatype."
#~ msgstr ""

#~ msgid "The input function."
#~ msgstr ""

#~ msgid "The global module."
#~ msgstr ""

#~ msgid "**result** -- The output function."
#~ msgstr ""

#~ msgid "Note that this will not give the exact same interface as before cps:"
#~ msgstr ""

#~ msgid "If the input/output is higher order, they will still be in cps form."
#~ msgstr ""

#~ msgid "The input function"
#~ msgstr ""

#~ msgid "**result** -- The output function"
#~ msgstr ""

