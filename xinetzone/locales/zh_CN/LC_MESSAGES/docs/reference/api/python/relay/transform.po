# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-13 12:27+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../notebook/docs/reference/api/python/relay/transform.rst:19
msgid "tvm.relay.transform"
msgstr ""

#: of tvm.relay.transform:1
msgid "The Relay IR namespace containing transformations."
msgstr ""

#: of tvm.relay.transform:1
msgid "**Functions:**"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`AlterOpLayout <tvm.relay.transform.AlterOpLayout>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Alternate the layouts of operators or replace primitive operators with "
"other expressions."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`AnnotateSpans <tvm.relay.transform.AnnotateSpans>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateSpans:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Annotate a program with span information by first generating its textual "
"representation and then parsing it back into a Relay AST annotated with "
"span information."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`AnnotateTarget <tvm.relay.transform.AnnotateTarget>`\\ "
"\\(targets\\[\\, include\\_non\\_call\\_ops\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Annotate ops in an experession with a provied compiler/target and then "
"use it for codegen."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`BackwardFoldScaleAxis "
"<tvm.relay.transform.BackwardFoldScaleAxis>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.BackwardFoldScaleAxis:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Backward fold axis scaling into weights of conv2d/dense."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`BatchingOps <tvm.relay.transform.BatchingOps>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.BatchingOps:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Batching parallel operators into one for Conv2D, Dense and BatchMatmul."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`CanonicalizeCast <tvm.relay.transform.CanonicalizeCast>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.CanonicalizeCast:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Canonicalize cast expressions to make operator fusion more efficient."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`CanonicalizeOps <tvm.relay.transform.CanonicalizeOps>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Canonicalize special operators to basic operators."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CapturePostDfsIndexInSpans "
"<tvm.relay.transform.CapturePostDfsIndexInSpans>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.CapturePostDfsIndexInSpans:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Captures the post-dfs index and dominator post-dfs index of (most) "
"expression nodes in their span, in the form \"index:<post-dfs "
"index>:<dominator post-dfs index>\"."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CollagePartition <tvm.relay.transform.CollagePartition>`\\ "
"\\(config\\[\\, cost\\_estimator\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Partition the bodies of all functions according to the available targets "
"so as to minimize model latency."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CombineParallelBatchMatmul "
"<tvm.relay.transform.CombineParallelBatchMatmul>`\\ "
"\\(\\[min\\_num\\_branches\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Combine multiple batch matmul operators into one."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CombineParallelConv2D "
"<tvm.relay.transform.CombineParallelConv2D>`\\ "
"\\(\\[min\\_num\\_branches\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelConv2D:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Combine multiple conv2d operators into one."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`CombineParallelDense "
"<tvm.relay.transform.CombineParallelDense>`\\ "
"\\(\\[min\\_num\\_branches\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Combine multiple dense operators into one."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`Conv2dToSparse <tvm.relay.transform.Conv2dToSparse>`\\ "
"\\(weight\\_name\\, weight\\_shape\\, ...\\)"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Rewrite qualified ```nn.conv2d operation``` to ```nn.sparse_conv2d```"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`Conv2dToSparse2 <tvm.relay.transform.Conv2dToSparse2>`\\ "
"\\(layout\\, kernel\\_size\\, ...\\)"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse2:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Rewrite freezed ```nn.conv2d``` operation to ```nn.sparse_conv2d```"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ConvertLayout <tvm.relay.transform.ConvertLayout>`\\ "
"\\(desired\\_layouts\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Given a dest layout, this pass transforms the expr such that most of the "
"ops input data layout is changed to the dest layout."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`DeadCodeElimination <tvm.relay.transform.DeadCodeElimination>`\\"
" \\(\\[inline\\_once\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.DeadCodeElimination:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Remove expressions that do not have any users (dead code)."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`Defunctionalization <tvm.relay.transform.Defunctionalization>`\\"
" \\(func\\, mod\\)"
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Performs defunctionalization on func, transforming func from a higher-"
"order program to a first-order program."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`DefuseOps <tvm.relay.transform.DefuseOps>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "The inverse operation of FuseOps."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`DenseToSparse <tvm.relay.transform.DenseToSparse>`\\ "
"\\(weight\\_name\\, weight\\_shape\\)"
msgstr ""

#: of tvm.relay.transform.transform.DenseToSparse:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Rewrite qualified ```nn.dense operation``` to ```nn.sparse_dense``` This "
"pass is used in ```data_dep_optimization.bsr_dense``` Parameters of this "
"pass is generated by ```analysis.sparse_dense.process_params```"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`DivToMul <tvm.relay.transform.DivToMul>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.DivToMul:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Transform division by a constant to multiplication by the inverse of the "
"constant"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`DynamicToStatic <tvm.relay.transform.DynamicToStatic>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.DynamicToStatic:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "If possible, convert tvm.relay.dynamic* ops to static versions"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`EliminateCommonSubexpr "
"<tvm.relay.transform.EliminateCommonSubexpr>`\\ \\(\\[fskip\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.EliminateCommonSubexpr:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Eliminate common subexpressions."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`EtaExpand <tvm.relay.transform.EtaExpand>`\\ "
"\\(\\[expand\\_constructor\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Add abstraction over a constructor or global variable bound to a function"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FakeQuantizationToInteger "
"<tvm.relay.transform.FakeQuantizationToInteger>`\\ \\(\\[hard\\_fail\\, "
"...\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Find regions of the graph of the form"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`FastMath <tvm.relay.transform.FastMath>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.FastMath:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Converts the expensive non linear functions to their fast but approximate"
" counterparts."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FirstOrderGradient <tvm.relay.transform.FirstOrderGradient>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Transforms all global functions in the module to return the original "
"result, paired with the gradients of the inputs."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FlattenAtrousConv <tvm.relay.transform.FlattenAtrousConv>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.FlattenAtrousConv:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"The purpose of this pass is to find a sequence of space_to_batch_nd-"
"conv2d-batch_to_space_nd operations:"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FoldConstant <tvm.relay.transform.FoldConstant>`\\ "
"\\(\\[fold\\_qnn\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.FoldConstant:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Fold the constant expressions in a Relay program."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FoldConstantExpr <tvm.relay.transform.FoldConstantExpr>`\\ "
"\\(expr\\, mod\\[\\, fold\\_qnn\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Fold the constant expressions in a Relay program. Parameters ---------- "
"expr: Expr     The expression to fold mod: IRModule     The module the "
"expr lives in (for global calls) fold_qnn: bool     Whether to fold "
"constants for QNN operations."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FoldExplicitPadding <tvm.relay.transform.FoldExplicitPadding>`\\"
" \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.FoldExplicitPadding:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"FoldExplicitPadding finds explict padding before an op that can support "
"implicit padding and fuses them."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`FoldScaleAxis <tvm.relay.transform.FoldScaleAxis>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.ForwardFoldScaleAxis:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Fold the scaling of axis into weights of conv2d/dense."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ForwardFoldScaleAxis "
"<tvm.relay.transform.ForwardFoldScaleAxis>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`FuseOps <tvm.relay.transform.FuseOps>`\\ "
"\\(\\[fuse\\_opt\\_level\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.FuseOps:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Fuse operators in an expr to a larger operator according to some rules."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`InferType <tvm.relay.transform.InferType>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.InferType:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Infer the type of an expr."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`InferTypeLocal <tvm.relay.transform.InferTypeLocal>`\\ \\(expr\\)"
msgstr ""

#: of tvm.relay.transform.transform.InferTypeLocal:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Infer the type of a single expr, reusing type information to do so."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`Inline <tvm.relay.transform.Inline>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Perform inlining on the given Relay IR module."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`InlineCompilerFunctionsBoundTo "
"<tvm.relay.transform.InlineCompilerFunctionsBoundTo>`\\ "
"\\(global\\_vars\\)"
msgstr ""

#: of tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Inlines all global functions bound to a global var in global_vars."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`LambdaLift <tvm.relay.transform.LambdaLift>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.LambdaLift:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Lift the closure to global function."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`LazyGradientInit <tvm.relay.transform.LazyGradientInit>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.LazyGradientInit:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Reduces memory usage of gradient tensors"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`Legalize <tvm.relay.transform.Legalize>`\\ "
"\\(\\[legalize\\_map\\_attr\\_name\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Legalizes an expression with another expression."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ManifestLifetimes <tvm.relay.transform.ManifestLifetimes>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.ManifestLifetimes:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Manifest the lifetimes of variables after allocations have been "
"manifested, by inserting kill operations once variables become dead."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`MarkCompilerFunctionsAsExtern "
"<tvm.relay.transform.MarkCompilerFunctionsAsExtern>`\\ "
"\\(\\[compiler\\_filter\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Marks all global functions which have a \"Compiler\" attribute matching "
"compiler_filter as 'extern'."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`MergeCompilerRegions "
"<tvm.relay.transform.MergeCompilerRegions>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.MergeCompilerRegions:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Merge together compiler regions."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`MergeComposite <tvm.relay.transform.MergeComposite>`\\ "
"\\(pattern\\_table\\)"
msgstr ""

#: of tvm.relay.transform.transform.MergeComposite:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Merge multiple operators into a single composite relay function."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`OutlineCompilerFunctionsWithExistingGlobalSymbols "
"<tvm.relay.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols>`\\"
" \\(\\[...\\]\\)"
msgstr ""

#: of
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Outlines all literal functions in direct call positions which have a "
"\"Compiler\" attribute."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`PartialEvaluate <tvm.relay.transform.PartialEvaluate>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.PartialEvaluate:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Evaluate the static fragment of the code."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`PartitionGraph <tvm.relay.transform.PartitionGraph>`\\ "
"\\(\\[mod\\_name\\, bind\\_constants\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.PartitionGraph:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Partition a Relay program into regions that can be executed on different "
"backends."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`PlanDevices <tvm.relay.transform.PlanDevices>`\\ \\(config\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
"Uses existing \"on_device\" and \"device_copy\" calls to infer the "
"virtual device on which every Relay sub-expression should run and the "
"result stored."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`RemoveUnusedFunctions "
"<tvm.relay.transform.RemoveUnusedFunctions>`\\ "
"\\(\\[entry\\_functions\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.RemoveUnusedFunctions:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Remove unused global relay functions in a relay module."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`SimplifyExpr <tvm.relay.transform.SimplifyExpr>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.SimplifyExpr:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Simplify the Relay expression, including merging consecutive reshapes."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`SimplifyFCTranspose <tvm.relay.transform.SimplifyFCTranspose>`\\"
" \\(target\\_weight\\_name\\)"
msgstr ""

#: of tvm.relay.transform.transform.SimplifyFCTranspose:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Rewrite ```y = nn.dense(x, transpose(w, [1, 0]))``` to ```y = nn.dense(x,"
" wt)``` This pass is used in "
"```data_dep_optimization.simplify_fc_transpose```"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`SimplifyInference <tvm.relay.transform.SimplifyInference>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Simplify the data-flow graph for inference phase."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`SplitArgs <tvm.relay.transform.SplitArgs>`\\ "
"\\(max\\_function\\_args\\)"
msgstr ""

#: of tvm.relay.transform.transform.SplitArgs:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Split function with huge number of arguments to smaller pieces."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`ToANormalForm <tvm.relay.transform.ToANormalForm>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Turn Graph Normal Form expression into A Normal Form Expression."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToANormalFormExpr <tvm.relay.transform.ToANormalFormExpr>`\\ "
"\\(e\\)"
msgstr ""

#: of tvm.relay.transform.transform.ToANormalFormExpr:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "ToANormalForm, but on expression level."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToBasicBlockNormalForm "
"<tvm.relay.transform.ToBasicBlockNormalForm>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Turn an expression to Basic Block Normal Form."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`ToCPS <tvm.relay.transform.ToCPS>`\\ \\(expr\\[\\, mod\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.ToCPS:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Turn expression into continuation passing style(CPS)."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToGraphNormalForm <tvm.relay.transform.ToGraphNormalForm>`\\ "
"\\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.ToGraphNormalForm:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Turn a Relay program in A Normal Form into Graph Normal Form"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`ToMixedPrecision <tvm.relay.transform.ToMixedPrecision>`\\ "
"\\(\\[mixed\\_precision\\_type\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Automatic mixed precision rewriter."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`build_config <tvm.relay.transform.build_config>`\\ "
"\\(\\[opt\\_level\\, required\\_pass\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Configure the build behavior by setting config variables."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`function_pass <tvm.relay.transform.function_pass>`\\ "
"\\(\\[pass\\_func\\, opt\\_level\\, name\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Decorate a function pass."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`gradient <tvm.relay.transform.gradient>`\\ \\(expr\\[\\, mod\\, "
"mode\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.gradient:1
#: tvm.relay.transform:1:<autosummary>:1
msgid ""
"Transform the input function, returning a function that calculate the "
"original result, paired with gradient of the input."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ""
":py:obj:`recast <tvm.relay.transform.recast>`\\ \\(expr\\, dtype\\, "
"out\\_dtype\\[\\, ops\\, ...\\]\\)"
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid "Convert the types of operations in a graph to a new value."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`to_cps <tvm.relay.transform.to_cps>`\\ \\(func\\[\\, mod\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.to_cps:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Turn expression into CPS expression."
msgstr ""

#: of tvm.relay.transform:1:<autosummary>:1
msgid ":py:obj:`un_cps <tvm.relay.transform.un_cps>`\\ \\(func\\)"
msgstr ""

#: of tvm.relay.transform.transform.un_cps:1
#: tvm.relay.transform:1:<autosummary>:1
msgid "Turn an cps function into a Function without the continuation argument."
msgstr ""

#: of tvm.relay.transform:1
msgid "**Classes:**"
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:1:<autosummary>:1
msgid ""
":py:obj:`ChangeBatch <tvm.relay.transform.ChangeBatch>`\\ \\(data\\[\\, "
"batch\\_size\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:1:<autosummary>:1
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:1
msgid "Change the batch size."
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:1:<autosummary>:1
msgid ""
":py:obj:`FlexibleShapeDispatch "
"<tvm.relay.transform.FlexibleShapeDispatch>`\\ \\(buckets\\[\\, axis\\, "
"...\\]\\)"
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:1
#: tvm.relay.transform.transform.AlterOpLayout:1:<autosummary>:1
msgid "Enable inference of multiple shaped inputs in one module."
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:1:<autosummary>:1
msgid ":py:obj:`FunctionPass <tvm.relay.transform.FunctionPass>`\\ \\(\\)"
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:1:<autosummary>:1
msgid "A pass that works on each tvm.relay.Function in a module."
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:1:<autosummary>:1
msgid ""
":py:obj:`LayoutConfig <tvm.relay.transform.LayoutConfig>`\\ "
"\\(\\[skip\\_layers\\]\\)"
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:1:<autosummary>:1
#: tvm.relay.transform.transform.LayoutConfig:1
msgid "A structure for customizing the ConvertLayout pass."
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:1
msgid ""
"Alternate the layouts of operators or replace primitive operators with "
"other expressions. This pass can be used for computing convolution in "
"custom layouts or other general weight pre-transformation."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:50
#: tvm.relay.transform.recast.recast:27
#: tvm.relay.transform.transform.AlterOpLayout:7
#: tvm.relay.transform.transform.AnnotateSpans:6
#: tvm.relay.transform.transform.AnnotateTarget:13
#: tvm.relay.transform.transform.BackwardFoldScaleAxis:4
#: tvm.relay.transform.transform.BatchingOps:4
#: tvm.relay.transform.transform.CanonicalizeCast:4
#: tvm.relay.transform.transform.CanonicalizeOps:6
#: tvm.relay.transform.transform.CapturePostDfsIndexInSpans:12
#: tvm.relay.transform.transform.CollagePartition:12
#: tvm.relay.transform.transform.CombineParallelBatchMatmul:26
#: tvm.relay.transform.transform.CombineParallelConv2D:10
#: tvm.relay.transform.transform.CombineParallelDense:36
#: tvm.relay.transform.transform.Conv2dToSparse:15
#: tvm.relay.transform.transform.Conv2dToSparse2:12
#: tvm.relay.transform.transform.ConvertLayout:24
#: tvm.relay.transform.transform.DeadCodeElimination:11
#: tvm.relay.transform.transform.Defunctionalization:20
#: tvm.relay.transform.transform.DefuseOps:5
#: tvm.relay.transform.transform.DenseToSparse:14
#: tvm.relay.transform.transform.DynamicToStatic:4
#: tvm.relay.transform.transform.EliminateCommonSubexpr:10
#: tvm.relay.transform.transform.EtaExpand:12
#: tvm.relay.transform.transform.FakeQuantizationToInteger:51
#: tvm.relay.transform.transform.FastMath:4
#: tvm.relay.transform.transform.FirstOrderGradient:7
#: tvm.relay.transform.transform.FlattenAtrousConv:18
#: tvm.relay.transform.transform.FoldConstant:15
#: tvm.relay.transform.transform.FoldConstantExpr:12
#: tvm.relay.transform.transform.FoldExplicitPadding:5
#: tvm.relay.transform.transform.FoldScaleAxis:5
#: tvm.relay.transform.transform.ForwardFoldScaleAxis:4
#: tvm.relay.transform.transform.FuseOps:10
#: tvm.relay.transform.transform.InferType:4
#: tvm.relay.transform.transform.InferTypeLocal:12
#: tvm.relay.transform.transform.Inline:6
#: tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:15
#: tvm.relay.transform.transform.LambdaLift:4
#: tvm.relay.transform.transform.LazyGradientInit:7
#: tvm.relay.transform.transform.Legalize:13
#: tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:19
#: tvm.relay.transform.transform.MergeCompilerRegions:4
#: tvm.relay.transform.transform.MergeComposite:14
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:19
#: tvm.relay.transform.transform.PartialEvaluate:11
#: tvm.relay.transform.transform.PartitionGraph:18
#: tvm.relay.transform.transform.PlanDevices:14
#: tvm.relay.transform.transform.RemoveUnusedFunctions:9
#: tvm.relay.transform.transform.SimplifyExpr:4
#: tvm.relay.transform.transform.SimplifyFCTranspose:11
#: tvm.relay.transform.transform.SimplifyInference:8
#: tvm.relay.transform.transform.SplitArgs:11
#: tvm.relay.transform.transform.ToANormalForm:7
#: tvm.relay.transform.transform.ToANormalFormExpr:9
#: tvm.relay.transform.transform.ToBasicBlockNormalForm:9
#: tvm.relay.transform.transform.ToCPS:6
#: tvm.relay.transform.transform.ToGraphNormalForm:4
#: tvm.relay.transform.transform.ToMixedPrecision:23
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:13
#: tvm.relay.transform.transform.build_config:38
#: tvm.relay.transform.transform.function_pass:23
#: tvm.relay.transform.transform.gradient:19
#: tvm.relay.transform.transform.to_cps:14
#: tvm.relay.transform.transform.un_cps:12
msgid "Returns"
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:51
#: tvm.relay.transform.transform.AlterOpLayout:8
#: tvm.relay.transform.transform.AnnotateSpans:7
#: tvm.relay.transform.transform.AnnotateTarget:15
#: tvm.relay.transform.transform.BackwardFoldScaleAxis:6
#: tvm.relay.transform.transform.CanonicalizeCast:5
#: tvm.relay.transform.transform.CapturePostDfsIndexInSpans:13
#: tvm.relay.transform.transform.CollagePartition:13
#: tvm.relay.transform.transform.Conv2dToSparse:16
#: tvm.relay.transform.transform.Conv2dToSparse2:13
#: tvm.relay.transform.transform.DefuseOps:6
#: tvm.relay.transform.transform.DenseToSparse:15
#: tvm.relay.transform.transform.DynamicToStatic:5
#: tvm.relay.transform.transform.EliminateCommonSubexpr:11
#: tvm.relay.transform.transform.FakeQuantizationToInteger:52
#: tvm.relay.transform.transform.FirstOrderGradient:8
#: tvm.relay.transform.transform.FlattenAtrousConv:19
#: tvm.relay.transform.transform.FoldConstant:16
#: tvm.relay.transform.transform.FoldExplicitPadding:6
#: tvm.relay.transform.transform.FoldScaleAxis:7
#: tvm.relay.transform.transform.ForwardFoldScaleAxis:6
#: tvm.relay.transform.transform.FuseOps:11
#: tvm.relay.transform.transform.InferType:5
#: tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:16
#: tvm.relay.transform.transform.LambdaLift:5
#: tvm.relay.transform.transform.Legalize:14
#: tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:20
#: tvm.relay.transform.transform.MergeCompilerRegions:5
#: tvm.relay.transform.transform.MergeComposite:16
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:20
#: tvm.relay.transform.transform.PlanDevices:15
#: tvm.relay.transform.transform.RemoveUnusedFunctions:10
#: tvm.relay.transform.transform.SimplifyExpr:5
#: tvm.relay.transform.transform.SimplifyFCTranspose:12
#: tvm.relay.transform.transform.SplitArgs:12
#: tvm.relay.transform.transform.ToANormalForm:8
#: tvm.relay.transform.transform.ToANormalFormExpr:10
#: tvm.relay.transform.transform.ToGraphNormalForm:5
#: tvm.relay.transform.transform.ToMixedPrecision:24
msgid "ret"
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:-1
#: tvm.relay.transform.transform.AnnotateSpans:-1
#: tvm.relay.transform.transform.AnnotateTarget:-1
#: tvm.relay.transform.transform.BackwardFoldScaleAxis:-1
#: tvm.relay.transform.transform.CanonicalizeCast:-1
#: tvm.relay.transform.transform.CapturePostDfsIndexInSpans:-1
#: tvm.relay.transform.transform.CollagePartition:-1
#: tvm.relay.transform.transform.Conv2dToSparse:-1
#: tvm.relay.transform.transform.Conv2dToSparse2:-1
#: tvm.relay.transform.transform.DefuseOps:-1
#: tvm.relay.transform.transform.DenseToSparse:-1
#: tvm.relay.transform.transform.DynamicToStatic:-1
#: tvm.relay.transform.transform.EliminateCommonSubexpr:-1
#: tvm.relay.transform.transform.FakeQuantizationToInteger:-1
#: tvm.relay.transform.transform.FirstOrderGradient:-1
#: tvm.relay.transform.transform.FlattenAtrousConv:-1
#: tvm.relay.transform.transform.FoldConstant:-1
#: tvm.relay.transform.transform.FoldExplicitPadding:-1
#: tvm.relay.transform.transform.FoldScaleAxis:-1
#: tvm.relay.transform.transform.ForwardFoldScaleAxis:-1
#: tvm.relay.transform.transform.FuseOps:-1
#: tvm.relay.transform.transform.InferType:-1
#: tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:-1
#: tvm.relay.transform.transform.LambdaLift:-1
#: tvm.relay.transform.transform.Legalize:-1
#: tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:-1
#: tvm.relay.transform.transform.MergeCompilerRegions:-1
#: tvm.relay.transform.transform.MergeComposite:-1
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:-1
#: tvm.relay.transform.transform.RemoveUnusedFunctions:-1
#: tvm.relay.transform.transform.SimplifyExpr:-1
#: tvm.relay.transform.transform.SimplifyFCTranspose:-1
#: tvm.relay.transform.transform.SplitArgs:-1
#: tvm.relay.transform.transform.ToGraphNormalForm:-1
#: tvm.relay.transform.transform.ToMixedPrecision:-1
msgid "tvm.transform.Pass"
msgstr ""

#: of tvm.relay.transform.transform.AlterOpLayout:9
msgid "The registered pass that alters the layout of operators."
msgstr ""

#: of tvm.relay.transform.transform.AnnotateSpans:8
msgid "The registered AnnotateSpans pass."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:28
#: tvm.relay.transform.recast.recast:11
#: tvm.relay.transform.transform.AnnotateTarget:5
#: tvm.relay.transform.transform.CollagePartition:5
#: tvm.relay.transform.transform.CombineParallelBatchMatmul:20
#: tvm.relay.transform.transform.CombineParallelConv2D:4
#: tvm.relay.transform.transform.CombineParallelDense:26
#: tvm.relay.transform.transform.Conv2dToSparse:4
#: tvm.relay.transform.transform.Conv2dToSparse2:4
#: tvm.relay.transform.transform.ConvertLayout:16
#: tvm.relay.transform.transform.DeadCodeElimination:4
#: tvm.relay.transform.transform.Defunctionalization:9
#: tvm.relay.transform.transform.DenseToSparse:6
#: tvm.relay.transform.transform.EliminateCommonSubexpr:4
#: tvm.relay.transform.transform.EtaExpand:4
#: tvm.relay.transform.transform.FakeQuantizationToInteger:21
#: tvm.relay.transform.transform.FoldConstant:10
#: tvm.relay.transform.transform.FuseOps:4
#: tvm.relay.transform.transform.InferTypeLocal:7
#: tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:10
#: tvm.relay.transform.transform.LazyGradientInit:4
#: tvm.relay.transform.transform.Legalize:8
#: tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:14
#: tvm.relay.transform.transform.MergeComposite:4
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:14
#: tvm.relay.transform.transform.PartitionGraph:5
#: tvm.relay.transform.transform.PlanDevices:9
#: tvm.relay.transform.transform.RemoveUnusedFunctions:4
#: tvm.relay.transform.transform.SimplifyFCTranspose:5
#: tvm.relay.transform.transform.SplitArgs:4
#: tvm.relay.transform.transform.ToANormalFormExpr:4
#: tvm.relay.transform.transform.ToMixedPrecision:5
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:4
#: tvm.relay.transform.transform.build_config:6
#: tvm.relay.transform.transform.function_pass:8
#: tvm.relay.transform.transform.gradient:6
#: tvm.relay.transform.transform.to_cps:6
#: tvm.relay.transform.transform.un_cps:7
msgid "Parameters"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:6
msgid "targets"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:-1
msgid "str or List[str]"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:7
msgid "The list of target compilers used for codegen."
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:10
msgid "include_non_call_ops"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:-1
#: tvm.relay.transform.transform.FakeQuantizationToInteger:-1
msgid "boolean"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:9
msgid ""
"If True then non-call ops also will be annotated with targets If False "
"then non-call ops will not be processed"
msgstr ""

#: of tvm.relay.transform.transform.AnnotateTarget:15
msgid "The annotated pass that wrapps ops with subgraph_start and subgraph_end."
msgstr ""

#: of tvm.relay.transform.transform.BackwardFoldScaleAxis:6
msgid "The registered pass to backward fold expressions."
msgstr ""

#: of tvm.relay.transform.transform.BackwardFoldScaleAxis:9
#: tvm.relay.transform.transform.FoldScaleAxis:10
#: tvm.relay.transform.transform.ForwardFoldScaleAxis:9
#: tvm.relay.transform.transform.PartialEvaluate:4
msgid "Note"
msgstr ""

#: of tvm.relay.transform.transform.BackwardFoldScaleAxis:10
msgid ""
"It is recommended to call backward_fold_scale_axis before using "
"forward_fold_scale_axis as backward folding targets the common conv->bn "
"pattern."
msgstr ""

#: of tvm.relay.transform.transform.BatchingOps:5
#: tvm.relay.transform.transform.CanonicalizeOps:7
#: tvm.relay.transform.transform.CombineParallelBatchMatmul:27
#: tvm.relay.transform.transform.CombineParallelConv2D:11
#: tvm.relay.transform.transform.CombineParallelDense:37
#: tvm.relay.transform.transform.DeadCodeElimination:12
#: tvm.relay.transform.transform.EtaExpand:13
#: tvm.relay.transform.transform.FastMath:5
#: tvm.relay.transform.transform.Inline:7
#: tvm.relay.transform.transform.LazyGradientInit:9
#: tvm.relay.transform.transform.PartialEvaluate:12
#: tvm.relay.transform.transform.PartitionGraph:19
#: tvm.relay.transform.transform.SimplifyInference:9
#: tvm.relay.transform.transform.ToBasicBlockNormalForm:10
msgid "ret: tvm.transform.Pass"
msgstr ""

#: of tvm.relay.transform.transform.BatchingOps:6
msgid "The sequential pass which apply batching for different operator types."
msgstr ""

#: of tvm.relay.transform.transform.CanonicalizeCast:6
msgid "The registered pass that canonicalizes cast expression."
msgstr ""

#: of tvm.relay.transform.transform.CanonicalizeOps:1
msgid ""
"Canonicalize special operators to basic operators. This can simplify "
"followed analysis, e.g. expanding bias_add to expand_dims and "
"broadcast_add."
msgstr ""

#: of tvm.relay.transform.transform.CanonicalizeOps:8
msgid "The registered pass performing the canonicalization."
msgstr ""

#: of tvm.relay.transform.transform.CapturePostDfsIndexInSpans:4
msgid ""
"This is useful for debugging since a) it helps identify pretty-printed "
"sub-expressions within the overall model and b) the indexes are heavily "
"used by Collage for its compact representation of sub-graphs."
msgstr ""

#: of tvm.relay.transform.transform.CapturePostDfsIndexInSpans:8
msgid ""
"Note that Op and Constructor nodes are not changed even though they are "
"assigned an post-dfs index."
msgstr ""

#: of tvm.relay.transform.transform.CapturePostDfsIndexInSpans:14
#: tvm.relay.transform.transform.CollagePartition:14
#: tvm.relay.transform.transform.ConvertLayout:26
#: tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:17
#: tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:21
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:21
#: tvm.relay.transform.transform.PlanDevices:16
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:15
msgid "The pass."
msgstr ""

#: of
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:7
msgid "data: Dict[relay.Var, int]"
msgstr ""

#: of
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:6
msgid ""
"A dictionary of all the params to change. The keys are all params, and "
"the values are which dimension hold the batch."
msgstr ""

#: of
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:10
msgid "batch_size: int"
msgstr ""

#: of
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:10
msgid "The batch size to change to."
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:25
#: tvm.relay.transform.transform._wrap_class_function_pass.<locals>.PyFunctionPass:14
msgid "pass: FunctionPass"
msgstr ""

#: of tvm.relay.transform.transform.CollagePartition:1
msgid ""
"Partition the bodies of all functions according to the available targets "
"so as to minimize model latency. See https://github.com/apache/tvm-"
"rfcs/blob/main/rfcs/0062-collage.md."
msgstr ""

#: of tvm.relay.transform.transform.CollagePartition:6
#: tvm.relay.transform.transform.PlanDevices:11
msgid "config"
msgstr ""

#: of tvm.relay.transform.transform.CollagePartition:-1
msgid "CompilationConfig"
msgstr ""

#: of tvm.relay.transform.transform.CollagePartition:7
msgid "The available targets."
msgstr ""

#: of tvm.relay.transform.transform.CollagePartition:9
msgid "cost_estimator"
msgstr ""

#: of tvm.relay.transform.transform.CollagePartition:-1
msgid "CostEstimator, optional"
msgstr ""

#: of tvm.relay.transform.transform.CollagePartition:9
msgid "The custom cost estimator to use for costing each candidate partition."
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:1
msgid "Combine multiple batch matmul operators into one. For example:"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:9
#: tvm.relay.transform.transform.CombineParallelDense:9
msgid "Would become:"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:23
#: tvm.relay.transform.transform.CombineParallelConv2D:7
#: tvm.relay.transform.transform.CombineParallelDense:29
msgid "min_num_branches"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:-1
#: tvm.relay.transform.transform.CombineParallelConv2D:-1
#: tvm.relay.transform.transform.CombineParallelDense:-1
#: tvm.relay.transform.transform.Conv2dToSparse2:-1
#: tvm.relay.transform.transform.FuseOps:-1
#: tvm.relay.transform.transform.function_pass:-1
msgid "int"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:22
#: tvm.relay.transform.transform.CombineParallelConv2D:6
#: tvm.relay.transform.transform.CombineParallelDense:28
msgid ""
"The minimum number of required parallel branches for performing this "
"optimization."
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelBatchMatmul:28
#: tvm.relay.transform.transform.CombineParallelDense:38
msgid "The registered pass that combines parallel dense operators."
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelConv2D:12
msgid "The registered pass that combines parallel conv2d operators."
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelDense:1
msgid "Combine multiple dense operators into one. For example:"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelDense:17
msgid "or (if to_batch=False)"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelDense:33
msgid "to_batch_matmul"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelDense:-1
msgid "bool"
msgstr ""

#: of tvm.relay.transform.transform.CombineParallelDense:32
msgid ""
"If True, combine parallel dense ops into batch_matmul op. If False, "
"combine parallel dense ops into dense op."
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:6
#: tvm.relay.transform.transform.DenseToSparse:8
#: tvm.relay.transform.transform.SimplifyFCTranspose:8
msgid "weight_name: Array[String]"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:6
#: tvm.relay.transform.transform.DenseToSparse:8
msgid "Names of weights which qualified sparse contrains"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:9
#: tvm.relay.transform.transform.DenseToSparse:11
msgid "weight_shape: Array[Array[IntImm]]"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:9
#: tvm.relay.transform.transform.DenseToSparse:11
msgid "Weights shape in BSR format."
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:12
#: tvm.relay.transform.transform.Conv2dToSparse2:6
msgid "layout"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:-1
#: tvm.relay.transform.transform.Conv2dToSparse2:-1
#: tvm.relay.transform.transform.Legalize:-1
msgid "str"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:12
#: tvm.relay.transform.transform.Conv2dToSparse2:6
msgid "layout of data"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse:17
#: tvm.relay.transform.transform.Conv2dToSparse2:14
#: tvm.relay.transform.transform.DenseToSparse:16
msgid "The registered DenseToSparse pass."
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse2:9
msgid "kernel_size"
msgstr ""

#: of tvm.relay.transform.transform.Conv2dToSparse2:9
msgid "kernel size of conv2d"
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:1
msgid ""
"Given a dest layout, this pass transforms the expr such that most of the "
"ops input data layout is changed to the dest layout. In ideal situation, "
"there are only 2 layout transforms, one at the start and one at the end."
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:5
msgid ""
"This pass is not a part of relay.build and is expected to be called "
"between framework-relay parser and relay.build call. This is very helpful"
" for hardware backends that support/prefer only type of data layout."
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:9
msgid "RFC - https://discuss.tvm.apache.org/t/layout-conversion-pass/4009"
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:11
msgid ""
"This pass uses most of the AlterOpLayout and InferCorrectLayout "
"infrastructure. We can define new layouts for conv2d ops for now. Most of"
" the other operators try to adapt to their input layout using the "
"InferCorrectLayout infrastructure."
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:21
msgid "desired_layouts"
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:-1
msgid "map of op_name to list of layouts"
msgstr ""

#: of tvm.relay.transform.transform.ConvertLayout:18
msgid ""
"Specify a mapping of operator names to a list of layouts to convert to, "
"in the order defined by the operator. An example for nn.conv2d could be: "
"{\"nn.conv2d\", [\"NHWC\", \"OHWI]}, where the first item in the list "
"specifies the data layout and the second specifies the kernel layout."
msgstr ""

#: of tvm.relay.transform.transform.DeadCodeElimination:5
msgid "inline_once: Optional[Bool]"
msgstr ""

#: of tvm.relay.transform.transform.DeadCodeElimination:6
msgid "Whether to inline a binding that is referenced exactly once."
msgstr ""

#: of tvm.relay.transform.transform.DeadCodeElimination:8
msgid "ignore_impurity: Optional[Bool]"
msgstr ""

#: of tvm.relay.transform.transform.DeadCodeElimination:8
msgid "Whether to ignore possible side-effects in let-bound expressions."
msgstr ""

#: of tvm.relay.transform.transform.DeadCodeElimination:13
msgid "The registered pass that eliminates the dead code in a Relay program."
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:4
msgid ""
"At each call site, the function is cloned and type parameters are "
"substituted in. Function arguments are encoded as datatypes and "
"additional apply functions are used for application."
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:13
msgid "func"
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:-1
msgid "tvm.relay.Function"
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:11
msgid ""
"The input function, which should not be polymorphic or be higher-order. "
"This is because all types must be known and we can't encode function "
"arguments to the program itself."
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:17
msgid "mod"
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:-1
msgid "tvm.IRModule"
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:16
msgid ""
"The IRModule containing function and type definitions, which is also "
"mutated during this pass."
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:21
#: tvm.relay.transform.transform.gradient:8
#: tvm.relay.transform.transform.gradient:20
msgid "expr"
msgstr ""

#: of tvm.relay.transform.transform.Defunctionalization:22
#: tvm.relay.transform.transform.to_cps:16
msgid "The output function."
msgstr ""

#: of tvm.relay.transform.transform.DefuseOps:1
msgid ""
"The inverse operation of FuseOps. It transforms a fused program returned "
"by FuseOps into the program before FuseOps. (i.e., x == "
"DefuseOps(FuseOps(x)))"
msgstr ""

#: of tvm.relay.transform.transform.DefuseOps:7
msgid "The registered pass for operator defusion."
msgstr ""

#: of tvm.relay.transform.transform.DynamicToStatic:6
msgid "The registered pass for dynamic->static conversion."
msgstr ""

#: of tvm.relay.transform.transform.EliminateCommonSubexpr:7
msgid "fskip: Callable"
msgstr ""

#: of tvm.relay.transform.transform.EliminateCommonSubexpr:6
msgid ""
"The callback function that decides whether an expression should be "
"skipped."
msgstr ""

#: of tvm.relay.transform.transform.EliminateCommonSubexpr:12
msgid "The registered pass that eliminates common subexpressions."
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:6
msgid "expand_constructor: bool"
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:6
msgid "Whether to expand constructors."
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:9
msgid "expand_global_var: bool"
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:9
msgid "Whether to expand global variables."
msgstr ""

#: of tvm.relay.transform.transform.EtaExpand:14
msgid "The registered pass that eta expands an expression."
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:15
msgid ""
"where ``q == qnn.quantize`` and ``dq = qnn.dequantize`` and rewrite them "
"into integer versions of ``op1`` and ``op2``"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:18
msgid "Rules for rewriting indivdual ops are in fake_quantization_to_integer.py"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:25
msgid "hard_fail"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:23
msgid ""
"How do deal with errors during graph rewriting. If true, raise an error. "
"If false, skip rewriting the subgraph."
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:43
msgid "use_qat"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:28
msgid ""
"To perform an additional QAT pass - convert enabled operations with "
"dequantized inputs. Example: in the graph above op2 is not registered "
"with the FakeQuantizationToInteger attribute, op1 operation can still be "
"converted. Converted pattern below:"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:48
msgid "optional_qnn_ops"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:-1
msgid "List[str]"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:46
msgid ""
"Specify a list of operator names to explicitly enable conversion for "
"specific ops disabled by default. Example: ['nn.softmax']"
msgstr ""

#: of tvm.relay.transform.transform.FakeQuantizationToInteger:53
msgid "The registered FakeQuantizationToInteger pass."
msgstr ""

#: of tvm.relay.transform.transform.FastMath:6
msgid "The registered pass to perform fast math operations."
msgstr ""

#: of tvm.relay.transform.transform.FirstOrderGradient:1
msgid ""
"Transforms all global functions in the module to return the original "
"result, paired with the gradients of the inputs. This pass transforms "
"each global function independently and does not support interprocedural "
"AD. Additionally, this pass does not support any control-flow or "
"references, and should only be used on pure data-flow graphs."
msgstr ""

#: of tvm.relay.transform.transform.FirstOrderGradient:9
msgid "The registered FirstOrderGradient pass."
msgstr ""

#: of tvm.relay.transform.transform.FlattenAtrousConv:14
msgid ""
"and convert them into subgraphs with a convolution with the modified "
"\"dilation\" and recalculated \"padding\" parameters."
msgstr ""

#: of tvm.relay.transform.transform.FlattenAtrousConv:20
msgid "The registered FlattenAtrousConv pass."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:3
msgid ""
"This transformation adds a handler around a module that checks input "
"shapes and dispatches to a subgraph specialized to handle the specific "
"shapes of that input. If no exactly matching subgraph is available, the "
"input will be run using full dynamism. For best performance, specify all "
"the sizes the module will be likely to see using the buckets argument."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:10
msgid ""
"By default, this pass will dispatch shapes that exactly match one of the "
"buckets to a corresponding subgraph. All non-matching shapes use the same"
" fully dynamic fallback. This can be detrimental to performance for those"
" non-matching shapes. Setting auto_pad to True causes this pass to round-"
"up the shape of non-matching inputs to the closest bucket. This allows "
"them to use the tuned kernels of bucket shapes which can improve "
"performance."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:18
msgid ""
"Models that have multiple inputs sharing a dynamic axis, which is common "
"for batch size or sequence length dynamism, are supported through the "
"input_indices argument."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:22
msgid ""
"Many types of dynamism such as batching affect both the input and output "
"shape, however this is not always the case. If the output shape is "
"independent of the input, the affects_output argument of this pass must "
"be set to False."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:31
msgid "buckets: list[int]"
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:30
msgid ""
"The sizes of the input dimension that should be explicitly handled. Each "
"value in buckets will have a corresponding subgraph constructed to handle"
" it."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:34
msgid "axis: int"
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:34
msgid ""
"The dimension of the input that should be made flexible. This will most "
"often be used for the batch dimension."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:37
msgid "auto_pad: Optional[bool]"
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:37
msgid ""
"If True, then padding will be inserted to values that don't match one of "
"the provided buckets."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:39
msgid "pad_value: Optional[float]"
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:40
msgid "When auto_pad is true, padding will be done with this value."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:42
msgid "input_indices: Optional[List[int]]"
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:42
msgid ""
"Which inputs should be dispatched dynamically, provided by index. All "
"inputs must share the same dynamic axis."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:47
msgid "affects_output: Optional[bool]"
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:45
msgid ""
"Whether the change in input shape has a corresponding effect on the "
"output shape. Batching for example effects both the input and output "
"whereas changing sequence length in an NLP model typically does not."
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:-1
msgid "FlexibleShapeDispatch"
msgstr ""

#: of tvm.relay.transform.flexible_shape.FlexibleShapeDispatch:52
msgid "A pass that can be applied to a module to add flexible shape handling."
msgstr ""

#: of tvm.relay.transform.transform.FoldConstant:3
msgid ""
"Because of backward compatibility reason it skips QNN primitives from "
"folding by default. There are some transformation passes like "
"FakeQuantizationToInteger, which requires to keep QNN primitives for "
"constant subgraphs. Uncontrolled constant folding of QNN primitives may "
"break applicability of FakeQuantizationToInteger. We suggest to use "
"FoldConstant pass with none default fold_qnn=True value only when all "
"other QNN sensitive passes were already applied."
msgstr ""

#: of tvm.relay.transform.transform.FoldConstant:12
#: tvm.relay.transform.transform.FoldConstantExpr:9
msgid "fold_qnn: bool"
msgstr ""

#: of tvm.relay.transform.transform.FoldConstant:12
#: tvm.relay.transform.transform.FoldConstantExpr:9
msgid "Whether to fold constants for QNN operations."
msgstr ""

#: of tvm.relay.transform.transform.FoldConstant:17
msgid "The registered pass for constant folding."
msgstr ""

#: of tvm.relay.transform.transform.FoldConstantExpr:1
msgid ""
"Fold the constant expressions in a Relay program. Parameters ---------- "
"expr: Expr"
msgstr ""

#: of tvm.relay.transform.transform.FoldConstantExpr:5
msgid "The expression to fold"
msgstr ""

#: of tvm.relay.transform.transform.FoldConstantExpr:6
msgid "mod: IRModule"
msgstr ""

#: of tvm.relay.transform.transform.FoldConstantExpr:7
msgid "The module the expr lives in (for global calls)"
msgstr ""

#: of tvm.relay.transform.transform.FoldConstantExpr:13
msgid "new_expr: Expr"
msgstr ""

#: of tvm.relay.transform.transform.FoldConstantExpr:14
msgid "The expr after Constant Folding"
msgstr ""

#: of tvm.relay.transform.transform.FoldExplicitPadding:7
msgid "The registered ImplicitPadding pass."
msgstr ""

#: of tvm.relay.transform.transform.FoldScaleAxis:1
msgid ""
"Fold the scaling of axis into weights of conv2d/dense. This pass will "
"invoke both forward and backward scale folding."
msgstr ""

#: of tvm.relay.transform.transform.FoldScaleAxis:7
msgid "The registered pass to fold expressions."
msgstr ""

#: of tvm.relay.transform.transform.FoldScaleAxis:11
msgid ""
"Internally, we will call backward_fold_scale_axis before using "
"forward_fold_scale_axis as backward folding targets the common conv->bn "
"pattern."
msgstr ""

#: of tvm.relay.transform.transform.ForwardFoldScaleAxis:6
msgid "The registered pass to forward fold expressions."
msgstr ""

#: of tvm.relay.transform.transform.ForwardFoldScaleAxis:10
msgid ""
"It is recommended to call backward_fold_scale_axis before using "
"forward_fold_scale_axis, as backward folding targets the common conv->bn "
"pattern."
msgstr ""

#: of tvm.relay.transform.transform.FunctionPass:1
msgid ""
"A pass that works on each tvm.relay.Function in a module. A function pass"
" class should be created through `function_pass`."
msgstr ""

#: of tvm.relay.transform.transform.FuseOps:7
msgid "fuse_opt_level"
msgstr ""

#: of tvm.relay.transform.transform.FuseOps:6
msgid ""
"The level of fuse optimization. -1 indicates that the level will be "
"inferred from pass context."
msgstr ""

#: of tvm.relay.transform.transform.FuseOps:12
msgid "The registered pass for operator fusion."
msgstr ""

#: of tvm.relay.transform.transform.InferType:6
msgid "The registered type inference pass."
msgstr ""

#: of tvm.relay.transform.transform.InferTypeLocal:3
msgid ""
"This populates the checked_type field in expr. We assume existing type "
"information in the graph is correct!"
msgstr ""

#: of tvm.relay.transform.transform.InferTypeLocal:9
msgid "expr: relay.Expr"
msgstr ""

#: of tvm.relay.transform.transform.InferTypeLocal:9
msgid "The expression we want to know the type of"
msgstr ""

#: of tvm.relay.transform.transform.InferTypeLocal:13
msgid "type: relay.Type"
msgstr ""

#: of tvm.relay.transform.transform.InferTypeLocal:14
msgid "The type of the expression"
msgstr ""

#: of tvm.relay.transform.transform.Inline:1
msgid ""
"Perform inlining on the given Relay IR module. The global functions that "
"are marked as `inline` should be always inlined. A cost model will be "
"needed in the future to decide if it is profitable to inline the "
"function."
msgstr ""

#: of tvm.relay.transform.transform.Inline:8
msgid "The registered pass that performs inlining for a Relay IR module."
msgstr ""

#: of tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:3
msgid ""
"Both the global \"Compiler\" attributed function, and any calls to "
"\"Composite\" functions it its body are inlined."
msgstr ""

#: of tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:6
msgid ""
"This pass may be useful for external codegen which needs to undo "
"partitioning based on properties of the entire partition."
msgstr ""

#: of tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:12
msgid "global_vars"
msgstr ""

#: of tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:-1
msgid "Array[tvm.relay.GlobalVar]"
msgstr ""

#: of tvm.relay.transform.transform.InlineCompilerFunctionsBoundTo:12
msgid "The global vars of all 'Compiler' functions to inline."
msgstr ""

#: of tvm.relay.transform.transform.LambdaLift:6
msgid "The registered pass that lifts the lambda function."
msgstr ""

#: of tvm.relay.transform.transform.LazyGradientInit:9
msgid ""
"A pass which delays and/or reduces memory allocation, by lazily "
"allocating 0 or one filled tensors."
msgstr ""

#: of tvm.relay.transform.transform.Legalize:1
msgid ""
"Legalizes an expression with another expression. This pass can be used to"
" replace an expr with another expr for target dependent optimizations. "
"For example, one expr, though semnatically equivalent to the other, can "
"have better performance on a target. This pass can be used to legalize "
"the expr in a target-dependent manner."
msgstr ""

#: of tvm.relay.transform.transform.Legalize:10
msgid "legalize_map_attr_name"
msgstr ""

#: of tvm.relay.transform.transform.Legalize:10
msgid "The Op's attr name which corresponds to the legalize rule function."
msgstr ""

#: of tvm.relay.transform.transform.Legalize:15
msgid "The registered pass that rewrites an expr."
msgstr ""

#: of tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:4
msgid ""
"The function's attributes are replaced with a single \"Extern\" "
"attribute, and all calls to the function are switched to use the "
"'call_lowered' calling convention."
msgstr ""

#: of tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:7
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:7
msgid ""
"If compiler_filter is non-empty only functions with that as their "
"attribute value are outlined."
msgstr ""

#: of tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:10
msgid ""
"This pass may be useful for external codegen using the \"RelayToTIR\" "
"custom pass mechanism to cleanup the IRModule after custom lowering."
msgstr ""

#: of tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:16
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:16
msgid "compiler_filter"
msgstr ""

#: of tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:-1
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:-1
msgid "String"
msgstr ""

#: of tvm.relay.transform.transform.MarkCompilerFunctionsAsExtern:16
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:16
msgid "If non-empty, the \"Compiler\" attribute to filter on."
msgstr ""

#: of tvm.relay.transform.transform.MergeCompilerRegions:6
msgid "The registered pass that merges compiler regions."
msgstr ""

#: of tvm.relay.transform.transform.MergeComposite:11
msgid "pattern_table"
msgstr ""

#: of tvm.relay.transform.transform.MergeComposite:-1
msgid "List[Tuple[str, tvm.relay.dataflow_pattern.DFPattern, Function]]"
msgstr ""

#: of tvm.relay.transform.transform.MergeComposite:6
msgid ""
"A list of (pattern_name, pattern, check) tuples. The order of the "
"patterns in the list will determine the order of priority in which they "
"are matched. 'check' is a function to check whether an extracted pattern "
"matches. It can be implemented by pattern writer but if not specified it "
"will always return True."
msgstr ""

#: of tvm.relay.transform.transform.MergeComposite:16
msgid ""
"The registered pass that merges operators into a single composite relay "
"function."
msgstr ""

#: of
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:4
msgid ""
"The outlined functions are bound to unique global vars according to their"
" existing \"global_symbol\" attribute. At most one function with the same"
" global symbol is outlined."
msgstr ""

#: of
#: tvm.relay.transform.transform.OutlineCompilerFunctionsWithExistingGlobalSymbols:10
msgid ""
"This pass may be useful for external codegen using the \"RelayToTIR\" "
"custom pass mechanism to prepare the IRModule before custom lowering."
msgstr ""

#: of tvm.relay.transform.transform.PartialEvaluate:5
msgid ""
"This transformation could be either `Module -> Module` or `Expr -> Expr`."
" It will directly transform the input expression to a new one if the "
"target expression is provided. Otherwise, it will rely on the pass "
"manager to carry out transformation."
msgstr ""

#: of tvm.relay.transform.transform.PartialEvaluate:13
msgid "The registered pass that performs partial evaluation on an expression."
msgstr ""

#: of tvm.relay.transform.transform.PartitionGraph:9
msgid "mod_name"
msgstr ""

#: of tvm.relay.transform.transform.PartitionGraph:-1
msgid "string"
msgstr ""

#: of tvm.relay.transform.transform.PartitionGraph:7
msgid ""
"Controls the prefix of the name of each partitioned subraph. If "
"`mod_name` is None, then `tvmgen_` prefix is used. Otherwise, "
"`tvmgen_mod_name_` prefix is used."
msgstr ""

#: of tvm.relay.transform.transform.PartitionGraph:15
msgid "bind_constants: bool"
msgstr ""

#: of tvm.relay.transform.transform.PartitionGraph:12
msgid ""
"Whether or not to bind constants in partitioned subgraphs. Note that the "
"codegen needs to maintain the bound constants; Otherwise the constants "
"will be maintained by the metadata module. So it is recommended for "
"C-source based codegens to set bind_constants=False to avoid embedding "
"large constants in a C source file."
msgstr ""

#: of tvm.relay.transform.transform.PartitionGraph:20
msgid "The registered pass that partitions the Relay program."
msgstr ""

#: of tvm.relay.transform.transform.PlanDevices:1
msgid ""
"Uses existing \"on_device\" and \"device_copy\" calls to infer the "
"virtual device on which every Relay sub-expression should run and the "
"result stored. Captures the result of that analysis using new "
"\"on_device\" and \"device_copy\" calls. Sub-expressions which are not "
"otherwise constrained are assigned to the default primitive virtual "
"device describe by config. However data and computations which must be "
"hosted on a CPU (such as shapes and shape functions) use the host virtual"
" device of the config."
msgstr ""

#: of tvm.relay.transform.transform.PlanDevices:-1
msgid "tvm.CompilationConfig"
msgstr ""

#: of tvm.relay.transform.transform.PlanDevices:11
msgid ""
"The compilation configuration, specifying available targets and default "
"devices."
msgstr ""

#: of tvm.relay.transform.transform.PlanDevices:-1
msgid "tvm.transforms.Pass"
msgstr ""

#: of tvm.relay.transform.transform.RemoveUnusedFunctions:6
msgid "entry_functions: list[string]"
msgstr ""

#: of tvm.relay.transform.transform.RemoveUnusedFunctions:6
msgid "The set of entry functions to start from."
msgstr ""

#: of tvm.relay.transform.transform.RemoveUnusedFunctions:11
msgid "The registered pass to remove unused functions."
msgstr ""

#: of tvm.relay.transform.transform.SimplifyExpr:6
msgid "The registered SimplifyExpr pass."
msgstr ""

#: of tvm.relay.transform.transform.SimplifyFCTranspose:7
msgid ""
"Names of weights which qualified ```y = nn.dense(x, transpose(w, [1, "
"0]))``` This parameter is generated by ```analysis.search_fc_transpose```"
" function"
msgstr ""

#: of tvm.relay.transform.transform.SimplifyFCTranspose:13
msgid "The registered SimplifyFCTranspose pass."
msgstr ""

#: of tvm.relay.transform.transform.SimplifyInference:1
msgid ""
"Simplify the data-flow graph for inference phase. An simplified "
"expression which is semantically equal to the input expression will be "
"returned."
msgstr ""

#: of tvm.relay.transform.transform.SimplifyInference:4
msgid ""
"Note that batch norms will only be simplified if their result is indexed "
"at tuple index 0."
msgstr ""

#: of tvm.relay.transform.transform.SimplifyInference:10
msgid "The registered pass to perform operator simplification."
msgstr ""

#: of tvm.relay.transform.transform.SplitArgs:8
msgid "max_function_args: int"
msgstr ""

#: of tvm.relay.transform.transform.SplitArgs:6
msgid ""
"Maximum number of function arguments. If it equals 0 then SplitArgs "
"shouldn't split the function."
msgstr ""

#: of tvm.relay.transform.transform.SplitArgs:13
#: tvm.relay.transform.transform.ToMixedPrecision:25
msgid "The registered pass."
msgstr ""

#: of tvm.relay.transform.transform.ToANormalForm:1
msgid ""
"Turn Graph Normal Form expression into A Normal Form Expression. The "
"scope of the root expression is the global scope. The scope of any non "
"root expression is the least common ancestor of all it's scope. Values "
"are ordered by post-DFS order in each scope."
msgstr ""

#: of tvm.relay.transform.transform.ToANormalForm:-1
msgid "Union[tvm.transform.Pass, tvm.relay.Expr]"
msgstr ""

#: of tvm.relay.transform.transform.ToANormalForm:9
msgid "The registered pass that transforms an expression into A Normal Form."
msgstr ""

#: of tvm.relay.transform.transform.ToANormalFormExpr:6
msgid "e"
msgstr ""

#: of tvm.relay.transform.transform.ToANormalFormExpr:-1
msgid "Expr"
msgstr ""

#: of tvm.relay.transform.transform.ToANormalFormExpr:6
msgid "The graph expression."
msgstr ""

#: of tvm.relay.transform.transform.ToANormalFormExpr:11
msgid "The transformed expresion."
msgstr ""

#: of tvm.relay.transform.transform.ToBasicBlockNormalForm:1
msgid ""
"Turn an expression to Basic Block Normal Form. We define a block as a "
"group of expressions implied by the scope structure. Each graph node can "
"only belong to a single block. For any value that is being used in "
"multiple blocks, it has to be referred by a Var which is defined in a "
"block, whose scope is the least common ancestor of blocks this value is "
"used."
msgstr ""

#: of tvm.relay.transform.transform.ToBasicBlockNormalForm:11
msgid ""
"The registered pass that transforms an expression into Basic Block Normal"
" Form."
msgstr ""

#: of tvm.relay.transform.transform.ToCPS:3
#: tvm.relay.transform.transform.to_cps:3
msgid "Every intermediate compute will be passed to a continuation."
msgstr ""

#: of tvm.relay.transform.transform.ToCPS:7
msgid "result: tvm.transform.Pass"
msgstr ""

#: of tvm.relay.transform.transform.ToCPS:8
msgid "The registered pass that transforms an expression into CPS."
msgstr ""

#: of tvm.relay.transform.transform.ToGraphNormalForm:6
msgid "The registered pass that transforms an expression into Graph Normal Form."
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:1
msgid ""
"Automatic mixed precision rewriter. Rewrite an FP32 relay graph into a "
"version where as many operations as possible are in the target "
"mixed_precision_type."
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:7
msgid "mixed_precision_type: str"
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:7
msgid "The target datatype to transform operations in the graph to use."
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:13
msgid "missing_op_mode: int"
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:13
msgid ""
"Determines how to handle ops not registered with "
"FTVMMixedPrecisionConversionType"
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:11
msgid ""
"0: Does not allow any missing ops. Will throw errors when encountering "
"any. 1: Allow missing ops but emit warnings. 2: Allow missing ops and "
"silently ignore them."
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:20
msgid "relay.ToMixedPrecision.keep_orig_output_dtype: boolean"
msgstr ""

#: of tvm.relay.transform.transform.ToMixedPrecision:16
msgid ""
"Defines if outputs should be retained in original data type or convert to"
" mixed_precision_type. By default this parameter is False and "
"transformation modifies the data types of outputs to "
"mixed_precision_type. This parameter is not part of explicit arguments of"
" the transformation, but should be passed through "
"tvm.transform.PassContext."
msgstr ""

#: of tvm.relay.transform.transform.build_config:1
msgid ""
"Configure the build behavior by setting config variables. This function "
"will be deprecated in TVM v0.7. Instead, we should directly use "
"tvm.transform.PassContext."
msgstr ""

#: of tvm.relay.transform.transform.build_config:26
msgid "opt_level: int, optional"
msgstr ""

#: of tvm.relay.transform.transform.build_config:8
msgid ""
"Optimization level. The optimization pass name and level are as the "
"following:"
msgstr ""

#: of tvm.relay.transform.transform.build_config:29
msgid "required_pass: set of str, optional"
msgstr ""

#: of tvm.relay.transform.transform.build_config:29
msgid "Optimization passes that are required regardless of optimization level."
msgstr ""

#: of tvm.relay.transform.transform.build_config:32
msgid "disabled_pass: set of str, optional"
msgstr ""

#: of tvm.relay.transform.transform.build_config:32
msgid "Optimization passes to be disabled during optimization."
msgstr ""

#: of tvm.relay.transform.transform.build_config:35
msgid "trace: Callable[[IRModule, PassInfo, bool], None]"
msgstr ""

#: of tvm.relay.transform.transform.build_config:35
msgid "A tracing function for debugging or introspection."
msgstr ""

#: of tvm.relay.transform.transform.build_config:39
msgid "pass_context: PassContext"
msgstr ""

#: of tvm.relay.transform.transform.build_config:40
msgid "The pass context for optimizations."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:3
msgid ""
"This function returns a callback when pass_func is provided. Otherwise, "
"it returns the created function pass using the given optimization "
"function."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:10
msgid "pass_func"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:-1
msgid "Optional[Callable[(Function, Module, PassContext) -> Function]]"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:10
msgid "The transformation function or class."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:13
msgid "opt_level"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:13
msgid "The optimization level of this module pass."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:17
msgid "name"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:-1
msgid "Optional[str]"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:16
msgid ""
"The name of the function pass. The name could be empty. In this case, the"
" name of the optimization function will be used as the pass name."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:20
msgid "required"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:-1
msgid "Optional[List[str]]"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:20
msgid "The list of passes that the module pass is dependent on."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:24
msgid "create_function_pass : Union[Callable, FunctionPass]"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:26
msgid ""
"A decorator will be returned if pass_func is not provided, otherwise "
"return the decorated result. The returned decorator has two behaviors "
"depending on the input: A new FunctionPass will be returned when we "
"decorate a pass function. A new FunctionPass class will be returned when "
"we decorate a class type."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:33
msgid "Examples"
msgstr ""

#: of tvm.relay.transform.transform.function_pass:34
msgid "The following code block decorates a function pass class."
msgstr ""

#: of tvm.relay.transform.transform.function_pass:58
msgid ""
"The following code creates a function pass by decorating a user defined "
"transform function."
msgstr ""

#: of tvm.relay.transform.transform.gradient:-1
msgid "tvm.relay.Expr"
msgstr ""

#: of tvm.relay.transform.transform.gradient:8
msgid "The input expression, which is a Function or a GlobalVar."
msgstr ""

#: of tvm.relay.transform.transform.gradient:10
msgid "mod : Optional[tvm.IRModule]"
msgstr ""

#: of tvm.relay.transform.transform.gradient:16
msgid "mode"
msgstr ""

#: of tvm.relay.transform.transform.gradient:-1
msgid "Optional[String]"
msgstr ""

#: of tvm.relay.transform.transform.gradient:13
msgid ""
"The mode of the automatic differentiation algorithm. 'first_order' only "
"works on first order code, but will not produce reference nor closure. "
"'higher_order' works on all code using reference and closure."
msgstr ""

#: of tvm.relay.transform.transform.gradient:21
msgid "The transformed expression."
msgstr ""

#: of tvm.relay.transform.recast.recast:1
msgid ""
"Convert the types of operations in a graph to a new value. Note that this"
" is primarily useful for testing performance of individual operations at "
"the new datatype. In a real setting, this pass will almost certainly do a"
" poor job converting from one datatype to another as it just applies hard"
" casting. For example, when recasting from float to integer, many small "
"values will simply be set to 0. Although this will allow autotuning and "
"benchmarking to produce proper timings at the new data type, the output "
"of the model will of course be heavily impacted."
msgstr ""

#: of tvm.relay.transform.recast.recast:12
msgid "expr: tvm.relay.Expr, tvm.relay.Function, or tvm.ir.IRModule"
msgstr ""

#: of tvm.relay.transform.recast.recast:13
msgid "The original function that will have its type changed."
msgstr ""

#: of tvm.relay.transform.recast.recast:14
msgid "dtype: str"
msgstr ""

#: of tvm.relay.transform.recast.recast:15
msgid "The target type to cast to."
msgstr ""

#: of tvm.relay.transform.recast.recast:16
msgid "out_dtype: str"
msgstr ""

#: of tvm.relay.transform.recast.recast:17
msgid "The output type to cast to."
msgstr ""

#: of tvm.relay.transform.recast.recast:19
msgid "ops: List[str]"
msgstr ""

#: of tvm.relay.transform.recast.recast:19
msgid ""
"A list of operations that should have their type changed, others will be "
"left as is."
msgstr ""

#: of tvm.relay.transform.recast.recast:24
msgid "skip_layers: List[int]"
msgstr ""

#: of tvm.relay.transform.recast.recast:22
msgid ""
"A list of integers indicating operations that should not have their type "
"changed, counted starting with the first valid operation encountered. "
"Negative indices are allowed and indicate starting at the last layer."
msgstr ""

#: of tvm.relay.transform.recast.recast:28
msgid "output_expr"
msgstr ""

#: of tvm.relay.transform.recast.recast:-1
msgid "tvm.relay.Expr, tvm.relay.Function, or tvm.ir.IRModule"
msgstr ""

#: of tvm.relay.transform.recast.recast:29
msgid "The graph after recasting to the specified datatype."
msgstr ""

#: of tvm.relay.transform.transform.to_cps:8
#: tvm.relay.transform.transform.un_cps:9
msgid "func: tvm.relay.Function"
msgstr ""

#: of tvm.relay.transform.transform.to_cps:8
msgid "The input function."
msgstr ""

#: of tvm.relay.transform.transform.to_cps:11
msgid "mod: Optional[tvm.IRModule]"
msgstr ""

#: of tvm.relay.transform.transform.to_cps:11
msgid "The global module."
msgstr ""

#: of tvm.relay.transform.transform.to_cps:15
#: tvm.relay.transform.transform.un_cps:13
msgid "result: tvm.relay.Function"
msgstr ""

#: of tvm.relay.transform.transform.un_cps:4
msgid "Note that this will not give the exact same interface as before cps:"
msgstr ""

#: of tvm.relay.transform.transform.un_cps:4
msgid "If the input/output is higher order, they will still be in cps form."
msgstr ""

#: of tvm.relay.transform.transform.un_cps:9
msgid "The input function"
msgstr ""

#: of tvm.relay.transform.transform.un_cps:14
msgid "The output function"
msgstr ""

#~ msgid ""
#~ ":py:obj:`FunctionPass "
#~ "<tvm.relay.transform.tvm.relay.transform.FunctionPass>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FunctionPass "
#~ "<tvm.relay.transform.tvm.relay.transform.FunctionPass>`\\"
#~ msgstr ""

#~ msgid "The Relay IR namespace containing transformations."
#~ msgstr ""

#~ msgid "**Functions:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AlterOpLayout "
#~ "<tvm.relay.transform.tvm.relay.transform.AlterOpLayout>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Alternate the layouts of operators or"
#~ " replace primitive operators with other "
#~ "expressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AnnotateSpans "
#~ "<tvm.relay.transform.tvm.relay.transform.AnnotateSpans>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Annotate a program with span information"
#~ " by first generating its textual "
#~ "representation and then parsing it back"
#~ " into a Relay AST annotated with "
#~ "span information."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`AnnotateTarget "
#~ "<tvm.relay.transform.tvm.relay.transform.AnnotateTarget>`\\ "
#~ "\\(targets\\[\\, include\\_non\\_call\\_ops\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Annotate ops in an experession with "
#~ "a provied compiler/target and then use"
#~ " it for codegen."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BackwardFoldScaleAxis "
#~ "<tvm.relay.transform.tvm.relay.transform.BackwardFoldScaleAxis>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Backward fold axis scaling into weights of conv2d/dense."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`BatchingOps "
#~ "<tvm.relay.transform.tvm.relay.transform.BatchingOps>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Batching parallel operators into one for Conv2D, Dense and BatchMatmul."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CanonicalizeCast "
#~ "<tvm.relay.transform.tvm.relay.transform.CanonicalizeCast>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Canonicalize cast expressions to make operator fusion more efficient."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CanonicalizeOps "
#~ "<tvm.relay.transform.tvm.relay.transform.CanonicalizeOps>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Canonicalize special operators to basic operators."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CombineParallelBatchMatmul "
#~ "<tvm.relay.transform.tvm.relay.transform.CombineParallelBatchMatmul>`\\"
#~ " \\(\\[min\\_num\\_branches\\]\\)"
#~ msgstr ""

#~ msgid "Combine multiple batch matmul operators into one."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CombineParallelConv2D "
#~ "<tvm.relay.transform.tvm.relay.transform.CombineParallelConv2D>`\\ "
#~ "\\(\\[min\\_num\\_branches\\]\\)"
#~ msgstr ""

#~ msgid "Combine multiple conv2d operators into one."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`CombineParallelDense "
#~ "<tvm.relay.transform.tvm.relay.transform.CombineParallelDense>`\\ "
#~ "\\(\\[min\\_num\\_branches\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Combine multiple dense operators into one."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Conv2dToSparse "
#~ "<tvm.relay.transform.tvm.relay.transform.Conv2dToSparse>`\\ "
#~ "\\(weight\\_name\\, weight\\_shape\\, ...\\)"
#~ msgstr ""

#~ msgid "Rewrite qualified ```nn.conv2d operation``` to ```nn.sparse_conv2d```"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Conv2dToSparse2 "
#~ "<tvm.relay.transform.tvm.relay.transform.Conv2dToSparse2>`\\ "
#~ "\\(layout\\, kernel\\_size\\, ...\\)"
#~ msgstr ""

#~ msgid "Rewrite freezed ```nn.conv2d``` operation to ```nn.sparse_conv2d```"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ConvertLayout "
#~ "<tvm.relay.transform.tvm.relay.transform.ConvertLayout>`\\ "
#~ "\\(desired\\_layouts\\)"
#~ msgstr ""

#~ msgid ""
#~ "Given a dest layout, this pass "
#~ "transforms the expr such that most "
#~ "of the ops input data layout is"
#~ " changed to the dest layout."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DeadCodeElimination "
#~ "<tvm.relay.transform.tvm.relay.transform.DeadCodeElimination>`\\ "
#~ "\\(\\[inline\\_once\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Remove expressions that do not have any users (dead code)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Defunctionalization "
#~ "<tvm.relay.transform.tvm.relay.transform.Defunctionalization>`\\ "
#~ "\\(func\\, mod\\)"
#~ msgstr ""

#~ msgid ""
#~ "Performs defunctionalization on func, "
#~ "transforming func from a higher-order"
#~ " program to a first-order program."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DefuseOps "
#~ "<tvm.relay.transform.tvm.relay.transform.DefuseOps>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "The inverse operation of FuseOps."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DenseToSparse "
#~ "<tvm.relay.transform.tvm.relay.transform.DenseToSparse>`\\ "
#~ "\\(weight\\_name\\, weight\\_shape\\)"
#~ msgstr ""

#~ msgid ""
#~ "Rewrite qualified ```nn.dense operation``` to"
#~ " ```nn.sparse_dense``` This pass is used"
#~ " in ```data_dep_optimization.bsr_dense``` Parameters"
#~ " of this pass is generated by "
#~ "```analysis.sparse_dense.process_params```"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`DynamicToStatic "
#~ "<tvm.relay.transform.tvm.relay.transform.DynamicToStatic>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "If possible, convert tvm.relay.dynamic* ops to static versions"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`EliminateCommonSubexpr "
#~ "<tvm.relay.transform.tvm.relay.transform.EliminateCommonSubexpr>`\\"
#~ " \\(\\[fskip\\]\\)"
#~ msgstr ""

#~ msgid "Eliminate common subexpressions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`EtaExpand "
#~ "<tvm.relay.transform.tvm.relay.transform.EtaExpand>`\\ "
#~ "\\(\\[expand\\_constructor\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Add abstraction over a constructor or"
#~ " global variable bound to a function"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FakeQuantizationToInteger "
#~ "<tvm.relay.transform.tvm.relay.transform.FakeQuantizationToInteger>`\\"
#~ " \\(\\[hard\\_fail\\]\\)"
#~ msgstr ""

#~ msgid "Find regions of the graph of the form"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FastMath "
#~ "<tvm.relay.transform.tvm.relay.transform.FastMath>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Converts the expensive non linear "
#~ "functions to their fast but approximate"
#~ " counterparts."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FirstOrderGradient "
#~ "<tvm.relay.transform.tvm.relay.transform.FirstOrderGradient>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Transforms all global functions in the"
#~ " module to return the original "
#~ "result, paired with the gradients of "
#~ "the inputs."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FoldConstant "
#~ "<tvm.relay.transform.tvm.relay.transform.FoldConstant>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Fold the constant expressions in a Relay program."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FoldConstantExpr "
#~ "<tvm.relay.transform.tvm.relay.transform.FoldConstantExpr>`\\ "
#~ "\\(expr\\, mod\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FoldExplicitPadding "
#~ "<tvm.relay.transform.tvm.relay.transform.FoldExplicitPadding>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "FoldExplicitPadding finds explict padding "
#~ "before an op that can support "
#~ "implicit padding and fuses them."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FoldScaleAxis "
#~ "<tvm.relay.transform.tvm.relay.transform.FoldScaleAxis>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Fold the scaling of axis into weights of conv2d/dense."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ForwardFoldScaleAxis "
#~ "<tvm.relay.transform.tvm.relay.transform.ForwardFoldScaleAxis>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`FuseOps "
#~ "<tvm.relay.transform.tvm.relay.transform.FuseOps>`\\ "
#~ "\\(\\[fuse\\_opt\\_level\\]\\)"
#~ msgstr ""

#~ msgid "Fuse operators in an expr to a larger operator according to some rules."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InferType "
#~ "<tvm.relay.transform.tvm.relay.transform.InferType>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Infer the type of an expr."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`InferTypeLocal "
#~ "<tvm.relay.transform.tvm.relay.transform.InferTypeLocal>`\\ "
#~ "\\(expr\\)"
#~ msgstr ""

#~ msgid "Infer the type of a single expr, reusing type information to do so."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Inline "
#~ "<tvm.relay.transform.tvm.relay.transform.Inline>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Perform inlining on the given Relay IR module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LambdaLift "
#~ "<tvm.relay.transform.tvm.relay.transform.LambdaLift>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Lift the closure to global function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LazyGradientInit "
#~ "<tvm.relay.transform.tvm.relay.transform.LazyGradientInit>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Reduces memory usage of gradient tensors"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`Legalize "
#~ "<tvm.relay.transform.tvm.relay.transform.Legalize>`\\ "
#~ "\\(\\[legalize\\_map\\_attr\\_name\\]\\)"
#~ msgstr ""

#~ msgid "Legalizes an expression with another expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ManifestLifetimes "
#~ "<tvm.relay.transform.tvm.relay.transform.ManifestLifetimes>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Manifest the lifetimes of variables "
#~ "after allocations have been manifested, "
#~ "by inserting kill operations once "
#~ "variables become dead."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MergeCompilerRegions "
#~ "<tvm.relay.transform.tvm.relay.transform.MergeCompilerRegions>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Merge together compiler regions."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MergeComposite "
#~ "<tvm.relay.transform.tvm.relay.transform.MergeComposite>`\\ "
#~ "\\(pattern\\_table\\)"
#~ msgstr ""

#~ msgid "Merge multiple operators into a single composite relay function."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PartialEvaluate "
#~ "<tvm.relay.transform.tvm.relay.transform.PartialEvaluate>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Evaluate the static fragment of the code."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PartitionGraph "
#~ "<tvm.relay.transform.tvm.relay.transform.PartitionGraph>`\\ "
#~ "\\(\\[mod\\_name\\, bind\\_constants\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Partition a Relay program into regions"
#~ " that can be executed on different"
#~ " backends."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PlanDevices "
#~ "<tvm.relay.transform.tvm.relay.transform.PlanDevices>`\\ "
#~ "\\(config\\)"
#~ msgstr ""

#~ msgid ""
#~ "Uses existing \"on_device\" and "
#~ "\"device_copy\" calls to infer the "
#~ "virtual device on which every Relay "
#~ "sub-expression should run and the "
#~ "result stored."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RemoveUnusedFunctions "
#~ "<tvm.relay.transform.tvm.relay.transform.RemoveUnusedFunctions>`\\ "
#~ "\\(\\[entry\\_functions\\]\\)"
#~ msgstr ""

#~ msgid "Remove unused global relay functions in a relay module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SimplifyExpr "
#~ "<tvm.relay.transform.tvm.relay.transform.SimplifyExpr>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Simplify the Relay expression, including merging consecutive reshapes."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SimplifyFCTranspose "
#~ "<tvm.relay.transform.tvm.relay.transform.SimplifyFCTranspose>`\\ "
#~ "\\(target\\_weight\\_name\\)"
#~ msgstr ""

#~ msgid ""
#~ "Rewrite ```y = nn.dense(x, transpose(w, "
#~ "[1, 0]))``` to ```y = nn.dense(x, "
#~ "wt)``` This pass is used in "
#~ "```data_dep_optimization.simplify_fc_transpose```"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SimplifyInference "
#~ "<tvm.relay.transform.tvm.relay.transform.SimplifyInference>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Simplify the data-flow graph for inference phase."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SplitArgs "
#~ "<tvm.relay.transform.tvm.relay.transform.SplitArgs>`\\ "
#~ "\\(max\\_function\\_args\\)"
#~ msgstr ""

#~ msgid "Split function with huge number of arguments to smaller pieces."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToANormalForm "
#~ "<tvm.relay.transform.tvm.relay.transform.ToANormalForm>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Turn Graph Normal Form expression into A Normal Form Expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToANormalFormExpr "
#~ "<tvm.relay.transform.tvm.relay.transform.ToANormalFormExpr>`\\ "
#~ "\\(e\\)"
#~ msgstr ""

#~ msgid "ToANormalForm, but on expression level."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToBasicBlockNormalForm "
#~ "<tvm.relay.transform.tvm.relay.transform.ToBasicBlockNormalForm>`\\"
#~ " \\(\\)"
#~ msgstr ""

#~ msgid "Turn an expression to Basic Block Normal Form."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToCPS "
#~ "<tvm.relay.transform.tvm.relay.transform.ToCPS>`\\ \\(expr\\[\\,"
#~ " mod\\]\\)"
#~ msgstr ""

#~ msgid "Turn expression into continuation passing style(CPS)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToGraphNormalForm "
#~ "<tvm.relay.transform.tvm.relay.transform.ToGraphNormalForm>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Turn a Relay program in A Normal Form into Graph Normal Form"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ToMixedPrecision "
#~ "<tvm.relay.transform.tvm.relay.transform.ToMixedPrecision>`\\ "
#~ "\\(\\[mixed\\_precision\\_type\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Automatic mixed precision rewriter."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`build_config "
#~ "<tvm.relay.transform.tvm.relay.transform.build_config>`\\ "
#~ "\\(\\[opt\\_level\\, required\\_pass\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Configure the build behavior by setting config variables."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`function_pass "
#~ "<tvm.relay.transform.tvm.relay.transform.function_pass>`\\ "
#~ "\\(\\[pass\\_func\\, opt\\_level\\, name\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Decorate a function pass."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`gradient "
#~ "<tvm.relay.transform.tvm.relay.transform.gradient>`\\ "
#~ "\\(expr\\[\\, mod\\, mode\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Transform the input function, returning "
#~ "a function that calculate the original"
#~ " result, paired with gradient of the"
#~ " input."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`recast "
#~ "<tvm.relay.transform.tvm.relay.transform.recast>`\\ \\(expr\\,"
#~ " dtype\\, out\\_dtype\\[\\, ops\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Convert the types of operations in a graph to a new value."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`to_cps "
#~ "<tvm.relay.transform.tvm.relay.transform.to_cps>`\\ "
#~ "\\(func\\[\\, mod\\]\\)"
#~ msgstr ""

#~ msgid "Turn expression into CPS expression."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`un_cps "
#~ "<tvm.relay.transform.tvm.relay.transform.un_cps>`\\ \\(func\\)"
#~ msgstr ""

#~ msgid "Turn an cps function into a Function without the continuation argument."
#~ msgstr ""

#~ msgid "**Classes:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ChangeBatch "
#~ "<tvm.relay.transform.tvm.relay.transform.ChangeBatch>`\\ "
#~ "\\(data\\[\\, batch\\_size\\]\\)"
#~ msgstr ""

#~ msgid "Change the batch size."
#~ msgstr ""

#~ msgid "A pass that works on each tvm.relay.Function in a module."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LayoutConfig "
#~ "<tvm.relay.transform.tvm.relay.transform.LayoutConfig>`\\ "
#~ "\\(\\[skip\\_layers\\]\\)"
#~ msgstr ""

#~ msgid "A structure for customizing the ConvertLayout pass."
#~ msgstr ""

#~ msgid ""
#~ "Alternate the layouts of operators or"
#~ " replace primitive operators with other "
#~ "expressions. This pass can be used "
#~ "for computing convolution in custom "
#~ "layouts or other general weight pre-"
#~ "transformation."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that alters the layout of operators."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "**ret** -- The registered AnnotateSpans pass."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "The list of target compilers used for codegen."
#~ msgstr ""

#~ msgid ""
#~ "If True then non-call ops also "
#~ "will be annotated with targets If "
#~ "False then non-call ops will not"
#~ " be processed"
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The annotated pass that "
#~ "wrapps ops with subgraph_start and "
#~ "subgraph_end."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to backward fold expressions."
#~ msgstr ""

#~ msgid ""
#~ "It is recommended to call "
#~ "backward_fold_scale_axis before using "
#~ "forward_fold_scale_axis as backward folding "
#~ "targets the common conv->bn pattern."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The sequential pass which "
#~ "apply batching for different operator "
#~ "types."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that canonicalizes cast expression."
#~ msgstr ""

#~ msgid ""
#~ "Canonicalize special operators to basic "
#~ "operators. This can simplify followed "
#~ "analysis, e.g. expanding bias_add to "
#~ "expand_dims and broadcast_add."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass performing the canonicalization."
#~ msgstr ""

#~ msgid ""
#~ "A dictionary of all the params to"
#~ " change. The keys are all params, "
#~ "and the values are which dimension "
#~ "hold the batch."
#~ msgstr ""

#~ msgid "The batch size to change to."
#~ msgstr ""

#~ msgid "**pass** -- The pass."
#~ msgstr ""

#~ msgid "Combine multiple batch matmul operators into one. For example:"
#~ msgstr ""

#~ msgid "Would become:"
#~ msgstr ""

#~ msgid ""
#~ "The minimum number of required parallel"
#~ " branches for performing this optimization."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that combines parallel dense operators."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that combines parallel conv2d operators."
#~ msgstr ""

#~ msgid "Combine multiple dense operators into one. For example:"
#~ msgstr ""

#~ msgid "or (if to_batch=False)"
#~ msgstr ""

#~ msgid ""
#~ "If True, combine parallel dense ops "
#~ "into batch_matmul op. If False, combine"
#~ " parallel dense ops into dense op."
#~ msgstr ""

#~ msgid "Names of weights which qualified sparse contrains"
#~ msgstr ""

#~ msgid "Weights shape in BSR format."
#~ msgstr ""

#~ msgid "layout of data"
#~ msgstr ""

#~ msgid "**ret** -- The registered DenseToSparse pass."
#~ msgstr ""

#~ msgid "kernel size of conv2d"
#~ msgstr ""

#~ msgid ""
#~ "Given a dest layout, this pass "
#~ "transforms the expr such that most "
#~ "of the ops input data layout is"
#~ " changed to the dest layout. In "
#~ "ideal situation, there are only 2 "
#~ "layout transforms, one at the start "
#~ "and one at the end."
#~ msgstr ""

#~ msgid ""
#~ "This pass is not a part of "
#~ "relay.build and is expected to be "
#~ "called between framework-relay parser "
#~ "and relay.build call. This is very "
#~ "helpful for hardware backends that "
#~ "support/prefer only type of data layout."
#~ msgstr ""

#~ msgid "RFC - https://discuss.tvm.apache.org/t/layout-conversion-pass/4009"
#~ msgstr ""

#~ msgid ""
#~ "This pass uses most of the "
#~ "AlterOpLayout and InferCorrectLayout infrastructure."
#~ " We can define new layouts for "
#~ "conv2d ops for now. Most of the"
#~ " other operators try to adapt to "
#~ "their input layout using the "
#~ "InferCorrectLayout infrastructure."
#~ msgstr ""

#~ msgid ""
#~ "Specify a mapping of operator names "
#~ "to a list of layouts to convert"
#~ " to, in the order defined by "
#~ "the operator. An example for nn.conv2d"
#~ " could be: {\"nn.conv2d\", [\"NHWC\", "
#~ "\"OHWI]}, where the first item in "
#~ "the list specifies the data layout "
#~ "and the second specifies the kernel "
#~ "layout."
#~ msgstr ""

#~ msgid "Whether to inline a binding that is referenced exactly once."
#~ msgstr ""

#~ msgid "Whether to ignore possible side-effects in let-bound expressions."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "eliminates the dead code in a "
#~ "Relay program."
#~ msgstr ""

#~ msgid ""
#~ "At each call site, the function is"
#~ " cloned and type parameters are "
#~ "substituted in. Function arguments are "
#~ "encoded as datatypes and additional "
#~ "apply functions are used for "
#~ "application."
#~ msgstr ""

#~ msgid ""
#~ "The input function, which should not "
#~ "be polymorphic or be higher-order. "
#~ "This is because all types must be"
#~ " known and we can't encode function"
#~ " arguments to the program itself."
#~ msgstr ""

#~ msgid ""
#~ "The IRModule containing function and "
#~ "type definitions, which is also mutated"
#~ " during this pass."
#~ msgstr ""

#~ msgid "**expr** -- The output function."
#~ msgstr ""

#~ msgid ""
#~ "The inverse operation of FuseOps. It "
#~ "transforms a fused program returned by"
#~ " FuseOps into the program before "
#~ "FuseOps. (i.e., x == DefuseOps(FuseOps(x)))"
#~ msgstr ""

#~ msgid "**ret** -- The registered pass for operator defusion."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass for dynamic->static conversion."
#~ msgstr ""

#~ msgid ""
#~ "The callback function that decides "
#~ "whether an expression should be skipped."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that eliminates common subexpressions."
#~ msgstr ""

#~ msgid "Whether to expand constructors."
#~ msgstr ""

#~ msgid "Whether to expand global variables."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that eta expands an expression."
#~ msgstr ""

#~ msgid ""
#~ "where ``q == qnn.quantize`` and ``dq "
#~ "= qnn.dequantize`` and rewrite them into"
#~ " integer versions of ``op1`` and "
#~ "``op2``"
#~ msgstr ""

#~ msgid ""
#~ "Rules for rewriting indivdual ops are"
#~ " in fake_quantization_to_integer.py"
#~ msgstr ""

#~ msgid ""
#~ "How do deal with errors during "
#~ "graph rewriting. If true, raise an "
#~ "error. If false, skip rewriting the "
#~ "subgraph."
#~ msgstr ""

#~ msgid "**ret** -- The registered SimplifyExpr pass."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to perform fast math operations."
#~ msgstr ""

#~ msgid ""
#~ "Transforms all global functions in the"
#~ " module to return the original "
#~ "result, paired with the gradients of "
#~ "the inputs. This pass transforms each"
#~ " global function independently and does "
#~ "not support interprocedural AD. Additionally,"
#~ " this pass does not support any "
#~ "control-flow or references, and should"
#~ " only be used on pure data-flow"
#~ " graphs."
#~ msgstr ""

#~ msgid "**ret** -- The registered FirstOrderGradient pass."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass for constant folding."
#~ msgstr ""

#~ msgid ""
#~ "Fold the constant expressions in a "
#~ "Relay program. :param expr: The "
#~ "expression to fold :type expr: Expr "
#~ ":param mod: The module the expr "
#~ "lives in (for global calls) :type "
#~ "mod: IRModule"
#~ msgstr ""

#~ msgid "**new_expr** -- The expr after Constant Folding"
#~ msgstr ""

#~ msgid "**ret** -- The registered ImplicitPadding pass."
#~ msgstr ""

#~ msgid ""
#~ "Fold the scaling of axis into "
#~ "weights of conv2d/dense. This pass will"
#~ " invoke both forward and backward "
#~ "scale folding."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to fold expressions."
#~ msgstr ""

#~ msgid ""
#~ "Internally, we will call "
#~ "backward_fold_scale_axis before using "
#~ "forward_fold_scale_axis as backward folding "
#~ "targets the common conv->bn pattern."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to forward fold expressions."
#~ msgstr ""

#~ msgid ""
#~ "It is recommended to call "
#~ "backward_fold_scale_axis before using "
#~ "forward_fold_scale_axis, as backward folding "
#~ "targets the common conv->bn pattern."
#~ msgstr ""

#~ msgid ""
#~ "A pass that works on each "
#~ "tvm.relay.Function in a module. A "
#~ "function pass class should be created"
#~ " through `function_pass`."
#~ msgstr ""

#~ msgid ""
#~ "The level of fuse optimization. -1 "
#~ "indicates that the level will be "
#~ "inferred from pass context."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass for operator fusion."
#~ msgstr ""

#~ msgid "**ret** -- The registered type inference pass."
#~ msgstr ""

#~ msgid ""
#~ "This populates the checked_type field in"
#~ " expr. We assume existing type "
#~ "information in the graph is correct!"
#~ msgstr ""

#~ msgid "The expression we want to know the type of"
#~ msgstr ""

#~ msgid "**type** -- The type of the expression"
#~ msgstr ""

#~ msgid ""
#~ "Perform inlining on the given Relay "
#~ "IR module. The global functions that "
#~ "are marked as `inline` should be "
#~ "always inlined. A cost model will "
#~ "be needed in the future to decide"
#~ " if it is profitable to inline "
#~ "the function."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "performs inlining for a Relay IR "
#~ "module."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that lifts the lambda function."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- A pass which delays "
#~ "and/or reduces memory allocation, by "
#~ "lazily allocating 0 or one filled "
#~ "tensors."
#~ msgstr ""

#~ msgid ""
#~ "Legalizes an expression with another "
#~ "expression. This pass can be used "
#~ "to replace an expr with another "
#~ "expr for target dependent optimizations. "
#~ "For example, one expr, though "
#~ "semnatically equivalent to the other, "
#~ "can have better performance on a "
#~ "target. This pass can be used to"
#~ " legalize the expr in a target-"
#~ "dependent manner."
#~ msgstr ""

#~ msgid "The Op's attr name which corresponds to the legalize rule function."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that rewrites an expr."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that merges compiler regions."
#~ msgstr ""

#~ msgid ""
#~ "A list of (pattern_name, pattern, check)"
#~ " tuples. The order of the patterns"
#~ " in the list will determine the "
#~ "order of priority in which they "
#~ "are matched. 'check' is a function "
#~ "to check whether an extracted pattern"
#~ " matches. It can be implemented by"
#~ " pattern writer but if not specified"
#~ " it will always return True."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "merges operators into a single composite"
#~ " relay function."
#~ msgstr ""

#~ msgid ""
#~ "This transformation could be either "
#~ "`Module -> Module` or `Expr -> "
#~ "Expr`. It will directly transform the"
#~ " input expression to a new one "
#~ "if the target expression is provided."
#~ " Otherwise, it will rely on the "
#~ "pass manager to carry out "
#~ "transformation."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "performs partial evaluation on an "
#~ "expression."
#~ msgstr ""

#~ msgid ""
#~ "Controls the prefix of the name of"
#~ " each partitioned subraph. If `mod_name`"
#~ " is None, then `tvmgen_` prefix is"
#~ " used. Otherwise, `tvmgen_mod_name_` prefix "
#~ "is used."
#~ msgstr ""

#~ msgid ""
#~ "Whether or not to bind constants "
#~ "in partitioned subgraphs. Note that the"
#~ " codegen needs to maintain the bound"
#~ " constants; Otherwise the constants will"
#~ " be maintained by the metadata "
#~ "module. So it is recommended for "
#~ "C-source based codegens to set "
#~ "bind_constants=False to avoid embedding large"
#~ " constants in a C source file."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass that partitions the Relay program."
#~ msgstr ""

#~ msgid ""
#~ "Uses existing \"on_device\" and "
#~ "\"device_copy\" calls to infer the "
#~ "virtual device on which every Relay "
#~ "sub-expression should run and the "
#~ "result stored. Captures the result of"
#~ " that analysis using new \"on_device\" "
#~ "and \"device_copy\" calls. Sub-expressions "
#~ "which are not otherwise constrained are"
#~ " assigned to the default primitive "
#~ "virtual device describe by config. "
#~ "However data and computations which must"
#~ " be hosted on a CPU (such as"
#~ " shapes and shape functions) use the"
#~ " host virtual device of the config."
#~ msgstr ""

#~ msgid ""
#~ "The compilation configuration, specifying "
#~ "available targets and default devices."
#~ msgstr ""

#~ msgid "**ret** -- The pass."
#~ msgstr ""

#~ msgid "The set of entry functions to start from."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to remove unused functions."
#~ msgstr ""

#~ msgid ""
#~ "Names of weights which qualified ```y"
#~ " = nn.dense(x, transpose(w, [1, 0]))``` "
#~ "This parameter is generated by "
#~ "```analysis.search_fc_transpose``` function"
#~ msgstr ""

#~ msgid "**ret** -- The registered SimplifyFCTranspose pass."
#~ msgstr ""

#~ msgid ""
#~ "Simplify the data-flow graph for "
#~ "inference phase. An simplified expression "
#~ "which is semantically equal to the "
#~ "input expression will be returned."
#~ msgstr ""

#~ msgid ""
#~ "Note that batch norms will only be"
#~ " simplified if their result is "
#~ "indexed at tuple index 0."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass to perform operator simplification."
#~ msgstr ""

#~ msgid ""
#~ "Turn Graph Normal Form expression into"
#~ " A Normal Form Expression. The scope"
#~ " of the root expression is the "
#~ "global scope. The scope of any non"
#~ " root expression is the least common"
#~ " ancestor of all it's scope. Values"
#~ " are ordered by post-DFS order "
#~ "in each scope."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "transforms an expression into A Normal"
#~ " Form."
#~ msgstr ""

#~ msgid "The graph expression."
#~ msgstr ""

#~ msgid "**ret** -- The transformed expresion."
#~ msgstr ""

#~ msgid ""
#~ "Turn an expression to Basic Block "
#~ "Normal Form. We define a block as"
#~ " a group of expressions implied by"
#~ " the scope structure. Each graph node"
#~ " can only belong to a single "
#~ "block. For any value that is being"
#~ " used in multiple blocks, it has "
#~ "to be referred by a Var which "
#~ "is defined in a block, whose scope"
#~ " is the least common ancestor of "
#~ "blocks this value is used."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "transforms an expression into Basic "
#~ "Block Normal Form."
#~ msgstr ""

#~ msgid "Every intermediate compute will be passed to a continuation."
#~ msgstr ""

#~ msgid ""
#~ "**result** -- The registered pass that"
#~ " transforms an expression into CPS."
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The registered pass that "
#~ "transforms an expression into Graph "
#~ "Normal Form."
#~ msgstr ""

#~ msgid ""
#~ "Automatic mixed precision rewriter. Rewrite"
#~ " an FP32 relay graph into a "
#~ "version where as many operations as "
#~ "possible are in the target "
#~ "mixed_precision_type."
#~ msgstr ""

#~ msgid "The target datatype to transform operations in the graph to use."
#~ msgstr ""

#~ msgid ""
#~ "Determines how to handle ops not "
#~ "registered with FTVMMixedPrecisionConversionType   "
#~ "0: Does not allow any missing ops."
#~ " Will throw errors when encountering "
#~ "any.   1: Allow missing ops but "
#~ "emit warnings.   2: Allow missing ops"
#~ " and silently ignore them."
#~ msgstr ""

#~ msgid ""
#~ "Determines how to handle ops not "
#~ "registered with FTVMMixedPrecisionConversionType"
#~ msgstr ""

#~ msgid ""
#~ "0: Does not allow any missing ops."
#~ " Will throw errors when encountering "
#~ "any. 1: Allow missing ops but emit"
#~ " warnings. 2: Allow missing ops and"
#~ " silently ignore them."
#~ msgstr ""

#~ msgid "**ret** -- The registered pass."
#~ msgstr ""

#~ msgid ""
#~ "Configure the build behavior by setting"
#~ " config variables. This function will "
#~ "be deprecated in TVM v0.7. Instead, "
#~ "we should directly use "
#~ "tvm.transform.PassContext."
#~ msgstr ""

#~ msgid ""
#~ "Optimization level. The optimization pass "
#~ "name and level are as the "
#~ "following:  .. code-block:: python      "
#~ "OPT_PASS_LEVEL = {         \"SimplifyInference\":"
#~ " 0,         \"OpFusion\": 1,         "
#~ "\"FoldConstant\": 2,         \"FoldScaleAxis\": 3,"
#~ "         \"AlterOpLayout\": 3,         "
#~ "\"CanonicalizeOps\": 3,         \"CanonicalizeCast\": "
#~ "3,         \"EliminateCommonSubexpr\": 3,         "
#~ "\"CombineParallelConv2D\": 4,         "
#~ "\"CombineParallelDense\": 4,         "
#~ "\"CombineParallelBatchMatmul\": 4,         \"FastMath\":"
#~ " 4     }"
#~ msgstr ""

#~ msgid ""
#~ "Optimization level. The optimization pass "
#~ "name and level are as the "
#~ "following:"
#~ msgstr ""

#~ msgid "Optimization passes that are required regardless of optimization level."
#~ msgstr ""

#~ msgid "Optimization passes to be disabled during optimization."
#~ msgstr ""

#~ msgid "A tracing function for debugging or introspection."
#~ msgstr ""

#~ msgid "**pass_context** -- The pass context for optimizations."
#~ msgstr ""

#~ msgid ""
#~ "This function returns a callback when"
#~ " pass_func is provided. Otherwise, it "
#~ "returns the created function pass using"
#~ " the given optimization function."
#~ msgstr ""

#~ msgid "The transformation function or class."
#~ msgstr ""

#~ msgid "The optimization level of this module pass."
#~ msgstr ""

#~ msgid ""
#~ "The name of the function pass. The"
#~ " name could be empty. In this "
#~ "case, the name of the optimization "
#~ "function will be used as the pass"
#~ " name."
#~ msgstr ""

#~ msgid "The list of passes that the module pass is dependent on."
#~ msgstr ""

#~ msgid ""
#~ "**create_function_pass** -- A decorator will"
#~ " be returned if pass_func is not "
#~ "provided, otherwise return the decorated "
#~ "result. The returned decorator has two"
#~ " behaviors depending on the input: A"
#~ " new FunctionPass will be returned "
#~ "when we decorate a pass function. "
#~ "A new FunctionPass class will be "
#~ "returned when we decorate a class "
#~ "type."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "The following code block decorates a function pass class."
#~ msgstr ""

#~ msgid ""
#~ "The following code creates a function"
#~ " pass by decorating a user defined"
#~ " transform function."
#~ msgstr ""

#~ msgid "The input expression, which is a Function or a GlobalVar."
#~ msgstr ""

#~ msgid ""
#~ "The mode of the automatic "
#~ "differentiation algorithm. 'first_order' only "
#~ "works on first order code, but "
#~ "will not produce reference nor closure."
#~ " 'higher_order' works on all code "
#~ "using reference and closure."
#~ msgstr ""

#~ msgid "**expr** -- The transformed expression."
#~ msgstr ""

#~ msgid ""
#~ "Convert the types of operations in "
#~ "a graph to a new value. Note "
#~ "that this is primarily useful for "
#~ "testing performance of individual operations"
#~ " at the new datatype. In a real"
#~ " setting, this pass will almost "
#~ "certainly do a poor job converting "
#~ "from one datatype to another as it"
#~ " just applies hard casting. For "
#~ "example, when recasting from float to"
#~ " integer, many small values will "
#~ "simply be set to 0. Although this"
#~ " will allow autotuning and benchmarking "
#~ "to produce proper timings at the "
#~ "new data type, the output of the"
#~ " model will of course be heavily "
#~ "impacted."
#~ msgstr ""

#~ msgid "The original function that will have its type changed."
#~ msgstr ""

#~ msgid "The target type to cast to."
#~ msgstr ""

#~ msgid "The output type to cast to."
#~ msgstr ""

#~ msgid ""
#~ "A list of operations that should "
#~ "have their type changed, others will "
#~ "be left as is."
#~ msgstr ""

#~ msgid ""
#~ "A list of integers indicating operations"
#~ " that should not have their type "
#~ "changed, counted starting with the first"
#~ " valid operation encountered. Negative "
#~ "indices are allowed and indicate "
#~ "starting at the last layer."
#~ msgstr ""

#~ msgid "**output_expr** -- The graph after recasting to the specified datatype."
#~ msgstr ""

#~ msgid "The input function."
#~ msgstr ""

#~ msgid "The global module."
#~ msgstr ""

#~ msgid "**result** -- The output function."
#~ msgstr ""

#~ msgid "Note that this will not give the exact same interface as before cps:"
#~ msgstr ""

#~ msgid "If the input/output is higher order, they will still be in cps form."
#~ msgstr ""

#~ msgid "The input function"
#~ msgstr ""

#~ msgid "**result** -- The output function"
#~ msgstr ""

