# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-31 18:33+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../docs/reference/api/python/relay/frontend.rst:20
msgid "tvm.relay.frontend"
msgstr ""

#~ msgid "Frontends for constructing Relay programs."
#~ msgstr "用于构建 Relay 程序的前端。"

#~ msgid "Contains the model importers currently defined for Relay."
#~ msgstr "包含当前为 Relay 定义的模型导入器。"

#~ msgid "**Classes:**"
#~ msgstr "**类**："

#~ msgid ""
#~ ":py:obj:`ChangeDatatype <tvm.relay.frontend.ChangeDatatype>`\\"
#~ " \\(src\\, dst\\)"
#~ msgstr ""

#~ msgid "Mutator for changing the datatype of Relay programs."
#~ msgstr "改变 Relay 程序的数据类型的变体。"

#~ msgid "**Functions:**"
#~ msgstr "**函数**："

#~ msgid ""
#~ ":py:obj:`from_caffe <tvm.relay.frontend.from_caffe>`\\ "
#~ "\\(init\\_net\\, predict\\_net\\, ...\\)"
#~ msgstr ""

#~ msgid "Convert from caffe model into compatible relay Function."
#~ msgstr "将 caffe 模型转换为兼容的 relay  函数。"

#~ msgid ""
#~ ":py:obj:`from_caffe2 <tvm.relay.frontend.from_caffe2>`\\ "
#~ "\\(init\\_net\\, predict\\_net\\[\\, shape\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Load caffe2 graph which contains "
#~ "init_net and predict_net into Relay "
#~ "Function."
#~ msgstr "将包含 init_net 和 predict_net 的 caffe2 图加载到 Relay Function 中。"

#~ msgid ""
#~ ":py:obj:`from_coreml <tvm.relay.frontend.from_coreml>`\\ "
#~ "\\(model\\[\\, shape\\]\\)"
#~ msgstr ""

#~ msgid "Convert from coreml model into Relay Function."
#~ msgstr "将 coreml 模型转换为 Relay Function。"

#~ msgid ""
#~ ":py:obj:`from_darknet <tvm.relay.frontend.from_darknet>`\\ "
#~ "\\(net\\[\\, shape\\, dtype\\]\\)"
#~ msgstr ""

#~ msgid "Convert from Darknet's model into compatible relay Function."
#~ msgstr "将 Darknet 模型转换为兼容的 Relay  函数。"

#~ msgid ""
#~ ":py:obj:`from_keras <tvm.relay.frontend.from_keras>`\\ "
#~ "\\(model\\[\\, shape\\, layout\\]\\)"
#~ msgstr ""

#~ msgid "Convert keras model to relay Function."
#~ msgstr "将 keras 模型转换为 Relay Function。"

#~ msgid ""
#~ ":py:obj:`from_mxnet <tvm.relay.frontend.from_mxnet>`\\ "
#~ "\\(symbol\\[\\, shape\\, dtype\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Convert from MXNet\"s model into compatible relay Function."
#~ msgstr "将 MXNet 模型转换为兼容的 Relay  函数。"

#~ msgid ""
#~ ":py:obj:`from_onnx <tvm.relay.frontend.from_onnx>`\\ "
#~ "\\(model\\[\\, shape\\, dtype\\, opset\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Convert a ONNX model into an equivalent Relay Function."
#~ msgstr "将 ONNX 模型转换为等效的 Relay Function。"

#~ msgid ""
#~ ":py:obj:`from_paddle <tvm.relay.frontend.from_paddle>`\\ "
#~ "\\(program\\_or\\_layer\\[\\, shape\\_dict\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Convert a PaddlePaddle model into an equivalent Relay Function."
#~ msgstr "将PaddlePaddle 模型转换成等效的 Relay Function。"

#~ msgid ""
#~ ":py:obj:`from_pytorch <tvm.relay.frontend.from_pytorch>`\\ "
#~ "\\(script\\_module\\, input\\_infos\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Load PyTorch model in the form of"
#~ " a scripted PyTorch model and convert"
#~ " into relay."
#~ msgstr "以脚本化 PyTorch 模型的形式加载 PyTorch 模型，并转换为 relay。"

#~ msgid ""
#~ ":py:obj:`from_tensorflow <tvm.relay.frontend.from_tensorflow>`\\"
#~ " \\(graph\\[\\, layout\\, shape\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Load tensorflow graph which is a "
#~ "python tensorflow graph object into "
#~ "relay."
#~ msgstr "将 python tensorflow 计算图对象加载到 relay 中。"

#~ msgid ""
#~ ":py:obj:`from_tflite <tvm.relay.frontend.from_tflite>`\\ "
#~ "\\(model\\[\\, shape\\_dict\\, dtype\\_dict\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid "Convert from tflite model into compatible relay Function."
#~ msgstr "从 tflite 模型转换成兼容的 relay Function。"

#~ msgid ""
#~ ":py:obj:`quantize_conv_bias_mkldnn_from_var "
#~ "<tvm.relay.frontend.quantize_conv_bias_mkldnn_from_var>`\\ "
#~ "\\(bias\\_var\\, ...\\)"
#~ msgstr ""

#~ msgid "Quantized conv2d bias"
#~ msgstr "量化 conv2d bias"

#~ msgid ""
#~ "This pass should be useful for "
#~ "users of the Bring Your Own "
#~ "Datatypes framework. TODO(@gussmith23 "
#~ "@hypercubestart) Add link to documentation "
#~ "when it exists"
#~ msgstr ""
#~ "此传递对于“带上你自己的数据类型”框架的用户应该很有用。TODO(@gussmith23 @hypercubestart)"
#~ " 当文档存在时，添加链接到文档"

#~ msgid "Example:"
#~ msgstr "示例："

#~ msgid "参数"
#~ msgstr ""

#~ msgid ""
#~ "The source datatype name, e.g. \"float\""
#~ " or \"posites2\" (but not \"float32\" "
#~ "or \"custom[posites2]32\")."
#~ msgstr ""
#~ "源码的数据类型名称，例如 \"float\" 或 \"posites2\" （但不是 "
#~ "\"float32\" 或 \"custom[posites2]32\"）。"

#~ msgid "The destination datatype name, in the same format."
#~ msgstr "目标数据的类型名称，格式相同。"

#~ msgid "返回"
#~ msgstr ""

#~ msgid ""
#~ "**mod** -- Module where all nodes "
#~ "of dtype `src` have been changed "
#~ "to have dtype `dst`."
#~ msgstr "**mod** —— 所有 dtype 为 `src` 的节点都被更改为 dtype `dst` 的模块。"

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid "caffemodel"
#~ msgstr ""

#~ msgid "caffe prototxt"
#~ msgstr ""

#~ msgid "Input shapes of the model."
#~ msgstr ""

#~ msgid "Input types of the model."
#~ msgstr ""

#~ msgid ""
#~ "* **mod** (*tvm.IRModule*) -- The relay"
#~ " module for compilation. * **params** "
#~ "(*dict of str to tvm.NDArray*) -- "
#~ "The parameter dict to be used by"
#~ " relay"
#~ msgstr ""

#~ msgid "**mod** (*tvm.IRModule*) -- The relay module for compilation."
#~ msgstr ""

#~ msgid ""
#~ "**params** (*dict of str to "
#~ "tvm.NDArray*) -- The parameter dict to"
#~ " be used by relay"
#~ msgstr ""

#~ msgid "Caffe2 NetDef containing the weights"
#~ msgstr ""

#~ msgid "Caffe2 NetDef containing the graph"
#~ msgstr ""

#~ msgid "The input shape to the graph"
#~ msgstr ""

#~ msgid "The input types to the graph"
#~ msgstr ""

#~ msgid ""
#~ "* **mod** (*tvm.IRModule*) -- The module"
#~ " that optimizations will be performed "
#~ "on. * **params** (*dict of str to"
#~ " tvm.nd.NDArray*) -- Dict of converted "
#~ "parameters stored in tvm.nd.NDArray format"
#~ msgstr ""

#~ msgid ""
#~ "**mod** (*tvm.IRModule*) -- The module "
#~ "that optimizations will be performed on."
#~ msgstr ""

#~ msgid ""
#~ "**params** (*dict of str to "
#~ "tvm.nd.NDArray*) -- Dict of converted "
#~ "parameters stored in tvm.nd.NDArray format"
#~ msgstr ""

#~ msgid "coremltools.models.MLModel of a NeuralNetworkClassifier"
#~ msgstr ""

#~ msgid "The input shapes"
#~ msgstr ""

#~ msgid ""
#~ "* **mod** (*tvm.IRModule*) -- The relay"
#~ " module for compilation. * **params** "
#~ "(*dict of str to tvm.nd.NDArray*) -- "
#~ "The parameter dict to be used by"
#~ " Relay."
#~ msgstr ""

#~ msgid ""
#~ "**params** (*dict of str to "
#~ "tvm.nd.NDArray*) -- The parameter dict "
#~ "to be used by Relay."
#~ msgstr ""

#~ msgid "Darknet net structure."
#~ msgstr ""

#~ msgid ""
#~ "* **mod** (*tvm.IRModule*) -- The relay"
#~ " module for compilation. * **params** "
#~ "(*dict of str to tvm.nd.NDArray*) -- "
#~ "The parameter dict to be used by"
#~ " relay"
#~ msgstr ""

#~ msgid ""
#~ "**params** (*dict of str to "
#~ "tvm.nd.NDArray*) -- The parameter dict "
#~ "to be used by relay"
#~ msgstr ""

#~ msgid "The keras model to be converted."
#~ msgstr ""

#~ msgid "Input shapes of the model, optional"
#~ msgstr ""

#~ msgid ""
#~ "One of 'NCHW' or 'NHWC', indicates "
#~ "how data should be arranged in the"
#~ " output model. Default layout is "
#~ "'NCHW' as it in general performs "
#~ "better across TVM."
#~ msgstr ""

#~ msgid "MXNet symbol."
#~ msgstr ""

#~ msgid "The argument parameters in mxnet"
#~ msgstr ""

#~ msgid "The auxiliary parameters in mxnet"
#~ msgstr ""

#~ msgid ""
#~ "* **mod** (*tvm.IRModule*) -- The relay"
#~ " module for compilation * **params** "
#~ "(*dict of str to tvm.nd.NDArray*) -- "
#~ "The parameter dict to be used by"
#~ " nnvm"
#~ msgstr ""

#~ msgid "**mod** (*tvm.IRModule*) -- The relay module for compilation"
#~ msgstr ""

#~ msgid ""
#~ "**params** (*dict of str to "
#~ "tvm.nd.NDArray*) -- The parameter dict "
#~ "to be used by nnvm"
#~ msgstr ""

#~ msgid ""
#~ "ONNX graphs are represented as Python"
#~ " Protobuf objects. The companion parameters"
#~ " will be handled automatically. However,"
#~ " the input names from onnx graph "
#~ "is vague, mixing inputs and network "
#~ "weights/bias such as \"1\", \"2\"... For"
#~ " convenience, we rename the `real` "
#~ "input names to \"input_0\", \"input_1\"... "
#~ "And renaming parameters to \"param_0\", "
#~ "\"param_1\"..."
#~ msgstr ""

#~ msgid ""
#~ "By default, ONNX defines models in "
#~ "terms of dynamic shapes. The ONNX "
#~ "importer retains that dynamism upon "
#~ "import, and the compiler attempts to "
#~ "convert the model into a static "
#~ "shapes at compile time. If this "
#~ "fails, there may still be dynamic "
#~ "operations in the model. Not all "
#~ "TVM kernels currently support dynamic "
#~ "shapes, please file an issue on "
#~ "discuss.tvm.apache.org if you hit an "
#~ "error with dynamic kernels."
#~ msgstr ""

#~ msgid "ONNX ModelProto after ONNX v1.1.0"
#~ msgstr ""

#~ msgid "Override to autodetected opset. This can be helpful for some testing."
#~ msgstr ""

#~ msgid ""
#~ "If this parameter is true, the "
#~ "importer will take any provided onnx "
#~ "input values (weights, shapes, etc) and"
#~ " embed them into the relay model "
#~ "as Constants instead of variables. This"
#~ " allows more aggressive optimizations at"
#~ " compile time and helps in making "
#~ "models static if certain inputs "
#~ "represent attributes relay would traditionally"
#~ " consider compile-time constants."
#~ msgstr ""

#~ msgid ""
#~ "Default config:     use_nt_batch_matmul : bool"
#~ " = True         True to convert "
#~ "qualified onnx `matmul` to `nn.batch_matmul`"
#~ " strict to NT format         "
#~ "(transpose_a=False, transpose_b=True)."
#~ msgstr ""

#~ msgid "Default config:"
#~ msgstr ""

#~ msgid "use_nt_batch_matmul"
#~ msgstr ""

#~ msgid "bool = True"
#~ msgstr ""

#~ msgid ""
#~ "True to convert qualified onnx `matmul`"
#~ " to `nn.batch_matmul` strict to NT "
#~ "format (transpose_a=False, transpose_b=True)."
#~ msgstr ""

#~ msgid ""
#~ "* **mod** (*tvm.IRModule*) -- The relay"
#~ " module for compilation * **params** "
#~ "(*dict of str to tvm.nd.NDArray*) -- "
#~ "The parameter dict to be used by"
#~ " relay"
#~ msgstr ""

#~ msgid ""
#~ "Convert a PaddlePaddle model into an "
#~ "equivalent Relay Function. PaddlePaddle "
#~ "Program/TranslatedLayer represent the computation"
#~ " graph of PaddlePaddle model, and "
#~ "PaddlePaddle scope stores all the "
#~ "weights of PaddlePaddle model."
#~ msgstr ""

#~ msgid ""
#~ "Loaded model by `paddle.static.load_inference_model`"
#~ " or `paddle.jit.load`"
#~ msgstr ""

#~ msgid "The input shape of model"
#~ msgstr ""

#~ msgid ""
#~ "The scope that saves all the "
#~ "weights of model, use "
#~ "`paddle.static.global_scope` by default"
#~ msgstr ""

#~ msgid ""
#~ "* **mod** (*tvm.IRModule*) -- The relay"
#~ " module for compilation * **params** "
#~ "(*dict of str to tvm.nd.NDArray*)"
#~ msgstr ""

#~ msgid "**params** (*dict of str to tvm.nd.NDArray*)"
#~ msgstr ""

#~ msgid ""
#~ "Load PyTorch model in the form of"
#~ " a scripted PyTorch model and convert"
#~ " into relay. The companion parameters "
#~ "will be handled automatically."
#~ msgstr ""

#~ msgid ""
#~ "TorchScripted PyTorch graph Note: We "
#~ "currently only support traces (ie: "
#~ "torch.jit.trace(model, input))"
#~ msgstr ""

#~ msgid ""
#~ "Can be (input name, input shape) "
#~ "or (input name, (input shape, input "
#~ "types)) Graph level input shape and "
#~ "type list The same input names "
#~ "need to be used for deployment, so"
#~ " choose easy to remember names (such"
#~ " as: input0, input1) e.g. [('input0', "
#~ "(1, 2)), ('input1', (3, 4))] or "
#~ "[('input0', ((1, 2), 'int')), ('input1', "
#~ "((3, 4), 'float'))]"
#~ msgstr ""

#~ msgid "A custom op conversion map in the same format as _convert_map above"
#~ msgstr ""

#~ msgid ""
#~ "The default dtype to use when type"
#~ " information is not provided by "
#~ "PyTorch."
#~ msgstr ""

#~ msgid ""
#~ "When True, replace '.' with `_' in"
#~ " a original parameter name. The Relay"
#~ " text parser treats a variable name"
#~ " followed by a period as a "
#~ "tuple element access, so a variable "
#~ "name like \"dense.weight\" cannot be "
#~ "parsed correctly. Use this option when"
#~ " you want to run the AnnotateSpans"
#~ " pass on the imported module."
#~ msgstr ""

#~ msgid ""
#~ "Return quantized weights and bias, "
#~ "rather than float ones. PyTorch stores"
#~ " quantized weights in a custom "
#~ "format, so we cannot directly access "
#~ "8 bit weights as Numpy arrays. We"
#~ " use a PyTorch function to unpack "
#~ "quantized weights into float32 arrays "
#~ "and quantization parameters. By default, "
#~ "we return float32 weights and rely "
#~ "on the QNN lowering and the Relay"
#~ " constant folding pass to quantize "
#~ "weights at compile time. In BYOC "
#~ "use cases, however, we cannot apply "
#~ "the constant folding pass on a QNN"
#~ " graph. If keep_quantized_weight is True,"
#~ " we quantize weights in the frontend"
#~ " using a function that is equivalent"
#~ " to qnn.op.quantize(...) operating on Numpy"
#~ " arrays."
#~ msgstr ""

#~ msgid ""
#~ "* **mod** (*tvm.IRModule*) -- The module"
#~ " that optimizations will be performed "
#~ "on. * **params** (*dict of str to"
#~ " tvm.runtime.NDArray*) -- Dict of converted"
#~ " parameters stored in tvm.runtime.ndarray "
#~ "format"
#~ msgstr ""

#~ msgid ""
#~ "**params** (*dict of str to "
#~ "tvm.runtime.NDArray*) -- Dict of converted "
#~ "parameters stored in tvm.runtime.ndarray "
#~ "format"
#~ msgstr ""

#~ msgid ""
#~ "Load tensorflow graph which is a "
#~ "python tensorflow graph object into "
#~ "relay. The companion parameters will be"
#~ " handled automatically."
#~ msgstr ""

#~ msgid "Tensorflow GraphDef"
#~ msgstr ""

#~ msgid "NCHW only supported now to enable NHWC models on GPU."
#~ msgstr ""

#~ msgid "Graph level input shape dictionary."
#~ msgstr ""

#~ msgid "if not specified then the last node is assumed as graph output."
#~ msgstr ""

#~ msgid ""
#~ "Default config:     use_dense : bool ="
#~ " True         Ture to convert `tf.matmul`"
#~ " to `nn.dense`, else to `nn.matmul`."
#~ "         The `nn.dense` op requires the "
#~ "data tensor to be non-transposed "
#~ "and weight tensor         to be "
#~ "transposed, may insert extra `transpose` "
#~ "to the original graph.     use_nt_batch_matmul"
#~ " : bool = True         True to "
#~ "convert `tf.batch_matmul` to `nn.batch_matmul` "
#~ "strict to NT format         "
#~ "(transpose_a=False, transpose_b=True)."
#~ msgstr ""

#~ msgid "use_dense"
#~ msgstr ""

#~ msgid ""
#~ "Ture to convert `tf.matmul` to "
#~ "`nn.dense`, else to `nn.matmul`. The "
#~ "`nn.dense` op requires the data tensor"
#~ " to be non-transposed and weight "
#~ "tensor to be transposed, may insert "
#~ "extra `transpose` to the original graph."
#~ msgstr ""

#~ msgid ""
#~ "True to convert `tf.batch_matmul` to "
#~ "`nn.batch_matmul` strict to NT format "
#~ "(transpose_a=False, transpose_b=True)."
#~ msgstr ""

#~ msgid "tflite.Model or tflite.Model.Model (depending on tflite version)"
#~ msgstr ""

