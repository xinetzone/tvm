# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the tvm package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2024.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tvm doc\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2024-10-09 21:52+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../../doc/docs/faq.rst:20
msgid "Frequently Asked Questions"
msgstr "常见问题解答"

#: ../../doc/docs/faq.rst:24
msgid "How to Install"
msgstr "如何安装"

#: ../../doc/docs/faq.rst:25
msgid "See :ref:`installation`."
msgstr "查阅 :ref:`installation`。"

#: ../../doc/docs/faq.rst:29
msgid "How to add a new Hardware Backend"
msgstr "如何添加新的硬件后端"

#: ../../doc/docs/faq.rst:31
msgid ""
"If the hardware backend has LLVM support, then we can directly generate "
"the code by setting the correct target triple as in "
":py:mod:`~tvm.target`."
msgstr ""
"如果硬件后端支持 LLVM，那么可以通过设置正确的目标三元组来直接生成代码，如 :py:mod:`~tvm.target` 中所示。"

#: ../../doc/docs/faq.rst:33
msgid ""
"If the target hardware is a GPU, try to use the cuda, opencl or vulkan "
"backend."
msgstr ""
"如果目标硬件是 GPU，尝试使用 cuda、opencl 或 vulkan 后端。"

#: ../../doc/docs/faq.rst:34
msgid ""
"If the target hardware is a special accelerator, checkout :ref:`vta-"
"index` and :ref:`relay-bring-your-own-codegen`."
msgstr ""
"如果目标硬件是专用加速器，请查阅 :ref:`vta-index` 和 :ref:`relay-bring-your-own-codegen`。"

#: ../../doc/docs/faq.rst:36
msgid ""
"For all of the above cases, You may want to add target specific "
"optimization templates using AutoTVM, see :ref:`tutorials-autotvm-sec`."
msgstr ""
"对于上述所有情况，您可能希望使用 AutoTVM 添加特定于目标的优化模板，参见：:ref:`tutorials-autotvm-sec`。"

#: ../../doc/docs/faq.rst:38
msgid ""
"Besides using LLVM's vectorization, we can also embed micro-kernels to "
"leverage hardware intrinsics, see :ref:`tutorials-tensorize`."
msgstr ""
"除了使用 LLVM 的矢量化功能外，还可以嵌入微内核以利用硬件内部函数，参见：:ref:`tutorials-tensorize`。"

#: ../../doc/docs/faq.rst:43
msgid "TVM's relation to Other IR/DSL Projects"
msgstr "TVM与其他IR/DSL项目的关系"

#: ../../doc/docs/faq.rst:44
msgid ""
"There are usually two levels of abstractions of IR in the deep learning "
"systems. TensorFlow's XLA and Intel's ngraph both use a computation graph"
" representation. This representation is high level, and can be helpful to"
" perform generic optimizations such as memory reuse, layout "
"transformation and automatic differentiation."
msgstr ""
"深度学习系统中的 IR（中间表示）通常具有两个层次的抽象。"
"TensorFlow 的 XLA 和 Intel 的 nGraph 都采用了计算图的表示方式。"
"这种表示方式处于较高层次，有助于执行通用的优化措施，例如内存重用、布局变换以及自动微分。"

#: ../../doc/docs/faq.rst:49
msgid ""
"TVM adopts a low-level representation, that explicitly express the choice"
" of memory layout, parallelization pattern, locality and hardware "
"primitives etc. This level of IR is closer to directly target hardwares. "
"The low-level IR adopts ideas from existing image processing languages "
"like Halide, darkroom and loop transformation tools like loopy and "
"polyhedra-based analysis. We specifically focus on expressing deep "
"learning workloads (e.g. recurrence), optimization for different hardware"
" backends and embedding with frameworks to provide end-to-end compilation"
" stack."
msgstr ""
"TVM 采用了一种低层次的表示方法，它明确地表达了内存布局的选择、并行化模式、局部性以及硬件原语等。"
"这种层次的 IR 更接近于直接针对硬件。低层次的 IR 借鉴了现有的图像处理语言（如 Halide、Darkroom）和循环转换工具（如 Loopy 和基于多面体的分析工具）的思想。"
"特别关注于表达深度学习工作负载（例如递归）、针对不同硬件后端的优化，以及与框架的集成，以提供端到端的编译堆栈。"

#: ../../doc/docs/faq.rst:60
msgid "TVM's relation to libDNN, cuDNN"
msgstr "TVM 与 libDNN、cuDNN 的关系"

#: ../../doc/docs/faq.rst:61
msgid ""
"TVM can incorporate these libraries as external calls. One goal of TVM is"
" to be able to generate high-performing kernels. We will evolve TVM an "
"incremental manner as we learn from the techniques of manual kernel "
"crafting and add these as primitives in DSL. See also top for recipes of "
"operators in TVM."
msgstr ""
"TVM 可以将这些库作为外部调用进行整合。TVM 的目标是能够生成高性能的内核。"
"将以渐进的方式发展 TVM，从手工内核制作的技术中学习，并将这些技术作为领域特定语言（DSL）中的原语添加进去。另请参阅 TVM 中算子的实现方法。"

#: ../../doc/docs/faq.rst:68
msgid "Security"
msgstr "安全"

#: ../../doc/docs/faq.rst:69
msgid "See :ref:`dev-security`"
msgstr "查阅 :ref:`dev-security`"

