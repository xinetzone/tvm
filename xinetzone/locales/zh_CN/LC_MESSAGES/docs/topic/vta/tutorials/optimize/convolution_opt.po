# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-05-27 12:49+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:10002
msgid "2D 卷积优化"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:10004
msgid "**原作者**: [Thierry Moreau](https://homes.cs.washington.edu/~moreau/)"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:10006
msgid "本教程提供了关于如何使用 TVM 映射二维卷积工作负载有效的 VTA 设计的概述。建议先学习 {ref}`vta-mat-mult-opt` 教程。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:10008
msgid ""
"二维卷积在大多数计算机视觉深度神经网络中占主导地位。在本教程中，将演示 TVM 调度优化，将 NCHW 布局中的 2D 卷积算子映射到 "
"VTA。还引入了延迟隐藏（latency hiding）的概念，它允许最大化 VTA 的计算和内存资源利用。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:10010
msgid "RPC 设置"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:10012
msgid "首先编程 Pynq 的 FPGA 并构建它的 RPC 运行时。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:30002
msgid "声明计算"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:30004
msgid "作为第一步，需要用 NCHW 格式描述 2D 卷积计算。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:30006
msgid "通过 batch size、空间维度、输入通道、输出通道、核维度、核维度、填充维度和步长维度来定义二维卷积形状。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:30008
msgid "选择 ResNet-18 架构的第 9 个卷积层的形状作为卷积 workload 参数。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:30010
msgid "在 2D 卷积中添加了额外的算子，用于对输出进行移位和剪切，以模拟定点卷积之后的修正线性激活。将二维卷积层的 TVM 数据流图描述如下："
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:30016
msgid ""
"这个计算被故意设置得太大，以至于不能一次全部放入 VTA 的 on-chip "
"buffers。因此，在调度阶段，将依靠计算分块策略将计算分解为可管理的块。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:30018
msgid "空间填充"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:30021
msgid ""
"注意，需要导入 TOPI 库来对输入特征映射张量应用空间填充（Spatial padding）。空间填充有助于在 2D "
"卷积环境中分块，因为如果卷积核窗口大小大于 1，那么任何给定层的输入特征映射的相同 `(x, y)` 空间位置将被读取多次。在 CPU 和 GPU"
" 上，当并行工作时，提高内存访问效率的一种方法是空间打包（spatial packing），这需要重新布局数据。VTA load DMA "
"引擎可以自动插入填充，这样原始的输入特征映射就不必在内存中重新打包。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:30023
msgid "当数据从 DRAM load 到 VTA 的 SRAM 时，下面展示了 VTA 对动态空间填充的影响，随后是 2D 跨步和填充内存读取。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:50002
msgid "调度计算"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:50004
msgid "将看到一组必要的调度变换，以有效的方式将 2D 卷积映射到 VTA。这些包括："
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:50006
#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:70002
msgid "分块计算"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:50007
msgid "增加计算利用率（compute utilization）的虚拟线程（Virtual threading）"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:50008
msgid "Lowering 到 VTA 硬件 intrinsics"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:70004
msgid ""
"默认情况下，2D 卷积太大，激活或卷积核权重无法同时适应 VTA 的 on-chip "
"buffer。沿着输入通道、输出通道和高度空间维度应用分块。不沿宽度空间维度进行分块，因为它是 NCHW "
"布局中的最内层维度（因此，为了增加局部性，最好不要沿最内层维度进行分块）。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:90002
msgid "虚拟线程"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:90004
msgid ""
"虚拟线程是一种在 VTA 硬件设计中增加任务级管道并行性的机制。换句话说，它通过隐藏内存访问延迟（hiding memory access "
"latency）提高了计算资源的利用率。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:90006
msgid "在下面的实现中，虚拟线程将工作分配给沿输出通道轴划分的两个线程。在下面的图中，展示了计算 2D 卷积时工作是如何分割的。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:110002
msgid "Lowering Copies 到 DMA Transfers"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:110004
msgid ""
"接下来，设置相应的 on-chip VTA SRAM buffers 的 buffers 作用域。将 load 循环移动到 2D 卷积计算循环，以"
" stage 内存加载，以便它们适合 on-chip SRAM buffers。最后，用 DMA 复制 pragma 注解了 load/store"
" 循环外轴，以便在 VTA 上执行大容量内存传输。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:130002
msgid "Lowering 计算到 VTA 计算 Intrinsics"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:130004
msgid ""
"最后阶段是通过将二维卷积映射为张量 intrinsics，并将位移和剪切计算映射为向量 ALU，从而将计算循环 lower 到 VTA 硬件 "
"intrinsics。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:150002
msgid "TVM 计算和验证"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:150004
msgid "在指定调度之后，可以将其编译为 TVM 函数。保存模块，这样就可以通过 RPC 发送它。运行该函数并对 numpy 实现进行验证，以确保其正确性。"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:170002
msgid "小结"
msgstr ""

#: ../../../xin/docs/topic/vta/tutorials/optimize/convolution_opt.ipynb:170004
msgid ""
"本教程演示如何使用 TVM 调度原语 lower 硬件加速器 intrinsics 的 2D "
"卷积，利用特定于硬件的优化，比如使用带虚拟线程的隐藏延迟。"
msgstr ""

