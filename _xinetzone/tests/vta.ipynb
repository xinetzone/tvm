{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import set_env\n",
    "\n",
    "import os\n",
    "import tvm\n",
    "from tvm import te\n",
    "import vta\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = vta.get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ext_dev -keys=vta,cpu -device=vta -model=sim_1x16_i8w8a32_15_15_18_17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "config_path = f'{vta.environment.get_vta_hw_path()}/config/vta_config.json'\n",
    "with open(config_path) as fp:\n",
    "    cfg = json.load(fp)\n",
    "\n",
    "cfg.update({'TARGET': 'sim'})\n",
    "vta.environment.Environment.current = vta.environment.Environment(cfg)\n",
    "env = vta.get_env()\n",
    "env.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要 TVM RPC 模块和 VTA 仿真器模块\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils\n",
    "from vta.testing import simulator\n",
    "\n",
    "# 从操作系统环境中读取 Pynq RPC 主机 IP 地址和端口号\n",
    "host = os.environ.get(\"VTA_RPC_HOST\", \"192.168.2.99\")\n",
    "port = int(os.environ.get(\"VTA_RPC_PORT\", \"9091\"))\n",
    "\n",
    "# 在 Pynq 上配置 bitstream 和 runtime 系统，以匹配 vta_config.json 文件指定的 VTA 配置\n",
    "if env.TARGET == \"pynq\" or env.TARGET == \"de10nano\":\n",
    "\n",
    "    # 确保使用 RPC=1 编译 TVM\n",
    "    assert tvm.runtime.enabled(\"rpc\")\n",
    "    remote = rpc.connect(host, port)\n",
    "\n",
    "    # 重新配置 JIT 运行时\n",
    "    vta.reconfig_runtime(remote)\n",
    "\n",
    "    # 用预编译的 VTA bitstream 编程 FPGA。\n",
    "    # 通过将路径传递到 bitstream 文件而不是 None，\n",
    "    # 您可以使用自己的自定义 bitstream 来编程 FPGA。\n",
    "    vta.program_fpga(remote, bitstream=None)\n",
    "\n",
    "# 在仿真模式下，本地托管 RPC 服务器。\n",
    "elif env.TARGET in (\"sim\", \"tsim\", \"intelfocl\"):\n",
    "    remote = rpc.LocalSession()\n",
    "\n",
    "    if env.TARGET in [\"intelfocl\"]:\n",
    "        # 编程 intelfocl aocx \n",
    "        vta.program_fpga(remote, bitstream=\"vta.bitstream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出通道因子 m - 总计 64 x 16 = 1024 输出通道\n",
    "m = 64\n",
    "# Batch 因子 o - 总计 1 x 1 = 1\n",
    "o = 1\n",
    "# tiled 数据格式的占位符张量 A\n",
    "A = te.placeholder((o, m, env.BATCH, env.BLOCK_OUT), name=\"A\", dtype=env.acc_dtype)\n",
    "# tiled 数据格式的占位符张量 B\n",
    "B = te.placeholder((o, m, env.BATCH, env.BLOCK_OUT), name=\"B\", dtype=env.acc_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A copy buffer\n",
    "A_buf = te.compute((o, m, env.BATCH, env.BLOCK_OUT), lambda *i: A(*i), \"A_buf\")\n",
    "# B copy buffer\n",
    "B_buf = te.compute((o, m, env.BATCH, env.BLOCK_OUT), lambda *i: B(*i), \"B_buf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the in-VTA vector addition\n",
    "C_buf = te.compute(\n",
    "    (o, m, env.BATCH, env.BLOCK_OUT),\n",
    "    lambda *i: A_buf(*i).astype(env.acc_dtype) + B_buf(*i).astype(env.acc_dtype),\n",
    "    name=\"C_buf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to output type, and send to main memory\n",
    "C = te.compute(\n",
    "    (o, m, env.BATCH, env.BLOCK_OUT), lambda *i: C_buf(*i).astype(env.inp_dtype), name=\"C\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(int32), int32, [1024], []),\n",
      "             B: Buffer(B_2: Pointer(int32), int32, [1024], []),\n",
      "             C: Buffer(C_2: Pointer(int8), int8, [1024], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  allocate(A_buf: Pointer(global int32), int32, [1024]), storage_scope = global;\n",
      "  allocate(B_buf: Pointer(global int32), int32, [1024]), storage_scope = global {\n",
      "    for (i1: int32, 0, 64) {\n",
      "      for (i3: int32, 0, 16) {\n",
      "        let cse_var_1: int32 = ((i1*16) + i3)\n",
      "        A_buf_1: Buffer(A_buf, int32, [1024], [])[cse_var_1] = A[cse_var_1]\n",
      "      }\n",
      "    }\n",
      "    for (i1_1: int32, 0, 64) {\n",
      "      for (i3_1: int32, 0, 16) {\n",
      "        let cse_var_2: int32 = ((i1_1*16) + i3_1)\n",
      "        B_buf_1: Buffer(B_buf, int32, [1024], [])[cse_var_2] = B[cse_var_2]\n",
      "      }\n",
      "    }\n",
      "    for (i1_2: int32, 0, 64) {\n",
      "      for (i3_2: int32, 0, 16) {\n",
      "        let cse_var_3: int32 = ((i1_2*16) + i3_2)\n",
      "        A_buf_2: Buffer(A_buf, int32, [1024], [])[cse_var_3] = (A_buf_1[cse_var_3] + B_buf_1[cse_var_3])\n",
      "      }\n",
      "    }\n",
      "    for (i1_3: int32, 0, 64) {\n",
      "      for (i3_3: int32, 0, 16) {\n",
      "        let cse_var_4: int32 = ((i1_3*16) + i3_3)\n",
      "        C[cse_var_4] = cast(int8, A_buf_2[cse_var_4])\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the generated schedule\n",
    "s = te.create_schedule(C.op)\n",
    "\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stage(C_buf, compute(C_buf, body=[(A_buf[i0, i1, i2, i3] + B_buf[i0, i1, i2, i3])], axis=[iter_var(i0, range(min=0, ext=1)), iter_var(i1, range(min=0, ext=64)), iter_var(i2, range(min=0, ext=1)), iter_var(i3, range(min=0, ext=16))], reduce_axis=[], tag=, attrs={}))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[A_buf].set_scope(env.acc_scope)\n",
    "s[B_buf].set_scope(env.acc_scope)\n",
    "s[C_buf].set_scope(env.acc_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the buffer copies with the DMA pragma to map a copy loop to a\n",
    "# DMA transfer operation\n",
    "s[A_buf].pragma(s[A_buf].op.axis[0], env.dma_copy)\n",
    "s[B_buf].pragma(s[B_buf].op.axis[0], env.dma_copy)\n",
    "s[C].pragma(s[C].op.axis[0], env.dma_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(int32), int32, [1024], []),\n",
      "             B: Buffer(B_2: Pointer(int32), int32, [1024], []),\n",
      "             C: Buffer(C_2: Pointer(int8), int8, [1024], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  attr [IterVar(vta: int32, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 2 {\n",
      "    @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), A_2, 0, 64, 1, 64, 0, 0, 0, 0, 0, 3, dtype=int32)\n",
      "    @tir.call_extern(\"VTALoadBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), B_2, 0, 64, 1, 64, 0, 0, 0, 0, 64, 3, dtype=int32)\n",
      "    attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_uop_scope\" = \"VTAPushALUOp\" {\n",
      "      @tir.call_extern(\"VTAUopLoopBegin\", 64, 1, 1, 0, dtype=int32)\n",
      "      @tir.vta.uop_push(1, 0, 0, 64, 0, 2, 0, 0, dtype=int32)\n",
      "      @tir.call_extern(\"VTAUopLoopEnd\", dtype=int32)\n",
      "    }\n",
      "    @tir.vta.coproc_dep_push(2, 3, dtype=int32)\n",
      "  }\n",
      "  attr [IterVar(vta, (nullptr), \"ThreadIndex\", \"vta\")] \"coproc_scope\" = 3 {\n",
      "    @tir.vta.coproc_dep_pop(2, 3, dtype=int32)\n",
      "    @tir.call_extern(\"VTAStoreBuffer2D\", @tir.tvm_thread_context(@tir.vta.command_handle(, dtype=handle), dtype=handle), 0, 4, C_2, 0, 64, 1, 64, dtype=int32)\n",
      "  }\n",
      "  @tir.vta.coproc_sync(, dtype=int32)\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tell TVM that the computation needs to be performed\n",
    "# on VTA's vector ALU\n",
    "s[C_buf].pragma(C_buf.op.axis[0], env.alu)\n",
    "\n",
    "# Let's take a look at the finalized schedule\n",
    "print(vta.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vadd = vta.build(\n",
    "    s, [A, B, C], tvm.target.Target(\"ext_dev\", host=env.target_host), name=\"my_vadd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the compiled module into an object file.\n",
    "temp = utils.tempdir()\n",
    "my_vadd.save(temp.relpath(\"vadd.o\"))\n",
    "\n",
    "# Send the executable over RPC\n",
    "remote.upload(temp.relpath(\"vadd.o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = remote.load_module(\"vadd.o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the remote device context\n",
    "ctx = remote.ext_dev(0)\n",
    "\n",
    "# Initialize the A and B arrays randomly in the int range of (-128, 128]\n",
    "A_orig = np.random.randint(-128, 128, size=(o * env.BATCH, m * env.BLOCK_OUT)).astype(A.dtype)\n",
    "B_orig = np.random.randint(-128, 128, size=(o * env.BATCH, m * env.BLOCK_OUT)).astype(B.dtype)\n",
    "\n",
    "# Apply packing to the A and B arrays from a 2D to a 4D packed layout\n",
    "A_packed = A_orig.reshape(o, env.BATCH, m, env.BLOCK_OUT).transpose((0, 2, 1, 3))\n",
    "B_packed = B_orig.reshape(o, env.BATCH, m, env.BLOCK_OUT).transpose((0, 2, 1, 3))\n",
    "\n",
    "# Format the input/output arrays with tvm.nd.array to the DLPack standard\n",
    "A_nd = tvm.nd.array(A_packed, ctx)\n",
    "B_nd = tvm.nd.array(B_packed, ctx)\n",
    "C_nd = tvm.nd.array(np.zeros((o, m, env.BATCH, env.BLOCK_OUT)).astype(C.dtype), ctx)\n",
    "\n",
    "# Invoke the module to perform the computation\n",
    "f(A_nd, B_nd, C_nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful vector add test!\n"
     ]
    }
   ],
   "source": [
    "# Compute reference result with numpy\n",
    "C_ref = (A_orig.astype(env.acc_dtype) + B_orig.astype(env.acc_dtype)).astype(C.dtype)\n",
    "C_ref = C_ref.reshape(o, env.BATCH, m, env.BLOCK_OUT).transpose((0, 2, 1, 3))\n",
    "np.testing.assert_equal(C_ref, C_nd.numpy())\n",
    "print(\"Successful vector add test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ee5142ba8a2589df39b0df03e82f50c3ae535c49aaf7d83abad1a0d572c7e37"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tvm-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
