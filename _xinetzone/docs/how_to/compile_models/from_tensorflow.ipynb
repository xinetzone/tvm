{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Installs the latest dev build of TVM from PyPI. If you wish to build\n",
        "# from source, see https://tvm.apache.org/docs/install/from_source.html\n",
        "pip install apache-tvm --pre"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 编译 Tensorflow 模型\n",
        "\n",
        "本文是介绍性教程，旨在演示如何使用 TVM 部署 TensorFlow 模型。\n",
        "\n",
        "在开始之前，需要安装 TensorFlow Python 模块。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install tensorflow"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "参考 https://www.tensorflow.org/install\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-08 14:39:08.227239: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-06-08 14:39:08.274278: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-06-08 14:39:08.275306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-08 14:39:09.150430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-06-08 14:39:10.587663: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "# tvm, relay\n",
        "import tvm\n",
        "from tvm import te\n",
        "from tvm import relay\n",
        "\n",
        "# os and numpy\n",
        "import numpy as np\n",
        "import os.path\n",
        "\n",
        "# Tensorflow imports\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Ask tensorflow to limit its GPU memory to what's actually needed\n",
        "# instead of gobbling everything that's available.\n",
        "# https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n",
        "# This way this tutorial is a little more friendly to sphinx-gallery.\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(\"tensorflow will use experimental.set_memory_growth(True)\")\n",
        "    except RuntimeError as e:\n",
        "        print(\"experimental.set_memory_growth option is not available: {}\".format(e))\n",
        "\n",
        "\n",
        "try:\n",
        "    tf_compat_v1 = tf.compat.v1\n",
        "except ImportError:\n",
        "    tf_compat_v1 = tf\n",
        "\n",
        "# Tensorflow utility functions\n",
        "import tvm.relay.testing.tf as tf_testing\n",
        "\n",
        "# Base location for model related files.\n",
        "repo_base = \"https://github.com/dmlc/web-data/raw/main/tensorflow/models/InceptionV1/\"\n",
        "\n",
        "# Test image\n",
        "img_name = \"elephant-299.jpg\"\n",
        "image_url = os.path.join(repo_base, img_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 教程\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = \"classify_image_graph_def-with_shapes.pb\"\n",
        "model_url = os.path.join(repo_base, model_name)\n",
        "\n",
        "# Image label map\n",
        "map_proto = \"imagenet_2012_challenge_label_map_proto.pbtxt\"\n",
        "map_proto_url = os.path.join(repo_base, map_proto)\n",
        "\n",
        "# Human readable text for labels\n",
        "label_map = \"imagenet_synset_to_human_label_map.txt\"\n",
        "label_map_url = os.path.join(repo_base, label_map)\n",
        "\n",
        "# Target settings\n",
        "# Use these commented settings to build for cuda.\n",
        "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
        "# layout = \"NCHW\"\n",
        "# dev = tvm.cuda(0)\n",
        "target = tvm.target.Target(\"llvm\", host=\"llvm\")\n",
        "layout = None\n",
        "dev = tvm.cpu(0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 下载必须的文件\n",
        "\n",
        "下载如下文件："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.contrib.download import download_testdata\n",
        "\n",
        "img_path = download_testdata(image_url, img_name, module=\"data\")\n",
        "model_path = download_testdata(model_url, model_name, module=[\"tf\", \"InceptionV1\"])\n",
        "map_proto_path = download_testdata(map_proto_url, map_proto, module=\"data\")\n",
        "label_path = download_testdata(label_map_url, label_map, module=\"data\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 加载模型\n",
        "\n",
        "从 protobuf 文件创建 TensorFlow graph 定义。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-08 14:39:18.045745: W tensorflow/core/framework/op_def_util.cc:369] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /media/pc/data/lxw/ai/tvm/xinetzone/__pypackages__/3.10/lib/tvm/relay/testing/tf.py:136: convert_variables_to_constants (from tensorflow.python.framework.convert_to_constants) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n",
            "WARNING:tensorflow:From /media/pc/data/tmp/cache/conda/envs/tvmz/lib/python3.10/site-packages/tensorflow/python/framework/convert_to_constants.py:952: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n"
          ]
        }
      ],
      "source": [
        "with tf_compat_v1.gfile.GFile(model_path, \"rb\") as f:\n",
        "    graph_def = tf_compat_v1.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())\n",
        "    graph = tf.import_graph_def(graph_def, name=\"\")\n",
        "    # Call the utility to import the graph definition into default graph.\n",
        "    graph_def = tf_testing.ProcessGraphDefParam(graph_def)\n",
        "    # Add shapes to the graph.\n",
        "    with tf_compat_v1.Session() as sess:\n",
        "        graph_def = tf_testing.AddShapesToGraphDef(sess, \"softmax\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 解码图像"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image = Image.open(img_path).resize((299, 299))\n",
        "\n",
        "x = np.array(image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 导入 graph 到 Relay\n",
        "\n",
        "将 TensorFlow graph 定义导入到 Relay 前端。\n",
        "\n",
        "结果：\n",
        "\n",
        "The result is:\n",
        "\n",
        "- sym: Relay 表达式，表示给定的 tensorflow protobuf。\n",
        "- params: 从 tensorflow params（张量 protobuf）转换而来的参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/pc/data/lxw/ai/tvm/xinetzone/__pypackages__/3.10/lib/tvm/relay/frontend/tensorflow.py:537: UserWarning: Ignore the passed shape. Shape in graphdef will be used for operator DecodeJpeg/contents.\n",
            "  warnings.warn(\n",
            "/media/pc/data/lxw/ai/tvm/xinetzone/__pypackages__/3.10/lib/tvm/relay/frontend/tensorflow_ops.py:1036: UserWarning: DecodeJpeg: It's a pass through, please handle preprocessing before input\n",
            "  warnings.warn(\"DecodeJpeg: It's a pass through, please handle preprocessing before input\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow protobuf imported to relay frontend.\n"
          ]
        }
      ],
      "source": [
        "shape_dict = {\"DecodeJpeg/contents\": x.shape}\n",
        "dtype_dict = {\"DecodeJpeg/contents\": \"uint8\"}\n",
        "mod, params = relay.frontend.from_tensorflow(graph_def, layout=layout, shape=shape_dict)\n",
        "\n",
        "print(\"Tensorflow protobuf imported to relay frontend.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Relay 构建\n",
        "\n",
        "使用给定的输入规格将图编译为 LLVM 目标。 \n",
        "\n",
        "结果：\n",
        "  \n",
        "- `graph`：编译后的最终计算图。\n",
        "- `params`：编译后的最终参数。\n",
        "- `lib`：可以在具有 TVM 运行时的目标上部署的目标库。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with tvm.transform.PassContext(opt_level=3):\n",
        "    lib = relay.build(mod, target, params=params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 在 TVM 上执行 portable graph\n",
        "\n",
        "现在我们可以尝试在目标设备上部署编译好的模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.contrib import graph_executor\n",
        "\n",
        "dtype = \"uint8\"\n",
        "m = graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "# set inputs\n",
        "m.set_input(\"DecodeJpeg/contents\", tvm.nd.array(x.astype(dtype)))\n",
        "# execute\n",
        "m.run()\n",
        "# get outputs\n",
        "tvm_output = m.get_output(0, tvm.nd.empty(((1, 1008)), \"float32\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 处理输出\n",
        "\n",
        "将 InceptionV1 模型的输出处理成可读的文本形式。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "African elephant, Loxodonta africana (score = 0.58335)\n",
            "tusker (score = 0.33901)\n",
            "Indian elephant, Elephas maximus (score = 0.02391)\n",
            "banana (score = 0.00025)\n",
            "vault (score = 0.00021)\n"
          ]
        }
      ],
      "source": [
        "predictions = tvm_output.numpy()\n",
        "predictions = np.squeeze(predictions)\n",
        "\n",
        "# Creates node ID --> English string lookup.\n",
        "node_lookup = tf_testing.NodeLookup(label_lookup_path=map_proto_path, uid_lookup_path=label_path)\n",
        "\n",
        "# Print top 5 predictions from TVM output.\n",
        "top_k = predictions.argsort()[-5:][::-1]\n",
        "for node_id in top_k:\n",
        "    human_string = node_lookup.id_to_string(node_id)\n",
        "    score = predictions[node_id]\n",
        "    print(\"%s (score = %.5f)\" % (human_string, score))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 在 tensorflow 上推理\n",
        "\n",
        "在 TensorFlow 上运行相应的模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-08 14:43:57.510715: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== TENSORFLOW RESULTS =======\n",
            "African elephant, Loxodonta africana (score = 0.58394)\n",
            "tusker (score = 0.33909)\n",
            "Indian elephant, Elephas maximus (score = 0.03186)\n",
            "banana (score = 0.00022)\n",
            "desk (score = 0.00019)\n"
          ]
        }
      ],
      "source": [
        "def create_graph():\n",
        "    \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
        "    # Creates graph from saved graph_def.pb.\n",
        "    with tf_compat_v1.gfile.GFile(model_path, \"rb\") as f:\n",
        "        graph_def = tf_compat_v1.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "        graph = tf.import_graph_def(graph_def, name=\"\")\n",
        "        # Call the utility to import the graph definition into default graph.\n",
        "        graph_def = tf_testing.ProcessGraphDefParam(graph_def)\n",
        "\n",
        "\n",
        "def run_inference_on_image(image):\n",
        "    \"\"\"Runs inference on an image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image: String\n",
        "        Image file name.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Nothing\n",
        "    \"\"\"\n",
        "    if not tf_compat_v1.gfile.Exists(image):\n",
        "        tf.logging.fatal(\"File does not exist %s\", image)\n",
        "    image_data = tf_compat_v1.gfile.GFile(image, \"rb\").read()\n",
        "\n",
        "    # Creates graph from saved GraphDef.\n",
        "    create_graph()\n",
        "\n",
        "    with tf_compat_v1.Session() as sess:\n",
        "        softmax_tensor = sess.graph.get_tensor_by_name(\"softmax:0\")\n",
        "        predictions = sess.run(softmax_tensor, {\"DecodeJpeg/contents:0\": image_data})\n",
        "\n",
        "        predictions = np.squeeze(predictions)\n",
        "\n",
        "        # Creates node ID --> English string lookup.\n",
        "        node_lookup = tf_testing.NodeLookup(\n",
        "            label_lookup_path=map_proto_path, uid_lookup_path=label_path\n",
        "        )\n",
        "\n",
        "        # Print top 5 predictions from tensorflow.\n",
        "        top_k = predictions.argsort()[-5:][::-1]\n",
        "        print(\"===== TENSORFLOW RESULTS =======\")\n",
        "        for node_id in top_k:\n",
        "            human_string = node_lookup.id_to_string(node_id)\n",
        "            score = predictions[node_id]\n",
        "            print(\"%s (score = %.5f)\" % (human_string, score))\n",
        "\n",
        "\n",
        "run_inference_on_image(img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
