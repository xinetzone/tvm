# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-09 00:02+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10003
msgid "用 Python 接口编译和优化模型（AutoTVM）"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10005
msgid "**原作者**: [Chris Hoge](https://github.com/hogepodge>)"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10007
msgid ""
"在 [TVMC 教程](tvmc_command_line_driver) 中，介绍了如何使用 TVM 的命令行界面 TVMC "
"来编译、运行和微调预训练的视觉模型 ResNet-50 v2。不过，TVM 不仅仅是命令行工具，它也是优化框架，其 API "
"可用于许多不同的语言，在处理机器学习模型方面给你带来巨大的灵活性。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10009
msgid ""
"在本教程中，将涵盖与 TVMC 相同的内容，但展示如何用 Python API 来完成它。完成本节后，将使用 TVM 的 Python API "
"来完成以下任务："
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10011
msgid "编译预训练的 ResNet-50 v2 模型供 TVM 运行时使用。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10012
msgid "使用编译后的模型，运行真实图像，并解释输出和评估模型性能。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10013
msgid "使用 TVM 在 CPU 上调度该模型。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10014
msgid "使用 TVM 收集的调度数据重新编译已优化的模型。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10015
msgid "通过优化后的模型运行图像，并比较输出和模型的性能。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10017
msgid "本节的目的是让你了解 TVM 的能力以及如何通过 Python API 使用它们。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10019
msgid ""
"TVM 是一个深度学习编译器框架，有许多不同的模块可用于处理深度学习模型和算子。在本教程中，我们将研究如何使用 Python API "
"加载、编译和优化一个模型。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:10021
msgid ""
"首先要导入一些依赖关系，包括用于加载和转换模型的 ``mxnet``，用于下载测试数据的辅助工具，用于处理图像数据的 Python "
"图像库，用于图像数据预处理和后处理的 ``numpy``，TVM Relay 框架，以及 TVM Graph Executor。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:30002
msgid "下载和加载前端模型"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:30004
msgid ""
"在本教程中，使用 ResNet-50 v2。ResNet-50 是卷积神经网络，有 50 "
"层深度，旨在对图像进行分类。该模型已经在超过一百万张图片上进行了预训练，有 1000 种不同的分类。该网络的输入图像大小为 224x224。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:30007
msgid ""
"如果你有兴趣探索更多关于 ResNet-50 模型的结构，建议下载免费的 ML 模型查看器 "
"[Netron](https://netron.app)。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:30010
msgid "TVM 提供了辅助库来下载预训练的模型。通过该模块提供模型的 URL、文件名和模型类型，TVM 将下载模型并保存到磁盘。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:30012
msgid "与其他模型格式一起工作"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:30013
msgid "TVM 支持许多流行的模型格式。清单可以在 TVM 文档的 [编译深度学习模型](tutorial-frontend) 部分找到。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:30017
msgid "可以直接使用如下方式下载预训练的模型（以 ONNX 为例）："
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:30032
msgid "MXNet 可直接载入模型："
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:50002
msgid "下载、预处理和加载测试图像"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:50004
msgid ""
"当涉及到预期的张量形状、格式和数据类型时，每个模型都很特别。出于这个原因，大多数模型需要一些预处理和后处理，以确保输入是有效的，并解释输出。TVMC"
" 对输入和输出数据都采用了 NumPy 的 ``.npz`` 格式。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:50006
msgid "作为本教程的输入，将使用一只猫的图像，但你可以自由地用你选择的任何图像来代替这个图像。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:50010
msgid "下载图像数据，然后将其转换成 numpy 数组，作为模型的输入。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:70002
msgid "用 Relay 编译模型"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:70004
msgid ""
"下一步是编译 ResNet 模型。使用 {func}`~tvm.relay.frontend.from_mxnet` 导入器将模型导入到 "
"{mod}`~tvm.relay`。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:70006
msgid "不同的模型类型，输入的名称可能不同。你可以使用 Netron 这样的工具来检查输入名称。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:90002
msgid "将模型与标准优化一起构建成 TVM 库。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:90004
msgid "定义正确的目标"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:90005
msgid ""
"指定正确的目标可以对编译后的模块的性能产生巨大影响，因为它可以利用目标上可用的硬件特性。欲了解更多信息，请参考为 [x86 CPU "
"自动调整卷积网络](tune_relay_x86)。建议确定你运行的是哪种 CPU，以及可选的功能，并适当地设置目标。例如，对于某些处理器， "
"`target = \"llvm -mcpu=skylake\"`，或者对于具有 AVX-512 向量指令集的处理器， `target = "
"\"llvm-mcpu=skylake-avx512\"`。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:110002
msgid "从该库中创建 TVM graph 运行时模块。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:130002
msgid "在 TVM 运行时上执行"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:130004
msgid "已经编译了模型，下面可以使用 TVM 运行时来进行预测。要使用 TVM 来运行模型并进行预测，需要两样东西："
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:130006
msgid "编译后的模型，也就是我们刚刚制作的模块 `module`。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:130007
msgid "对模型的有效输入，以便进行预测。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:150002
msgid "收集基本性能数据"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:150004
msgid ""
"想收集一些与这个未优化的模型相关的基本性能数据，并在以后与调整后的模型进行比较。为了帮助说明 CPU "
"的噪音，在多个批次的重复中运行计算，然后收集一些关于平均值、中位数和标准差的基础统计数据。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:170002
msgid "对输出进行后处理"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:170004
msgid "如前所述，每个模型都有自己提供输出张量的特殊方式。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:170006
msgid "在案例中，需要运行一些后处理，利用为模型提供的查找表，将 ResNet-50 v2 的输出渲染成更适合人类阅读的形式。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:190002
msgid "调优模型"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:190004
msgid "之前的模型是为了在 TVM 运行时工作而编译的，但不包括任何特定平台的优化。在本节中，将向你展示如何使用 TVM 建立针对你工作平台的优化模型。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:190006
msgid ""
"在某些情况下，当使用编译的模块运行推理时，可能无法获得预期的性能。在这种情况下，可以利用自动调谐器，为模型找到更好的配置，获得性能的提升。TVM "
"中的调谐是指对模型进行优化以在给定目标上更快地运行的过程。这与训练或微调不同，因为它不影响模型的准确性，而只影响运行时的性能。作为调优过程的一部分，TVM"
" 将尝试运行许多不同的算子实现变体，以观察哪些算子表现最佳。这些运行的结果被储存在调优记录文件中。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:190008
msgid "在最简单的形式下，调优需要你提供三样东西："
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:190010
msgid "你打算在上面运行这个模型的设备的目标规格"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:190011
msgid "输出文件的路径，调优记录将被存储在该文件中"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:190012
msgid "要调优的模型的路径"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:210002
msgid ""
"为运行器设置一些基本参数。运行器采用一组特定参数生成的编译代码，并测量其性能。``number`` 指定我们将测试的不同配置的数量，而 "
"``repeat`` 指定我们将对每个配置进行多少次测量。``min_repeat_ms`` "
"是一个值，指定需要多长时间运行配置测试。如果重复次数低于这个时间，它将被增加。这个选项对于在 GPU 上进行精确的调优是必要的，而对于 CPU "
"的调优则不需要。把这个值设置为 0 可以禁用它。``timeout`` 为每个测试的配置运行训练代码的时间设置了上限。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:230002
msgid ""
"创建简单的结构来保存调谐选项。使用 XGBoost 算法来指导搜索。对于生产作业来说，你会想把试验的数量设置得比这里使用的 10 的值大。对于 "
"CPU，推荐 1500，对于 GPU，推荐 "
"3000-4000。所需的试验次数可能取决于特定的模型和处理器，因此值得花一些时间来评估各种数值的性能，以找到调整时间和模型优化之间的最佳平衡。因为运行调谐是需要时间的，我们将试验次数设置为"
" 10 次，但不建议使用这么小的值。``early_stopping`` "
"参数是在应用提前停止搜索的条件之前，要运行的最小轨数。``measure`` "
"选项表示将在哪里建立试验代码，以及将在哪里运行。在这种情况下，使用刚刚创建的 ``LocalRunner`` 和 "
"``LocalBuilder``。``tuning_records`` 选项指定了文件来写入调整数据。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:250002
msgid "定义调谐搜索算法"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:250003
msgid "默认情况下，这种搜索是使用 XGBoost 网格算法指导的。根据你的模型的复杂性和可用的时间量，你可能想选择一个不同的算法。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:250006
msgid "设置调谐参数"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:250007
msgid ""
"在这个例子中，为了节省时间，我们将试验次数和提前停止设置为 "
"10。如果你把这些值设置得更高，你可能会看到更多的性能改进，但这是以花时间调整为代价的。收敛所需的试验次数将取决于模型和目标平台的具体情况。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:270002
msgid "用调优数据编译优化后的模型"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:270004
msgid ""
"作为上述调优过程的输出，我们获得了存储在 ``resnet-50-v2-autotuning.json`` "
"的调优记录。编译器将使用这些结果，在你指定的目标上为模型生成高性能代码。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:270006
msgid "现在，模型的调优数据已经收集完毕，可以使用优化的算子重新编译模型，以加快计算速度。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:290002
msgid "验证优化后的模型是否运行并产生相同的结果："
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:310002
msgid "比较已调谐和未调谐的模型"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:310004
msgid "我们想收集一些与这个优化模型相关的基本性能数据，将其与未优化的模型进行比较。根据你的底层硬件、迭代次数和其他因素，你应该看到优化后的模型与未优化的模型相比有性能的提高。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:330002
msgid "小结"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:330004
msgid ""
"在本教程中，我们举了一个简短的例子，说明如何使用 TVM Python API "
"来编译、运行和调整一个模型。我们还讨论了对输入和输出进行预处理和后处理的必要性。在调优过程之后，我们演示了如何比较未优化和优化后的模型的性能。"
msgstr ""

#: ../../xin/docs/tutorial/autotvm_relay_x86.ipynb:330006
msgid "这里我们介绍了使用 ResNet-50 v2 本地的简单例子。然而，TVM 支持更多的功能，包括交叉编译、远程执行和剖析/基准测试。"
msgstr ""

#~ msgid ""
#~ "在本教程中，我们将使用 ResNet-50 v2。ResNet-50 是卷积神经网络，有 "
#~ "50 层深度，旨在对图像进行分类。该模型已经在超过一百万张图片上进行了预训练，有 1000 "
#~ "种不同的分类。该网络的输入图像大小为 224x224。如果你有兴趣探索更多关于 ResNet-50 "
#~ "模型的结构，我们建议下载免费的 ML 模型查看器 "
#~ "[Netron](https://netron.app)。"
#~ msgstr ""

#~ msgid ""
#~ "创建一个简单的结构来保存调谐选项。我们使用一个 XGBoost "
#~ "算法来指导搜索。对于一个生产作业来说，你会想把试验的数量设置得比这里使用的 10 的值大。对于 "
#~ "CPU，我们推荐 1500，对于 GPU，推荐 "
#~ "3000-4000。所需的试验次数可能取决于特定的模型和处理器，因此值得花一些时间来评估各种数值的性能，以找到调整时间和模型优化之间的最佳平衡。因为运行调谐是需要时间的，我们将试验次数设置为"
#~ " 10 次，但不建议使用这么小的值。``early_stopping`` "
#~ "参数是在应用提前停止搜索的条件之前，要运行的最小轨数。``measure`` "
#~ "选项表示将在哪里建立试验代码，以及将在哪里运行。在这种情况下，我们使用刚刚创建的 ``LocalRunner`` "
#~ "和一个 ``LocalBuilder``。``tuning_records`` 选项指定了一个文件来写入调整数据。"
#~ msgstr ""

#~ msgid "这里我们介绍了一个使用 ResNet-50 v2 本地的简单例子。然而，TVM 支持更多的功能，包括交叉编译、远程执行和剖析/基准测试。"
#~ msgstr ""

#~ msgid ""
#~ "之前的模型是为了在 TVM 运行时工作而编译的，但不包括任何特定平台的优化。在本节中，我们将向你展示如何使用 "
#~ "TVM 建立一个针对你工作平台的优化模型。"
#~ msgstr ""

#~ msgid ""
#~ "在某些情况下，当使用我们编译的模块运行推理时，我们可能无法获得预期的性能。在这种情况下，我们可以利用自动调谐器，为我们的模型找到一个更好的配置，获得性能的提升。TVM"
#~ " "
#~ "中的调谐是指对模型进行优化以在给定目标上更快地运行的过程。这与训练或微调不同，因为它不影响模型的准确性，而只影响运行时的性能。作为调优过程的一部分，TVM"
#~ " 将尝试运行许多不同的运算器实现变体，以观察哪些运算器表现最佳。这些运行的结果被储存在调优记录文件中。"
#~ msgstr ""

#~ msgid "现在，模型的调优数据已经收集完毕，我们可以使用优化的运算符重新编译模型，以加快我们的计算速度。"
#~ msgstr ""

