# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-09 00:02+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/arch/relay_intro.rst:21
msgid "Relay IR 简介"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:23
msgid "本文介绍了第二代 NNVM： Relay IR。希望读者具有两种背景——具有编程语言背景和熟悉计算图表示的深度学习框架开发人员。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:25
msgid "在这里简要地总结了设计目标，并将在本文的后面部分涉及这些要点。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:27
msgid "支持传统的数据流图风格编程和变换。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:28
msgid "支持函数式的作用域，let-binding，并使其成为功能齐全的可微分语言。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:29
msgid "允许用户混合两种编程风格。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:33
msgid "使用 Relay 构建计算图"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:35
msgid ""
"传统的深度学习框架使用计算图作为其中间表示。计算图（或数据流图）是表示计算的有向无环图（directed acyclic graph，简称 "
"DAG）。 "
"虽然由于缺乏控制流，数据流图在它们能够表达的计算方面受到了限制，但它们的简单性使其更容易实现对异构执行环境的自动微分和编译（例如，在专门的硬件上执行图的一部分）。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:42
msgid ""
"您可以使用 Relay 来构建计算（数据流）图。具体来说，上面的代码展示了如何构造简单的双节点图。 可以发现，该示例的语法与现有的计算图 IR（如"
" NNVMv1）没有太大区别，惟一的区别是术语上的不同："
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:45
msgid "现有框架通常使用 graph 和 subgraph"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:46
#, python-format
msgid "Relay 使用函数，例如—— ``fn (%x)`` 来表示 graph"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:48
#, python-format
msgid ""
"每个数据流节点都是 Relay 中的 CallNode。Relay Python DSL 允许快速构造数据流图。 "
"在上面的代码中，要强调的一件事是，我们显式地构造了 Add 节点，它的两个输入点都指向 ``%1``。 "
"当深度学习框架评估上述程序时，它将按拓扑顺序计算节点，``%1`` 只计算一次。 "
"虽然这一事实对于深度学习框架的构建者来说是非常自然的，但它可能会让 PL 研究人员首先感到惊讶。 如果实现简单的 visitor "
"来打印结果，并将结果作为嵌套的 Call 表达式处理，那么它将变成 ``log(%x) + log(%x)``。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:54
msgid ""
"当 DAG 中存在共享节点时，对程序语义的不同解释导致了这种分歧。在普通的函数式编程 IR 中，嵌套表达式被视为表达式树，而不考虑 ``%1`` "
"实际上在 ``%2`` 中被重用了两次的事实。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:56
msgid ""
"Relay IR 注意到了这种差异。通常，深度学习框架用户以这种方式构建计算图，其中经常出现 DAG 节点重用。 因此，当以文本格式打印 "
"Relay 程序时，每行打印一个 CallNode，并为每个 CallNode 分配一个临时 id ``(%1, "
"%2)``，以便在程序的后面部分中引用每个公共节点。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:60
msgid "模块：支持多函数（Graphs）"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:62
msgid ""
"到目前为止，已经介绍了如何将数据流图构建为函数。 人们可能会很自然地问：能支持多个函数并使它们能够相互调用吗？Relay "
"允许在模块（module）中分组多个函数；下面的代码显示了一个函数调用另一个函数的示例。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:79
msgid ""
"Module 可以被视为 ``Map<GlobalVar, Function>``。这里 ``GlobalVar`` 只是一个 "
"id，用来表示模块中的函数。 在上面的例子中 ``@muladd`` 和 ``@myfunc`` 是 ``GlobalVars``。当 "
"CallNode 用于调用另一个函数时，相应的 ``GlobalVar`` 存储在该 ``CallNode`` 的 ``op`` 字段中。 "
"它包含了一个间接级别，我们需要使用对应的 ``GlobalVar`` 从模块中查找被调用函数的函数体。在这种特殊情况下，还可以直接将对 "
"``Function`` 的引用存储为 ``CallNode`` 中的 ``op``。 那么，为什么需要引入 "
"``GlobalVar``？主要原因是 ``GlobalVar`` 解耦了函数的定义/声明，并支持递归和延迟声明（delayed "
"declaration）。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:98
msgid ""
"在上面的例子中，``@myfunc`` 递归地调用自己。使用 GlobalVar ``@myfunc`` 表示函数可以避免数据结构中的循环依赖。 "
"至此，我们已经介绍了 Relay 中的基本概念。值得注意的是，Relay 对 NNVMv1 有以下改进："
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:101
msgid "简洁的文本格式，易于调试写入 passes。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:102
msgid ""
"在联合的模块中对 subgraphs-functions "
"的一级支持，这获得进一步的联合优化机会，如内联（inlining）和调用约定规范（calling convention "
"specification）。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:103
msgid ""
"简单的前端语言互操作（interop），例如，所有的数据结构都可以在 Python 中访问，这允许在 Python 中快速构建优化原型，并将它们与"
" C++ 代码混合。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:106
msgid "Let Binding and Scopes"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:108
msgid "到目前为止，已经介绍了如何用深度学习框架中使用的老式方法来构建计算图。本节将讨论 Relay 引入的新的重要构造—— let 绑定。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:110
msgid ""
"Let 绑定在每一种高级编程语言中都使用。在 Relay 中，它是包含三个字段 ``Let(var, value, body)`` 的数据结构。 "
"当求 let 表达式的值时，首先 evaluate ``value`` 部分，将其赋值给 ``var``，然后在 ``body`` "
"表达式中返回求值结果。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:113
msgid "可以使用 let 绑定序列来构造与数据流图程序在逻辑上等价的程序。下面的代码示例显示了带有两种形式的程序。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:119
msgid ""
"嵌套的 let 绑定称为 A-normal（A-范式） 形式，它通常在函数式编程语言中用作 IRs。现在，请仔细看一下 AST 结构。 "
"虽然这两个程序在语义上是相同的（它们的文本表示也是相同的，只是 A-normal 有前缀），但它们的 AST 结构是不同的。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:122
msgid ""
"由于程序优化使用这些 AST 数据结构并变换它们，这两种不同的结构将影响将要编写的编译器代码。例如，如果想检测模式 ``add(log(x), "
"y)``："
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:124
msgid "在数据流图形式中，可以首先访问 ``add`` 节点，然后直接查看它的第一个参数，以确定它是否是 log"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:125
msgid ""
"在 A-normal 形式中，我们不能再直接进行检查了，因为 add 的第一个输入是 "
"``%v1``——我们需要保存一个从变量到其绑定值的映射，并查找该映射，以便知道 ``%v1`` 是 log。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:127
msgid ""
"不同的数据结构将影响您编写变换的方式，我们需要记住这一点。所以现在，作为深度学习框架的开发人员，您可能会问，为什么我们需要 let 绑定？ 你的 "
"PL 朋友总是会告诉你，let 很重要，因为 PL 是相当成熟的领域，这背后一定有一些智慧。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:131
msgid "为什么可能需要 Let 绑定"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:133
msgid "let 绑定的一个关键用法是指定计算范围。让我们看一下下面的示例，它不使用 Let 绑定。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:138
msgid ""
"当我们试图决定在哪里计算节点 ``%1`` 时，问题就来了。 特别是，虽然文本格式似乎建议我们应该在 if 范围外计算节点 ``%1``，但 "
"AST（如图所示）不建议这样做。 实际上，数据流图从来没有定义它的评估范围。这在语义上引入了一些歧义。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:142
msgid "当我们有闭包时，这种模糊性变得更加有趣。考虑下面的程序，它返回一个闭包。我们不知道应该在哪里计算 ``%1``；它可以在闭包的内部或外部。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:154
#, python-format
msgid ""
"let 绑定解决了这个问题，因为值的计算发生在 let 节点。 在两个程序中，如果我们将 ``%1 = log(%x)`` 更改为 ``let "
"%v1 = log(%x)``，就可以明确指定计算位置在 if 范围和闭包之外。 正如你所看到的，let-binding "
"给出了更精确的计算站点规范，当我们生成后端代码时可能会很有用（因为这样的规范在 IR 中）。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:158
msgid ""
"另一方面，数据流图形式，它没有指定计算范围，确实有它自己的优点——即，我们不需要担心在生成代码时将 let 放在哪里。 "
"数据流图形式还为后面的传递提供了更多的自由，以决定将评估点放在哪里。因此，在优化的初始阶段，当您觉得方便时，使用程序的数据流形式可能不是坏主意。 "
"目前，Relay 中的许多优化都是为了优化数据流图程序而编写的。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:162
msgid ""
"然而，当我们将 IR 降低到实际的运行时程序时，需要精确地确定计算的范围。 特别是，希望显式指定在使用子函数和闭包时计算范围应该发生在哪里。let"
" 绑定可以在后期执行特定的优化中解决这个问题。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:166
msgid "IR 变换的意义"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:168
msgid "希望现在你已经熟悉了这两种表示。大多数函数式编程语言以 A-normal 形式进行分析，其中 analyzer 不需要注意表达式是 DAG。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:170
msgid ""
"Relay 选择同时支持数据流图形式和 let 绑定。我们相信让框架开发人员选择他们熟悉的表示是很重要的。然而，这确实对我们如何写 passes "
"有一些影响："
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:172
msgid ""
"如果你来自数据流图背景，想要处理 let，保留 var 到表达式的映射，这样你就可以在遇到 var "
"时执行查找。这可能意味着最小的更改，因为我们已经需要从表达式到变换后的表达式的映射。注意，这将有效地删除程序中的所有 let。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:173
msgid "如果您来自 PL 背景，并且喜欢 A-normal 形式，我们将提供数据流图到 A-normal 形式的 pass。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:174
msgid ""
"对于 PL 人来说，当你在实现某些东西（比如数据流图到 ANF 变换）时，要注意表达式可以是 DAG，这通常意味着我们应该用 "
"``Map<Expr, Result>`` 并且只计算一次转换后的结果，因此得到的表达式保持了通用结构。"
msgstr ""

#: ../../xin/docs/arch/relay_intro.rst:176
msgid ""
"还有一些其他的高级概念，如 symbolic shape inference，polymorphic "
"functions，这些都没有在本材料中涵盖；欢迎您查阅其他资料。"
msgstr ""

#~ msgid "Introduction to Relay IR"
#~ msgstr ""

#~ msgid ""
#~ "This article introduces Relay IR -- "
#~ "the second generation of NNVM. We "
#~ "expect readers from two kinds of "
#~ "background -- those who have a "
#~ "programming language background and deep "
#~ "learning framework developers who are "
#~ "familiar with the computational graph "
#~ "representation."
#~ msgstr ""

#~ msgid ""
#~ "We briefly summarize the design goal "
#~ "here, and will touch upon these "
#~ "points in the later part of the"
#~ " article."
#~ msgstr ""

#~ msgid "Support traditional data flow-style programming and transformations."
#~ msgstr ""

#~ msgid ""
#~ "Support functional-style scoping, let-"
#~ "binding and making it a fully "
#~ "featured differentiable language."
#~ msgstr ""

#~ msgid "Being able to allow the user to mix the two programming styles."
#~ msgstr ""

#~ msgid "Build a Computational Graph with Relay"
#~ msgstr ""

#~ msgid ""
#~ "Traditional deep learning frameworks use "
#~ "computational graphs as their intermediate "
#~ "representation. A computational graph (or "
#~ "dataflow graph), is a directed acyclic"
#~ " graph (DAG) that represents the "
#~ "computation. Though dataflow graphs are "
#~ "limited in terms of the computations "
#~ "they are capable of expressing due "
#~ "to lacking control flow, their "
#~ "simplicity makes it easier to implement"
#~ " automatic differentiation and compile for"
#~ " heterogeneous execution environments (e.g., "
#~ "executing parts of the graph on "
#~ "specialized hardware)."
#~ msgstr ""

#~ msgid ""
#~ "You can use Relay to build a "
#~ "computational (dataflow) graph. Specifically, "
#~ "the above code shows how to "
#~ "construct a simple two-node graph. "
#~ "You can find that the syntax of"
#~ " the example is not that different"
#~ " from existing computational graph IR "
#~ "like NNVMv1, with the only difference"
#~ " in terms of terminology:"
#~ msgstr ""

#~ msgid "Existing frameworks usually use graph and subgraph"
#~ msgstr ""

#~ msgid "Relay uses function e.g. --  ``fn (%x)``, to indicate the graph"
#~ msgstr ""

#~ msgid ""
#~ "Each dataflow node is a CallNode "
#~ "in Relay. The Relay Python DSL "
#~ "allows you to construct a dataflow "
#~ "graph quickly. One thing we want "
#~ "to highlight in the above code --"
#~ " is that we explicitly constructed an"
#~ " Add node with both input point "
#~ "to ``%1``.  When a deep learning "
#~ "framework evaluates the above program, "
#~ "it will compute the nodes in "
#~ "topological order, and ``%1`` will only"
#~ " be computed once. While this fact"
#~ " is very natural to deep learning "
#~ "framework builders, it is something that"
#~ " might surprise a PL researcher in"
#~ " the first place.  If we implement"
#~ " a simple visitor to print out "
#~ "the result and treat the result as"
#~ " nested Call expression, it becomes "
#~ "``log(%x) + log(%x)``."
#~ msgstr ""

#~ msgid ""
#~ "Such ambiguity is caused by different"
#~ " interpretations of program semantics when"
#~ " there is a shared node in the"
#~ " DAG. In a normal functional "
#~ "programming IR, nested expressions are "
#~ "treated as expression trees, without "
#~ "considering the fact that the ``%1`` "
#~ "is actually reused twice in ``%2``."
#~ msgstr ""

#~ msgid ""
#~ "The Relay IR is mindful of this"
#~ " difference. Usually, deep learning "
#~ "framework users build the computational "
#~ "graph in this fashion, where a DAG"
#~ " node reuse often occurs. As a "
#~ "result, when we print out the "
#~ "Relay program in the text format, "
#~ "we print one CallNode per line and"
#~ " assign a temporary id ``(%1, %2)``"
#~ " to each CallNode so each common "
#~ "node can be referenced in later "
#~ "parts of the program."
#~ msgstr ""

#~ msgid "Module: Support Multiple Functions (Graphs)"
#~ msgstr ""

#~ msgid ""
#~ "So far we have introduced how can"
#~ " we build a dataflow graph as a"
#~ " function. One might naturally ask: "
#~ "Can we support multiple functions and"
#~ " enable them to call each other? "
#~ "Relay allows grouping multiple functions "
#~ "together in a module; the code "
#~ "below shows an example of a "
#~ "function calling another function."
#~ msgstr ""

#~ msgid ""
#~ "The Module can be viewed as a "
#~ "``Map<GlobalVar, Function>``. Here GlobalVar "
#~ "is just an id that is used "
#~ "to represent the functions in the "
#~ "module. ``@muladd`` and ``@myfunc`` are "
#~ "GlobalVars in the above example. When"
#~ " a CallNode is used to call "
#~ "another function, the corresponding GlobalVar"
#~ " is stored in the op field of"
#~ " the CallNode. It contains a level"
#~ " of indirection -- we need to "
#~ "look up body of the called "
#~ "function from the module using the "
#~ "corresponding GlobalVar. In this particular"
#~ " case, we could also directly store"
#~ " the reference to the Function as "
#~ "op in the CallNode. So, why do "
#~ "we need to introduce GlobalVar? The "
#~ "main reason is that GlobalVar decouples"
#~ " the definition/declaration and enables "
#~ "recursion and delayed declaration of the"
#~ " function."
#~ msgstr ""

#~ msgid ""
#~ "In the above example, ``@myfunc`` "
#~ "recursively calls itself. Using GlobalVar "
#~ "``@myfunc`` to represent the function "
#~ "avoids the cyclic dependency in the "
#~ "data structure. At this point, we "
#~ "have introduced the basic concepts in"
#~ " Relay. Notably, Relay has the "
#~ "following improvements over NNVMv1:"
#~ msgstr ""

#~ msgid "Succinct text format that eases debugging of writing passes."
#~ msgstr ""

#~ msgid ""
#~ "First-class support for subgraphs-"
#~ "functions, in a joint module, this "
#~ "enables further chance of joint "
#~ "optimizations such as inlining and "
#~ "calling convention specification."
#~ msgstr ""

#~ msgid ""
#~ "Naive front-end language interop, for"
#~ " example, all the data structure can"
#~ " be visited in Python, which allows"
#~ " quick prototyping of optimizations in "
#~ "Python and mixing them with C++ "
#~ "code."
#~ msgstr ""

#~ msgid ""
#~ "So far, we have introduced how to"
#~ " build a computational graph in the"
#~ " good old way used in deep "
#~ "learning frameworks. This section will "
#~ "talk about a new important construct "
#~ "introduced by Relay -- let bindings."
#~ msgstr ""

#~ msgid ""
#~ "Let binding is used in every "
#~ "high-level programming language. In Relay,"
#~ " it is a data structure with "
#~ "three fields ``Let(var, value, body)``. "
#~ "When we evaluate a let expression, "
#~ "we first evaluate the value part, "
#~ "assign it to the var, then return"
#~ " the evaluated result in the body "
#~ "expression."
#~ msgstr ""

#~ msgid ""
#~ "You can use a sequence of let "
#~ "bindings to construct a logically "
#~ "equivalent program to a dataflow "
#~ "program. The code example below shows"
#~ " one program with two forms side "
#~ "by side."
#~ msgstr ""

#~ msgid ""
#~ "The nested let binding is called "
#~ "A-normal form, and it is commonly "
#~ "used as IRs in functional programming"
#~ " languages. Now, please take a close"
#~ " look at the AST structure. While "
#~ "the two programs are semantically "
#~ "identical (so are their textual "
#~ "representations, except that A-normal form "
#~ "has let prefix), their AST structures"
#~ " are different."
#~ msgstr ""

#~ msgid ""
#~ "Since program optimizations take these "
#~ "AST data structures and transform them,"
#~ " the two different structures will "
#~ "affect the compiler code we are "
#~ "going to write. For example, if we"
#~ " want to detect a pattern "
#~ "``add(log(x), y)``:"
#~ msgstr ""

#~ msgid ""
#~ "In the data-flow form, we can "
#~ "first access the add node, then "
#~ "directly look at its first argument "
#~ "to see if it is a log"
#~ msgstr ""

#~ msgid ""
#~ "In the A-normal form, we cannot "
#~ "directly do the check anymore, because"
#~ " the first input to add is "
#~ "``%v1`` -- we will need to keep"
#~ " a map from variable to its "
#~ "bound values and look up that map,"
#~ " in order to know that ``%v1`` "
#~ "is a log."
#~ msgstr ""

#~ msgid ""
#~ "Different data structures will impact "
#~ "how you might write transformations, and"
#~ " we need to keep that in mind."
#~ " So now, as a deep learning "
#~ "framework developer, you might ask, Why"
#~ " do we need let bindings? Your "
#~ "PL friends will always tell you "
#~ "that let is important -- as PL "
#~ "is a quite established field, there "
#~ "must be some wisdom behind that."
#~ msgstr ""

#~ msgid "Why We Might Need Let Binding"
#~ msgstr ""

#~ msgid ""
#~ "One key usage of let binding is"
#~ " that it specifies the scope of "
#~ "computation. Let us take a look at"
#~ " the following example, which does "
#~ "not use let bindings."
#~ msgstr ""

#~ msgid ""
#~ "The problem comes when we try to"
#~ " decide where we should evaluate node"
#~ " ``%1``. In particular, while the "
#~ "text format seems to suggest that "
#~ "we should evaluate node ``%1`` outside"
#~ " the if scope, the AST(as shown "
#~ "in the picture) does not suggest "
#~ "so. Actually, a dataflow graph never "
#~ "defines its scope of the evaluation. "
#~ "This introduces some ambiguity in the"
#~ " semantics."
#~ msgstr ""

#~ msgid ""
#~ "This ambiguity becomes more interesting "
#~ "when we have closures. Consider the "
#~ "following program, which returns a "
#~ "closure. We don’t know where should "
#~ "we compute ``%1``; it can be "
#~ "either inside or outside the closure."
#~ msgstr ""

#~ msgid ""
#~ "A let binding solves this problem, "
#~ "as the computation of the value "
#~ "happens at the let node. In both"
#~ " programs, if we change ``%1 = "
#~ "log(%x)`` to ``let %v1 = log(%x)``, "
#~ "we clearly specify the computation "
#~ "location to be outside of the if"
#~ " scope and closure. As you can "
#~ "see let-binding gives a more "
#~ "precise specification of the computation "
#~ "site and could be useful when we"
#~ " generate backend code (as such "
#~ "specification is in the IR)."
#~ msgstr ""

#~ msgid ""
#~ "On the other hand, the dataflow "
#~ "form, which does not specify the "
#~ "scope of computation, does have its "
#~ "own advantages -- namely, we don’t "
#~ "need to worry about where to put"
#~ " the let when we generate the "
#~ "code. The dataflow form also gives "
#~ "more freedom to the later passes "
#~ "to decide where to put the "
#~ "evaluation point. As a result, it "
#~ "might not be a bad idea to "
#~ "use data flow form of the program"
#~ " in the initial phases of "
#~ "optimizations when you find it is "
#~ "convenient. Many optimizations in Relay "
#~ "today are written to optimize dataflow"
#~ " programs."
#~ msgstr ""

#~ msgid ""
#~ "However, when we lower the IR to"
#~ " an actual runtime program, we need"
#~ " to be precise about the scope "
#~ "of computation. In particular, we want"
#~ " to explicitly specify where the "
#~ "scope of computation should happen when"
#~ " we are using sub-functions and "
#~ "closures. Let-binding can be used "
#~ "to solve this problem in later "
#~ "stage execution specific optimizations."
#~ msgstr ""

#~ msgid "Implication on IR Transformations"
#~ msgstr ""

#~ msgid ""
#~ "Hopefully, by now you are familiar "
#~ "with the two kinds of representations."
#~ " Most functional programming languages do"
#~ " their analysis in A-normal form, "
#~ "where the analyzer does not need "
#~ "to be mindful that the expressions "
#~ "are DAGs."
#~ msgstr ""

#~ msgid ""
#~ "Relay choose to support both the "
#~ "dataflow form and let bindings. We "
#~ "believe that it is important to "
#~ "let the framework developer choose the"
#~ " representation they are familiar with. "
#~ "This does, however, have some "
#~ "implications on how we write passes:"
#~ msgstr ""

#~ msgid ""
#~ "If you come from a dataflow "
#~ "background and want to handle lets, "
#~ "keep a map of var to the "
#~ "expressions so you can perform lookup"
#~ " when encountering a var. This likely"
#~ " means a minimum change as we "
#~ "already need a map from expressions "
#~ "to transformed expressions anyway. Note "
#~ "that this will effectively remove all"
#~ " the lets in the program."
#~ msgstr ""

#~ msgid ""
#~ "If you come from a PL background"
#~ " and like A-normal form, we will "
#~ "provide a dataflow to A-normal form "
#~ "pass."
#~ msgstr ""

#~ msgid ""
#~ "For PL folks, when you are "
#~ "implementing something (like a dataflow-"
#~ "to-ANF transformation), be mindful that "
#~ "expressions can be DAGs, and this "
#~ "usually means that we should visit "
#~ "expressions with a ``Map<Expr, Result>`` "
#~ "and only compute the transformed result"
#~ " once, so the resulting expression "
#~ "keeps the common structure."
#~ msgstr ""

#~ msgid ""
#~ "There are additional advanced concepts "
#~ "such as symbolic shape inference, "
#~ "polymorphic functions that are not "
#~ "covered by this material; you are "
#~ "more than welcome to look at other"
#~ " materials."
#~ msgstr ""

