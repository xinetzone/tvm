# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-09 00:02+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../xin/docs/reference/api/python/autotvm.rst:19
msgid "tvm.autotvm"
msgstr ""

#: of tvm.autotvm:1
msgid "The auto-tuning module of tvm"
msgstr ""

#: of tvm.autotvm:3
msgid "This module includes:"
msgstr ""

#: of tvm.autotvm:5
msgid "Tuning space definition API"
msgstr ""

#: of tvm.autotvm:7
msgid "Efficient auto-tuners"
msgstr ""

#: of tvm.autotvm:9
msgid "Tuning result and database support"
msgstr ""

#: of tvm.autotvm:11
msgid "Distributed measurement to scale up tuning"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyHistoryBest:1
msgid "Apply the history best config"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.__init__:6
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest:9
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest:11
msgid "records"
msgstr ""

#: of
msgid "None, Records, or iterator of Records objects, where a"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyHistoryBest:6
msgid ""
"Records object is a path-like object, a file-like object, or an iterator "
"of (MeasureInput, MeasureResult)."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyHistoryBest:9
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.load:7
msgid ""
"Collection of tuning records. If multiple Records objects are passed, "
"their contents will be merged."
msgstr ""

#: ../../xin/docs/reference/api/python/autotvm.rst:24
msgid "tvm.autotvm.measure"
msgstr ""

#: of tvm.autotvm.measure.measure:1
msgid "User facing API for specifying how to measure the generated code"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:1
msgid "Stores all the necessary inputs for a measurement."
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:5
#: tvm.autotvm.task.dispatcher.ApplyGraphBest._query_inside:5
#: tvm.autotvm.task.task.create:9
msgid "target"
msgstr ""

#: of
msgid "tvm.target.Target"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:6
msgid "The target device"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:7
msgid "task"
msgstr ""

#: of
msgid "task.Task"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:8
msgid "Task function"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:9
#: tvm.autotvm.task.dispatcher.ApplyConfig:7
msgid "config"
msgstr ""

#: of
msgid "ConfigEntity"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureInput:10
msgid "Specific configuration."
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:1
msgid "Stores all the results of a measurement"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:6
msgid "costs: Array of float or Array of Exception"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:6
msgid ""
"If no error occurs during measurement, it is an array of measured running"
" times. If an error occurs during measurement, it is an array of the "
"exception objections."
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:8
msgid "error_no: int"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:9
msgid "Denote error type, defined by MeasureErrorNo"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:10
msgid "all_cost: float"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:11
msgid "All cost of this measure, including rpc, compilation, test runs"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:12
msgid "timestamp: float"
msgstr ""

#: of tvm.autotvm.measure.measure.MeasureResult:13
msgid "The absolute time stamp when we finish measurement."
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:1
msgid ""
"Set options for measure. To measure a config, we will build it and run "
"it. So we have to set options for these two steps. They have their own "
"options on timeout, parallel, etc."
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:7
msgid "builder: Builder"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:8
msgid "Specify how to build programs"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:10
msgid "runner: Runner"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:10
msgid "Specify how to run programs"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:14
msgid ""
"# example setting for using local devices >>> measure_option = "
"autotvm.measure_option( >>>     builder=autotvm.LocalBuilder(),      # "
"use all local cpu cores for compilation >>>     "
"runner=autotvm.LocalRunner(          # measure them sequentially >>>"
"         number=10, >>>         timeout=5) >>> )"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:22
msgid ""
"# example setting for using remote devices >>> measure_option = "
"autotvm.measure_option( >>>    builder=autotvm.LocalBuilder(),  # use all"
" local cpu cores for compilation >>>    runner=autotvm.RPCRunner( >>>"
"        'rasp3b', 'locahost', 9190, # device key, host and port of the "
"rpc tracker >>>        number=4, >>>        timeout=4) # timeout of a run"
" on the device. RPC request waiting time is excluded. >>>)"
msgstr ""

#: of tvm.autotvm.measure.measure.measure_option:33
msgid ""
"To make measurement results accurate, you should pick the correct value "
"for the argument `number` and `repeat` in Runner(). Some devices need a "
"certain minimum running time to \"warm up,\" such as GPUs that need time "
"to reach a performance power state. Using `min_repeat_ms` can dynamically"
" adjusts `number`, so it is recommended. The typical value for NVIDIA GPU"
" is 150 ms."
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:1
msgid "Get a standard measure_batch function."
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:5
msgid "task: tvm.autotvm.task.Task"
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:6
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner:6
msgid "The tuning task"
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:9
msgid "option: dict"
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:8
msgid ""
"The option for measuring generated code. You should use the return value "
"of function :any:`measure_option` for this argument."
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:13
msgid "measure_batch: callable"
msgstr ""

#: of tvm.autotvm.measure.measure.create_measure_batch:14
msgid "a callback function to measure a batch of configs"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:1
msgid "Run compilation on local machine"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:5
#: tvm.autotvm.measure.measure_methods.LocalRunner:5
#: tvm.autotvm.measure.measure_methods.RPCRunner:6
msgid "timeout: float"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:6
#: tvm.autotvm.measure.measure_methods.LocalRunner:6
msgid "The timeout of a compilation"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:7
#: tvm.autotvm.measure.measure_methods.RPCRunner:8
msgid "n_parallel: int"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:8
#: tvm.autotvm.measure.measure_methods.RPCRunner:9
msgid "The number of tasks run in parallel. \"None\" will use all cpu cores"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:10
msgid "build_kwargs: dict"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:10
msgid ""
"If supplied, additional kwargs passed to build_func. Overrides any "
"build_kwargs supplied by the Runner."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:15
msgid "build_func: callable or str"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:13
msgid ""
"If is 'default', use default build function If is 'ndk', use function for"
" android ndk If id 'stackvm', use function for stackvm If is callable, "
"use it as custom build function, expect lib_format field."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:17
msgid "do_fork: bool"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:18
msgid "If False, do not fork when building. Requires n_parallel=1."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:19
msgid "runtime: Optional[Runtime]"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalBuilder:20
msgid "Specify the runtime to generate artifacts for"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:1
msgid ""
"Run generated code on remove devices. This function will ask a RPC "
"Tracker to get device for measurement."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:7
msgid "The timeout of a RPCRunner measurement task"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:10
#: tvm.autotvm.record.measure_str_key:12
msgid "key: str"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:11
msgid "The key of the device registered in the tracker"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:12
msgid "host: str"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:13
msgid "The host address of RPC Tracker"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:14
msgid "port: int"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:15
msgid "The port of RPC Tracker"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:8
#: tvm.autotvm.measure.measure_methods.RPCRunner:17
msgid "number: int"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:8
#: tvm.autotvm.measure.measure_methods.RPCRunner:17
msgid ""
"The number of times to run the generated code for taking average. We call"
" these runs as one `repeat` of measurement."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:14
#: tvm.autotvm.measure.measure_methods.RPCRunner:23
msgid "repeat"
msgstr ""

#: of
msgid "int, optional"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:20
msgid ""
"The number of times to repeat the measurement. In total, the generated "
"code will be run (1 + number x repeat) times, where the first \"1\" is "
"warm up and will be discarded. The returned result contains `repeat` "
"costs, each of which is an average of `number` costs."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:21
#: tvm.autotvm.measure.measure_methods.RPCRunner:30
msgid "min_repeat_ms: int, optional"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:17
#: tvm.autotvm.measure.measure_methods.RPCRunner:26
msgid ""
"The minimum duration of one `repeat` in milliseconds. By default, one "
"`repeat` contains `number` runs. If this parameter is set, the parameters"
" `number` will be dynamically adjusted to meet the minimum duration "
"requirement of one `repeat`. i.e., When the run time of one `repeat` "
"falls below this time, the `number` parameter will be automatically "
"increased."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:23
#: tvm.autotvm.measure.measure_methods.RPCRunner:32
msgid "cooldown_interval: float, optional"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:24
#: tvm.autotvm.measure.measure_methods.RPCRunner:33
msgid "The cool down interval between two measurements."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:29
#: tvm.autotvm.measure.measure_methods.RPCRunner:38
msgid "enable_cpu_cache_flush: bool"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:26
#: tvm.autotvm.measure.measure_methods.RPCRunner:35
msgid ""
"Whether to flush cache on CPU between repeated measurements. Flushing "
"cache can make the measured latency of one operator closer to its actual "
"latency during end-to-end inference. To make this option effective, the "
"argument `number` should also be set to 1. This is only has effect on CPU"
" task."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:41
msgid "module_loader"
msgstr ""

#: of
msgid "ModuleLoader"
msgstr ""

#: of tvm.autotvm.measure.measure_methods.RPCRunner:41
msgid ""
"If given, a context manager that loads the module to be timed into the "
"remote runtime. If not given, default_module_loader is used."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:1
msgid "Run generated code on local devices."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:11
msgid ""
"The number of times to repeat the measurement. In total, the generated "
"code will be run (1 + number x repeat) times, where the first one is warm"
" up and will be discarded. The returned result contains `repeat` costs, "
"each of which is an average of `number` costs."
msgstr ""

#: of tvm.autotvm.measure.measure_methods.LocalRunner:33
msgid ""
"This is a \"fake\" local mode. We start a silent rpc tracker and rpc "
"server for the user. In this way we reuse timeout/isolation mechanism in "
"RPC infrastructure."
msgstr ""

#: ../../xin/docs/reference/api/python/autotvm.rst:44
msgid "tvm.autotvm.tuner"
msgstr ""

#: of tvm.autotvm.tuner:1
msgid ""
"A tuner takes a task as input. It proposes some promising "
":any:`ConfigEntity` in the :any:`ConfigSpace` and measure them on the "
"real hardware. Then it proposed the next batch of :any:`ConfigEntity` "
"according to the measure results. This tuning loop is repeated."
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner:1
msgid "Base class for tuners"
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.RandomTuner:6
#: tvm.autotvm.tuner.tuner.Tuner:7
msgid "task: autotvm.task.Task"
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.RandomTuner:6
#: tvm.autotvm.tuner.tuner.Tuner:6
msgid "Tuning Task"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.has_next:1
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.has_next:1
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.has_next:1
#: tvm.autotvm.tuner.tuner.Tuner.has_next:1
msgid "Whether has next untried config in the space"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.has_next:5
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.has_next:5
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.has_next:5
#: tvm.autotvm.tuner.tuner.Tuner.has_next:5
msgid "has_next: bool"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.load_history:1
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.load_history:1
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.load_history:1
#: tvm.autotvm.tuner.tuner.Tuner.load_history:1
msgid "load history data for transfer learning"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.load_history:5
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.load_history:5
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.load_history:5
#: tvm.autotvm.tuner.tuner.Tuner.load_history:5
msgid ""
"data_set: Array of (autotvm.measure.MeasureInput, "
"autotvm.measure.MeasureResult) pair"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.load_history:6
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.load_history:6
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.load_history:6
#: tvm.autotvm.tuner.tuner.Tuner.load_history:6
msgid "Previous tuning records"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.load_history:10
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.load_history:10
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.load_history:10
#: tvm.autotvm.tuner.tuner.Tuner.load_history:10
msgid "min_seed_records: int"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.load_history:8
#: tvm.autotvm.tuner.index_based_tuner.IndexBaseTuner.load_history:8
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.load_history:8
#: tvm.autotvm.tuner.tuner.Tuner.load_history:8
msgid ""
"Defaults to 500. Indicates the minimum number of records to train the "
"tuner with. If there are less than `min_seed_records` number of records "
"in `data_set`, no training of the tuner will be done."
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.next_batch:1
#: tvm.autotvm.tuner.index_based_tuner.GridSearchTuner.next_batch:1
#: tvm.autotvm.tuner.index_based_tuner.RandomTuner.next_batch:1
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.next_batch:1
#: tvm.autotvm.tuner.tuner.Tuner.next_batch:1
msgid "get the next batch of configs to be measure on real hardware"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.next_batch:6
#: tvm.autotvm.tuner.index_based_tuner.GridSearchTuner.next_batch:6
#: tvm.autotvm.tuner.index_based_tuner.RandomTuner.next_batch:6
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.next_batch:6
#: tvm.autotvm.tuner.tuner.Tuner.next_batch:6
msgid "batch_size: int"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.next_batch:6
#: tvm.autotvm.tuner.index_based_tuner.GridSearchTuner.next_batch:6
#: tvm.autotvm.tuner.index_based_tuner.RandomTuner.next_batch:6
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.next_batch:6
#: tvm.autotvm.tuner.tuner.Tuner.next_batch:6
msgid "The size of the batch"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.next_batch:10
#: tvm.autotvm.tuner.index_based_tuner.GridSearchTuner.next_batch:10
#: tvm.autotvm.tuner.index_based_tuner.RandomTuner.next_batch:10
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.next_batch:10
#: tvm.autotvm.tuner.tuner.Tuner.next_batch:10
msgid "a batch of configs"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.reset:1
msgid "reset the status of tuner"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.set_error_threshold:1
msgid "Modify error counter threshold, which controls switch to debug mode"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.set_error_threshold:5
msgid "threshold: New threshold value"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:1
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:1
msgid "Begin tuning"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:5
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:5
msgid "n_trial: int"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:6
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:6
msgid "Maximum number of configs to try (measure on real hardware)"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:8
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:8
msgid "measure_option: dict"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:8
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:8
msgid ""
"The options for how to measure generated code. You should use the return "
"value ot autotvm.measure_option for this argument."
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:10
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:10
msgid "early_stopping: int, optional"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:11
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:11
msgid ""
"Early stop the tuning when not finding better configs in this number of "
"trials"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:15
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:15
msgid "callbacks: List of callable"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:13
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:13
msgid ""
"A list of callback functions. The signature of callback function is "
"(Tuner, List of MeasureInput, List of MeasureResult) with no return "
"value. These callback functions will be called on every measurement pair."
" See autotvm/tuner/callback.py for some examples."
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:9
#: tvm.autotvm.tuner.tuner.Tuner.tune:17
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:17
msgid "si_prefix: str"
msgstr ""

#: of tvm.autotvm.tuner.tuner.Tuner.tune:18
#: tvm.autotvm.tuner.xgboost_tuner.XGBTuner.tune:18
msgid ""
"One of tvm.autotvm.utils.SI_PREFIXES. The SI prefix to use when reporting"
" FLOPS."
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.update:1
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.update:1
#: tvm.autotvm.tuner.tuner.Tuner.update:1
msgid "Update parameters of the tuner according to measurement results"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.update:5
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.update:5
#: tvm.autotvm.tuner.tuner.Tuner.update:5
msgid "inputs: Array of autotvm.measure.MeasureInput"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.update:6
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.update:6
#: tvm.autotvm.tuner.tuner.Tuner.update:6
msgid "The input for measurement"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.update:7
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.update:7
#: tvm.autotvm.tuner.tuner.Tuner.update:7
msgid "results: Array of autotvm.measure.MeasureResult"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner.update:8
#: tvm.autotvm.tuner.model_based_tuner.ModelBasedTuner.update:8
#: tvm.autotvm.tuner.tuner.Tuner.update:8
msgid "result for measurement"
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.RandomTuner:1
msgid "Enumerate the search space in a random order"
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.RandomTuner:10
msgid "range_idx: Optional[Tuple[int, int]]"
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.RandomTuner:9
msgid "A tuple of index range to random"
msgstr ""

#: of tvm.autotvm.tuner.index_based_tuner.GridSearchTuner:1
msgid "Enumerate the search space in a grid search order"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:1
msgid ""
"Tuner with genetic algorithm. This tuner does not have a cost model so it"
" always run measurement on real machines. This tuner expands the "
":code:`ConfigEntity` as gene."
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:7
msgid "pop_size: int"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:8
msgid "number of genes in one generation"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:9
msgid "elite_num: int"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:10
msgid "number of elite to keep"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:13
msgid "mutation_prob: float"
msgstr ""

#: of tvm.autotvm.tuner.ga_tuner.GATuner:12
msgid "probability of mutation of a knob in a gene"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:1
msgid "Tuner that uses xgboost as cost model"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:5
msgid "task: Task"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:8
msgid "plan_size: int"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:8
msgid ""
"The size of a plan. After `plan_size` trials, the tuner will refit a new "
"cost model and do planing for the next `plan_size` trials."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:25
msgid "feature_type: str, optional"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:11
msgid ""
"If is 'itervar', use features extracted from IterVar (loop variable). If "
"is 'knob', use flatten ConfigEntity directly. If is 'curve', use sampled "
"curve feature (relation feature)."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:15
msgid ""
"Note on choosing feature type: For single task tuning, 'itervar' and "
"'knob' are good. 'itervar' is more accurate but 'knob' is much faster. "
"There are some constraints on 'itervar', if you meet problems with "
"feature extraction when using 'itervar', you can switch to 'knob'."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:22
msgid ""
"For cross-shape tuning (e.g. many convolutions with different shapes), "
"'itervar' and 'curve' has better transferability, 'knob' is faster."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:26
msgid "For cross-device or cross-operator tuning, you can use 'curve' only."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:31
msgid "loss_type: str"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:28
msgid ""
"If is 'reg', use regression loss to train cost model. The cost model "
"predicts the normalized flops. If is 'rank', use pairwise rank loss to "
"train cost model. The cost model predicts relative rank score."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:34
msgid "num_threads: int, optional"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:34
msgid "The number of threads."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:38
msgid "optimizer: str or ModelOptimizer, optional"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:37
msgid ""
"If is 'sa', use a default simulated annealing optimizer. Otherwise it "
"should be a ModelOptimizer object."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:43
msgid "diversity_filter_ratio: int or float, optional"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:41
msgid ""
"If is not None, the tuner will first select top-(plan_size * "
"diversity_filter_ratio) candidates according to the cost model and then "
"pick batch_size of them according to the diversity metric."
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:49
msgid "log_interval: int = 50"
msgstr ""

#: of tvm.autotvm.tuner.xgboost_tuner.XGBTuner:46
msgid ""
"The verbose level. If is 0, output nothing. Otherwise, output debug "
"information every `verbose` iterations."
msgstr ""

#: of tvm.autotvm.tuner.callback:1
msgid "Namespace of callback utilities of AutoTVM"
msgstr ""

#: of tvm.autotvm.tuner.callback.Monitor:1
msgid "A monitor to collect statistic during tuning"
msgstr ""

#: of tvm.autotvm.tuner.callback.Monitor.trial_scores:1
msgid "get scores (currently is flops) of all trials"
msgstr ""

#: of tvm.autotvm.tuner.callback.Monitor.trial_timestamps:1
msgid "get wall clock time stamp of all trials"
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_database:1
msgid "Save the tuning records to a database object."
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_database:5
msgid "db: Database"
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_database:6
msgid "The database"
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:1
msgid ""
"Log the tuning records into file. The rows of the log are stored in the "
"format of autotvm.record.encode."
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:6
msgid "file_out"
msgstr ""

#: of
msgid "File or str"
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:7
msgid "The file to log to."
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:9
msgid "protocol: str, optional"
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:9
msgid "The log protocol. Can be 'json' or 'pickle'"
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:13
msgid "callback"
msgstr ""

#: of
msgid "callable"
msgstr ""

#: of tvm.autotvm.tuner.callback.log_to_file:14
msgid "Callback function to do the logging."
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:1
msgid "Display progress bar for tuning"
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:5
msgid "total: int"
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:6
msgid "The total number of trials"
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:7
msgid "prefix: str"
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:8
msgid "The prefix of output message"
msgstr ""

#: of tvm.autotvm.tuner.callback.progress_bar:10
msgid "SI prefix for flops"
msgstr ""

#: ../../xin/docs/reference/api/python/autotvm.rst:71
msgid "tvm.autotvm.task"
msgstr ""

#: of tvm.autotvm.task:1
msgid "Task is a tunable composition of template functions."
msgstr ""

#: of tvm.autotvm.task:3
msgid ""
"Tuner takes a tunable task and optimizes the joint configuration space of"
" all the template functions in the task. This module defines the task "
"data structure, as well as a collection(zoo) of typical tasks of "
"interest."
msgstr ""

#: of tvm.autotvm.task.task:1
msgid "Definition of task function."
msgstr ""

#: of tvm.autotvm.task.task:3
msgid ""
"Task can be constructed from tuple of func, args, and kwargs. func is a "
"state-less function, or a string that registers the standard task."
msgstr ""

#: of tvm.autotvm.task.task.FlopCalculationError:1
msgid "Error happens when estimating FLOP for a compute op"
msgstr ""

#: of tvm.autotvm.task.task.MissingTask:1
msgid ""
"Dummy task template for a task lookup which cannot be resolved. This can "
"occur if the task being requested from _lookup_task() has not been "
"imported in this run."
msgstr ""

#: of tvm.autotvm.task.task.Task:1
msgid "A Tunable Task"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.__getitem__:6
#: tvm.autotvm.task.space.ConfigSpace.define_annotate:5
#: tvm.autotvm.task.space.ConfigSpace.define_knob:5
#: tvm.autotvm.task.space.ConfigSpace.define_reorder:5
#: tvm.autotvm.task.space.ConfigSpace.define_split:5
#: tvm.autotvm.task.space.FallbackConfigEntity.__setitem__:5
#: tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:5
#: tvm.autotvm.task.space.VirtualAxis:10 tvm.autotvm.task.task.Task:5
#: tvm.autotvm.task.task._register_customized_task:6
#: tvm.autotvm.task.task._register_task_compute:6
#: tvm.autotvm.task.task._register_task_schedule:6
msgid "name: str"
msgstr ""

#: of tvm.autotvm.task.task.Task:6
msgid "The name of the task."
msgstr ""

#: of tvm.autotvm.task.task.Task:9
msgid "args: Tuple"
msgstr ""

#: of tvm.autotvm.task.task.Task:8
msgid "Positional argument of func"
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:1
msgid ""
"Instantiate this task function (template) with a config. Returns "
"corresponding schedule."
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:7
msgid "config: template.ConfigEntity"
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:7
msgid "parameter config for this template"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:5
#: tvm.autotvm.task.space.ReorderEntity.apply:5
#: tvm.autotvm.task.space.SplitEntity.apply:5
#: tvm.autotvm.task.task.Task.instantiate:11
#: tvm.autotvm.task.task.compute_flop:6
msgid "sch: tvm.te.schedule.Schedule"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:6
#: tvm.autotvm.task.space.ReorderEntity.apply:6
#: tvm.autotvm.task.space.SplitEntity.apply:6
#: tvm.autotvm.task.task.Task.instantiate:12
msgid "The tvm schedule"
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:13
msgid "arg_bufs: Array of te.tensor.Tensor"
msgstr ""

#: of tvm.autotvm.task.task.Task.instantiate:14
msgid "The input/output buffers"
msgstr ""

#: of tvm.autotvm.task.task.TaskTemplate:1
msgid "Task template is used to creates a tunable AutoTVM task."
msgstr ""

#: of tvm.autotvm.task.task.TaskTemplate:3
msgid ""
"It can be defined by a pair of compute and schedule function using "
"`_register_task_compute` and `_register_task_schedule`, or by a "
"customized task creation function that is more flexible using "
"`_register_customized_task`."
msgstr ""

#: of tvm.autotvm.task.task.TaskTemplate:8
msgid ""
"Note that when customized func is registered, compute and schedule "
"function will be ignored"
msgstr ""

#: of tvm.autotvm.task.task._register_customized_task:1
msgid "Register a customized function to AutoTVM task."
msgstr ""

#: of tvm.autotvm.task.task._register_customized_task:6
#: tvm.autotvm.task.task._register_task_compute:6
#: tvm.autotvm.task.task._register_task_schedule:6
#: tvm.autotvm.task.task.template:6
msgid "The task name"
msgstr ""

#: of tvm.autotvm.task.task._register_customized_task:10
#: tvm.autotvm.task.task._register_task_compute:10
#: tvm.autotvm.task.task._register_task_schedule:10
#: tvm.autotvm.task.task.template:11
#: tvm.autotvm.task.topi_integration.register_topi_compute:15
#: tvm.autotvm.task.topi_integration.register_topi_schedule:18
msgid "func: None or callable"
msgstr ""

#: of tvm.autotvm.task.task._register_customized_task:9
#: tvm.autotvm.task.task._register_task_compute:9
#: tvm.autotvm.task.task._register_task_schedule:9
#: tvm.autotvm.task.topi_integration.register_topi_compute:14
#: tvm.autotvm.task.topi_integration.register_topi_schedule:17
msgid "If it is None, return a decorator. If is callable, decorate this function."
msgstr ""

#: of tvm.autotvm.task.task._register_customized_task:14
#: tvm.autotvm.task.task._register_task_compute:14
#: tvm.autotvm.task.task._register_task_schedule:14
#: tvm.autotvm.task.topi_integration.register_topi_compute:20
#: tvm.autotvm.task.topi_integration.register_topi_schedule:23
msgid "decorator: callable"
msgstr ""

#: of tvm.autotvm.task.task._register_customized_task:15
#: tvm.autotvm.task.task._register_task_compute:15
#: tvm.autotvm.task.task._register_task_schedule:15
#: tvm.autotvm.task.topi_integration.register_topi_compute:20
#: tvm.autotvm.task.topi_integration.register_topi_schedule:23
msgid "A decorator"
msgstr ""

#: of tvm.autotvm.task.task._register_task_compute:1
msgid "Register compute function to autotvm task"
msgstr ""

#: of tvm.autotvm.task.task._register_task_schedule:1
msgid "Register schedule function to autotvm task"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:1
msgid ""
"Convert argument list to hashable workload tuple. This function will "
"convert list to tuple, tvm node to python value and flatten "
"te.tensor.Tensor to a tuple"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:8 tvm.autotvm.task.task.create:5
msgid "task_name"
msgstr ""

#: of
msgid "str"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:8 tvm.autotvm.task.task.create:6
#: tvm.autotvm.task.topi_integration.register_topi_compute:11
#: tvm.autotvm.task.topi_integration.register_topi_schedule:14
msgid "The AutoTVM task name"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:11 tvm.autotvm.task.task.create:7
msgid "args"
msgstr ""

#: of
msgid "list of args"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:11
msgid "The arguments to the function"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:15
msgid "ret: hashable"
msgstr ""

#: of tvm.autotvm.task.task.args_to_workload:16
msgid "The hashable value"
msgstr ""

#: of tvm.autotvm.task.task.compute_flop:1
msgid ""
"Calculate number of FLOP (floating number operations) of the compute ops "
"in a schedule"
msgstr ""

#: of tvm.autotvm.task.task.compute_flop:6
msgid "schedule"
msgstr ""

#: of tvm.autotvm.task.task.compute_flop:10
msgid "flop: int"
msgstr ""

#: of tvm.autotvm.task.task.compute_flop:11
msgid "number of FLOP in this schedule"
msgstr ""

#: of tvm.autotvm.task.task.create:1
msgid "Create a tuning task and initialize its search space"
msgstr ""

#: of
msgid "List"
msgstr ""

#: of tvm.autotvm.task.task.create:8
msgid "Positional arguments"
msgstr ""

#: of
msgid "Target"
msgstr ""

#: of tvm.autotvm.task.task.create:10
msgid "The compilation target"
msgstr ""

#: of tvm.autotvm.task.task.create:12
msgid "target_host: Target, optional"
msgstr ""

#: of tvm.autotvm.task.task.create:12
msgid "The compilation target for host side"
msgstr ""

#: of tvm.autotvm.task.task.create:16
msgid "tsk: Task"
msgstr ""

#: of tvm.autotvm.task.task.create:17
msgid "a task object"
msgstr ""

#: of tvm.autotvm.task.task.deserialize_args:1
msgid "The inverse function of :code:`serialize_args`."
msgstr ""

#: of tvm.autotvm.task.task.deserialize_args:5
#: tvm.autotvm.task.task.serialize_args:5
msgid "args: list of hashable or Tensor"
msgstr ""

#: of tvm.autotvm.task.task.get_config:1
msgid "Get current config object"
msgstr ""

#: of tvm.autotvm.task.task.get_config:5
msgid "cfg: ConfigSpace or ConfigEntity"
msgstr ""

#: of tvm.autotvm.task.task.get_config:6
msgid "The current config"
msgstr ""

#: of tvm.autotvm.task.task.serialize_args:1
msgid "serialize arguments of a topi function to a hashable tuple."
msgstr ""

#: of tvm.autotvm.task.task.template:1
msgid "Decorate a function as a tunable schedule template."
msgstr ""

#: of tvm.autotvm.task.task.template:6
#: tvm.autotvm.task.topi_integration.TaskExtractEnv.add_task:6
#: tvm.autotvm.task.topi_integration.register_topi_compute:11
#: tvm.autotvm.task.topi_integration.register_topi_schedule:14
msgid "task_name: str"
msgstr ""

#: of tvm.autotvm.task.task.template:9
msgid ""
"A callable template function. If it is None, return a decorator. If is "
"callable, decorate this function."
msgstr ""

#: of tvm.autotvm.task.task.template:16
msgid "func: callable"
msgstr ""

#: of tvm.autotvm.task.task.template:16
msgid "The decorated function"
msgstr ""

#: of tvm.autotvm.task.task.template:20
msgid ""
"The following code is a tunable template for a blocked matrix "
"multiplication"
msgstr ""

#: of tvm.autotvm.task.space:1
msgid "Template configuration space."
msgstr ""

#: of tvm.autotvm.task.space:3
msgid ""
"Each template function can be parameterized by a ConfigSpace. The space "
"is declared when we invoke the template function with ConfigSpace. During"
" evaluation, we pass in a ConfigEntity, which contains a specific entity "
"in the space. This entity contains deterministic parameters."
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity:1
msgid "An annotation operation with detailed parameters that can apply to axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity:7
msgid "anns: Array of string"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity:6
msgid "The annotations of axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:1
msgid "Apply annotation to an array of axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:7
#: tvm.autotvm.task.space.ReorderEntity.apply:7
#: tvm.autotvm.task.space.SplitEntity.apply:7
msgid "op: tvm.te.Operation"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:8
#: tvm.autotvm.task.space.ReorderEntity.apply:8
#: tvm.autotvm.task.space.SplitEntity.apply:8
msgid "The stage to be applied"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:9
#: tvm.autotvm.task.space.ConfigSpace.define_annotate:7
#: tvm.autotvm.task.space.ConfigSpace.define_reorder:7
msgid "axes: Array of tvm.te.schedule.IterVar"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:10
#: tvm.autotvm.task.space.ConfigSpace.define_split:8
#: tvm.autotvm.task.space.ReorderEntity.apply:10
#: tvm.autotvm.task.space.SplitEntity.apply:10
msgid "axis to split"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:11
msgid "axis_lens: Array of int, optional"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:12
msgid "the length of axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:13
msgid "max_unroll: int, optional"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:14
msgid "maximum unroll step"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:15
msgid "vec_size: Array of int, optional"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:16
msgid "valid vector lanes for vectorization"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:17
msgid "cfg: ConfigEntity, optional"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:18
msgid "cfg for recording error"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:20
msgid "source: Array of Array tensor, optional"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:20
msgid "source tensor for attaching cache"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:24
#: tvm.autotvm.task.space.ReorderEntity.apply:14
#: tvm.autotvm.task.space.SplitEntity.apply:14
msgid "axes"
msgstr ""

#: of
msgid "list of tvm.te.schedule.IterVar"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateEntity.apply:25
msgid "The transformed axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateSpace:1
msgid "The parameter space for annotating an array of axes"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateSpace._generate_space:1
#: tvm.autotvm.task.space.SplitSpace._generate_space:1
msgid "Generate space by DFS"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateSpace.get_num_output:1
#: tvm.autotvm.task.space.OtherOptionSpace.get_num_output:1
#: tvm.autotvm.task.space.ReorderSpace.get_num_output:1
#: tvm.autotvm.task.space.SplitSpace.get_num_output:1
#: tvm.autotvm.task.space.TransformSpace.get_num_output:1
#: tvm.autotvm.task.space.VirtualAxis.get_num_output:1
msgid "get number of output axes after this transform"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateSpace.get_num_output:5
#: tvm.autotvm.task.space.OtherOptionSpace.get_num_output:5
#: tvm.autotvm.task.space.ReorderSpace.get_num_output:5
#: tvm.autotvm.task.space.SplitSpace.get_num_output:5
#: tvm.autotvm.task.space.TransformSpace.get_num_output:5
#: tvm.autotvm.task.space.VirtualAxis.get_num_output:5
#: tvm.autotvm.task.space.get_factors:6 tvm.autotvm.task.space.get_pow2s:6
msgid "n: int"
msgstr ""

#: of tvm.autotvm.task.space.AnnotateSpace.get_num_output:6
#: tvm.autotvm.task.space.OtherOptionSpace.get_num_output:6
#: tvm.autotvm.task.space.ReorderSpace.get_num_output:6
#: tvm.autotvm.task.space.SplitSpace.get_num_output:6
#: tvm.autotvm.task.space.TransformSpace.get_num_output:6
#: tvm.autotvm.task.space.VirtualAxis.get_num_output:6
msgid "number of output axes"
msgstr ""

#: ../../../xinetzone/docstring of tvm.autotvm.task.space.Axis.index:1
msgid "Alias for field number 1"
msgstr ""

#: ../../../xinetzone/docstring of tvm.autotvm.task.space.Axis.space:1
msgid "Alias for field number 0"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:1
msgid "A configuration with detailed parameters"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:5
#: tvm.autotvm.task.space.ConfigSpace.get:6
#: tvm.autotvm.task.space.ConfigSpace.get_next_index:5
#: tvm.autotvm.task.space.ConfigSpace.is_index_valid:6
#: tvm.autotvm.task.space.TransformSpace.__getitem__:5
msgid "index: int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:6
msgid "index of this config in space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:7
msgid "code_hash: str"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:8
msgid "hash of schedule code"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:9
msgid "entity_map: dict"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:10
msgid "map name to transform entity"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:13
msgid "constraints"
msgstr ""

#: of
msgid "list"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity:12
msgid "List of constraints"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.from_json_dict:1
msgid "Build a ConfigEntity from json serializable dictionary"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.from_json_dict:7
#: tvm.autotvm.task.space.ConfigEntity.to_json_dict:5
msgid "json_dict: dict"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.from_json_dict:6
msgid ""
"Json serializable dictionary. This should be the return value of "
":any:`to_json_dict`."
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.from_json_dict:11
#: tvm.autotvm.task.space.ConfigSpace.get:10
msgid "config: ConfigEntity"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.from_json_dict:12
msgid "The corresponding config object"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.get_flatten_feature:1
msgid "flatten entities to a numerical one-dimensional feature vector"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.get_flatten_feature:5
msgid "fea: np.array"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.get_flatten_feature:6
msgid "one dimensional float32 array"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.get_other_option:3
msgid "other_option: dict"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.get_other_option:4
msgid "other tunable parameters (tunable parameters defined by `cfg.define_knob`)"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.to_json_dict:1
msgid "convert to a json serializable dictionary"
msgstr ""

#: of tvm.autotvm.task.space.ConfigEntity.to_json_dict:6
msgid "a json serializable dictionary"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace:1
msgid ""
"The configuration space of a schedule. Pass it as config in template to "
"collect transformation space and build transform graph of axes"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.__getitem__:2
msgid "get the transform entity(knob) of this entity by name"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.__getitem__:2
msgid ""
"do not use this to get a ConfigEntity of this space (should use "
"ConfigSpace.get instead)"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.__getitem__:7
msgid "name of the transform"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.__len__:1
msgid "Returns the number of valid indexes in the space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace._add_new_transform:1
msgid "Add a new transform space in template"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.add_flop:1
msgid "Add float operation statistics for this tuning task"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.add_flop:5
msgid "flop: int or float or IntImm or FloatImm"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.add_flop:6
msgid "number of float operations"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.axis:1
msgid "get a virtual axis (axis placeholder)"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.axis:7
#: tvm.autotvm.task.space.VirtualAxis:8
msgid "var: int or tvm.te.schedule.IterVar"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.axis:6
msgid ""
"If is int, return an axis whose length is the provided argument. If is "
"IterVar, return an axis whose length is extracted from the IterVar's "
"extent domain."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.clear_cache:1
msgid "Clears the cache of index validity"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:1
msgid "Define a new tunable knob which annotates a list of axes"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:6
#: tvm.autotvm.task.space.ConfigSpace.define_reorder:6
#: tvm.autotvm.task.space.ConfigSpace.define_split:6
msgid "name to index the entity of this space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:8
msgid "axes to annotate"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:14
#: tvm.autotvm.task.space.ConfigSpace.define_reorder:14
#: tvm.autotvm.task.space.ConfigSpace.define_split:13
msgid "policy: str"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:10
msgid ""
"name of policy If is 'unroll', unroll the axes. If is 'try_unroll', try "
"to unroll the axes. If is 'try_unroll_vec', try to unroll or vectorize "
"the axes. If is 'bind_gpu', bind the first few axes to gpu threads. If is"
" 'locate_cache', choose n axes to attach shared/local cache."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:16
#: tvm.autotvm.task.space.ConfigSpace.define_reorder:16
msgid "kwargs: dict"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_annotate:17
#: tvm.autotvm.task.space.ConfigSpace.define_reorder:17
#: tvm.autotvm.task.space.ConfigSpace.define_split:16
msgid "extra arguments for policy"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_knob:1
msgid "Define a tunable knob with a list of candidates"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_knob:6
msgid "name key of that option"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_knob:7
msgid "candidate: list"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_knob:8
msgid "list of candidates"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_reorder:1
msgid "Define a new tunable knob which reorders a list of axes"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_reorder:8
msgid "axes to reorder"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_reorder:10
msgid ""
"name of policy If is 'identity', do an identity permutation. If is 'all',"
" try all permutations. If is 'interval_all', try all permutations of an "
"interval of axes. If is 'candidate', try listed candidate. If is "
"'interleave', interleave chains of spatial axes and chains of reduction "
"axes."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:1
msgid "Define a new tunable knob which splits an axis into a list of axes"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:7
#: tvm.autotvm.task.space.ReorderEntity.apply:10
#: tvm.autotvm.task.space.SplitEntity.apply:10
msgid "axis: tvm.te.schedule.IterVar"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:10
msgid ""
"name of policy. If is 'factors', the tuner will try all divisible "
"factors. If is 'power2', the tuner will try power-of-two factors less or "
"equal to the length. If is 'verbose', the tuner will try all candidates "
"in above two policies. If is 'candidate', try given candidates."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:27
msgid "**kwargs:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:18
msgid "``max_factor``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:19
msgid "the maximum split factor (`int`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:20
msgid "``filter``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:21
msgid "see examples below for how to use filter (`Callable[[int], bool]`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:22
msgid "``num_outputs``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:23
msgid "the total number of axis after split (`int`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:24
msgid "``no_tail``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:25
msgid "should we only include divisible numbers as split factors (`bool`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:27
msgid "``candidate``:"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.define_split:27
msgid "(policy=candidate) manual candidate list (`List`)."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.dims:1
msgid "Dimensions in the space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get:1
msgid "Get a config entity with detailed parameters from this space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get:6
msgid "index in the space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get:11
msgid "config corresponds to the index"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:1
msgid "Returns the nth valid next index or None if out of range"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:6
#: tvm.autotvm.task.space.ConfigSpace.get_rand_index:6
msgid "specifying at which position to start, inclusive"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:8
msgid "n: int, optional"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:8
msgid ""
"step by using to find the next index, for the opposite direction a "
"negative number should be used"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:10
msgid "start: list, optional"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:11
#: tvm.autotvm.task.space.ConfigSpace.subrange_length:6
msgid "start of subrange, inclusive"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:13
msgid "end: list, optional"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:13
#: tvm.autotvm.task.space.ConfigSpace.subrange_length:8
msgid "end of subrange, exclusive"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:17
msgid "next: int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_next_index:18
msgid "next index in the space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_rand_index:1
msgid "Returns a random valid index unlisted to exclusion"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_rand_index:5
msgid "start: int, optional"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_rand_index:7
msgid "end: int, optional"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_rand_index:8
msgid "specifying at which position to end, exclusive"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_rand_index:10
msgid "to_exclude: list, optional"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_rand_index:10
msgid "determines unsuitable values"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_rand_index:15
msgid "rand: int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_rand_index:15
msgid "random index in the space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.get_rand_index:19
msgid "Excluding all valid space indexes will lead to an infinite loop."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.is_index_valid:1
msgid "Checks if the index satisfies the multi_filter condition"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.is_index_valid:6
msgid "index from the range of the space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.is_index_valid:10
#: tvm.autotvm.task.space.ConfigSpace.valid:10
msgid "valid: bool"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.is_index_valid:11
msgid "whether the index meets all the constraints"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.knob2point:1
msgid "Convert knob form (vector) to point form (single integer)"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.knob2point:6
#: tvm.autotvm.task.space.ConfigSpace.point2knob:10
msgid "knob: list"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.knob2point:6
msgid "knob to convert"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.knob2point:10
#: tvm.autotvm.task.space.ConfigSpace.point2knob:6
#: tvm.autotvm.task.space.ConfigSpace.random_walk:6
msgid "point: int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.knob2point:11
msgid "point of the knob representation"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.multi_filter:1
msgid ""
"The filter can restrict combination of parameters in difference to the "
"knob filter, that restricts only single parameter"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.multi_filter:7
msgid "filter: function"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.multi_filter:7
msgid "predicate with one argument (Callable[[int], bool])"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.multi_filter:11
msgid ""
"Using this filter causes additional restrictions on the use of __len__. "
"Normally, it define the count of valid indexes and the range of space, "
"but when multi_filter enabled, it requires to use __len__ for getting the"
" count of valid indexes or range_length for the range of space. It is "
"recommended to use: ``is_index_valid``, ``get_next_index``, "
"``get_rand_index`` to bypass the space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.point2knob:1
msgid "Convert point form (single integer) to knob (vector)"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.point2knob:6
msgid "point to convert"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.point2knob:11
msgid "knob representation of the point"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.raise_error:1
msgid ""
"register error in config Using this to actively detect error when "
"scheduling. Otherwise these error will occur during runtime, which will "
"cost more time."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.raise_error:8
msgid "msg: str"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.random_walk:1
msgid "random walk as local transition"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.random_walk:6
msgid "index of the ConfigEntity"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.random_walk:10
msgid "new_point: int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.random_walk:11
msgid "new neighborhood index"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.range_length:1
msgid "Length of the index range in the space"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.sample_ints:1
msgid ""
"Sample m different integer numbers from [0, self.range_length) without "
"replacement This function is an alternative of `np.random.choice` when "
"self.range_length > 2 ^ 32, in which case numpy does not work."
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.sample_ints:8
msgid "m: int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.sample_ints:8
msgid "The number of sampled int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.sample_ints:12
msgid "ints: an numpy array of size m"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.subrange_length:1
msgid ""
"Returns the number of valid indexes within the limited range from [start,"
" end]"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.subrange_length:5
msgid "start: int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.subrange_length:8
msgid "end: int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.subrange_length:12
msgid "count: int"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.subrange_length:13
msgid "number of valid indexes"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.valid:1
msgid "Check whether the config meets all the constraints"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.valid:5
msgid ""
"This check should be called after instantiation of task, because the "
"ConfigEntity/ConfigSpace collects errors during instantiation"
msgstr ""

#: of tvm.autotvm.task.space.ConfigSpace.valid:11
msgid "whether the config meets all the constraints"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity:1
msgid "The config entity created to support fallback"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.__setitem__:1
msgid "set the entity(knob) of by name"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.__setitem__:6
msgid "name of the entity"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.__setitem__:7
msgid "entity: SplitEntity, ReorderEntity, AnnotateEntity, OtherOptionEntity"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.__setitem__:8
msgid "value of the entity"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:1
msgid "Fallback a split knob"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:6
msgid "name of the knob"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:8
msgid "constraints: List of int"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:8
msgid "The maximum tile size for every dimension. Value `-1` means no constraint."
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:12
msgid ""
"If you use cfg.define_split('tile_0', 128, num_outputs=3), Then "
"cfg.fallback_split('tile_0', [-1, 8, 4]) will give you cfg['tile_0'].size"
" = [4, 8, 4]"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_split:15
msgid ""
"If you use cfg.define_split('tile_0', 49, num_outputs=3), Then "
"cfg.fallback_split('tile_0', [-1, 8, 4]) will give you cfg['tile_0'].size"
" = [7, 7, 1]"
msgstr ""

#: of tvm.autotvm.task.space.FallbackConfigEntity.fallback_with_reference_log:1
msgid ""
"A data driven fallback mechanism. We use tuned parameters from TopHub as "
"reference data. For an unseen shape, we find the most similar tuned one "
"from TopHub and mimic its parameters. Note that we are not matching by "
"workload (e.g., input size, kernel size), but instead matching by "
"configuration space. The idea is that if two workloads have similar "
"configuration space, their optimal configurations are also likely to be "
"similar."
msgstr ""

#: of
#: tvm.autotvm.task.space.FallbackConfigEntity.fallback_with_reference_log:11
msgid ""
"ref_log: List of (autotvm.measure.MeasureInput, "
"autotvm.measure.MeasureResult)"
msgstr ""

#: of
#: tvm.autotvm.task.space.FallbackConfigEntity.fallback_with_reference_log:12
msgid "The reference log"
msgstr ""

#: of tvm.autotvm.task.space.InstantiationError:1
msgid ""
"Actively detected error in instantiating a template with a config, raised"
" by cfg.raise_error e.g. too many unrolling, too many threads in a block"
msgstr ""

#: of tvm.autotvm.task.space.OtherOptionEntity:1
msgid "The parameter entity for general option, with a detailed value"
msgstr ""

#: of tvm.autotvm.task.space.OtherOptionSpace:1
msgid "The parameter space for general option"
msgstr ""

#: of tvm.autotvm.task.space.ReorderEntity:1
msgid "A reorder operation with detailed parameters that can apply to axes"
msgstr ""

#: of tvm.autotvm.task.space.ReorderEntity:7
msgid "perm: Array of int"
msgstr ""

#: of tvm.autotvm.task.space.ReorderEntity:6
msgid "define the permutation"
msgstr ""

#: of tvm.autotvm.task.space.ReorderEntity.apply:1
msgid "Apply reorder to an array of axes"
msgstr ""

#: of
msgid "list of Axis"
msgstr ""

#: of tvm.autotvm.task.space.ReorderEntity.apply:15
#: tvm.autotvm.task.space.SplitEntity.apply:15
msgid "The transformed axes."
msgstr ""

#: of tvm.autotvm.task.space.ReorderSpace:1
msgid "The parameter space for ordering an array of axes"
msgstr ""

#: of tvm.autotvm.task.space.ReorderSpace._merge_chain:1
msgid "generate all combinations of merge some chains"
msgstr ""

#: of tvm.autotvm.task.space.SplitEntity:1
msgid "A split operation with detailed parameters that can apply to an axis"
msgstr ""

#: of tvm.autotvm.task.space.SplitEntity:10
msgid "size: Array of int"
msgstr ""

#: of tvm.autotvm.task.space.SplitEntity:7
msgid ""
"the size of every axis after split. e.g. an axis of extent 128, we split "
"it into 3 axes, a possible size is [4, 4, 8] (4x4x8 = 128)."
msgstr ""

#: of tvm.autotvm.task.space.SplitEntity.apply:1
msgid "Apply split to an axis"
msgstr ""

#: of tvm.autotvm.task.space.SplitSpace:1
msgid "Split an axis for several times"
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace:1
msgid ""
"Base class for transform space TransformSpace is the node in the "
"computation graph of axes"
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace:6
msgid ""
"We can regard our schedule code as a transformation graph of axes. "
"Starting from raw axes in the definition of te.compute, we can transform "
"these axes by some operators. The operator includes 'split', 'reorder' "
"and 'annotate'. Each operator has some tunable parameters (e.g. the split"
" factor). Then the tuning process is just to find good parameters of "
"these op."
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace:12
msgid ""
"So all the combinations of the parameters of these op form our search "
"space."
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace:14
msgid ""
"Naming convention: We call the set of all possible values as XXXSpace. "
"(XXX can be Split, Reorder, Config ...) We call a specific entity in a "
"space as XXXEntity."
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace.__getitem__:1
msgid "Get an entity of the space by index"
msgstr ""

#: of tvm.autotvm.task.space.TransformSpace.__getitem__:9
msgid "transform entity"
msgstr ""

#: of tvm.autotvm.task.space.VirtualAxis:1
msgid "Axis placeholder in template"
msgstr ""

#: of tvm.autotvm.task.space.VirtualAxis:6
msgid ""
"If is int, return a virtual axis whose length is the provided argument. "
"If is IterVar, return a virtual axis whose length is extracted from the "
"IterVar's extent domain."
msgstr ""

#: of tvm.autotvm.task.space.get_factors:1
msgid "return all factors of an integer"
msgstr ""

#: of tvm.autotvm.task.space.get_factors:6
msgid "integer to factorize"
msgstr ""

#: of tvm.autotvm.task.space.get_factors:10 tvm.autotvm.task.space.get_pow2s:10
msgid "factors: list"
msgstr ""

#: of tvm.autotvm.task.space.get_factors:11
msgid "List of all factors"
msgstr ""

#: of tvm.autotvm.task.space.get_pow2s:1
msgid "return all power-of-two numbers that are less or equal than the integer"
msgstr ""

#: of tvm.autotvm.task.space.get_pow2s:6
msgid "integer for reference"
msgstr ""

#: of tvm.autotvm.task.space.get_pow2s:11
msgid "List of all power-of-two numbers"
msgstr ""

#: of tvm.autotvm.task.dispatcher:1
msgid "Template dispatcher module."
msgstr ""

#: of tvm.autotvm.task.dispatcher:3
msgid ""
"A dispatcher is a function that can contains multiple behaviors. Its "
"specific behavior is can be controlled by DispatchContext."
msgstr ""

#: of tvm.autotvm.task.dispatcher:6
msgid ""
"DispatchContext is used in two ways, usually via different implementation"
" of the DispatchContext base class."
msgstr ""

#: of tvm.autotvm.task.dispatcher:9
msgid "During search, we can use it to pass the current proposal from tuner."
msgstr ""

#: of tvm.autotvm.task.dispatcher:10
msgid "During evaluation, we can use it to set pick the best policy."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyConfig:1
msgid "Apply a deterministic config entity for all queries."
msgstr ""

#: of
msgid "ConfigSpace or ConfigEntity"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyConfig:6
msgid "The specific configuration we care about."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyConfig._query_inside:1
#: tvm.autotvm.task.dispatcher.ApplyFixedConfig._query_inside:1
msgid "Override query"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyConfig.update:1
#: tvm.autotvm.task.dispatcher.ApplyFixedConfig.update:1
msgid "Override update"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyFixedConfig:1
msgid ""
"Apply a config of a deterministic schedule. This is used for building a "
"single Relay operator with deterministic schedule for testing schedules "
"at Relay level."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyFixedConfig:7
msgid "tasks"
msgstr ""

#: of
msgid "list[tvm.autotvm.task.task.Task]"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyFixedConfig:8
msgid "List of autoTVM tasks."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyFixedConfig:11
msgid "schedule_names"
msgstr ""

#: of
msgid "str, List[str]"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyFixedConfig:10
msgid "Name of schedules to use."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest:1
msgid "Load the graph level tuning optimal schedules."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest:3
msgid ""
"The input records should be in the ascending order of node index for "
"target operator. Usually this can be obtained with graph tuner."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest:7
msgid ""
"This context maintains an internal counter to indicate the current node "
"index."
msgstr ""

#: of
msgid ""
"str or iterator of (autotvm.measure.MeasureInput, "
"autotvm.measure.MeasureResult)"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.__init__:4
msgid ""
"Collection of tuning records. If is str, then it should be the filename "
"of a records log file."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.__init__:6
msgid "Each row of this file is an encoded record pair."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.__init__:7
msgid "Otherwise, it is an iterator."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest._query_inside:1
msgid "Query the context to get config from records."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest._query_inside:6
#: tvm.autotvm.task.dispatcher.ApplyGraphBest.update:6
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:6
#: tvm.autotvm.task.dispatcher.DispatchContext._query_inside:7
#: tvm.autotvm.task.dispatcher.DispatchContext.query:8
#: tvm.autotvm.task.dispatcher.DispatchContext.update:6
#: tvm.autotvm.task.dispatcher.FallbackContext.clear_cache:7
#: tvm.autotvm.task.dispatcher.FallbackContext.update:6
#: tvm.autotvm.task.dispatcher.clear_fallback_cache:7
msgid "The current target"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest._query_inside:8
#: tvm.autotvm.task.dispatcher.ApplyGraphBest.update:7
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:7
#: tvm.autotvm.task.dispatcher.DispatchContext._query_inside:9
#: tvm.autotvm.task.dispatcher.DispatchContext.query:10
#: tvm.autotvm.task.dispatcher.DispatchContext.update:7
#: tvm.autotvm.task.dispatcher.FallbackContext.clear_cache:8
#: tvm.autotvm.task.dispatcher.FallbackContext.update:7
#: tvm.autotvm.task.dispatcher.clear_fallback_cache:9
msgid "workload"
msgstr ""

#: of
msgid "Workload"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest._query_inside:8
#: tvm.autotvm.task.dispatcher.ApplyGraphBest.update:8
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:8
#: tvm.autotvm.task.dispatcher.DispatchContext._query_inside:9
#: tvm.autotvm.task.dispatcher.DispatchContext.query:10
#: tvm.autotvm.task.dispatcher.DispatchContext.update:8
#: tvm.autotvm.task.dispatcher.FallbackContext.clear_cache:9
#: tvm.autotvm.task.dispatcher.FallbackContext.update:8
#: tvm.autotvm.task.dispatcher.clear_fallback_cache:9
msgid "The current workload."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest._query_inside:12
#: tvm.autotvm.task.dispatcher.ApplyGraphBest.update:10
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:10
#: tvm.autotvm.task.dispatcher.DispatchContext._query_inside:13
#: tvm.autotvm.task.dispatcher.DispatchContext.query:14
#: tvm.autotvm.task.dispatcher.DispatchContext.update:10
#: tvm.autotvm.task.dispatcher.FallbackContext.update:10
msgid "cfg"
msgstr ""

#: of
msgid "ConfigSpace"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest._query_inside:13
#: tvm.autotvm.task.dispatcher.ApplyGraphBest.update:10
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:10
#: tvm.autotvm.task.dispatcher.DispatchContext._query_inside:14
#: tvm.autotvm.task.dispatcher.DispatchContext.query:15
#: tvm.autotvm.task.dispatcher.DispatchContext.update:10
#: tvm.autotvm.task.dispatcher.FallbackContext.update:10
msgid "The specific configuration."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:1
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:1
#: tvm.autotvm.task.dispatcher.DispatchContext.update:1
#: tvm.autotvm.task.dispatcher.FallbackContext.update:1
msgid "Update context with a specific config."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:5
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:5
#: tvm.autotvm.task.dispatcher.DispatchContext._query_inside:6
#: tvm.autotvm.task.dispatcher.DispatchContext.query:7
#: tvm.autotvm.task.dispatcher.DispatchContext.update:5
#: tvm.autotvm.task.dispatcher.FallbackContext.clear_cache:6
#: tvm.autotvm.task.dispatcher.FallbackContext.update:5
#: tvm.autotvm.task.dispatcher.clear_fallback_cache:6
msgid "target: Target"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:14
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:14
#: tvm.autotvm.task.dispatcher.DispatchContext.update:14
#: tvm.autotvm.task.dispatcher.FallbackContext.update:14
msgid ""
"This interface is for cases when TVM decides to replace an operator in "
"the graph. For example, `AlterOpLayout` pass (enables when `opt_level = "
"3`) replaces `NCHW` convolution with `NCHW[x]c` implementation on x86 "
"CPUs. Thus in TOPI, we first query schedule using original `NCHW` "
"workload, then update the dispatcher with the new `NCHW[x]c` workload. So"
" that later on, `NCHW[x]c` convolution can get schedule from the "
"dispatcher using its own workload directly."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyGraphBest.update:40
#: tvm.autotvm.task.dispatcher.ApplyHistoryBest.update:40
#: tvm.autotvm.task.dispatcher.DispatchContext.update:40
#: tvm.autotvm.task.dispatcher.FallbackContext.update:40
msgid ""
"We directly store `config` back because `conv2d_NCHW` and `conv2d_NCHWc` "
"share the same schedule parameters. One can construct a new "
"`ConfigEntity` if this is not the case."
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyHistoryBest.load:1
msgid "Load records to this dispatch context"
msgstr ""

#: of tvm.autotvm.task.dispatcher.ApplyHistoryBest.load:5
msgid ""
"records : str, list of str, or iterator of (autotvm.measure.MeasureInput,"
"                                                    "
"autotvm.measure.MeasureResult)"
msgstr ""

#: of tvm.autotvm.task.dispatcher.DispatchContext:1
msgid "Base class of dispatch context."
msgstr ""

#: of tvm.autotvm.task.dispatcher.DispatchContext:3
msgid ""
"DispatchContext enables the target and workload specific dispatch "
"mechanism for templates."
msgstr ""

#: of tvm.autotvm.task.dispatcher.DispatchContext._query_inside:1
msgid ""
"Query the context to get the specific config for a template. This "
"function only query config inside this context."
msgstr ""

#: of tvm.autotvm.task.dispatcher.DispatchContext.query:1
msgid ""
"Query the context to get the specific config for a template. If cannot "
"find the result inside this context, this function will query it from the"
" upper contexts."
msgstr ""

#: of tvm.autotvm.task.dispatcher.FallbackContext:1
msgid "A fallback dispatch context."
msgstr ""

#: of tvm.autotvm.task.dispatcher.FallbackContext:3
msgid ""
"Any tunable template can be called under this context. This is the root "
"context."
msgstr ""

#: of tvm.autotvm.task.dispatcher.FallbackContext.clear_cache:1
#: tvm.autotvm.task.dispatcher.clear_fallback_cache:1
msgid ""
"Clear fallback cache. Pass the same argument as _query_inside to this "
"function to clean the cache."
msgstr ""

#: of tvm.autotvm.task.dispatcher.clear_fallback_cache:13
msgid ""
"This is used in alter_op_layout to clear the bad cache created before "
"call topi compute function"
msgstr ""

#: of tvm.autotvm.task.topi_integration:1
msgid "Decorators for registering tunable templates to TOPI."
msgstr ""

#: of tvm.autotvm.task.topi_integration:3
msgid ""
"These decorators can make your simple implementation be able to use "
"different configurations for different workloads. Here we directly use "
"all arguments to the TOPI call as \"workload\", so make sure all the "
"arguments (except tvm.te.Tensor) in you calls are hashable. For "
"tvm.te.Tensor, we will serialize it to a hashable tuple."
msgstr ""

#: of tvm.autotvm.task.topi_integration:9
#: tvm.autotvm.task.topi_integration.register_topi_compute:24
#: tvm.autotvm.task.topi_integration.register_topi_schedule:27
msgid "See tvm/topi/python/topi/arm_cpu/depthwise_conv2d.py for example usage."
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv:1
msgid "Global environment for extracting tuning tasks from graph"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.add_task:1
msgid "Add AutoTVM task"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.add_task:6
msgid "AutoTVM task name."
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.add_task:8
msgid "args: tuple"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.add_task:9
msgid "Arguments to the TOPI function."
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get:1
msgid "Get the single instance of TaskExtractEnv"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get:8
msgid "allow_duplicate"
msgstr ""

#: of
msgid "boolean"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get:6
msgid ""
"Whether to fetch all workloads in the network, even though some of them "
"are the same. This is useful for graph tuning."
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get:12
msgid "env: TaskExtractEnv"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get:13
msgid "The single instance of TaskExtractEnv"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get_tasks:1
msgid "Get collected tasks"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get_tasks:5
msgid "tasks: List of tuple(name, args)"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.get_tasks:6
msgid "A list of tasks extracted from the graph"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.reset:1
msgid "Reset task collections"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.reset:5
msgid "wanted_relay_ops: List of tvm.ir.Op"
msgstr ""

#: of tvm.autotvm.task.topi_integration.TaskExtractEnv.reset:6
msgid "The relay ops to be extracted"
msgstr ""

#: of tvm.autotvm.task.topi_integration.get_workload:1
msgid "Retrieve the workload from outputs"
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_compute:1
msgid "Register a tunable template for a topi compute function."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_compute:3
msgid ""
"The registration will wrap this topi compute to take `cfg` as the first "
"argument, followed by the original argument list. It uses all its "
"argument as workload and stores this \"workload\" to its final ComputeOp,"
" which can be used to reconstruct \"workload\" in the following "
"topi_schedule call."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_schedule:1
msgid "Register a tunable template for a topi schedule function."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_schedule:3
msgid ""
"The registration will wrap this topi schedule to take `cfg` as the first "
"argument, followed by the original argument list."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_schedule:6
msgid ""
"Note that this function will try to find \"workload\" from all the "
"ComputeOp in the input. You can attach \"workload\" to your compute op by"
" using :any:`register_topi_compute`."
msgstr ""

#: of tvm.autotvm.task.topi_integration.register_topi_schedule:9
msgid ""
"The task name has to be the same as that of the corresponding topi "
"compute function."
msgstr ""

#: ../../xin/docs/reference/api/python/autotvm.rst:88
msgid "tvm.autotvm.record"
msgstr ""

#: of tvm.autotvm.record:1
msgid "Tuning record and serialization format"
msgstr ""

#: of tvm.autotvm.record.decode:1
msgid "Decode encoded record string to python object"
msgstr ""

#: of tvm.autotvm.record.decode:6
msgid "row"
msgstr ""

#: of tvm.autotvm.record.decode:6 tvm.autotvm.record.encode:14
msgid "a row in the logger file"
msgstr ""

#: of tvm.autotvm.record.decode:9
msgid "protocol"
msgstr ""

#: of tvm.autotvm.record.decode:9 tvm.autotvm.record.encode:9
msgid "log protocol, json or pickle"
msgstr ""

#: of tvm.autotvm.record.decode:13
msgid "ret"
msgstr ""

#: of
msgid ""
"tuple(autotvm.measure.MeasureInput, autotvm.measure.MeasureResult), or "
"None"
msgstr ""

#: of tvm.autotvm.record.decode:14
msgid ""
"The tuple of input and result, or None if input uses old version log "
"format."
msgstr ""

#: of tvm.autotvm.record.encode:1
msgid "encode (MeasureInput, MeasureResult) pair to a string"
msgstr ""

#: of tvm.autotvm.record.encode:5
msgid "inp: autotvm.measure.MeasureInput result: autotvm.measure.MeasureResult"
msgstr ""

#: of tvm.autotvm.record.encode:7
msgid "pair of input/result"
msgstr ""

#: of tvm.autotvm.record.encode:9
msgid "protocol: str"
msgstr ""

#: of tvm.autotvm.record.encode:13
msgid "row: str"
msgstr ""

#: of tvm.autotvm.record.load_from_buffer:1
msgid ""
"Generator: load records from buffer. This is a generator that yields the "
"records."
msgstr ""

#: of tvm.autotvm.record.load_from_buffer:6
msgid "file: io.TextIOBase"
msgstr ""

#: of tvm.autotvm.record.load_from_buffer:10
#: tvm.autotvm.record.load_from_file:10
msgid "input: autotvm.measure.MeasureInput result: autotvm.measure.MeasureResult"
msgstr ""

#: of tvm.autotvm.record.load_from_file:1
msgid ""
"Generator: load records from path. This is a generator that yields the "
"records."
msgstr ""

#: of tvm.autotvm.record.load_from_file:6
msgid "filepath: str, bytes, or os.PathLike"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:1
msgid "get unique str key for MeasureInput"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:5
msgid "inp: autotvm.measure.MeasureInput"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:6
msgid "input for the measure"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:8
msgid "include_config: bool, optional"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:8
msgid "whether includes config in the str key"
msgstr ""

#: of tvm.autotvm.record.measure_str_key:13
msgid "The str representation of key"
msgstr ""

#: of tvm.autotvm.record.pick_best:1
msgid ""
"Pick the best entries from a file and store them to another file. This "
"function distills the useful log entries from a large log file. If "
"out_file already exists, the best entries from both in_file and out_file "
"will be saved."
msgstr ""

#: of tvm.autotvm.record.pick_best:8 tvm.autotvm.record.split_workload:6
msgid "in_file: str"
msgstr ""

#: of tvm.autotvm.record.pick_best:9
msgid "The filename of input"
msgstr ""

#: of tvm.autotvm.record.pick_best:10
msgid "out_file: str or file"
msgstr ""

#: of tvm.autotvm.record.pick_best:11
msgid "The filename of output"
msgstr ""

#: of tvm.autotvm.record.split_workload:1
msgid ""
"Split a log file into separate files, each of which contains only a "
"single workload This function can also delete duplicated records in log "
"file"
msgstr ""

#: of tvm.autotvm.record.split_workload:7
msgid "input filename"
msgstr ""

#: of tvm.autotvm.record.split_workload:8
msgid "clean: bool"
msgstr ""

#: of tvm.autotvm.record.split_workload:9
msgid "whether delete duplicated items"
msgstr ""

#~ msgid "The auto-tuning module of tvm"
#~ msgstr ""

#~ msgid "This module includes:"
#~ msgstr ""

#~ msgid "Tuning space definition API"
#~ msgstr ""

#~ msgid "Efficient auto-tuners"
#~ msgstr ""

#~ msgid "Tuning result and database support"
#~ msgstr ""

#~ msgid "Distributed measurement to scale up tuning"
#~ msgstr ""

#~ msgid "Apply the history best config"
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid ""
#~ "Collection of tuning records. If is "
#~ "str, then it should be the "
#~ "filename of a records log file. "
#~ "Each row of this file is an "
#~ "encoded record pair. Otherwise, it is"
#~ " an iterator."
#~ msgstr ""

#~ msgid "User facing API for specifying how to measure the generated code"
#~ msgstr ""

#~ msgid "Stores all the necessary inputs for a measurement."
#~ msgstr ""

#~ msgid "The target device"
#~ msgstr ""

#~ msgid "Task function"
#~ msgstr ""

#~ msgid "Specific configuration."
#~ msgstr ""

#~ msgid "Stores all the results of a measurement"
#~ msgstr ""

#~ msgid ""
#~ "If no error occurs during measurement,"
#~ " it is an array of measured "
#~ "running times. If an error occurs "
#~ "during measurement, it is an array "
#~ "of the exception objections."
#~ msgstr ""

#~ msgid "Denote error type, defined by MeasureErrorNo"
#~ msgstr ""

#~ msgid "All cost of this measure, including rpc, compilation, test runs"
#~ msgstr ""

#~ msgid "The absolute time stamp when we finish measurement."
#~ msgstr ""

#~ msgid ""
#~ "Set options for measure. To measure "
#~ "a config, we will build it and "
#~ "run it. So we have to set "
#~ "options for these two steps. They "
#~ "have their own options on timeout, "
#~ "parallel, etc."
#~ msgstr ""

#~ msgid "Specify how to build programs"
#~ msgstr ""

#~ msgid "Specify how to run programs"
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid ""
#~ "# example setting for using local "
#~ "devices >>> measure_option = "
#~ "autotvm.measure_option( >>>     "
#~ "builder=autotvm.LocalBuilder(),      # use all "
#~ "local cpu cores for compilation >>>"
#~ "     runner=autotvm.LocalRunner(          # measure "
#~ "them sequentially >>>         number=10, >>>"
#~ "         timeout=5) >>> )"
#~ msgstr ""

#~ msgid ""
#~ "# example setting for using remote "
#~ "devices >>> measure_option = "
#~ "autotvm.measure_option( >>>    "
#~ "builder=autotvm.LocalBuilder(),  # use all "
#~ "local cpu cores for compilation >>>"
#~ "    runner=autotvm.RPCRunner( >>>        'rasp3b', "
#~ "'locahost', 9190, # device key, host "
#~ "and port of the rpc tracker >>>"
#~ "        number=4, >>>        timeout=4) # "
#~ "timeout of a run on the device."
#~ " RPC request waiting time is "
#~ "excluded. >>>)"
#~ msgstr ""

#~ msgid ""
#~ "To make measurement results accurate, "
#~ "you should pick the correct value "
#~ "for the argument `number` and `repeat`"
#~ " in Runner(). Some devices need a "
#~ "certain minimum running time to \"warm"
#~ " up,\" such as GPUs that need "
#~ "time to reach a performance power "
#~ "state. Using `min_repeat_ms` can dynamically"
#~ " adjusts `number`, so it is "
#~ "recommended. The typical value for "
#~ "NVIDIA GPU is 150 ms."
#~ msgstr ""

#~ msgid "Get a standard measure_batch function."
#~ msgstr ""

#~ msgid "The tuning task"
#~ msgstr ""

#~ msgid ""
#~ "The option for measuring generated code."
#~ " You should use the return value "
#~ "of function :any:`measure_option` for this "
#~ "argument."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "**measure_batch** -- a callback function to measure a batch of configs"
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "Run compilation on local machine"
#~ msgstr ""

#~ msgid "The timeout of a compilation"
#~ msgstr ""

#~ msgid "The number of tasks run in parallel. \"None\" will use all cpu cores"
#~ msgstr ""

#~ msgid ""
#~ "If supplied, additional kwargs passed to"
#~ " build_func. Overrides any build_kwargs "
#~ "supplied by the Runner."
#~ msgstr ""

#~ msgid ""
#~ "If is 'default', use default build "
#~ "function If is 'ndk', use function "
#~ "for android ndk If id 'stackvm', "
#~ "use function for stackvm If is "
#~ "callable, use it as custom build "
#~ "function, expect lib_format field."
#~ msgstr ""

#~ msgid "If False, do not fork when building. Requires n_parallel=1."
#~ msgstr ""

#~ msgid "Specify the runtime to generate artifacts for"
#~ msgstr ""

#~ msgid ""
#~ "Run generated code on remove devices."
#~ " This function will ask a RPC "
#~ "Tracker to get device for measurement."
#~ msgstr ""

#~ msgid "The timeout of a RPCRunner measurement task"
#~ msgstr ""

#~ msgid "The key of the device registered in the tracker"
#~ msgstr ""

#~ msgid "The host address of RPC Tracker"
#~ msgstr ""

#~ msgid "The port of RPC Tracker"
#~ msgstr ""

#~ msgid ""
#~ "The number of times to run the "
#~ "generated code for taking average. We"
#~ " call these runs as one `repeat` "
#~ "of measurement."
#~ msgstr ""

#~ msgid ""
#~ "The number of times to repeat the"
#~ " measurement. In total, the generated "
#~ "code will be run (1 + number "
#~ "x repeat) times, where the first "
#~ "\"1\" is warm up and will be "
#~ "discarded. The returned result contains "
#~ "`repeat` costs, each of which is "
#~ "an average of `number` costs."
#~ msgstr ""

#~ msgid ""
#~ "The minimum duration of one `repeat` "
#~ "in milliseconds. By default, one "
#~ "`repeat` contains `number` runs. If this"
#~ " parameter is set, the parameters "
#~ "`number` will be dynamically adjusted to"
#~ " meet the minimum duration requirement "
#~ "of one `repeat`. i.e., When the "
#~ "run time of one `repeat` falls "
#~ "below this time, the `number` parameter"
#~ " will be automatically increased."
#~ msgstr ""

#~ msgid "The cool down interval between two measurements."
#~ msgstr ""

#~ msgid ""
#~ "Whether to flush cache on CPU "
#~ "between repeated measurements. Flushing cache"
#~ " can make the measured latency of "
#~ "one operator closer to its actual "
#~ "latency during end-to-end inference. "
#~ "To make this option effective, the "
#~ "argument `number` should also be set "
#~ "to 1. This is only has effect "
#~ "on CPU task."
#~ msgstr ""

#~ msgid ""
#~ "If given, a context manager that "
#~ "loads the module to be timed into"
#~ " the remote runtime. If not given,"
#~ " default_module_loader is used."
#~ msgstr ""

#~ msgid "Run generated code on local devices."
#~ msgstr ""

#~ msgid ""
#~ "The number of times to repeat the"
#~ " measurement. In total, the generated "
#~ "code will be run (1 + number "
#~ "x repeat) times, where the first "
#~ "one is warm up and will be "
#~ "discarded. The returned result contains "
#~ "`repeat` costs, each of which is "
#~ "an average of `number` costs."
#~ msgstr ""

#~ msgid ""
#~ "This is a \"fake\" local mode. We"
#~ " start a silent rpc tracker and "
#~ "rpc server for the user. In this"
#~ " way we reuse timeout/isolation mechanism"
#~ " in RPC infrastructure."
#~ msgstr ""

#~ msgid ""
#~ "A tuner takes a task as input. "
#~ "It proposes some promising :any:`ConfigEntity`"
#~ " in the :any:`ConfigSpace` and measure "
#~ "them on the real hardware. Then it"
#~ " proposed the next batch of "
#~ ":any:`ConfigEntity` according to the measure"
#~ " results. This tuning loop is "
#~ "repeated."
#~ msgstr ""

#~ msgid "Base class for tuners"
#~ msgstr ""

#~ msgid "Tuning Task"
#~ msgstr ""

#~ msgid "Whether has next untried config in the space"
#~ msgstr ""

#~ msgid "**has_next**"
#~ msgstr ""

#~ msgid "load history data for transfer learning"
#~ msgstr ""

#~ msgid "Previous tuning records"
#~ msgstr ""

#~ msgid ""
#~ "Defaults to 500. Indicates the minimum"
#~ " number of records to train the "
#~ "tuner with. If there are less than"
#~ " `min_seed_records` number of records in"
#~ " `data_set`, no training of the tuner"
#~ " will be done."
#~ msgstr ""

#~ msgid "get the next batch of configs to be measure on real hardware"
#~ msgstr ""

#~ msgid "The size of the batch"
#~ msgstr ""

#~ msgid "reset the status of tuner"
#~ msgstr ""

#~ msgid "Begin tuning"
#~ msgstr ""

#~ msgid "Maximum number of configs to try (measure on real hardware)"
#~ msgstr ""

#~ msgid ""
#~ "The options for how to measure "
#~ "generated code. You should use the "
#~ "return value ot autotvm.measure_option for "
#~ "this argument."
#~ msgstr ""

#~ msgid ""
#~ "Early stop the tuning when not "
#~ "finding better configs in this number"
#~ " of trials"
#~ msgstr ""

#~ msgid ""
#~ "A list of callback functions. The "
#~ "signature of callback function is "
#~ "(Tuner, List of MeasureInput, List of"
#~ " MeasureResult) with no return value. "
#~ "These callback functions will be called"
#~ " on every measurement pair. See "
#~ "autotvm/tuner/callback.py for some examples."
#~ msgstr ""

#~ msgid ""
#~ "One of tvm.autotvm.utils.SI_PREFIXES. The SI"
#~ " prefix to use when reporting FLOPS."
#~ msgstr ""

#~ msgid "Update parameters of the tuner according to measurement results"
#~ msgstr ""

#~ msgid "The input for measurement"
#~ msgstr ""

#~ msgid "result for measurement"
#~ msgstr ""

#~ msgid "Enumerate the search space in a random order"
#~ msgstr ""

#~ msgid "A tuple of index range to random"
#~ msgstr ""

#~ msgid "Enumerate the search space in a grid search order"
#~ msgstr ""

#~ msgid ""
#~ "Tuner with genetic algorithm. This tuner"
#~ " does not have a cost model so"
#~ " it always run measurement on real"
#~ " machines. This tuner expands the "
#~ ":code:`ConfigEntity` as gene."
#~ msgstr ""

#~ msgid "number of genes in one generation"
#~ msgstr ""

#~ msgid "number of elite to keep"
#~ msgstr ""

#~ msgid "probability of mutation of a knob in a gene"
#~ msgstr ""

#~ msgid "Tuner that uses xgboost as cost model"
#~ msgstr ""

#~ msgid ""
#~ "The size of a plan. After "
#~ "`plan_size` trials, the tuner will refit"
#~ " a new cost model and do "
#~ "planing for the next `plan_size` trials."
#~ msgstr ""

#~ msgid ""
#~ "If is 'itervar', use features extracted"
#~ " from IterVar (loop variable). If is"
#~ " 'knob', use flatten ConfigEntity directly."
#~ " If is 'curve', use sampled curve "
#~ "feature (relation feature).  Note on "
#~ "choosing feature type: For single task"
#~ " tuning, 'itervar' and 'knob' are "
#~ "good. 'itervar' is more accurate but "
#~ "'knob' is much faster. There are "
#~ "some constraints on 'itervar', if you"
#~ " meet problems with feature extraction "
#~ "when using 'itervar', you can switch "
#~ "to 'knob'.  For cross-shape tuning "
#~ "(e.g. many convolutions with different "
#~ "shapes), 'itervar' and 'curve' has "
#~ "better transferability, 'knob' is faster.  "
#~ "For cross-device or cross-operator "
#~ "tuning, you can use 'curve' only."
#~ msgstr ""

#~ msgid ""
#~ "If is 'itervar', use features extracted"
#~ " from IterVar (loop variable). If is"
#~ " 'knob', use flatten ConfigEntity directly."
#~ " If is 'curve', use sampled curve "
#~ "feature (relation feature)."
#~ msgstr ""

#~ msgid ""
#~ "Note on choosing feature type: For "
#~ "single task tuning, 'itervar' and 'knob'"
#~ " are good. 'itervar' is more accurate"
#~ " but 'knob' is much faster. There "
#~ "are some constraints on 'itervar', if"
#~ " you meet problems with feature "
#~ "extraction when using 'itervar', you can"
#~ " switch to 'knob'."
#~ msgstr ""

#~ msgid ""
#~ "For cross-shape tuning (e.g. many "
#~ "convolutions with different shapes), 'itervar'"
#~ " and 'curve' has better transferability,"
#~ " 'knob' is faster."
#~ msgstr ""

#~ msgid "For cross-device or cross-operator tuning, you can use 'curve' only."
#~ msgstr ""

#~ msgid ""
#~ "If is 'reg', use regression loss "
#~ "to train cost model. The cost "
#~ "model predicts the normalized flops. If"
#~ " is 'rank', use pairwise rank loss"
#~ " to train cost model. The cost "
#~ "model predicts relative rank score."
#~ msgstr ""

#~ msgid "The number of threads."
#~ msgstr ""

#~ msgid ""
#~ "If is 'sa', use a default "
#~ "simulated annealing optimizer. Otherwise it"
#~ " should be a ModelOptimizer object."
#~ msgstr ""

#~ msgid ""
#~ "If is not None, the tuner will "
#~ "first select top-(plan_size * "
#~ "diversity_filter_ratio) candidates according to "
#~ "the cost model and then pick "
#~ "batch_size of them according to the "
#~ "diversity metric."
#~ msgstr ""

#~ msgid ""
#~ "The verbose level. If is 0, output"
#~ " nothing. Otherwise, output debug "
#~ "information every `verbose` iterations."
#~ msgstr ""

#~ msgid "Namespace of callback utilities of AutoTVM"
#~ msgstr ""

#~ msgid "A monitor to collect statistic during tuning"
#~ msgstr ""

#~ msgid "get scores (currently is flops) of all trials"
#~ msgstr ""

#~ msgid "get wall clock time stamp of all trials"
#~ msgstr ""

#~ msgid "Save the tuning records to a database object."
#~ msgstr ""

#~ msgid "The database"
#~ msgstr ""

#~ msgid ""
#~ "Log the tuning records into file. "
#~ "The rows of the log are stored "
#~ "in the format of autotvm.record.encode."
#~ msgstr ""

#~ msgid "The file to log to."
#~ msgstr ""

#~ msgid "The log protocol. Can be 'json' or 'pickle'"
#~ msgstr ""

#~ msgid "**callback** -- Callback function to do the logging."
#~ msgstr ""

#~ msgid "Display progress bar for tuning"
#~ msgstr ""

#~ msgid "The total number of trials"
#~ msgstr ""

#~ msgid "The prefix of output message"
#~ msgstr ""

#~ msgid "SI prefix for flops"
#~ msgstr ""

#~ msgid "Task is a tunable composition of template functions."
#~ msgstr ""

#~ msgid ""
#~ "Tuner takes a tunable task and "
#~ "optimizes the joint configuration space "
#~ "of all the template functions in "
#~ "the task. This module defines the "
#~ "task data structure, as well as a"
#~ " collection(zoo) of typical tasks of "
#~ "interest."
#~ msgstr ""

#~ msgid "Definition of task function."
#~ msgstr ""

#~ msgid ""
#~ "Task can be constructed from tuple "
#~ "of func, args, and kwargs. func is"
#~ " a state-less function, or a "
#~ "string that registers the standard task."
#~ msgstr ""

#~ msgid "Error happens when estimating FLOP for a compute op"
#~ msgstr ""

#~ msgid ""
#~ "Dummy task template for a task "
#~ "lookup which cannot be resolved. This"
#~ " can occur if the task being "
#~ "requested from _lookup_task() has not "
#~ "been imported in this run."
#~ msgstr ""

#~ msgid "A Tunable Task"
#~ msgstr ""

#~ msgid "The name of the task."
#~ msgstr ""

#~ msgid "Positional argument of func"
#~ msgstr ""

#~ msgid ""
#~ "Instantiate this task function (template) "
#~ "with a config. Returns corresponding "
#~ "schedule."
#~ msgstr ""

#~ msgid "parameter config for this template"
#~ msgstr ""

#~ msgid ""
#~ "* **sch** (*tvm.te.schedule.Schedule*) -- The"
#~ " tvm schedule * **arg_bufs** (*Array "
#~ "of te.tensor.Tensor*) -- The input/output "
#~ "buffers"
#~ msgstr ""

#~ msgid "**sch** (*tvm.te.schedule.Schedule*) -- The tvm schedule"
#~ msgstr ""

#~ msgid "**arg_bufs** (*Array of te.tensor.Tensor*) -- The input/output buffers"
#~ msgstr ""

#~ msgid "Task template is used to creates a tunable AutoTVM task."
#~ msgstr ""

#~ msgid ""
#~ "It can be defined by a pair "
#~ "of compute and schedule function using"
#~ " `_register_task_compute` and "
#~ "`_register_task_schedule`, or by a customized"
#~ " task creation function that is more"
#~ " flexible using `_register_customized_task`."
#~ msgstr ""

#~ msgid ""
#~ "Note that when customized func is "
#~ "registered, compute and schedule function "
#~ "will be ignored"
#~ msgstr ""

#~ msgid ""
#~ "Convert argument list to hashable "
#~ "workload tuple. This function will "
#~ "convert list to tuple, tvm node to"
#~ " python value and flatten te.tensor.Tensor"
#~ " to a tuple"
#~ msgstr ""

#~ msgid "The AutoTVM task name"
#~ msgstr ""

#~ msgid "The arguments to the function"
#~ msgstr ""

#~ msgid "**ret** -- The hashable value"
#~ msgstr ""

#~ msgid ""
#~ "Calculate number of FLOP (floating "
#~ "number operations) of the compute ops"
#~ " in a schedule"
#~ msgstr ""

#~ msgid "schedule"
#~ msgstr ""

#~ msgid "**flop** -- number of FLOP in this schedule"
#~ msgstr ""

#~ msgid "Create a tuning task and initialize its search space"
#~ msgstr ""

#~ msgid "Positional arguments"
#~ msgstr ""

#~ msgid "The compilation target"
#~ msgstr ""

#~ msgid "The compilation target for host side"
#~ msgstr ""

#~ msgid "**tsk** -- a task object"
#~ msgstr ""

#~ msgid "The inverse function of :code:`serialize_args`."
#~ msgstr ""

#~ msgid "Get current config object"
#~ msgstr ""

#~ msgid "**cfg** -- The current config"
#~ msgstr ""

#~ msgid "serialize arguments of a topi function to a hashable tuple."
#~ msgstr ""

#~ msgid "Decorate a function as a tunable schedule template."
#~ msgstr ""

#~ msgid "The task name"
#~ msgstr ""

#~ msgid ""
#~ "A callable template function. If it "
#~ "is None, return a decorator. If is"
#~ " callable, decorate this function."
#~ msgstr ""

#~ msgid "**func** -- The decorated function"
#~ msgstr ""

#~ msgid ""
#~ "The following code is a tunable "
#~ "template for a blocked matrix "
#~ "multiplication"
#~ msgstr ""

#~ msgid "Template configuration space."
#~ msgstr ""

#~ msgid ""
#~ "Each template function can be "
#~ "parameterized by a ConfigSpace. The "
#~ "space is declared when we invoke "
#~ "the template function with ConfigSpace. "
#~ "During evaluation, we pass in a "
#~ "ConfigEntity, which contains a specific "
#~ "entity in the space. This entity "
#~ "contains deterministic parameters."
#~ msgstr ""

#~ msgid "An annotation operation with detailed parameters that can apply to axes"
#~ msgstr ""

#~ msgid "The annotations of axes"
#~ msgstr ""

#~ msgid "Apply annotation to an array of axes"
#~ msgstr ""

#~ msgid "The tvm schedule"
#~ msgstr ""

#~ msgid "The stage to be applied"
#~ msgstr ""

#~ msgid "axis to split"
#~ msgstr ""

#~ msgid "the length of axes"
#~ msgstr ""

#~ msgid "maximum unroll step"
#~ msgstr ""

#~ msgid "valid vector lanes for vectorization"
#~ msgstr ""

#~ msgid "cfg for recording error"
#~ msgstr ""

#~ msgid "source tensor for attaching cache"
#~ msgstr ""

#~ msgid "**axes** -- The transformed axes"
#~ msgstr ""

#~ msgid "The parameter space for annotating an array of axes"
#~ msgstr ""

#~ msgid "get number of output axes after this transform"
#~ msgstr ""

#~ msgid "**n** -- number of output axes"
#~ msgstr ""

#~ msgid "Alias for field number 1"
#~ msgstr ""

#~ msgid "Alias for field number 0"
#~ msgstr ""

#~ msgid "A configuration with detailed parameters"
#~ msgstr ""

#~ msgid "index of this config in space"
#~ msgstr ""

#~ msgid "hash of schedule code"
#~ msgstr ""

#~ msgid "map name to transform entity"
#~ msgstr ""

#~ msgid "List of constraints"
#~ msgstr ""

#~ msgid "Build a ConfigEntity from json serializable dictionary"
#~ msgstr ""

#~ msgid ""
#~ "Json serializable dictionary. This should "
#~ "be the return value of "
#~ ":any:`to_json_dict`."
#~ msgstr ""

#~ msgid "**config** -- The corresponding config object"
#~ msgstr ""

#~ msgid "flatten entities to a numerical one-dimensional feature vector"
#~ msgstr ""

#~ msgid "**fea** -- one dimensional float32 array"
#~ msgstr ""

#~ msgid ""
#~ "**other_option** -- other tunable parameters"
#~ " (tunable parameters defined by "
#~ "`cfg.define_knob`)"
#~ msgstr ""

#~ msgid "convert to a json serializable dictionary"
#~ msgstr ""

#~ msgid "**json_dict** -- a json serializable dictionary"
#~ msgstr ""

#~ msgid ""
#~ "The configuration space of a schedule."
#~ " Pass it as config in template "
#~ "to collect transformation space and "
#~ "build transform graph of axes"
#~ msgstr ""

#~ msgid "Add float operation statistics for this tuning task"
#~ msgstr ""

#~ msgid "number of float operations"
#~ msgstr ""

#~ msgid "get a virtual axis (axis placeholder)"
#~ msgstr ""

#~ msgid ""
#~ "If is int, return an axis whose"
#~ " length is the provided argument. If"
#~ " is IterVar, return an axis whose "
#~ "length is extracted from the IterVar's"
#~ " extent domain."
#~ msgstr ""

#~ msgid "Define a new tunable knob which annotates a list of axes"
#~ msgstr ""

#~ msgid "name to index the entity of this space"
#~ msgstr ""

#~ msgid "axes to annotate"
#~ msgstr ""

#~ msgid ""
#~ "name of policy If is 'unroll', "
#~ "unroll the axes. If is 'try_unroll', "
#~ "try to unroll the axes. If is "
#~ "'try_unroll_vec', try to unroll or "
#~ "vectorize the axes. If is 'bind_gpu',"
#~ " bind the first few axes to gpu"
#~ " threads. If is 'locate_cache', choose "
#~ "n axes to attach shared/local cache."
#~ msgstr ""

#~ msgid "extra arguments for policy"
#~ msgstr ""

#~ msgid "Define a tunable knob with a list of candidates"
#~ msgstr ""

#~ msgid "name key of that option"
#~ msgstr ""

#~ msgid "list of candidates"
#~ msgstr ""

#~ msgid "Define a new tunable knob which reorders a list of axes"
#~ msgstr ""

#~ msgid "axes to reorder"
#~ msgstr ""

#~ msgid ""
#~ "name of policy If is 'identity', "
#~ "do an identity permutation. If is "
#~ "'all', try all permutations. If is "
#~ "'interval_all', try all permutations of "
#~ "an interval of axes. If is "
#~ "'candidate', try listed candidate. If is"
#~ " 'interleave', interleave chains of spatial"
#~ " axes and chains of reduction axes."
#~ msgstr ""

#~ msgid "Define a new tunable knob which splits an axis into a list of axes"
#~ msgstr ""

#~ msgid ""
#~ "name of policy. If is 'factors', "
#~ "the tuner will try all divisible "
#~ "factors. If is 'power2', the tuner "
#~ "will try power-of-two factors less"
#~ " or equal to the length. If is"
#~ " 'verbose', the tuner will try all"
#~ " candidates in above two policies. If"
#~ " is 'candidate', try given candidates."
#~ msgstr ""

#~ msgid ""
#~ "extra arguments for policy  ``max_factor``:"
#~ "     the maximum split factor (`int`). "
#~ "``filter``:     see examples below for "
#~ "how to use filter (`Callable[[int], "
#~ "bool]`). ``num_outputs``:     the total number"
#~ " of axis after split (`int`). "
#~ "``no_tail``:     should we only include "
#~ "divisible numbers as split factors "
#~ "(`bool`). `candidate``:     (policy=candidate) "
#~ "manual candidate list (`List`)."
#~ msgstr ""

#~ msgid "``max_factor``:"
#~ msgstr ""

#~ msgid "the maximum split factor (`int`)."
#~ msgstr ""

#~ msgid "``filter``:"
#~ msgstr ""

#~ msgid "see examples below for how to use filter (`Callable[[int], bool]`)."
#~ msgstr ""

#~ msgid "``num_outputs``:"
#~ msgstr ""

#~ msgid "the total number of axis after split (`int`)."
#~ msgstr ""

#~ msgid "``no_tail``:"
#~ msgstr ""

#~ msgid "should we only include divisible numbers as split factors (`bool`)."
#~ msgstr ""

#~ msgid "`candidate``:"
#~ msgstr ""

#~ msgid "(policy=candidate) manual candidate list (`List`)."
#~ msgstr ""

#~ msgid "Get a config entity with detailed parameters from this space"
#~ msgstr ""

#~ msgid "index in the space"
#~ msgstr ""

#~ msgid ""
#~ "register error in config Using this "
#~ "to actively detect error when "
#~ "scheduling. Otherwise these error will "
#~ "occur during runtime, which will cost"
#~ " more time."
#~ msgstr ""

#~ msgid "Check whether the config meets all the constraints"
#~ msgstr ""

#~ msgid ""
#~ "This check should be called after "
#~ "instantiation of task, because the "
#~ "ConfigEntity/ConfigSpace collects errors during "
#~ "instantiation"
#~ msgstr ""

#~ msgid "**valid** -- whether the config meets all the constraints"
#~ msgstr ""

#~ msgid "The config entity created to support fallback"
#~ msgstr ""

#~ msgid "Fallback a split knob"
#~ msgstr ""

#~ msgid "name of the knob"
#~ msgstr ""

#~ msgid ""
#~ "The maximum tile size for every "
#~ "dimension. Value `-1` means no "
#~ "constraint."
#~ msgstr ""

#~ msgid ""
#~ "If you use cfg.define_split('tile_0', 128, "
#~ "num_outputs=3), Then cfg.fallback_split('tile_0', "
#~ "[-1, 8, 4]) will give you "
#~ "cfg['tile_0'].size = [4, 8, 4]"
#~ msgstr ""

#~ msgid ""
#~ "If you use cfg.define_split('tile_0', 49, "
#~ "num_outputs=3), Then cfg.fallback_split('tile_0', "
#~ "[-1, 8, 4]) will give you "
#~ "cfg['tile_0'].size = [7, 7, 1]"
#~ msgstr ""

#~ msgid ""
#~ "A data driven fallback mechanism. We "
#~ "use tuned parameters from TopHub as "
#~ "reference data. For an unseen shape, "
#~ "we find the most similar tuned one"
#~ " from TopHub and mimic its "
#~ "parameters. Note that we are not "
#~ "matching by workload (e.g., input size,"
#~ " kernel size), but instead matching "
#~ "by configuration space. The idea is "
#~ "that if two workloads have similar "
#~ "configuration space, their optimal "
#~ "configurations are also likely to be "
#~ "similar."
#~ msgstr ""

#~ msgid "The reference log"
#~ msgstr ""

#~ msgid ""
#~ "Actively detected error in instantiating "
#~ "a template with a config, raised "
#~ "by cfg.raise_error e.g. too many "
#~ "unrolling, too many threads in a "
#~ "block"
#~ msgstr ""

#~ msgid "The parameter entity for general option, with a detailed value"
#~ msgstr ""

#~ msgid "The parameter space for general option"
#~ msgstr ""

#~ msgid "A reorder operation with detailed parameters that can apply to axes"
#~ msgstr ""

#~ msgid "define the permutation"
#~ msgstr ""

#~ msgid "Apply reorder to an array of axes"
#~ msgstr ""

#~ msgid "**axes** -- The transformed axes."
#~ msgstr ""

#~ msgid "The parameter space for ordering an array of axes"
#~ msgstr ""

#~ msgid "A split operation with detailed parameters that can apply to an axis"
#~ msgstr ""

#~ msgid ""
#~ "the size of every axis after "
#~ "split. e.g. an axis of extent 128,"
#~ " we split it into 3 axes, a "
#~ "possible size is [4, 4, 8] (4x4x8"
#~ " = 128)."
#~ msgstr ""

#~ msgid "Apply split to an axis"
#~ msgstr ""

#~ msgid "Split an axis for several times"
#~ msgstr ""

#~ msgid ""
#~ "Base class for transform space "
#~ "TransformSpace is the node in the "
#~ "computation graph of axes"
#~ msgstr ""

#~ msgid ""
#~ "We can regard our schedule code as"
#~ " a transformation graph of axes. "
#~ "Starting from raw axes in the "
#~ "definition of te.compute, we can "
#~ "transform these axes by some operators."
#~ " The operator includes 'split', 'reorder'"
#~ " and 'annotate'. Each operator has "
#~ "some tunable parameters (e.g. the split"
#~ " factor). Then the tuning process is"
#~ " just to find good parameters of "
#~ "these op."
#~ msgstr ""

#~ msgid ""
#~ "So all the combinations of the "
#~ "parameters of these op form our "
#~ "search space."
#~ msgstr ""

#~ msgid ""
#~ "Naming convention: We call the set "
#~ "of all possible values as XXXSpace. "
#~ "(XXX can be Split, Reorder, Config "
#~ "...) We call a specific entity in"
#~ " a space as XXXEntity."
#~ msgstr ""

#~ msgid "Axis placeholder in template"
#~ msgstr ""

#~ msgid ""
#~ "If is int, return a virtual axis"
#~ " whose length is the provided "
#~ "argument. If is IterVar, return a "
#~ "virtual axis whose length is extracted"
#~ " from the IterVar's extent domain."
#~ msgstr ""

#~ msgid "return all factors of an integer"
#~ msgstr ""

#~ msgid "integer to factorize"
#~ msgstr ""

#~ msgid "**factors** -- List of all factors"
#~ msgstr ""

#~ msgid "return all power-of-two numbers that are less or equal than the integer"
#~ msgstr ""

#~ msgid "integer for reference"
#~ msgstr ""

#~ msgid "**factors** -- List of all power-of-two numbers"
#~ msgstr ""

#~ msgid "Template dispatcher module."
#~ msgstr ""

#~ msgid ""
#~ "A dispatcher is a function that "
#~ "can contains multiple behaviors. Its "
#~ "specific behavior is can be controlled"
#~ " by DispatchContext."
#~ msgstr ""

#~ msgid ""
#~ "DispatchContext is used in two ways, "
#~ "usually via different implementation of "
#~ "the DispatchContext base class."
#~ msgstr ""

#~ msgid "During search, we can use it to pass the current proposal from tuner."
#~ msgstr ""

#~ msgid "During evaluation, we can use it to set pick the best policy."
#~ msgstr ""

#~ msgid "Apply a deterministic config entity for all queries."
#~ msgstr ""

#~ msgid "The specific configuration we care about."
#~ msgstr ""

#~ msgid "Override update"
#~ msgstr ""

#~ msgid "Load the graph level tuning optimal schedules."
#~ msgstr ""

#~ msgid ""
#~ "The input records should be in the"
#~ " ascending order of node index for"
#~ " target operator. Usually this can be"
#~ " obtained with graph tuner."
#~ msgstr ""

#~ msgid ""
#~ "This context maintains an internal "
#~ "counter to indicate the current node "
#~ "index."
#~ msgstr ""

#~ msgid "Update context with a specific config."
#~ msgstr ""

#~ msgid "The current target"
#~ msgstr ""

#~ msgid "The current workload."
#~ msgstr ""

#~ msgid "The specific configuration."
#~ msgstr ""

#~ msgid ""
#~ "This interface is for cases when "
#~ "TVM decides to replace an operator "
#~ "in the graph. For example, "
#~ "`AlterOpLayout` pass (enables when `opt_level"
#~ " = 3`) replaces `NCHW` convolution "
#~ "with `NCHW[x]c` implementation on x86 "
#~ "CPUs. Thus in TOPI, we first query"
#~ " schedule using original `NCHW` workload,"
#~ " then update the dispatcher with the"
#~ " new `NCHW[x]c` workload. So that "
#~ "later on, `NCHW[x]c` convolution can get"
#~ " schedule from the dispatcher using "
#~ "its own workload directly."
#~ msgstr ""

#~ msgid ""
#~ "We directly store `config` back because"
#~ " `conv2d_NCHW` and `conv2d_NCHWc` share the"
#~ " same schedule parameters. One can "
#~ "construct a new `ConfigEntity` if this"
#~ " is not the case."
#~ msgstr ""

#~ msgid "Load records to this dispatch context"
#~ msgstr ""

#~ msgid "Base class of dispatch context."
#~ msgstr ""

#~ msgid ""
#~ "DispatchContext enables the target and "
#~ "workload specific dispatch mechanism for "
#~ "templates."
#~ msgstr ""

#~ msgid ""
#~ "Query the context to get the "
#~ "specific config for a template. If "
#~ "cannot find the result inside this "
#~ "context, this function will query it "
#~ "from the upper contexts."
#~ msgstr ""

#~ msgid "**cfg** -- The specific configuration."
#~ msgstr ""

#~ msgid "A fallback dispatch context."
#~ msgstr ""

#~ msgid ""
#~ "Any tunable template can be called "
#~ "under this context. This is the "
#~ "root context."
#~ msgstr ""

#~ msgid ""
#~ "Clear fallback cache. Pass the same "
#~ "argument as _query_inside to this "
#~ "function to clean the cache."
#~ msgstr ""

#~ msgid ""
#~ "This is used in alter_op_layout to "
#~ "clear the bad cache created before "
#~ "call topi compute function"
#~ msgstr ""

#~ msgid "Decorators for registering tunable templates to TOPI."
#~ msgstr ""

#~ msgid ""
#~ "These decorators can make your simple"
#~ " implementation be able to use "
#~ "different configurations for different "
#~ "workloads. Here we directly use all "
#~ "arguments to the TOPI call as "
#~ "\"workload\", so make sure all the "
#~ "arguments (except tvm.te.Tensor) in you "
#~ "calls are hashable. For tvm.te.Tensor, "
#~ "we will serialize it to a hashable"
#~ " tuple."
#~ msgstr ""

#~ msgid "See tvm/topi/python/topi/arm_cpu/depthwise_conv2d.py for example usage."
#~ msgstr ""

#~ msgid "Global environment for extracting tuning tasks from graph"
#~ msgstr ""

#~ msgid "Add AutoTVM task"
#~ msgstr ""

#~ msgid "AutoTVM task name."
#~ msgstr ""

#~ msgid "Arguments to the TOPI function."
#~ msgstr ""

#~ msgid "Get the single instance of TaskExtractEnv"
#~ msgstr ""

#~ msgid ""
#~ "Whether to fetch all workloads in "
#~ "the network, even though some of "
#~ "them are the same. This is useful"
#~ " for graph tuning."
#~ msgstr ""

#~ msgid "**env** -- The single instance of TaskExtractEnv"
#~ msgstr ""

#~ msgid "Get collected tasks"
#~ msgstr ""

#~ msgid "**tasks** -- A list of tasks extracted from the graph"
#~ msgstr ""

#~ msgid "Reset task collections"
#~ msgstr ""

#~ msgid "The relay ops to be extracted"
#~ msgstr ""

#~ msgid "Retrieve the workload from outputs"
#~ msgstr ""

#~ msgid "Register a tunable template for a topi compute function."
#~ msgstr ""

#~ msgid ""
#~ "The registration will wrap this topi "
#~ "compute to take `cfg` as the first"
#~ " argument, followed by the original "
#~ "argument list. It uses all its "
#~ "argument as workload and stores this "
#~ "\"workload\" to its final ComputeOp, "
#~ "which can be used to reconstruct "
#~ "\"workload\" in the following topi_schedule"
#~ " call."
#~ msgstr ""

#~ msgid ""
#~ "If it is None, return a decorator."
#~ " If is callable, decorate this "
#~ "function."
#~ msgstr ""

#~ msgid "**decorator** -- A decorator"
#~ msgstr ""

#~ msgid "Register a tunable template for a topi schedule function."
#~ msgstr ""

#~ msgid ""
#~ "The registration will wrap this topi "
#~ "schedule to take `cfg` as the "
#~ "first argument, followed by the original"
#~ " argument list."
#~ msgstr ""

#~ msgid ""
#~ "Note that this function will try "
#~ "to find \"workload\" from all the "
#~ "ComputeOp in the input. You can "
#~ "attach \"workload\" to your compute op"
#~ " by using :any:`register_topi_compute`."
#~ msgstr ""

#~ msgid ""
#~ "The task name has to be the "
#~ "same as that of the corresponding "
#~ "topi compute function."
#~ msgstr ""

#~ msgid "Tuning record and serialization format"
#~ msgstr ""

#~ msgid "Decode encoded record string to python object"
#~ msgstr ""

#~ msgid "a row in the logger file"
#~ msgstr ""

#~ msgid "log protocol, json or pickle"
#~ msgstr ""

#~ msgid ""
#~ "**ret** -- The tuple of input and"
#~ " result, or None if input uses "
#~ "old version log format."
#~ msgstr ""

#~ msgid "encode (MeasureInput, MeasureResult) pair to a string"
#~ msgstr ""

#~ msgid "pair of input/result"
#~ msgstr ""

#~ msgid "**row** -- a row in the logger file"
#~ msgstr ""

#~ msgid ""
#~ "Generator: load records from file. This"
#~ " is a generator that yields the "
#~ "records."
#~ msgstr ""

#~ msgid ""
#~ msgstr ""

#~ msgid "**input** (*autotvm.measure.MeasureInput*)"
#~ msgstr ""

#~ msgid "**result** (*autotvm.measure.MeasureResult*)"
#~ msgstr ""

#~ msgid "get unique str key for MeasureInput"
#~ msgstr ""

#~ msgid "input for the measure"
#~ msgstr ""

#~ msgid "whether includes config in the str key"
#~ msgstr ""

#~ msgid "**key** -- The str representation of key"
#~ msgstr ""

#~ msgid ""
#~ "Pick the best entries from a file"
#~ " and store them to another file. "
#~ "This function distills the useful log"
#~ " entries from a large log file. "
#~ "If out_file already exists, the best "
#~ "entries from both in_file and out_file"
#~ " will be saved."
#~ msgstr ""

#~ msgid "The filename of input"
#~ msgstr ""

#~ msgid "The filename of output"
#~ msgstr ""

#~ msgid ""
#~ "Split a log file into separate "
#~ "files, each of which contains only "
#~ "a single workload This function can "
#~ "also delete duplicated records in log"
#~ " file"
#~ msgstr ""

#~ msgid "input filename"
#~ msgstr ""

#~ msgid "whether delete duplicated items"
#~ msgstr ""

#~ msgid ""
#~ "str, list of str, or iterator of"
#~ " (autotvm.measure.MeasureInput,"
#~ "                                                "
#~ "autotvm.measure.MeasureResult)"
#~ msgstr ""

#~ msgid ""
#~ "Collection of tuning records. If is "
#~ "str, then it should be the "
#~ "filename of a records log file. "
#~ "Each row of this file is an "
#~ "encoded record pair. If it is a"
#~ " list, it can either be a list"
#~ " of paths to log files that "
#~ "will be loaded jointly or an "
#~ "iterator or records."
#~ msgstr ""

#~ msgid ""
#~ "str, list of str, or iterator of"
#~ " (autotvm.measure.MeasureInput,"
#~ "                                                    "
#~ "autotvm.measure.MeasureResult)"
#~ msgstr ""

#~ msgid ""
#~ "Collection of tuning records. If is "
#~ "str, then it should be the "
#~ "filename of a records log file. "
#~ "Each row of this file is an "
#~ "encoded record pair. If it is a"
#~ " list it can either be a list"
#~ " of paths to logs that will be"
#~ " loaded jointly or an iterator of "
#~ "measurement results."
#~ msgstr ""

#~ msgid "filename: str"
#~ msgstr ""

