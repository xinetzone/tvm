# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022, xinetzone
# This file is distributed under the same license as the TVM package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: TVM \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-03-31 18:33+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../docs/reference/api/python/auto_scheduler.rst:19
msgid "tvm.auto_scheduler"
msgstr ""

#~ msgid "Namespace for TVM Auto-scheduler."
#~ msgstr ""

#~ msgid "**Classes:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ApplyHistoryBest "
#~ "<tvm.auto_scheduler.ApplyHistoryBest>`\\ \\(records\\[\\, "
#~ "n\\_lines\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Apply the history best config"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ApplyHistoryBestOrSample "
#~ "<tvm.auto_scheduler.ApplyHistoryBestOrSample>`\\ "
#~ "\\(records\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Apply the history best config, or "
#~ "sample a valid schedule if no "
#~ "config is found."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`ComputeDAG <tvm.auto_scheduler.ComputeDAG>`\\ "
#~ "\\(compute\\_or\\_sche\\)"
#~ msgstr ""

#~ msgid "The auto-scheduler's computational graph and related program analyses."
#~ msgstr ""

#~ msgid ":py:obj:`DispatchContext <tvm.auto_scheduler.DispatchContext>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Base class of dispatch context."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`EmptyPolicy <tvm.auto_scheduler.EmptyPolicy>`\\ "
#~ "\\(task\\[\\, init\\_search\\_callbacks\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "A simple example of the search "
#~ "policy which always returns the initial"
#~ " naive schedule (state)."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`HardwareParams <tvm.auto_scheduler.HardwareParams>`\\"
#~ " \\(\\[num\\_cores\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "The parameters of target hardware used to guide the search policy."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LayoutRewriteOption "
#~ "<tvm.auto_scheduler.LayoutRewriteOption>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Options for applying layout rewrite."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LocalBuilder <tvm.auto_scheduler.LocalBuilder>`\\ "
#~ "\\(\\[timeout\\, n\\_parallel\\, build\\_func\\]\\)"
#~ msgstr ""

#~ msgid "LocalBuilder use local CPU cores to build programs in parallel."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LocalRPCMeasureContext "
#~ "<tvm.auto_scheduler.LocalRPCMeasureContext>`\\ \\(\\[priority\\,"
#~ " ...\\]\\)"
#~ msgstr ""

#~ msgid "A context wrapper for running RPCRunner locally."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`LocalRunner <tvm.auto_scheduler.LocalRunner>`\\ "
#~ "\\(\\[timeout\\, number\\, repeat\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "LocalRunner that uses local CPU/GPU to"
#~ " measures the time cost of programs."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MeasureInput <tvm.auto_scheduler.MeasureInput>`\\ "
#~ "\\(task\\, state\\)"
#~ msgstr ""

#~ msgid "Store the input of a measurement."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`MeasureResult <tvm.auto_scheduler.MeasureResult>`\\ "
#~ "\\(costs\\, error\\_no\\, error\\_msg\\, ...\\)"
#~ msgstr ""

#~ msgid "Store the results of a measurement."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PreloadCustomSketchRule "
#~ "<tvm.auto_scheduler.PreloadCustomSketchRule>`\\ "
#~ "\\(meet\\_condition\\_func\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "A SearchCallback for SketchSearchPolicy that"
#~ " allows users to add custom sketch"
#~ " rule."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`PreloadMeasuredStates "
#~ "<tvm.auto_scheduler.PreloadMeasuredStates>`\\ \\(filename\\)"
#~ msgstr ""

#~ msgid ""
#~ "A SearchCallback to load measured states"
#~ " from the log file for a search"
#~ " policy."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RPCRunner <tvm.auto_scheduler.RPCRunner>`\\ "
#~ "\\(key\\, host\\, port\\[\\, priority\\, "
#~ "...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "RPCRunner that uses RPC call to "
#~ "measures the time cost of programs "
#~ "on remote devices."
#~ msgstr ""

#~ msgid ":py:obj:`RandomModel <tvm.auto_scheduler.RandomModel>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "A model that returns random estimation for all inputs"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RecordReader <tvm.auto_scheduler.RecordReader>`\\ "
#~ "\\(filename\\)"
#~ msgstr ""

#~ msgid "Reader of the json log file."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`RecordToFile <tvm.auto_scheduler.RecordToFile>`\\ "
#~ "\\(filename\\)"
#~ msgstr ""

#~ msgid "A measurement callback that writes measurement records into a file."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SearchTask <tvm.auto_scheduler.SearchTask>`\\ "
#~ "\\(\\[func\\, args\\, compute\\_dag\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "The computation information and hardware "
#~ "parameters for a schedule search task."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`SketchPolicy <tvm.auto_scheduler.SketchPolicy>`\\ "
#~ "\\(task\\[\\, program\\_cost\\_model\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "The search policy that searches in "
#~ "a hierarchical search space defined by"
#~ " sketches."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`TaskScheduler <tvm.auto_scheduler.TaskScheduler>`\\ "
#~ "\\(tasks\\[\\, task\\_weights\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Allocate the time resources when tuning multiple tasks together."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`TuningOptions <tvm.auto_scheduler.TuningOptions>`\\ "
#~ "\\(\\[num\\_measure\\_trials\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "This controls the options of performance tuning."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`XGBModel <tvm.auto_scheduler.XGBModel>`\\ "
#~ "\\(\\[verbose\\_eval\\, num\\_warmup\\_sample\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Train a XGBoost model to predict "
#~ "the normalized throughputs of programs."
#~ msgstr ""

#~ msgid "**Functions:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`auto_schedule <tvm.auto_scheduler.auto_schedule>`\\ "
#~ "\\(task\\[\\, search\\_policy\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "THIS API IS DEPRECATED."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`create_task <tvm.auto_scheduler.create_task>`\\ "
#~ "\\(func\\, args\\, target\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`extract_tasks <tvm.auto_scheduler.extract_tasks>`\\ "
#~ "\\(mod\\, params\\, target\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Extract tuning tasks from a relay program."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_shape_from_rewritten_layout "
#~ "<tvm.auto_scheduler.get_shape_from_rewritten_layout>`\\ "
#~ "\\(...\\)"
#~ msgstr ""

#~ msgid "Get the orginal shape from a rewritten layout string."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`is_auto_scheduler_enabled "
#~ "<tvm.auto_scheduler.is_auto_scheduler_enabled>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Return whether the auto-scheduler is enabled."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`load_best_record "
#~ "<tvm.auto_scheduler.load_best_record>`\\ \\(filename\\[\\, "
#~ "workload\\_key\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Return the best measurement pair form a log file."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`load_records <tvm.auto_scheduler.load_records>`\\ "
#~ "\\(filename\\)"
#~ msgstr ""

#~ msgid "Load measurement records from a file."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`make_workload_key "
#~ "<tvm.auto_scheduler.make_workload_key>`\\ \\(func\\, "
#~ "args\\)"
#~ msgstr ""

#~ msgid "Make a workload key by function and arguments."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`register_task_input_check_func "
#~ "<tvm.auto_scheduler.register_task_input_check_func>`\\ "
#~ "\\(func\\_name\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Register a function that checks the input buffer map."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`register_workload "
#~ "<tvm.auto_scheduler.register_workload>`\\ \\(func\\_name\\[\\,"
#~ " f\\, override\\]\\)"
#~ msgstr ""

#~ msgid "Register a function that generates a certain workload."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`remove_index_check "
#~ "<tvm.auto_scheduler.remove_index_check>`\\ \\(tensor\\)"
#~ msgstr ""

#~ msgid "Remove the safety check in the indexing function for a tensor."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`rewrite_compute_body "
#~ "<tvm.auto_scheduler.rewrite_compute_body>`\\ "
#~ "\\(compute\\_tensor\\, new\\_layout\\)"
#~ msgstr ""

#~ msgid ""
#~ "Rewrite the body of a ComputeOp "
#~ "according to a new layout of a "
#~ "placeholder"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`save_records <tvm.auto_scheduler.save_records>`\\ "
#~ "\\(filename\\, inputs\\, results\\)"
#~ msgstr ""

#~ msgid "Append measure records to file."
#~ msgstr ""

#~ msgid "参数"
#~ msgstr ""

#~ msgid ""
#~ "Collection of tuning records. If is "
#~ "str, then it should be the "
#~ "filename of a records log file. "
#~ "Each row of this file is an "
#~ "encoded record pair. Otherwise, it is"
#~ " an iterator."
#~ msgstr ""

#~ msgid "if it is not None, only load the first `n_lines` lines of log."
#~ msgstr ""

#~ msgid "When set to True, compatible records will also be considered."
#~ msgstr ""

#~ msgid "**Methods:**"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_workload_entry "
#~ "<tvm.auto_scheduler.ApplyHistoryBest.get_workload_entry>`\\ "
#~ "\\(best\\_records\\, target\\_key\\, ...\\)"
#~ msgstr ""

#~ msgid ""
#~ "Get the entry of the target key"
#~ " and workload key hash in the "
#~ "given best record map."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`load <tvm.auto_scheduler.ApplyHistoryBest.load>`\\ "
#~ "\\(records\\[\\, n\\_lines\\]\\)"
#~ msgstr ""

#~ msgid "Load records to this dispatch context"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`update <tvm.auto_scheduler.ApplyHistoryBest.update>`\\"
#~ " \\(target\\, workload\\_key\\, state\\)"
#~ msgstr ""

#~ msgid "Update the config for a workload"
#~ msgstr ""

#~ msgid "The best record map."
#~ msgstr ""

#~ msgid "The first key to the best_records."
#~ msgstr ""

#~ msgid "The workload key that can be decoded to workload hash and args."
#~ msgstr ""

#~ msgid "返回"
#~ msgstr ""

#~ msgid ""
#~ "* **entry** (*Dict[str, Any]*) -- The"
#~ " entry in best_records with target "
#~ "key and workload hash. * "
#~ "**workload_hash** (*str*) -- The workload "
#~ "hash decoded from workload_key. * "
#~ "**workload_args** (*Tuple[Any, ...]*) -- The"
#~ " hashable tuple of workload args "
#~ "decoded from workload_key."
#~ msgstr ""

#~ msgid ""
#~ "**entry** (*Dict[str, Any]*) -- The "
#~ "entry in best_records with target key"
#~ " and workload hash."
#~ msgstr ""

#~ msgid ""
#~ "**workload_hash** (*str*) -- The workload "
#~ "hash decoded from workload_key."
#~ msgstr ""

#~ msgid ""
#~ "**workload_args** (*Tuple[Any, ...]*) -- The"
#~ " hashable tuple of workload args "
#~ "decoded from workload_key."
#~ msgstr ""

#~ msgid "if it is not None, only load the first `n_lines` lines of log"
#~ msgstr ""

#~ msgid "The current target"
#~ msgstr ""

#~ msgid "The current workload_key."
#~ msgstr ""

#~ msgid "The state that stores schedule configuration for the workload"
#~ msgstr ""

#~ msgid ""
#~ "When False, sampling will not apply "
#~ "to simple workloads (w/o reduction)."
#~ msgstr ""

#~ msgid ""
#~ "The filename of the pre-trained "
#~ "XGBoost cost model. If not present, "
#~ "then random model will be used."
#~ msgstr ""

#~ msgid ""
#~ "Meausre the top-N rank of sampled "
#~ "schedules on the device. The default "
#~ "-1 means no measurement and simply "
#~ "return the top-1 schedule ranked by "
#~ "the cost model."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`query "
#~ "<tvm.auto_scheduler.ApplyHistoryBestOrSample.query>`\\ "
#~ "\\(target\\, workload\\_key\\, has\\_complex\\_op\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid "Query the context to get the specific config for a workload."
#~ msgstr ""

#~ msgid ""
#~ "Query the context to get the "
#~ "specific config for a workload. If "
#~ "this function cannot find the result "
#~ "inside this context, it will query "
#~ "the result from the upper contexts."
#~ msgstr ""

#~ msgid "The workload key"
#~ msgstr ""

#~ msgid "Whether this workload has at least one complex op."
#~ msgstr ""

#~ msgid "The ComputeDAG of the workload."
#~ msgstr ""

#~ msgid "The function name of this workload."
#~ msgstr ""

#~ msgid ""
#~ "**state** -- The state that stores "
#~ "schedule configuration for the workload"
#~ msgstr ""

#~ msgid "返回类型"
#~ msgstr ""

#~ msgid ""
#~ "We convert a compute declaration "
#~ "described by `tvm.compute` (could be a"
#~ " single operator or a subgraph) to"
#~ " a ComputeDAG. It keeps the "
#~ "input/output tensors, all operations in "
#~ "the DAG, and some static analysis "
#~ "results for the DAG (e.g. the "
#~ "total float operation count, consumer/producer"
#~ " relations of operations, whether an "
#~ "operation stage should be tiled/compute "
#~ "inlined). These analyses can help the"
#~ " search policy to make decisions "
#~ "during the search. ComputeDAG is also"
#~ " responsible for the interaction between"
#~ " auto-scheduler's `LoopState` and TVM "
#~ "schedule (e.g. applying the `LoopState` "
#~ "transform steps to a TVM schedule, "
#~ "providing `LoopState` with extra information"
#~ " got from TVM schedule)."
#~ msgstr ""

#~ msgid "Input/output tensors or workload key for a compute declaration."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`apply_steps_from_state "
#~ "<tvm.auto_scheduler.ComputeDAG.apply_steps_from_state>`\\ "
#~ "\\(state\\[\\, layout\\_rewrite\\]\\)"
#~ msgstr ""

#~ msgid "Apply the history transform steps from a State to get a TVM schedule."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_init_state "
#~ "<tvm.auto_scheduler.ComputeDAG.get_init_state>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Get the init state of this ComputeDAG."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`infer_bound_from_state "
#~ "<tvm.auto_scheduler.ComputeDAG.infer_bound_from_state>`\\ "
#~ "\\(state\\)"
#~ msgstr ""

#~ msgid "Infer and fill the bound of all iterators of a state."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`print_python_code_from_state "
#~ "<tvm.auto_scheduler.ComputeDAG.print_python_code_from_state>`\\ "
#~ "\\(state\\)"
#~ msgstr ""

#~ msgid ""
#~ "Print transform steps in the history "
#~ "of a State as TVM's python "
#~ "schedule code."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`rewrite_layout_from_state "
#~ "<tvm.auto_scheduler.ComputeDAG.rewrite_layout_from_state>`\\ "
#~ "\\(state\\)"
#~ msgstr ""

#~ msgid ""
#~ "Rewrite the layout of the DAG "
#~ "according to the history transform steps"
#~ " of a state."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`workload_key "
#~ "<tvm.auto_scheduler.ComputeDAG.workload_key>`\\ \\(\\)"
#~ msgstr ""

#~ msgid "Return the workload key of this compute DAG."
#~ msgstr ""

#~ msgid "The state from which we get transform steps."
#~ msgstr ""

#~ msgid ""
#~ "Rewrite the layout of placeholders "
#~ "specified by \"layout_free_placeholders\" attr "
#~ "to make it most friendly for the"
#~ " generated schedule to read from."
#~ msgstr ""

#~ msgid ""
#~ "A `te.schedule` and the a list of"
#~ " `te.Tensor` to be used in "
#~ "`tvm.lower` or `tvm.build`."
#~ msgstr ""

#~ msgid "**state** -- The initial State without any transform steps."
#~ msgstr ""

#~ msgid ""
#~ "The states may lose complete bound "
#~ "information after some transform steps "
#~ "(e.g., compute_at). We can call this "
#~ "function to infer and fill all the"
#~ " bound information. This function calls "
#~ "TVM InferBound pass internally to get"
#~ " the bound. The returned state of "
#~ "this function is guaranteed to have "
#~ "complete iterator extent information."
#~ msgstr ""

#~ msgid "**updated_state** -- The State with complete bound information."
#~ msgstr ""

#~ msgid ""
#~ "This is used to print transformation "
#~ "steps for debugging. Use "
#~ "`apply_steps_from_state` if you want to "
#~ "get a schedule for code generation."
#~ msgstr ""

#~ msgid "**str** -- The Python schedule code."
#~ msgstr ""

#~ msgid "**updated_dag** -- The compute dag with rewritten layout."
#~ msgstr ""

#~ msgid ""
#~ "Return the workload key of this "
#~ "compute DAG. The workload key is a"
#~ " JSON string from a tuple of "
#~ "(hash of DAG, tensor shapes...)"
#~ msgstr ""

#~ msgid "**key** -- The workload key of this compute DAG"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`query <tvm.auto_scheduler.DispatchContext.query>`\\ "
#~ "\\(target\\, workload\\_key\\, has\\_complex\\_op\\, "
#~ "...\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`update <tvm.auto_scheduler.DispatchContext.update>`\\"
#~ " \\(target\\, workload\\_key\\, state\\)"
#~ msgstr ""

#~ msgid "The SearchTask for the computation declaration."
#~ msgstr ""

#~ msgid "Callback functions called before the search process."
#~ msgstr ""

#~ msgid ""
#~ "When a parameter isn't provided, it "
#~ "will instead use the current machine's"
#~ " default value if target is "
#~ "specified. TODO(jcf94): This is considered "
#~ "to be merged with the new Target"
#~ " specification: https://discuss.tvm.apache.org/t/rfc-"
#~ "tvm-target-specification/6844 :param num_cores:"
#~ " The number of device cores. :type"
#~ " num_cores: int, optional :param "
#~ "vector_unit_bytes: The width of vector "
#~ "units in bytes. :type vector_unit_bytes: "
#~ "int, optional :param cache_line_bytes: The "
#~ "size of cache line in bytes. :type"
#~ " cache_line_bytes: int, optional :param "
#~ "max_shared_memory_per_block: The max shared "
#~ "memory per block in bytes. :type "
#~ "max_shared_memory_per_block: int, optional :param"
#~ " max_local_memory_per_block: The max local "
#~ "memory per block in bytes. :type "
#~ "max_local_memory_per_block: int, optional :param "
#~ "max_threads_per_block: The max number of "
#~ "threads per block. :type "
#~ "max_threads_per_block: int, optional :param "
#~ "max_vthread_extent: The max vthread extent."
#~ " :type max_vthread_extent: int, optional "
#~ ":param warp_size: The thread numbers of"
#~ " a warp. :type warp_size: int, "
#~ "optional :param target: The compilation "
#~ "target. Used to determine default values"
#~ " if provided. :type target: str or"
#~ " Target, optional :param target_host: The"
#~ " compilation target host. Used to "
#~ "determine default values if provided. "
#~ ":type target_host: str or Target, "
#~ "optional"
#~ msgstr ""

#~ msgid ""
#~ "The NO_REWRITE and INSERT_TRANSFORM_STAGE are"
#~ " expected to be used when tuning "
#~ "a standalone op, and the "
#~ "REWRITE_FOR_PRE_TRANSFORMED is expected to be"
#~ " used when tuning ops inside a "
#~ "network."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`get_target_default "
#~ "<tvm.auto_scheduler.LayoutRewriteOption.get_target_default>`\\ "
#~ "\\(target\\[\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Get the default layout rewrite option for the specified target."
#~ msgstr ""

#~ msgid ""
#~ "Get the default layout rewrite option"
#~ " for the specified target. Currently "
#~ "we only enable layout rewrite for "
#~ "cpu / mali backend for now"
#~ msgstr ""

#~ msgid "The compilation target."
#~ msgstr ""

#~ msgid "If this check is ask for relay integration."
#~ msgstr ""

#~ msgid ""
#~ "**layout_rewrite_option** -- The default "
#~ "layout rewrite option for the specified"
#~ " target."
#~ msgstr ""

#~ msgid ""
#~ "The timeout limit (in second) for "
#~ "each build thread. This is used in"
#~ " a wrapper of the "
#~ "multiprocessing.Process.join()."
#~ msgstr ""

#~ msgid "Number of threads used to build in parallel."
#~ msgstr ""

#~ msgid ""
#~ "If is 'default', use default build "
#~ "function If is 'ndk', use function "
#~ "for android ndk If is callable, "
#~ "use it as custom build function, "
#~ "expect lib_format field."
#~ msgstr ""

#~ msgid ""
#~ "A context wrapper for running RPCRunner"
#~ " locally. This will launch a local"
#~ " RPC Tracker and local RPC Server."
#~ msgstr ""

#~ msgid "The priority of this run request, larger is more prior."
#~ msgstr ""

#~ msgid "The number of tasks run in parallel."
#~ msgstr ""

#~ msgid ""
#~ "The timeout limit (in second) for "
#~ "each run. This is used in a "
#~ "wrapper of the multiprocessing.Process.join()."
#~ msgstr ""

#~ msgid ""
#~ "The number of times to run the "
#~ "generated code for taking average. We"
#~ " call these runs as one `repeat` "
#~ "of measurement."
#~ msgstr ""

#~ msgid ""
#~ "The number of times to repeat the"
#~ " measurement. In total, the generated "
#~ "code will be run (1 + number "
#~ "x repeat) times, where the first "
#~ "\"1\" is warm up and will be "
#~ "discarded. The returned result contains "
#~ "`repeat` costs, each of which is "
#~ "an average of `number` costs."
#~ msgstr ""

#~ msgid ""
#~ "The minimum duration of one `repeat` "
#~ "in milliseconds. By default, one "
#~ "`repeat` contains `number` runs. If this"
#~ " parameter is set, the parameters "
#~ "`number` will be dynamically adjusted to"
#~ " meet the minimum duration requirement "
#~ "of one `repeat`. i.e., When the "
#~ "run time of one `repeat` falls "
#~ "below this time, the `number` parameter"
#~ " will be automatically increased."
#~ msgstr ""

#~ msgid "The cool down interval between two measurements in seconds."
#~ msgstr ""

#~ msgid ""
#~ "Whether to flush cache on CPU "
#~ "between repeated measurements. Flushing cache"
#~ " can make the measured latency of "
#~ "one operator closer to its actual "
#~ "latency during end-to-end inference. "
#~ "To make this option effective, the "
#~ "argument `number` should also be set "
#~ "to 1. This is only has effect "
#~ "on CPU task."
#~ msgstr ""

#~ msgid "Which device to run on if multiple are available."
#~ msgstr ""

#~ msgid "The SearchTask of this measurement."
#~ msgstr ""

#~ msgid "The State to be measured."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`serialize "
#~ "<tvm.auto_scheduler.MeasureInput.serialize>`\\ \\(\\)"
#~ msgstr ""

#~ msgid ""
#~ "Custom serialization to workaround "
#~ "MeasureInput not exposing all its "
#~ "members to the TVM ffi interface."
#~ msgstr ""

#~ msgid ""
#~ "Note that we do not implement "
#~ "__getstate__ as it does not seem "
#~ "to work with initialization of the "
#~ "workload registry (maybe because of "
#~ "initialization order?)."
#~ msgstr ""

#~ msgid "The time costs of execution."
#~ msgstr ""

#~ msgid "The error code."
#~ msgstr ""

#~ msgid "The error message if there is any error."
#~ msgstr ""

#~ msgid "The time cost of build and run."
#~ msgstr ""

#~ msgid "The time stamps of this measurement."
#~ msgstr ""

#~ msgid "提示"
#~ msgstr ""

#~ msgid ""
#~ "This is an advanced feature. Make "
#~ "sure you're clear how it works and"
#~ " this should only be used in "
#~ "SketchSearchPolicy."
#~ msgstr ""

#~ msgid ""
#~ "A function with `(policy, state, "
#~ "stage_id) -> int`. Should return one "
#~ "of the result enumeration."
#~ msgstr ""

#~ msgid "A function with `(policy, state, stage_id) -> [[State, int], ...]`."
#~ msgstr ""

#~ msgid "The name of this custom sketch rule."
#~ msgstr ""

#~ msgid "This can resume the state of the search policy:"
#~ msgstr ""

#~ msgid ""
#~ "Making sure an already measured state"
#~ " in former searches will never be "
#~ "measured again."
#~ msgstr ""

#~ msgid ""
#~ "The history states can be used to"
#~ " speed up the search process(e.g. "
#~ "SketchPolicy uses history states as "
#~ "starting point to perform Evolutionary "
#~ "Search)."
#~ msgstr ""

#~ msgid "The name of the record file."
#~ msgstr ""

#~ msgid ""
#~ "RPCRunner that uses RPC call to "
#~ "measures the time cost of programs "
#~ "on remote devices. Or sometime we "
#~ "may need to use RPC even in "
#~ "local running to insulate the thread "
#~ "environment. (e.g. running CUDA programs)"
#~ msgstr ""

#~ msgid "The key of the device registered in the RPC tracker."
#~ msgstr ""

#~ msgid "The host address of the RPC Tracker."
#~ msgstr ""

#~ msgid "The port of RPC Tracker."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`predict <tvm.auto_scheduler.RandomModel.predict>`\\ "
#~ "\\(search\\_task\\, states\\)"
#~ msgstr ""

#~ msgid "Predict the scores of states"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`update <tvm.auto_scheduler.RandomModel.update>`\\ "
#~ "\\(inputs\\, results\\)"
#~ msgstr ""

#~ msgid ""
#~ "Update the cost model according to "
#~ "new measurement results (training data)."
#~ msgstr ""

#~ msgid "The search task of states"
#~ msgstr ""

#~ msgid "The input states"
#~ msgstr ""

#~ msgid "**scores** -- The predicted scores for all states"
#~ msgstr ""

#~ msgid "The measurement inputs"
#~ msgstr ""

#~ msgid "The measurement results"
#~ msgstr ""

#~ msgid "File name for this reader to load log from."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`check_workload_key "
#~ "<tvm.auto_scheduler.RecordReader.check_workload_key>`\\ "
#~ "\\(inputs\\)"
#~ msgstr ""

#~ msgid "Check and throw warnings for records with old format workload key."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`read_lines "
#~ "<tvm.auto_scheduler.RecordReader.read_lines>`\\ "
#~ "\\(\\[max\\_lines\\, skip\\_lines\\]\\)"
#~ msgstr ""

#~ msgid "Read multiple lines from the log file."
#~ msgstr ""

#~ msgid "The measure inputs to be checked."
#~ msgstr ""

#~ msgid "This checker could be deprecated in the future."
#~ msgstr ""

#~ msgid "The maximum number of lines. None to read all lines."
#~ msgstr ""

#~ msgid "Skip the first n lines."
#~ msgstr ""

#~ msgid ""
#~ "* **inputs** (*List[auto_scheduler.measure.MeasureInput]*)"
#~ " -- The MeasureInputs loaded from the"
#~ " log file. * **results** "
#~ "(*List[auto_scheduler.measure.MeasureResult]*) -- The "
#~ "MeasureResults loaded from the log file."
#~ msgstr ""

#~ msgid ""
#~ "**inputs** (*List[auto_scheduler.measure.MeasureInput]*) "
#~ "-- The MeasureInputs loaded from the "
#~ "log file."
#~ msgstr ""

#~ msgid ""
#~ "**results** (*List[auto_scheduler.measure.MeasureResult]*) "
#~ "-- The MeasureResults loaded from the"
#~ " log file."
#~ msgstr ""

#~ msgid ""
#~ "Some unimportant and expensive fields in"
#~ " the returned MeasureInput are not "
#~ "deserialized for faster read speed (e.g."
#~ " input.task.compute_dag, input.state.stages). If "
#~ "you want to use them, you can "
#~ "call the :code:`recover_measure_input` below "
#~ "to rebuild these fields."
#~ msgstr ""

#~ msgid "File name for this callback to write log to."
#~ msgstr ""

#~ msgid ""
#~ "The function that returns the compute"
#~ " declaration Tensors. Can be the a"
#~ " function or the function name."
#~ msgstr ""

#~ msgid "The args of the function."
#~ msgstr ""

#~ msgid "The ComputeDAG for the corresponding compute declaration."
#~ msgstr ""

#~ msgid "The workload key for the corresponding compute declaration."
#~ msgstr ""

#~ msgid "The target device of this search task."
#~ msgstr ""

#~ msgid "The target host device of this search task."
#~ msgstr ""

#~ msgid "Hardware parameters used in this search task."
#~ msgstr ""

#~ msgid ""
#~ "The layout rewrite option used for "
#~ "measuring programs. If None, the default"
#~ " value will be set depending on "
#~ "the specified target. Auto_scheduler will "
#~ "find a better schedule for the "
#~ "specified layout rewrite option. The "
#~ "NO_REWRITE and INSERT_TRANSFORM_STAGE are "
#~ "expected to be used when tuning a"
#~ " standalone op, and the "
#~ "REWRITE_FOR_PRE_TRANSFORMED is expected to be"
#~ " used when tuning ops inside a "
#~ "network."
#~ msgstr ""

#~ msgid ""
#~ "A dict maps the input names to "
#~ "input tensors or a list of input"
#~ " names. Some special Tensor used as"
#~ " inputs in program measuring. Usually "
#~ "we do not need to care about "
#~ "it, but for special workloads like "
#~ "Sparse computation the Sparse Tensor "
#~ "input are meaningful that we cannot "
#~ "use random input directly."
#~ msgstr ""

#~ msgid ""
#~ "Whether to overwrite the data if a"
#~ " name has already in the global "
#~ "table."
#~ msgstr ""

#~ msgid ""
#~ "Whether to save the data to a "
#~ "local file as well. This can be"
#~ " reused to resume the last tuning "
#~ "process."
#~ msgstr ""

#~ msgid "The description string of this task."
#~ msgstr ""

#~ msgid "实际案例"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`apply_best "
#~ "<tvm.auto_scheduler.SearchTask.apply_best>`\\ "
#~ "\\(log\\_file\\[\\, include\\_compatible\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Apply the history best from a log file and return the schedule."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`print_best "
#~ "<tvm.auto_scheduler.SearchTask.print_best>`\\ "
#~ "\\(log\\_file\\[\\, print\\_mode\\]\\)"
#~ msgstr ""

#~ msgid ""
#~ "Print the best schedule as python "
#~ "schedule API code or CUDA source "
#~ "code."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tune <tvm.auto_scheduler.SearchTask.tune>`\\ "
#~ "\\(tuning\\_options\\[\\, search\\_policy\\]\\)"
#~ msgstr ""

#~ msgid "Run auto scheduling search for a task"
#~ msgstr ""

#~ msgid "The name of the log file."
#~ msgstr ""

#~ msgid ""
#~ "When set to True, all compatible "
#~ "records in the log file will be"
#~ " considered."
#~ msgstr ""

#~ msgid "The layout rewrite option."
#~ msgstr ""

#~ msgid ""
#~ "A `te.Schedule` and the a list of"
#~ " `te.Tensor` to be used in "
#~ "`tvm.lower` or `tvm.build`."
#~ msgstr ""

#~ msgid "The name of the log file"
#~ msgstr ""

#~ msgid ""
#~ "if \"schedule\", print the best schedule"
#~ " as python schedule API code. if "
#~ "\"cuda\", print the best schedule as "
#~ "CUDA source code."
#~ msgstr ""

#~ msgid "**code** -- The best schedule code in python API or CUDA source code"
#~ msgstr ""

#~ msgid "Tuning and measurement options."
#~ msgstr ""

#~ msgid "The search policy to be used for schedule search."
#~ msgstr ""

#~ msgid ""
#~ "The search policy that searches in "
#~ "a hierarchical search space defined by"
#~ " sketches. The policy randomly samples "
#~ "programs from the space defined by "
#~ "sketches and use evolutionary search to"
#~ " fine-tune them."
#~ msgstr ""

#~ msgid "The cost model to estimate the complete schedules."
#~ msgstr ""

#~ msgid ""
#~ "Parameters of the search policy. See "
#~ "`src/auto_scheduler/search_policy/sketch_search_policy.h` for"
#~ " the definitions. See `DEFAULT_PARAMS` "
#~ "below to find the default values."
#~ msgstr ""

#~ msgid "Random seed."
#~ msgstr ""

#~ msgid ""
#~ "Verbosity level. 0 for silent, 1 "
#~ "to output information during schedule "
#~ "search."
#~ msgstr ""

#~ msgid ""
#~ "Callback functions called before the "
#~ "search process, usually used to do "
#~ "extra initializations. Possible callbacks:    "
#~ "- auto_scheduler.PreloadMeasuredStates   - "
#~ "auto_scheduler.PreloadCustomSketchRule"
#~ msgstr ""

#~ msgid ""
#~ "Callback functions called before the "
#~ "search process, usually used to do "
#~ "extra initializations. Possible callbacks:"
#~ msgstr ""

#~ msgid "auto_scheduler.PreloadMeasuredStates"
#~ msgstr ""

#~ msgid "auto_scheduler.PreloadCustomSketchRule"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`evolutionary_search "
#~ "<tvm.auto_scheduler.SketchPolicy.evolutionary_search>`\\ "
#~ "\\(init\\_populations\\, out\\_size\\)"
#~ msgstr ""

#~ msgid "Perform evolutionary search."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`generate_sketches "
#~ "<tvm.auto_scheduler.SketchPolicy.generate_sketches>`\\ "
#~ "\\(\\[print\\_for\\_debug\\]\\)"
#~ msgstr ""

#~ msgid "Generate the sketches."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`sample_initial_population "
#~ "<tvm.auto_scheduler.SketchPolicy.sample_initial_population>`\\ "
#~ "\\(\\)"
#~ msgstr ""

#~ msgid "Sample initial population."
#~ msgstr ""

#~ msgid ""
#~ "Perform evolutionary search. This python "
#~ "interface is mainly used for debugging"
#~ " and testing. The actual search is"
#~ " all done in c++."
#~ msgstr ""

#~ msgid "The initial population states"
#~ msgstr ""

#~ msgid "The size of generated states"
#~ msgstr ""

#~ msgid "**states** -- The generated states"
#~ msgstr ""

#~ msgid ""
#~ "Generate the sketches. This python "
#~ "interface is mainly used for debugging"
#~ " and testing. The actual search is"
#~ " all done in c++."
#~ msgstr ""

#~ msgid "Whether print out the sketches for debug."
#~ msgstr ""

#~ msgid "**sketches** -- The generated sketches of this search task."
#~ msgstr ""

#~ msgid ""
#~ "Sample initial population. This python "
#~ "interface is mainly used for debugging"
#~ " and testing. The actual search is"
#~ " all done in c++."
#~ msgstr ""

#~ msgid "**states** -- The sampled states"
#~ msgstr ""

#~ msgid ""
#~ "Allocate the time resources when tuning"
#~ " multiple tasks together. This implements"
#~ " two strategies: \"round-robin\" and "
#~ "\"gradient\"."
#~ msgstr ""

#~ msgid "All tasks to tune"
#~ msgstr ""

#~ msgid ""
#~ "The weights of tasks. If provided, "
#~ "the task scheduler will set the "
#~ "objective function to sum(weight[t] * "
#~ "latency[t]), where weight[t] is the "
#~ "weight of a task and the "
#~ "lantecy[t] is the lantecy of the "
#~ "task. If not provided, the task "
#~ "scheduer will assign equal weights to"
#~ " all tasks (i.e., the objective "
#~ "function is sum(latency[t]))."
#~ msgstr ""

#~ msgid ""
#~ "The objective function to be minimized."
#~ " The objective function accepts the "
#~ "current latencies of all tasks and "
#~ "returns the objective. If not provided,"
#~ " the objective is the weighted sum"
#~ " of the latencies of all tasks."
#~ msgstr ""

#~ msgid ""
#~ "The scheduling strategy. \"round-robin\": "
#~ "Tune tasks in round robin order. "
#~ "\"gradient\" : Tune tasks with gradient"
#~ " descent."
#~ msgstr ""

#~ msgid ""
#~ "Load pre-trained model from this "
#~ "file. If this is None, the cost"
#~ " model will be trained from scratch."
#~ msgstr ""

#~ msgid ""
#~ "Load measurement records from this file."
#~ " If it is not None, the status"
#~ " of the task scheduler, search "
#~ "policies and cost models will be "
#~ "restored according to this file."
#~ msgstr ""

#~ msgid "The level of verbosity. 0 means silent."
#~ msgstr ""

#~ msgid "The parameter used for 'gradient' strategy"
#~ msgstr ""

#~ msgid ""
#~ "The task scheduler callbacks that will"
#~ " be called before and after tuning"
#~ " a task. If None, PrintTableInfo and"
#~ " LogEstimatedLatency callback will be used."
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`tune <tvm.auto_scheduler.TaskScheduler.tune>`\\ "
#~ "\\(tune\\_option\\[\\, search\\_policy\\, ...\\]\\)"
#~ msgstr ""

#~ msgid "Tune a batch of tasks together."
#~ msgstr ""

#~ msgid "The tuning options applied to all tasks."
#~ msgstr ""

#~ msgid ""
#~ "The list of search policies. If it"
#~ " is str, \"default\" for the default"
#~ " policy (SketchPolicy + XGBModel), "
#~ "\"sketch.xgb\" for SketchPolicy + XGBModel,"
#~ " \"sketch.random\" for SketchPolicy + "
#~ "RandomModel."
#~ msgstr ""

#~ msgid "The parameters of the search policy"
#~ msgstr ""

#~ msgid ""
#~ "Option used by XGBModel to reduce "
#~ "the model training frequency when "
#~ "there're too many logs."
#~ msgstr ""

#~ msgid ""
#~ "Stop tuning a task early if "
#~ "getting no improvement after n "
#~ "measurements."
#~ msgstr ""

#~ msgid ""
#~ "The number of measurement trials. The"
#~ " search policy measures `num_measure_trials` "
#~ "schedules in total and returns the "
#~ "best one among them. With "
#~ "`num_measure_trials` == 0, the policy "
#~ "will do the schedule search but "
#~ "won't involve measurement. This can be"
#~ " used to get a runnable schedule "
#~ "quickly without auto-tuning."
#~ msgstr ""

#~ msgid "Stop the tuning early if getting no improvement after n measurements."
#~ msgstr ""

#~ msgid ""
#~ "The number of schedules to be "
#~ "measured at each search round. The "
#~ "whole schedule search process will try"
#~ " a total number of `num_measure_trials` "
#~ "in several rounds."
#~ msgstr ""

#~ msgid "ProgramBuilder which builds the program."
#~ msgstr ""

#~ msgid "ProgramRunner which runs the program and measures time costs."
#~ msgstr ""

#~ msgid ""
#~ "Callback functions called after each "
#~ "measurement. Candidates: - "
#~ "auto_scheduler.RecordToFile"
#~ msgstr ""

#~ msgid ""
#~ "Train a XGBoost model to predict "
#~ "the normalized throughputs of programs. "
#~ "Let the normalized throughput be the "
#~ "score of a program (higher is "
#~ "better). We predict the (approximate) "
#~ "score of a program = the sum "
#~ "of the scores of all stages in "
#~ "this program. i.e. score(P) = score_s0"
#~ " + score_s1 + ... + score_sn, "
#~ "where score_si is the score of "
#~ "Stage i in Program P. We extract"
#~ " feature for each stage and let "
#~ "the xgboost predict the score for "
#~ "each stage. We then sum up the "
#~ "predictions as the score of the "
#~ "whole program. We use RMSE as the"
#~ " loss function.  i.e. loss(P, y) ="
#~ " 1/2 * (score(P) - y)^2, where "
#~ "P is the program and y is "
#~ "the normalized throughput according to "
#~ "the ground truth (measurement). XGBoost "
#~ "does not support this loss function "
#~ "because `score(P)` is a sum of the"
#~ " prediction of several samples, so we"
#~ " implemented a custom loss function "
#~ "and call it pack-sum-rmse. It "
#~ "is called \"pack-sum\" because we "
#~ "combine several samples into a \"pack\""
#~ " and sum up their predictions."
#~ msgstr ""

#~ msgid "Print training log every `verbose_eval` iterations."
#~ msgstr ""

#~ msgid ""
#~ "The minimum number of samples to "
#~ "start to use the trained model. If"
#~ " the number of samples is less "
#~ "than this number, the model outputs "
#~ "random predictions."
#~ msgstr ""

#~ msgid "The random seed"
#~ msgstr ""

#~ msgid "If is not None, save model to this file after every update."
#~ msgstr ""

#~ msgid ""
#~ "Whether to use adapatie training, which"
#~ " reduces the training frequency when "
#~ "there are too many logs."
#~ msgstr ""

#~ msgid ":py:obj:`load <tvm.auto_scheduler.XGBModel.load>`\\ \\(file\\_name\\)"
#~ msgstr ""

#~ msgid ""
#~ "Load the model from a file :param"
#~ " file_name: The filename :type file_name:"
#~ " str"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`predict <tvm.auto_scheduler.XGBModel.predict>`\\ "
#~ "\\(task\\, states\\)"
#~ msgstr ""

#~ msgid ""
#~ "Predict the scores of states :param "
#~ "search_task: The search task of states"
#~ " :type search_task: SearchTask :param "
#~ "statse: The input states :type statse:"
#~ " List[State]"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`predict_stages "
#~ "<tvm.auto_scheduler.XGBModel.predict_stages>`\\ \\(task\\, "
#~ "states\\)"
#~ msgstr ""

#~ msgid "Predict the scores of all stages in states."
#~ msgstr ""

#~ msgid ":py:obj:`save <tvm.auto_scheduler.XGBModel.save>`\\ \\(file\\_name\\)"
#~ msgstr ""

#~ msgid ""
#~ "Save the model to a file :param"
#~ " file_name: The filename :type file_name:"
#~ " str"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`update <tvm.auto_scheduler.XGBModel.update>`\\ "
#~ "\\(inputs\\, results\\)"
#~ msgstr ""

#~ msgid ""
#~ ":py:obj:`update_from_file "
#~ "<tvm.auto_scheduler.XGBModel.update_from_file>`\\ "
#~ "\\(file\\_name\\[\\, n\\_lines\\]\\)"
#~ msgstr ""

#~ msgid "Load measure records from a log file to update the cost model."
#~ msgstr ""

#~ msgid ""
#~ "Predict the scores of all stages "
#~ "in states. This is the breakdown "
#~ "version of `predict`."
#~ msgstr ""

#~ msgid ""
#~ "**scores** -- The predicted scores for"
#~ " all stages in all states in "
#~ "the packed format"
#~ msgstr ""

#~ msgid ""
#~ "For faster data copy between c++ "
#~ "and python, the python part returns "
#~ "scores in a single flatten array "
#~ "using a packed format. The c++ "
#~ "part then unpacks the flatten array. "
#~ "The packed format is: {"
#~ msgstr ""

#~ msgid ""
#~ "float  scores[N];                 // scores[i] "
#~ "is the score for states[i]. int    "
#~ "n_stage_0;                 // the number of"
#~ " stages in states[0] float  "
#~ "stage_scores_0[[n_stage_0] // the scores for"
#~ " all stages in states[0] int    "
#~ "n_stage_1;                 // the number of"
#~ " stages in states[1] float  "
#~ "stage_scores_1[n_stage_1]; // the scores for"
#~ " all stages in states[1] ... int"
#~ "    n_stage_i;                 // the number "
#~ "of stages in states[i] float  "
#~ "stage_scores_1[n_stage_i]; // the scores for"
#~ " all stages in states[i] ...  // "
#~ "untill i == N - 1"
#~ msgstr ""

#~ msgid ""
#~ "} To implement this format, we "
#~ "also store int as float, so we "
#~ "can store all numbers into a "
#~ "single float array."
#~ msgstr ""

#~ msgid ""
#~ "Update the cost model according to "
#~ "new measurement results (training data). "
#~ "XGBoost does not support incremental "
#~ "training, so we re-train a new "
#~ "model every time. :param inputs: The "
#~ "measurement inputs :type inputs: "
#~ "List[MeasureInput] :param results: The "
#~ "measurement results :type results: "
#~ "List[MeasureResult]"
#~ msgstr ""

#~ msgid ""
#~ "Load measure records from a log "
#~ "file to update the cost model. "
#~ "This function can be used to "
#~ "pre-train the cost model with history"
#~ " log files. :param file_name: The "
#~ "filename :type file_name: str :param "
#~ "n_lines: Only load first n lines "
#~ "of the log file :type n_lines: "
#~ "Optional[int]"
#~ msgstr ""

#~ msgid "Run auto scheduling search for a task."
#~ msgstr ""

#~ msgid "Create a search task."
#~ msgstr ""

#~ msgid "**SearchTask**"
#~ msgstr ""

#~ msgid "The module or function to tune"
#~ msgstr ""

#~ msgid "The associated parameters of the program"
#~ msgstr ""

#~ msgid "The compilation target"
#~ msgstr ""

#~ msgid "The host compilation target"
#~ msgstr ""

#~ msgid "Hardware parameters used for the search tasks"
#~ msgstr ""

#~ msgid "Whether to extract simple tasks that do not include complicated ops."
#~ msgstr ""

#~ msgid ""
#~ "A file to dump an association "
#~ "between the workload keys and the "
#~ "actual DAG"
#~ msgstr ""

#~ msgid "The optimization level of the task extractions."
#~ msgstr ""

#~ msgid ""
#~ "* **tasks** (*List[SearchTask]*) -- The "
#~ "tasks in this network * **weights** "
#~ "(*List[int]*) -- The weight (i.e. the"
#~ " number of appearance) of extracted "
#~ "tasks"
#~ msgstr ""

#~ msgid "**tasks** (*List[SearchTask]*) -- The tasks in this network"
#~ msgstr ""

#~ msgid ""
#~ "**weights** (*List[int]*) -- The weight "
#~ "(i.e. the number of appearance) of "
#~ "extracted tasks"
#~ msgstr ""

#~ msgid "The layout after rewrite"
#~ msgstr ""

#~ msgid "Specify the order of axes by names"
#~ msgstr ""

#~ msgid "**shape** -- The original shape"
#~ msgstr ""

#~ msgid "Whether the auto-scheduler is enabled"
#~ msgstr ""

#~ msgid ""
#~ "Return the best measurement pair form"
#~ " a log file. This may return "
#~ "none results if there is no legal"
#~ " measure pair with the specified "
#~ "workload_key/target found from the log "
#~ "file."
#~ msgstr ""

#~ msgid "File name to load log from."
#~ msgstr ""

#~ msgid ""
#~ "The workload key of the compute "
#~ "declaration. With `None`, this returns "
#~ "the best measure pair of all "
#~ "workloads."
#~ msgstr ""

#~ msgid ""
#~ "The target device. With `None`, this "
#~ "returns the best measure pair of "
#~ "all target devices."
#~ msgstr ""

#~ msgid ""
#~ "* **input** (*auto_scheduler.measure.MeasureInput*) "
#~ "-- The best State's MeasureInput from"
#~ " this log fine. * **result** "
#~ "(*auto_scheduler.measure.MeasureResult*) -- The best"
#~ " State's MeasureResult from this log "
#~ "fine."
#~ msgstr ""

#~ msgid ""
#~ "**input** (*auto_scheduler.measure.MeasureInput*) -- "
#~ "The best State's MeasureInput from this"
#~ " log fine."
#~ msgstr ""

#~ msgid ""
#~ "**result** (*auto_scheduler.measure.MeasureResult*) -- "
#~ "The best State's MeasureResult from this"
#~ " log fine."
#~ msgstr ""

#~ msgid "**logs**"
#~ msgstr ""

#~ msgid ""
#~ "Some unimportant and expensive fields in"
#~ " the returned MeasureInput are not "
#~ "deserialized for faster read speed "
#~ "(e.g., input.task.compute_dag, input.state.stages). "
#~ "If you want to use them, you "
#~ "can call the :code:`recover_measure_input` "
#~ "below to rebuild these fields."
#~ msgstr ""

#~ msgid "**workload_key** -- The workload key of the function."
#~ msgstr ""

#~ msgid ""
#~ "The input function should take a "
#~ "list of Tensor wich indicate the "
#~ "Input/output Tensor of a TVM subgraph"
#~ " and return a Map from the "
#~ "input Tensor to its buffer name."
#~ msgstr ""

#~ msgid ""
#~ "The check function that returns the "
#~ "compute declaration Tensors or its "
#~ "function name."
#~ msgstr ""

#~ msgid "The check function to be registered."
#~ msgstr ""

#~ msgid "Whether to override existing entry."
#~ msgstr ""

#~ msgid ""
#~ "The input function should take hashable"
#~ " and jsonable arguments (int, float, "
#~ "tuple of int, tvm.tensor.Tensor, ...) "
#~ "and return a list of tvm.tensor.Tensor."
#~ msgstr ""

#~ msgid ""
#~ "The generation function that returns the"
#~ " compute declaration Tensors or its "
#~ "function name."
#~ msgstr ""

#~ msgid "The generation function to be registered."
#~ msgstr ""

#~ msgid ""
#~ "Remove the safety check in the "
#~ "indexing function for a tensor. This "
#~ "is done by monkey patching its "
#~ "indexing function. After removing the "
#~ "check, we are allowed to create a"
#~ " temporary wrong IR and fix it "
#~ "later in other places."
#~ msgstr ""

#~ msgid "The tensor to remove index check."
#~ msgstr ""

#~ msgid "File name to write log to."
#~ msgstr ""

#~ msgid "The MeasureInputs to be written."
#~ msgstr ""

#~ msgid "The MeasureResults to be written."
#~ msgstr ""

